import requests
import re
from bs4 import BeautifulSoup
#import base64
#from langdetect import detect


class ExtractText:

    def __init__(self, path):
        """
        Extract text data from url or local path to html file
        """
        self._path = path

        if self._path.startswith('http'):
            response = requests.get(self._path)
            data = response.content.decode('utf-8', errors="replace")
        elif self._path.endswith('.html'):
            data = open(self._path, encoding="utf8")
        else:
            raise ValueError("Input path must be url or loccal html file!")

        self._soup = BeautifulSoup(data, "lxml")

    def _get_title(self):

        try:
            title_ = self._soup.find('title')
            title = title_.getText()
        except:
            try:
                title_ = self._soup.find('h1')
                title = title_.getText()
            except:
                title = "NA"
        return title

    def _get_author(self):

        try:
            author_ = self._soup.find("span", class_="author")
            author = author_.getText()
        except:
            try:
                author_ = self._soup.find('meta', attrs={'name': 'author'})
                author = author_['content']
            except:
                author = "NA"
        return author

    def _get_date(self):

        try:
            date_ = self._soup.find(string=re.compile(r'^Document generated by Confluence on '))
            doc_date = date_.replace('Document generated by Confluence on ', '')
        except:
            try:
                doc_date_ = self._soup.find('meta', attrs={'property': 'article:published_time'})
                doc_date = doc_date_['content']
            except:
                doc_date = "NA"
        return doc_date

    def _get_hrefs(self):

        hrefs = self._soup.find_all('a', href=True)

        return hrefs

    def extract_text_from_html(self):

        title = self._get_title()
        # try:
        #    title = title_.getText()
        # except:
        #    title = "title"

        page = self._soup.findAll('p')  # works for wikipedia
        # page = self._soup.findAll(["div", {"class" : re.compile('*paragraph*')},]) # works for cnn news

        sentences = []
        for pp in page:  # loop over all <p> sentence</p>

            text = pp.getText().strip()

            sentences.append(text)
            # print(text)

        paragraph = ' \n'.join(sentences)
        return title, paragraph

    def extract_meta_data(self):

        author = self._get_author()
        date_doc = self._get_date()
        return author, date_doc


if __name__ == "__main__":

    import pandas as pd
    import os

    # testing url
    #url = "https://www.bbc.com/news/election-us-2020-54359993"
    url = "https://en.wikipedia.org/wiki/Topic_model"

    extractText = ExtractText(url)
    title, sentences = extractText.extract_text_from_html()

    print('=== testing url input: ===')
    print('=== Reading from : {}'.format(url))
    print('=== The title is: {}'.format(title))
    print('=== The 1st 100 chars are: {}'.format(sentences[:100]))
    print('=== The extracted text contains {} chars'.format(len(sentences)))

    # testing local html file
    local_file = '../test/data/news_debate.html'
    extractText = ExtractText(local_file)
    title, sentences = extractText.extract_text_from_html()

    print('=== testing local html file input: ===')
    print('=== Reading from : {}'.format(local_file))
    print('=== The title is: {}'.format(title))
    print('=== The 1st 100 chars are: {}'.format(sentences[:100]))
    print('=== The extracted text contains {} chars'.format(len(sentences)))
'''
    datadir = "/Users/zruxi/Downloads/EDST/"
    data_files = [i for i in os.listdir(datadir) if ".html" in i]
    #with open("urls_medium.txt", encoding="utf-8") as file:
    #    t = [l.rstrip("\n") for l in file]

    #urls = list(set(t))
    print(len(data_files))

    list_res = []
    for url in data_files:
        dict_res = {}
        extractText = ExtractText(datadir + url)
        print(url)
        title, sentences = extractText.extract_text_from_html()
        author, date_doc = extractText.extract_meta_data()
        dict_res['title'] = title
        dict_res['text'] = sentences
        dict_res['author'] = author
        dict_res['date'] = date_doc
        list_res.append(dict_res)

    df = pd.DataFrame(list_res)
    df.to_csv('confu_texts.csv', index=False)

'''
