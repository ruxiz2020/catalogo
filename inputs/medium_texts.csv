title,text,author,date
A Beginners Guide to Unsupervised Learning | by Mathanraj Sharma | Analytics Vidhya | Medium,"Whenever someone talks about Machine Learning they will always start with: Supervised learning, Unsupervised training, Reinforcement training are the main broad categories. 
But what is really behind that technique called unsupervised? Come, let us see with some examples. 
Most of the times we build models to predict or forecast something. This particular type of technique is very well known as supervised training. In this case, we know the labels and the patterns among the data. 
But Unsupervised learning is a bit different from that, where we train our models to find the hidden patterns among the data to label the unseen items in the future based on the learning. We do this 
Let me walk you through an example by using a “Fish Measurement” data set. This data set consists of: 
The task here is to cluster the fishes into correct species. But unfortunately, we don’t have any idea about how each of these features is correlated so labeling the fishes with correct species type ourselves is a hard task. So what can we do? 
This is where unsupervised learning comes in handy. There are many pre-defined algorithms like K-means clustering, Hierarchical Clustering, DBSCAN, and also we can build our own clustering models using Neural Networks as needed. In this article, I am not going to explain about those algorithms, for the simplicity we will use the K-Means clustering in our example. 
Before we need to train our model, we should know how many different types of species (clusters) we are going to label. So how can we figure it out? One way is we can simply get an idea from the use case. For example, if we are going to cluster apples and oranges based on some attributes, we know the clusters are 2. Likewise, we can get some idea from stakeholders. But the ideal way is to use the Inertia. Inertia is the sum of squared error for each cluster. Therefore the smaller the inertia the denser the cluster 
First, we cluster the data with different number of clusters and plot the number of clusters vs.inertia graph. 
As we can see, the inertia is getting lower and lower as the number of cluster increases. So what will be the best number of clusters? 
So, in our fish clustering case, there are 4 clusters, where the decreasing slope of inertia is lower. 
Since we know the number of clusters let’s build a model and visualize the result. 
Visually, it seems clustered perfectly. The markers in Diamond shapes are the center points(mean) of each cluster. But remember our eyes can cheat us! 
Let us double check our quality by counting how species are clustered into each species. A simple way to do this is using the pandas crosstab method. 
As we can see, the clustering is not perfect enough. Some of the fish from Bream species is clustered under the label 0,1 and 2. Moreover, the other species is also wrongly clustered into two or more different clusters (labels). 
So what might be the problem here? Yes, you are correct — the features we have are scaled in different metrics, so the mean and variance among the features are different, which makes the clustering imperfect. So how can we solve this and improve the model? 
These are the two different techniques used to solve the above issues. 
1 — Standardizing is the process where we bring down the standard deviation and mean of the features to 1 and 0, to the standard scale. 
2 — On the other hand, Normalization is the process where we bring down all the feature values ranges between 0 and 1. 
In our example, we are going to Standardize the data and see the cluster quality. There are many techniques available in Scikit learn to do this. We will use the “StandardScale” module to achieve our goal. 
Hooray, now the clusters seem much better than the previous one. We can still improve this by adding more data and tuning the Hyper Parameters of KMeans and using Feature Scaling techniques. I have talked about many things in this article. Get your hands dirty and try all the concepts out yourselves. 
Oops, don’t forget to give a star for the Github repo. 
Written by 
Written by",Mathanraj Sharma,2019-08-09T12:48:35.943Z
Importance of Distance Metrics in Machine Learning | by Alekhyo Banerjee | Medium,"You read the title right !!! You might be wondering how ‘Distance’ and ‘Machine Learning’ are related? Am I going out of context? 
No, if you have hands-on experience with Machine Learning algorithms, undoubtedly you came across ‘Distance’ as a parameter. This blog focuses on the need for distance metrics in Machine learning and it’s use cases so that you appreciate the concept more. 
The metric is a function that defines a concept of distance between any two members of the set, which are usually called points. 
The metric satisfies a few simple properties. 
d(i,j)≥0: Distance is a non-negative number. 
d(i, i)=0: The distance of an object to itself is 0. 
d(i,j)=d(j,i): Distance is a symmetric function. 
d(i,j)≤d(i,k)+d(k,j) 
Intuitively, distance can be thought of as a measure of similarity, so two closest observations can be categorized similarly. 
A supervised algorithm, K-Nearest Neighbors, and Unsupervised algorithms like K-mean clustering/Hierarchical clustering use distance metrics to understand patterns between data points. 
Various distance metrics have their use cases, but it is important to be aware of them while considering the best solution for a given situation to avoid accuracy issues and interpretation issues. 
This next section of the blog discusses some commonly used distance metrics in Machine Learning. 
You all have must be knowing Pythagoras Theorem from your junior school. 
Pythagoras formula can be extended to get the euclidean distance. 
Euclidean distance(also known as the Pythagorean metric) is an ordinary straight line between two points in a coordinate axis. 
The general formula for Euclidea distance in n-dimension is 
A real-life use case of Euclidean Distance will be calculating the distance of a flight between two countries 
Also known as Taxicab geometry, it calculates the distance between two points as the sum of the absolute differences of their Cartesian coordinates. 
Where, 
A real-life use case of Manhattan Distance will be calculating cab fare between two locations in a city. 
An illustration of the difference between Euclidean and Manhattan Distance 
Minkowski distance is a similarity measurement between two points in the normed vector space (N-dimensional real space). The definition sounds very mathematical. Let me break it down for you, normed vector space means a space where distances can be represented as a vector that has a length. 
Let’s assume that a map is a vector space. If we take a map, we see that distances between cities are normed vector space because we can draw a vector that connects two cities on the map. We can combine multiple vectors to create a route that connects more than two cities. Now, the adjective “normed.” It means that the vector has its length and no vector has a negative length. That constraint is met too because if we draw a line between cities on the map, we can measure its length. 
The formula for Minkowski Distance is given by 
Minkowski Distance, also known as Lp form can be generalized for Euclidean and Manhattan distance as well by changing the value of p. 
For p=2, we get the L2 form, which is Manhattan Distance 
p=1, we get the L1 form, which is Euclidean Distance 
As we know, Environmental interference and physical defects in the communication medium can cause random bit errors during data transmission. Error coding is a method of detecting and correcting these errors to ensure information is transferred intact from its source to its destination. 
Hamming distance is one such error-correcting code to measure the distance between two codewords, we just count the number of bits that differ between them. If we are doing this in hardware or software, we can just XOR the two codewords and count the number of 1 bit in the result. This count is called the Hamming distance. 
Suppose there are two strings 11011001 and 10011101. 
11011001 ⊕ 10011101 = 01000100. Since, this contains two 1s, the Hamming distance, d(11011001, 10011101) is 2. 
Here are a few useful references : 
These two terms are closely related and widely used in the recommendation system. Cosine similarity measures the similarity between two vectors by calculating the cosine of the angle between them 
Cosine distance=1-Cosine_Similarity. So, similarity increases when distance decreases and vice versa. 
Hence, we can say that Cosine Similarity lies between -1 and +1 as the range of Cos theta is [-1,+1] 
The crux of this whole discussion is to choose the best metric for recommendation systems. 
Let’s say I have a database of users who rate movies on a scale of 1–10. If I want to make a list of the top 5 most-watched movies similar to my profile; my first approach to finding similar users was to use Cosine Similarity and just treat user ratings as vector components. The main problem with this approach is that it just measures vector angles and doesn’t take the rating scale or magnitude into consideration. Magnitude matters in this case because I want the top 5 movies that are the closest to my user profile averages. Euclidean Distance can be used in such a scenario as it accounts for magnitude as well. 
Cosine Similarity is more useful for instances when you do not want magnitude to skew the results. This is most useful in word vectorization because normalizing the data makes a long document comparable to a short document. The euclidian distance will be very large between documents of different word length which would skew your results. 
Also, cosine similarity finds its application in text mining. 
Summing everything up, we can search for alternatives instead of mindlessly employing predefined metrics. We are not bound to use handcrafted designs since we can make machines come up with tailored solutions for our specific problems; this is what machine learning is about! 
Using the Distance Metric as a hyperparameter is a good optimisation feature. There are times where it will come in handy, and there are times where it won’t. The typical use cases are combating the dimensionality curse, saving computational costs, interpreting data, and improving accuracy. 
Written by 
Written by",Alekhyo Banerjee,2020-06-07T16:33:17.714Z
A gentle introduction to genetic algorithm | by Nguyễn Việt Hưng | The happy lone guy | Medium,"When starting to conduct research about artificial intelligent, you may have heard about something called ‘Genetic algorithm’. After seeing this term, you might reluctant to dig into it as it contains the word ‘algorithm’. Fear not! In this article, I’ll show you the simplicity of genetic algorithm and hopefully inspire you to build one for your own. 
Genetic algorithm is basically a method that heavily inspired by the process of natural selection to find the best solution to a problem. 
In nature, only the strong one survive, the process of eliminating the weak is called natural selection. Genetic algorithm use that same principle to eliminate the “weak” solutions and finally produce the best solution. 
Usually, genetic algorithm is used when you cannot know what the solution will be like. For instance, you want to create a car that can navigate through different kind of terrains. You cannot know how that car is going to look like, but you know the goal of the car. In this case, the genetic algorithm can be used to generate the car to travel of different terrains but the car design will be decided by the algorithm. 
As mentioned before, the genetic algorithm is heavily inspired by the natural selection so to see how genetic algorithm work, you should actually, look into how natural selection work. 
The diagram above illustrates the process of natural selection. It is clear that this process involves 5 main steps. The initial step is selection, in this step, nature will select individuals that has strong gene from the initial population, after that they begin to step into the next stage which is mating. After been through mating step, they’ll produce a child, we call this step: “reproduction”. That specific child then mutated to add some variation to the gene and finally moved back into the population. 
The process of genetic algorithm is pretty similar to the process above, the only different is that it added some additional details. 
To begin with, the working process of genetic algorithm also start by having an initial population. The first main stage is calculate fitness, calculate fitness can be consider as a part of the selection process as it basically used to calculate the “score” of each individuals to indicate whether its a strong individual or a weak one. All the strong entities then selected and passed to a stage called cross over, this stage is like the mating stage in the previous diagram. In this stage, 2 random parents is selected from the strong entity list to perform something called crossover which we will discuss later. After performing cross over, a child is created and also mutated to add some variation to the gene. Finally, this child rejoin the population and the process repeat again. 
In genetic algorithm, fitness plays a pivotal role in the selection stage. Fitness is the “score” for picking an entity that get to pass on its traits. For example, if a person have a serious illness, his “fitness score” will be low and less likely to have a chance to have kids as his kids would also inherit his illness. Therefore, if a solution’s fitness is low, you might not want to create another solution base on it as the result would be as bad as the old solution. 
Try to imagine you are given a problem, the problem is to find the best route for red riding hood to travel to her grandma house. Let's assume that she want to reach her grandma house in the shortest time as possible, therefore the fitness of a route can be calculated using the time it took her to travel. If there is a route that has the length of 400 meters and it took her 10 minutes to travel then its fitness will definitely less than the fitness of a route that is 500 meters long but only took her 8 minutes to travel as the fitness of a route is calculating base on the travel time, not the length of it. Therefore the 500 meters long route would be more likely to be selected to combine with other routes. 
Selecting the appropriate solution is what the genetic algorithm’s selection stage all about. After calculating the fitness score, the next step is to use some mysterious methods to select a list of solutions that can later be use to produce a better solution. 
Although you can create your own way of selecting the fitting solutions, there are some famous methods that you can use: 
The list above is not an adequate list, you can find out more methods here. 
Let’s take the first method as an example. Unlike its name, the actual concept behind this method is absurdly simple. The fitness proportionate selection A.K.A the roulette wheel selection, is the method of selecting “potential” solutions for recombination. 
Imagine, if there are 10 marbles in a bag, specifically, 5 blue marbles, 3 red marbles and 2 white marbles. You can easily calculate that if you pick out a random marble from the bag, you will have 5/10 chance to pick a blue one, 3/10 for red and 2/10 for white. That’s how the fitness proportionate selection works, however, the process is a bit different. 
In the fitness proportionate selection, a random number is first picked, then it will be used to compare with the fitness of a random picked solution. The fitness value is usually constrained to go from 0 to 1. If the random value is less than that fitness score then the solution will be picked. Thus, the higher the fitness of a solution is, the higher chance that it will be picked. For instance, if a random number goes from 0 to 1, there are 50% it will be less than 0.5 and 80% it will be less than 0.8, however, it will not likely to be less than 0.2, that means, if a solution has a 0.8 fitness, there are 80% it will be picked for recombination and solution that have 0.2 fitness will only have 20% to be picked. Although the 0.2 fitness solution is rarely picked but it also help variate the solution traits as that solution can contain some attributes that needed for later success. 
Crossover is the stage where selected solutions are combined to form new solutions. Just like the selecting stage, there are also many techniques of crossover and you can also find them here. 
Similar to selection stage, in crossover stage, you can also invent your own crossover techniques, however, there are also some techniques that you can use: 
For better understanding, I’ll explain the uniform crossover technique which is the technique that I reckon the most easiest technique to implement. 
The uniform crossover method involved picking random part of the gene of 2 solution randomly to combine and create a new (hopefully better) solution. 
The first step in uniform crossover is to select 2 random solutions from the set of solutions which are picked in the previous selection stage. Then each parts of the 2 solutions will be chose to add the the child solution base on a variable called the mixing ratio. The mixing ratio is the thing that decide the chance that a solution is likely to be picked to add to the child solution. For instance, there are 2 solutions: A and B and you want the child solution to have more parts which taken from solution A, you can increase the mixing ratio, after that loop through every parts of the solution A and B. In every loop, create a new random number and compare it with the mixing ratio, if it’s less than the mixing ratio then pick the solution A part else pick the B. Similarly, if the mixing ratio is 0.5 then A and B will have approximately 50% of being picked. 
The picture above, demonstrate the child solution C generated using solution A and B with the mixing ratio of 0.5 or 50%. Each part of both of the solutions was decided to be picked or not base on the mixing ratio and the mixing ratio was compared with a random number to decide. It is like tossing a coin to pick where A should be use instead of B and vice versa. 
No no no, we are not going to transform our children into some guys with metal claws and let them die on a random tree trunk! 
Instead, adding mutation to a child is like helping them to 
By thinking different, an individual can whether become the leader of the herd or a complete dumb-ass. 
As you can see, the red child is going on a different path compared to other children. The reason for this, is because, when the red child is following the herd, we added some mutation to him and therefore, help him to pick a different path. Of course, that different path can also be a dead end, but clearly, in this case, the path is leading to success! That’s the true purpose of mutation stage. 
To mutate a child, there are also a variety of techniques that you can use: 
You can find more of them here. 
To understand this better, I’ll demonstrate the “bit string mutation” technique. 
Imagine, you’re making a bot to avoid objects in front of it. The bot will have to avoid all objects to reach the final destination and its natural state is moving forward. To avoid object, its gene will be a series of ‘left’ and ‘right’ strings which indicates how to bot will move. 
The “bit string mutation” is usually used for binary string as this method flip 1 or more random selected bits in the gene. For example: 
Now, try to transform 1 and 0 into left and right and think of how the bot will perform. As the bot can get stuck when it encountered a large object, the mutation will help its offspring to pick a new direction to move and avoid that object. Imagine, for many generations, the bot only turns right when he meet an object and die, but, in the next generation, the bot suddenly turned left as a right string in its gene was flipped to left and the bot eventually reached its destination. That’s basically how the ‘bit string mutation’ works. 
After mutating the child, it will be joining back with other mutated child to reform a new population and the whole process start over. 
So when does it stop? The answer is extremely simple, when do you stop practicing typing keyboard with 10 fingers? When your typing skill is perfect of course. Same thing with genetic algorithm, the process will stop if a certain solution’s fitness reached the desired fitness. For instance, you want the bot in the previous example to reach the destination with the smallest amount of time as possible. 
You set the fitness threshold to 4 minutes, if the bot’s time to finish is more than 4 minutes, that means the process will repeat until the bot’s finish time is less than or equals to 4 minutes. 
That’s the end of my article, I hope that after this article, you will have a better insights into genetic algorithm have inspired to build one of your own. 
Here are some links for you to explore more about genetic algorithm: 
Written by 
Written by",Nguyễn Việt Hưng,2018-03-19T06:34:10.318Z
Recommendation Systems : User-based Collaborative Filtering using N Nearest Neighbors | by Ashay Pathak | SFU Professional Master’s Program in Computer Science | Medium,"Ashay Pathak, Chatana Mandava, Ritesh Patel 
Collaborative Filtering is a technique which is widely used in recommendation systems and is rapidly advancing research area. The two most commonly used methods are memory-based and model-based. 
In this post, we will only focus on (User-Based Collaborative Filtering) UB-CF which is a memory-based method. The main idea behind UB-CF is that people with similar characteristics share similar taste. For example, if you are interested in recommending a movie to our friend Bob, suppose Bob and I have seen many movies together and we rated them almost identically. It makes sense to think that in future as well we would continue to like similar movies and use this similarity metric to recommend movies. 
Let’s try to implement UB-CF and generate a list of movies that our friend Bob a.k.a active user might be interested in watching. The motivation behind writing this post is to deep dive into the algorithm and understand how UB-CF actually works. Most of the content of this post is inspired by a Course on Coursera. 
The method identifies users that are similar to the queried user and estimate the desired rating to be the weighted average of the ratings of these similar users. 
We would be doing the recommendation’s on MovieLens Dataset. The programming language used is python and the data analysis work is mostly done using pandas library. IDE used is jupyter notebook. 
So before starting I would like to give the list of libraries used : 
So lets move forward and understand the concepts behind recommendations. I have attached some code snippets and outputs in the blog for better understanding. The whole ipynb file is attached at the end of the blog. 
It’s easy to come up with a function for non-personalized collaborative filtering (i.e we don’t consider active user’s likes, dislikes and rating from the past) that returns a score taking user u and item i as the input parameters. The function outputs a score that quantifies how strongly does a user u likes/prefers item i. 
So this is usually done using the ratings of other people similar to the user. This all would be discussed in detail later. For now the formula I have used is, 
Where ‘s’ is the predicted score, ‘u’ is the user, ‘i’ is the item, ‘r’ is the rating given by the user and ‘w’ is the weight. 
In this case our score is equal to the sum of the ratings that each user gave to that item subtracting the average rating of that user multiplied with some weight which is of how much this user is similar or supposed to contribute to the predictions of other user. This is weight between user u and v. The score ranges between 0 to 1 where 0 is low and 1 is high. Everything looks perfect, then why did we subtract the average ratings from each users rating and why did we use weighted average instead of simple mean? 
The problem is with the types of users we are handling. It starts with the fact that people rate often on very different scales. I may be positive and optimistic user where I will rate the movie I liked as 4 out of 5 but some other user who is less optimistic or has some high standards may rate his favorite movie as 2 out of 5. Here his 2 is my 4. The tweaks to make it better is, we can increase the efficiency of this algorithm if we normalize user’s rating. One way to do that is to say we are going to compute s(u,i) i.e score as the average rating that user gives to each item plus some deviation and the deviation is going to be how much this item is better or worse than average. 
I have used the cosine similarity to calculate the weight given in the above formula. I have also used the notion of neighborhood which would be discussed in this blog as we move on. 
To normalize the data in the above manner, some data analysis is required in pandas. You can get whole code at the end. For the blog, i will focus on the important concepts. 
So now we are done calculating the normalized rating for a user. The above data would be used to calculate the final score for the user later. 
From here we will now focus on some important concepts related to recommendation systems. 
For the above formula we need to find the users who have similar thoughts. This sounds so interesting to find a user who has similar liking’s and disliking. But the question is how do we find the similarity? 
To answer this, we will use Cosine Similarity and see how similar the users are. It is usually calculated over the ratings that both the users have rated in the past. 
In our example, I have used cosine_similarity function of sklearn to calculate the similarity. But before that we have to perform some pre-processing and clean the data. 
This contains some lots of NaN value since every user has not seen all the movies and that’s the reason this type of matrix is called sparse matrix. Methods like matrix factorization are used to deal with this sparsity but we would not focus on it in this blog. Next step and one of the important step is to replace this NaN values. 
There are two methods commonly used for this : 
I have used both the methods and you can get it in the code below. But for explaining I would use the movie average method. 
Now, next step is to calculate the similarity between the users. 
Lets check our self whether what we calculated really makes sense !! 
From above image we can see that the similarity we generated is true since both the given users (370,86309) have almost same ratings and liking’s. 
So we are done with calculating the similarities between the users but I am not still happy. I would discuss the reason in the next step. 
So from above we can see that we have calculated the similarities for all the users. But since being a Big Data student, the complexity of the problem always drive me. By this I mean that the recommendation system works with the huge data and hence it becomes very important to maintain and capture only the important and necessary highlights from the data. 
To explain this using our example of movie recommendation system, the matrix we obtained above is (862*862), as there are 862 unique users in the data. This number is still small when compared to the data original system would be working on. Lets think about Amazon. It would have more than millions of users in its database and so while calculating the score for any item it would not be a good solution or method to look at all the other users all the time. Hence to overcome this we generate a notion of neighborhood. This includes only the set of (K) similar users for a particular user. 
Now let’s take further steps to implement the idea. In our example I have taken the value of k as 30. So we would have 30 nearest neighbor for all the users. 
I have used my custom function find_n_neighbours which takes the similarity matrix and the value of n as input and returns the nearest n neighbors for all the users. You can find the code in the notebook given at the end of the blog. 
So now if you think then we have seriously reduced the number of unnecessary computations. Now, we are ready to calculate the score for a item now. 
Wow! we are done with the process. I know we went through some real technical stuff, but the time we spent is worth as we have reached the last step. 
Here we will try to predict the score for the movie the given user has not seen. 
So our system predicted the score to be 4.25 which is really good. I think user (370) could like the movie with id (7371). 
Now lets give a final touch to our system. I think most of us would have used Netflix or Hotstar. So when we open the app it shows the item you make like. 
I always get fascinated by the recommendation as they turn out to be the movies I like. This all happens through the recommendation system itself. We will now do the same and try to predict the top 5 movies the given user may like. 
The logic is not that tough. We already generated the score for one item. Similarly we can generate the score for the other items with the same user. 
But, now again keeping Big Data into consideration does it make sense to generate score for all the other items ?? 
I think NOOOOOO!!! 
Lets consider user (U) and user (K). Now suppose K is not in the neighborhood of U ,is it possible for user U to like a movie which K has seen and rated. Mostly No. 
I think you got the trick. Yes, we are just interested in calculating the scores for the items that their neighbor users have seen. 
We are really successful. We reduced the computation from 2500 to N ( where N is the set of movies my neighborhood liked ) and which is very less than 2500. 
So finally lets recommend the movies. 
User_item_score1 is my custom function which uses our above discussion to calculate predictions 
At last!!!! We did it. We have our own recommendation system. 
The representation of the code below might not be very easy to read, so please go to my GitHub repository to access the code. 
The required Datasets can be obtained from here. 
I hope you liked the blog. For any queries you can mail me. Stay tuned for more blogs since this is one of my areas of interest and I would definitely post some new stuff. 
Thanks..!! 
📝 Read this story later in Journal. 
🗞 Wake up every Sunday morning to the week’s most noteworthy Tech stories, opinions, and news waiting in your inbox: Get the noteworthy newsletter > 
Written by 
Written by",Ashay Pathak,2019-03-09T21:23:12.017Z
Beginner’s guide to build Recommendation Engine in Python | by Amey Band | The Startup | Medium,"A while ago whenever we bought a specific product, it was probably recommended by our friends or trusted persons. But now the scenario has changed, what the product recommendations on amazon or movie recommendations on Netflix we are getting that are basically on our own interests. By analyzing the customer’s current site usage and his previous browsing history, a recommendation engine studies customer behaviour and his interests. Based on this information it is able to deliver relevant product recommendations. The data is collected in real-time so that when the customer’s interest changes, our recommendation model also get updated with the data and so on recommendations to the customer also changes. 
Naturally it is very feasible for every customer to get product recommendations based on his interests. Because that takes less time to deal with multiple choices and purchase the best one. Recommendation engine is the most widely used applications of machine learning, leads in company productive and marketing growth. 
Here are some few Recommendation System Benefits for your business :- 
In this article, we will go through various types of recommendation engine algorithms and fundamentals of designing them in python. We will see different types of recommendation engines along with we will create our own movie recommendation engine using different approaches and recommending some meaningful choices. 
Let’s understand the power of recommendation engine, 
The recommendation engine is used to give the recommendations of products that customer might wish to purchase based on customer’s behaviour, interests, browsing history and similarity with another likely customers. 
At first, recommendation engine collects the data from the customer in the form of ratings, comments then stores the data in standard database and then filter the data to extract the relevant meaningful information required to predict the final recommendations. 
There are different types of recommendation engines, let’s see it one by one.. 
Content-based filtering is based on the description of the product or the keywords used to specify more about the product. This filtering technique studies about user’s preferred choices and then deliver the most relevant recommendations. Consider, if you like a product in category ‘A’ then you may get most recommendations of products lied in category ‘A’ only. When a user usually watches movie in romance genre then it is considerable that he mostly like the romantic movies only then recommendation engine will also suggest most popular romantic movies to the user. 
There are mainly two vectors i.e. profile vector which contains the past behaviour of the user and item vector which contains the details of each movie like genre, overview etc. 
Now to deliver the most relevant recommendations, content-based filtering technique finds the cosine of the angle between the profile vector and item vector known as cosine similarity. 
Suppose A = profile vector and B = item vector, then the similarity between them can be calculated as: 
Based on the cosine similarity values, which ranges between -1 to 1, the movies are arranged in descending order and deliver the top most recommendations to the user. The main drawback of this technique is that it recommends movies in the same genre only. If we want recommendations from other movie genre then it might not perform well. 
Mainly collaborative filtering techniques deal with user’s preferences, activities and behaviour. They give recommendations to the user based on similarity with the other likely users. For the collaborative filtering techniques we don’t need any additional information just need to collect and analyse user’s behaviour. 
Further, there are several types of collaborative filtering algorithms:- 
As name suggests, this collaborative technique determines the similarity between the users. Based on similarity score, it brings out the similar users and then recommend the products to them whatever they purchased before. 
Users having higher correlation will tend to be similar. Let’s understand it with an example, consider user1 watched movie1, movie2, movie3 and user2 watched movie1, movie3, movie4 so based on the similarity with user1, user2 get recommendations to watch movie2 and similarly user1 get recommendations to watch movie4. 
To calculate the similarity for each user and prediction for each similarity score, it takes much time to compute. when the number of users is less then only this technique is very feasible to suggest recommendations. 
This technique is almost similar like the user-user collaborative technique, instead of finding similarities between users we find the similarity score between the items. 
So in our movie case example, we find similarities between the movies and then recommend them based on user’s interest and behaviour. Suppose movie1 and movie3 has the best similarity score and user has watched movie3, then definitely movie1 will be recommended to user. 
When a new user is introduced, it is hard to recommend products because he has no browsing history so we suggest the most popular products determined overall in the dataset. When a new product is introduced, we have to wait for the user action and then only we can give recommendations. 
Hybrid recommendation system is the combination of collaborative and content-based recommendation. This system can be implemented by making content-based and collaborative-based predictions separately and then combining them and vice-versa. 
In the user-item matrix, there are two dimensions: 
In case the user-item matrix is empty, then we can improve the algorithm performance by reducing the dimensions of the matrix. For that purpose you can use matrix factorization. 
In mathematical term, factorization is simply known as breaking of a large number into product of its multiples (factors) like 120 = 12 * 10 . 
Here, we are breaking down User-Item matrix into product of two small matrices i.e. User matrix and Item matrix. If the User-Item matrix has the dimension of a x b then it can reduced into product of two matrices having dimensions a x k and k x m respectively. 
Where k is the set of latent features which can define how a user rates the products. 
Read more on matrix factorization. 
Enough theory now moving towards implementation part, 
Let’s build the recommendation engine model, at first import all the required dependencies in your code editor, 
Now after importing dependencies, we have to import the dataset. I am using MovieLens datasets to build the recommendation engine model. You will get the required dataset here. 
Now you can see, we have plenty of columns which are not required to make predictions. So it will better for us to drop them from movies_df dataframe. 
Here, you can check that all the unrequired columns are dropped and we got the dataframe containing required columns only. 
As ‘genres’ column in dataframe contains dictionary with keys id and name so we have to extract the name which is movie genre from the dictionary and separate the column with name values only. 
You can see the output above. Now we have the modified data and we can make recommendations. 
Earlier we defined M value so now we are taking only those movies whose ‘vote_count’ is greater than required vote_counts to scale them. 
Now all set to calculate Weighted_average, 
We calculated the Weighted_average for every movie and after sorting df dataframe by Weighted_average values in descending order we store top most 500 movies whose weighted_average values is hightest in recm_movies dataframe. 
From the output, The Shawshank Redemption has the highest weighted_average and so on…using visualization we will get meaningful insights. 
Here you get the top most movie recommendations based on weighted_average calculated from votes_count and vote_average. 
Hope you got the approach 1 by weighted_average, now we deal with approach 2 by popularity and genre. 
Here, we defined a dataframe popular which contain movies scaled from high popularity to low popularity. 
We got the above output showing that Wonder Woman movie has the highest popularity and so on. Let’s visualize it, 
Now you can analyze it better and user also get recommended by these popular movies. 
If you want to get the recommendations from a particular genre of movies sorted according to the Weighted_average from highest to lowest. 
At first you know that the genres column in dataset have a list of multiple values, so we have to allocate only one single genres value in a row. 
From seeing the output in genre column you will get the idea about function we performed above as to get single value in genre column. 
Let us see our method in action by displaying the Top 10 Action Movies, 
We get the recommendations of the top 10 Action movies as seen above, Let’s visualize the results.. 
Here we get top 10 Action movie recommendations based on Weighted_average score. Now you can also try and get movie recommendations in genre drama, romance, crime etc. 
Let us first try to build a recommendation engine using movie overview. 
Now whenever we want to create the recommendation engine, for each and every movie we have to create a vector of matrix. The reason to create vector is that our recommendation engine depends upon the pairwise similarity. To create this similarity we have to design vectors for each movie. 
Now the overview column in dataset have sentences i.e. collection of strings. so we have to design tfidfVectorizer which used to create document matrix from this sentences. 
tfidfVectorizer do not convert directly raw data into useful features. Firstly, converting strings into vectors and each word has its own vector. Then we will use the technique for extracting the feature like Cosine Similarity which works on vector matrix. As we understand, we can’t directly pass the string to our recommendation model. So, tfidfVectorizer provides numeric values of the entire overview column for us. 
Since we have used the tfidfVectorizer which calculates the Dot Product will directly give us the Cosine Similarity Score. 
We have a pairwise cosine similarity matrix for all the movies in our dataset. 
Now we write a function that returns the 10 most similar movies based on the cos_sim score. 
Now calling the function with the movie title, 
Yeah, we got the best movie recommendations based on cos_sim score related with movie Star wars. Let’s check for another one movie.. 
Great, we accomplished our target. When you will implement it in your code editor you can also verify the recommendation engine by taking one or more examples and check the results. These movie recommendations are totally based on the cosine similarity score among the movies as discussed in earlier part. 
Product recommendation engine mainly runs on data. The data is in the form user’s ratings, comments, behaviour, preferences and many more. To drive customers to your business, recommendation engine is necessary. Along with that to generate revenue, create customer satisfaction, discover new shopping trends, personalize individual interest and provide reports in more effective manner, all these goals are accomplished with the help of recommendation engine. 
That’s all folks !! 
See you in my post !! 
All dataset files and python notebook is available on my github repo. 
Written by 
Written by",Amey Band,2020-05-05T18:45:11.716Z
Recommendation Engines 101. Get Started & Master the Basics | by Pierre Pfennig | data from the trenches | Medium,"Get Started & Master the Basics 
Recommendation engines are everywhere today, whether explicitly offered to users (e.g., Amazon or Netflix, the classic examples) or working behind the scenes to choose which content to surface without giving the user a choice. And while building a simple recommendation engine can be quite straightforward, the real challenge is to actually build one that works and where the business sees real uplift and value from its output. 
Here, I’ll focus on the straightforward bit, presenting the main components of a recommendation engine project so that you can (hopefully) get the easy part out of the way and focus on the business value. Specifically, we’ll explore building a collaborative filtering recommender (both item- and user-based) and a content-based recommender. 
We’ll work in SQL (Postgres flavor) and Python, andwe’ll work with two generic datasets throughout the rest of this post: 
where visit is a binary indicator (1 if the user visited the product, 0 otherwise).But this could also be a rating in case of explicit feedback. 
When preparing data to use for a recommendation engine, the first thing to do is some normalization since you’ll need it for any of the recommendation scores — normalization will scale every score between 0 and 1 so that it’s possible to compare things against each other to understand what is a good recommendation and what’s not. 
As the users and products often follow a long-tail distribution, we will also cut the long tail by filtering on users and products that occur frequently enough. You can see this part below: 
The following SQL query will produce a table called visit_normalized with format: 
product_id x user_id x nb_visit_user x nb_visit_product x visit_user_normed x visit_product_normed 
There are two main types of collaborative filtering: user-based and item-based. Note that the two are entirely symmetric (or more precisely the transpose of each other). For both versions, we first need to compute a score of similarity between two users or between two products. 
Again, with user-based collaborative filtering, the key is to calculate a user similarity score. The following will produce a table user_similarity with format: user_1 x user_2 x similarity_user 
As there are usually too many pairs of users to score, we often restrict this query (cf. the condition WHERE b.rank <= 10) by limiting ourselves to the 10 highest user similarity scores per user. The table has then format user_1 x user_2 x similarity_user x rank and our query becomes: 
We can then use this user similarity to score a product for a given user user_j : 
where 𝛿user, new_product (thanks Medium for this beautiful math rendering) is equal to 1 if the user has seen the new_product and 0 otherwise. 
More generally, we could change 𝛿user, new_product by: 
This is done using the following SQL script where the score_cf_user table will have the following format: 
user_id x product_id x score_user_product 
We proceed similarly for the item-based collaborative filtering; we just need to transpose the above to produce the table product_similarity containing: 
Product_1 x Product_2 x similarity_product 
As usual, we restrict to the products that have been seen enough times (I chose 25, in this case — you can see below that this is done by the filtered_visit table). 
And once again, we can then use this product similarity to score a product for a given user: 
where 𝛿i, new_product is equal to 1 if the user has seen the new_product and 0 otherwise. 
Again, let’s restrict the query to the 10 most similar products for a given product (cf. the condition WHERE final.rank <=10). The table score_cf_product has then format user_id x product_id x score_user_product x rank and our query becomes: 
With content-based recommendation engines, the overall philosophy is a bit different. Here, the goal is to use the description of the products to find products are similar to those that the user already bought or visited. To do that, we need a product classification, and we often have to build it from a text field. So that’s what we’ll walk through here. 
We can describe each product by the (non-zero) term frequency–inverse document frequency (TFIDF) value of the words of its description. 
The output dataset product_enriched_words has the following schema: 
product_id x word x value 
Given as follows: 
By summing the product vectors seen by each user, we obtain the user profile: 
This is done in with the following recipe whose output table user_profile_cb_words has format: user_id x word x value x value_normed 
Finally, thanks to the user’s profile, we can score products : 
(where <,> is your favorite scalar product) 
This will give us the table score_cb_words with format: user_id x product_id x score_cb x score_cb_normed 
Another way to proceed on a content-based recommendation engine is to extract topics from the vector description. This allows for drastically reducing the dimension space, expressing each product by a vector of length n_topics. 
Here, we present two versions of this method: 
This code is directly taken from the sklearn documentation. We only have to choose the number of desired topics, n_topics, and for displaying information about each topic, n_top_words. 
The output dataset product_enriched_topics has format: 
product_id x description x topic_0 x … x topic_9 
Like the TFIDF version, we can now deduce a user’s profile based on the NMF topics values. This first part computes the average topic values for each user, and the output table user_profile_cb has format: 
user_id x topic0 x…x topic9 x topic0_normed x … x topic9_normed : 
Now, we can rescale those values by the average topic values over all users: 
Finally, we can score new product in batch in Python with the following. The output dataset score_cb has then format: 
user_id x product_id x score_cb x score_cb_normed 
Starting from the NMF’s output product_enriched_topics, we can reshape this into a long format, product_enriched_topics_2, with schema: product_id x topic x value as follow : 
As before, we can now easily compute the user profile and this gives us the table user_profile_cb_2 with format: user_id x topic x value x value_normed 
Last but not least, we can finally score in SQL : 
Note that this may takes quite some time to run… but in the meantime — read on! 
We just saw how to compute several affinity scores, but you might be asking yourself: OK, which one do I take as my final result? Well, none of them, necessarily. 
It’d be silly to arbitrarily choose one. We should instead be able to make some “average” of these different scores to compute our final affinity User/Product. How can we do that ? 
The idea is to get the optimal combination between these scores. I’m sure it already rings a bell in your head… Yes, it’s a classical ML problem. We want to predict if a user will visit/buy/like a given product, and in order to do that we have a set of features which are the affinity scores we computed. 
However, there will be one major difficulty: we only have positive examples to learn. Indeed, we always have the visits/buys/likes, but in many cases we don’t have the counterfactual events (didn’t visit/buy/like). This means that we will have to “create” those. This is what we call “negative sampling”. 
The major difficulty in predicting if a user will visit/buy/like a given product is that we only have positive examples from which to learn. Indeed, we always have the visits, buys, and likes, but in many cases, we don’t have the counterfactual events (i.e., didn’t visit, didn’t buy, didn’t like). 
This means that we will have to create those using what we call “negative sampling.” Imagine that you have your visit/buy user data until a given date T (which is probably today or yesterday). You will compute your several affinity scores by taking into account the data since a date T — x days, where x is a buffer you chose. 
Then, you will be able to create a learning set with the data from T — x days until T. During this period of time, you have a set of User/Product couples that are “true” (i.e., the user bought the product during the period). But you don’t have negative examples, so you will have to create User/Product couples that are “false” (i.e., the user didn’t buy the product during the period). For each couple, your features will be the affinity scores you just computed, and the target will be “true” or “false”. You try to find the best combination to optimise the visits, buys, and likes. 
How many “false” couples should you create? Well, it’s a good question, and there is no real answer to this. It should be enough to have an unbalanced dataset, since in reality, the events visit, buy, or like are actually very unlikely to happen. How can I create «false» couples ? There are several strategies: 
Now that your learning set is ready, you just have to split into a train and test set to create and evaluate your model. For the best possible results, you should do this split on a time basis. 
Let’s say now that we have new columns date in our visit dataset: 
user_id x product_id x visit x date 
Here is the SQL code to create a learning set as described above with strategy number three: 
With the approach we’ve walked through here, note that once you trained a model based on your affinity scores, you will have to score all possible user/product couples. So obviously it can be extremely expensive if you have a huge catalog of products, which is why this approach better suits limited catalogs of products or content. We can think about ways to tackle this issue, but it will always mean limiting the set of available products (this limitation can be drawn by marketing rules, for example). 
We also see that ultimately, the choice of the algorithm will be very important, because the scoring time will be completely different depending on this choice. A logistic regression could be a good choice in that case since it’s a linear combination which allows a really fast scoring. 
So there you have it! You now have the basics with which to create a simple recommendation engine. We’ll have some more posts coming up that apply this to some real-life data (sneak preview: it may or may not involve beer [!!]), so stay tuned. 
Written by 
Written by",Pierre Pfennig,2018-01-08T14:39:09.840Z
Berkeley I School – Medium,"The UC Berkeley School of Information is a multi-disciplinary program devoted to enhancing the accessibility, usability, credibility & security of information.",NA,NA
"Machine Learning for Humans, Part 3: Unsupervised Learning | by Vishal Maini | Machine Learning for Humans | Medium","How do you find the underlying structure of a dataset? How do you summarize it and group it most usefully? How do you effectively represent data in a compressed format? These are the goals of unsupervised learning, which is called “unsupervised” because you start with unlabeled data (there’s no Y). 
The two unsupervised learning tasks we will explore are clustering the data into groups by similarity and reducing dimensionality to compress the data while maintaining its structure and usefulness. 
In contrast to supervised learning, it’s not always easy to come up with metrics for how well an unsupervised learning algorithm is doing. “Performance” is often subjective and domain-specific. 
An interesting example of clustering in the real world is marketing data provider Acxiom’s life stage clustering system, Personicx. This service segments U.S. households into 70 distinct clusters within 21 life stage groups that are used by advertisers when targeting Facebook ads, display ads, direct mail campaigns, etc. 
Their white paper reveals that they used centroid clustering and principal component analysis, both of which are techniques covered in this section. 
You can imagine how having access to these clusters is extremely useful for advertisers who want to (1) understand their existing customer base and (2) use their ad spend effectively by targeting potential new customers with relevant demographics, interests, and lifestyles. 
Let’s walk through a couple of clustering methods to develop intuition for how this task can be performed. 
“And k rings were given to the race of Centroids, who above all else, desire power.” 
The goal of clustering is to create groups of data points such that points in different clusters are dissimilar while points within a cluster are similar. 
With k-means clustering, we want to cluster our data points into k groups. A larger k creates smaller groups with more granularity, a lower k means larger groups and less granularity. 
The output of the algorithm would be a set of “labels” assigning each data point to one of the k groups. In k-means clustering, the way these groups are defined is by creating a centroid for each group. The centroids are like the heart of the cluster, they “capture” the points closest to them and add them to the cluster. 
Think of these as the people who show up at a party and soon become the centers of attention because they’re so magnetic. If there’s just one of them, everyone will gather around; if there are lots, many smaller centers of activity will form. 
That, in short, is how k-means clustering works! Check out this visualization of the algorithm — read it like a comic book. Each point in the plane is colored according the centroid that it is closest to at each moment. You’ll notice that the centroids (the larger blue, red, and green circles) start randomly and then quickly adjust to capture their respective clusters. 
Another real-life application of k-means clustering is classifying handwritten digits. Suppose we have images of the digits as a long vector of pixel brightnesses. Let’s say the images are black and white and are 64x64 pixels. Each pixel represents a dimension. So the world these images live in has 64x64=4,096 dimensions. In this 4,096-dimensional world, k-means clustering allows us to group the images that are close together and assume they represent the same digit, which can achieve pretty good results for digit recognition. 
“Let’s make a million options become seven options. Or five. Or twenty? Meh, we can decide later.” 
Hierarchical clustering is similar to regular clustering, except that you’re aiming to build a hierarchy of clusters. This can be useful when you want flexibility in how many clusters you ultimately want. For example, imagine grouping items on an online marketplace like Etsy or Amazon. On the homepage you’d want a few broad categories of items for simple navigation, but as you go into more specific shopping categories you’d want increasing levels of granularity, i.e. more distinct clusters of items. 
In terms of outputs from the algorithm, in addition to cluster assignments you also build a nice tree that tells you about the hierarchies between the clusters. You can then pick the number of clusters you want from this tree. 
“It is not the daily increase, but the daily decrease. Hack away at the unessential.” — Bruce Lee 
Dimensionality reduction looks a lot like compression. This is about trying to reduce the complexity of the data while keeping as much of the relevant structure as possible. If you take a simple 128 x 128 x 3 pixels image (length x width x RGB value), that’s 49,152 dimensions of data. If you’re able to reduce the dimensionality of the space in which these images live without destroying too much of the meaningful content in the images, then you’ve done a good job at dimensionality reduction. 
We’ll take a look at two common techniques in practice: principal component analysis and singular value decomposition. 
First, a little linear algebra refresher — let’s talk about spaces and bases. 
You’re familiar with the coordinate plane with origin O(0,0) and basis vectors i(1,0) and j(0,1). It turns out you can choose a completely different basis and still have all the math work out. For example, you can keep O as the origin and choose the basis to vectors i’=(2,1) and j’=(1,2). If you have the patience for it, you’ll convince yourself that the point labeled (2,2) in the i’, j’ coordinate system is labeled (6, 6) in the i, j system. 
This means we can change the basis of a space. Now imagine much higher-dimensional space. Like, 50K dimensions. You can select a basis for that space, and then select only the 200 most significant vectors of that basis. These basis vectors are called principal components, and the subset you select constitute a new space that is smaller in dimensionality than the original space but maintains as much of the complexity of the data as possible. 
To select the most significant principal components, we look at how much of the data’s variance they capture and order them by that metric. 
Another way of thinking about this is that PCA remaps the space in which our data exists to make it more compressible. The transformed dimension is smaller than the original dimension. 
By making use of the first several dimensions of the remapped space only, we can start gaining an understanding of the dataset’s organization. This is the promise of dimensionality reduction: reduce complexity (dimensionality in this case) while maintaining structure (variance). Here’s a fun paper Samer wrote on using PCA (and diffusion mapping, another technique) to try to make sense of the Wikileaks cable release. 
Let’s represent our data like a big A = m x n matrix. SVD is a computation that allows us to decompose that big matrix into a product of 3 smaller matrices (U=m x r, diagonal matrix Σ=r x r, and V=r x n where r is a small number). 
Here’s a more visual illustration of that product to start with: 
The values in the r*r diagonal matrix Σ are called singular values. What’s cool about them is that these singular values can be used to compress the original matrix. If you drop the smallest 20% of singular values and the associated columns in matrices U and V, you save quite a bit of space and still get a decent representation of the underlying matrix. 
To examine what that means more precisely, let’s work with this image of a dog: 
We’ll use the code written in Andrew Gibiansky’s post on SVD. First, we show that if we rank the singular values (the values of the matrix Σ) by magnitude, the first 50 singular values contain 85% of the magnitude of the whole matrix Σ. 
We can use this fact to discard the next 250 values of sigma (i.e., set them to 0) and just keep a “rank 50” version of the image of the dog. Here, we create a rank 200, 100, 50, 30, 20, 10, and 3 dog. Obviously, the picture is smaller, but let’s agree that the rank 30 dog is still good. Now let’s see how much compression we achieve with this dog. The original image matrix is 305*275 = 83,875 values. The rank 30 dog is 305*30+30+30*275=17,430 — almost 5 times fewer values with very little loss in image quality. The reason for the calculation above is that we also discard the parts of the matrix U and V that get multiplied by zeros when the operation UΣ’V is carried out (where Σ’ is the modified version of Σ that only has the first 30 values in it). 
Unsupervised learning is often used to preprocess the data. Usually, that means compressing it in some meaning-preserving way like with PCA or SVD before feeding it to a deep neural net or another supervised learning algorithm. 
Now that you’ve finished this section, you’ve earned an awful, horrible, never-to-be-mentioned-again joke about unsupervised learning. Here goes… 
Person-in-joke-#1: Y would u ever need to use unsupervised tho? 
Person-in-joke-#2: Y? there’s no Y. 
Next up… Part 4: Neural Networks & Deep Learning! 
Play around with this clustering visualization to build intuition for how the algorithm works. Then, take a look at this implementation of k-means clustering for handwritten digits and the associated tutorial. 
For a good reference on SVD, go no further than Andrew Gibiansky’s post. 
More from Machine Learning for Humans 🤖👶 
Written by 
Written by",Vishal Maini,2018-05-28T18:08:01.484Z
AI and Machine Learning in Cyber Security | by Formulatedby | Towards Data Science,"Zen monks have been using a tool called a ‘koan’ for hundreds of years to assist them in reaching enlightenment. These koans are like riddles or stories that can only be solved by letting go of ones narrowing believes and stories about how things should be. Zen students sit in silent meditation and observe how the koan is working on them, slowly transforming their way of looking at the world and revealing a tiny piece of the path to nirvana, that place of no suffering. 
“Zen is like a man hanging by his teeth in a tree over a precipice. His hands grasp no branch, his feet rest on no limb, and under the tree another man asks him, ‘Why did Bodhidharma come to China from the West?’ If the man in the tree does not answer, he misses the question, and if he answers, he falls and loses his life. Now what shall he do?” — Zen Koan — Case 5 of the Gateless Gate Collection. 
You might wonder what that has to do with cyber security. With the increased popularity of deep learning and the omni presence of the term artificial intelligence (AI), a lot of security practitioners are tricked into believing that these approaches are the magic silver bullet we have been waiting for to solve all of our cyber security challenges. But just like a koan, deep learning (or any other machine learning approach) is just a tool. It’s a tool you have to know how to apply in order for it to reveal true insight. And it’s not the only tool we need to use. We need to mix in experience. We have to work with experts to capture their knowledge for the algorithms to reveal actual security insights or issues. Just like with koan study, you work with a teacher (the expert) to have him guide you on your journey. 
Where do we stand today with artificial intelligence in cyber security? First of all, I will stop using the term artificial intelligence and revert back to using the term machine learning. We don’t have AI (or to be precise AGI) yet, so let’s not distract ourselves with these false concepts. 
Where are we with machine learning in security? To answer that question, we first need to look at what our goal is for applying machine learning to cyber security problems. To make a broad statement, we are trying to use machine learning to find anomalies. More precisely we use it to identify malicious behavior or malicious entities; call them hackers, attackers, malware, unwanted behavior, etc. But beware! To find anomalies, one of the biggest challenges is to define what’s normal. For example, can you define what is normal behavior for your laptop day in — day out? Don’t forget all the exceptional scenarios when you are traveling; or think of the time that you downloaded some ‘game’ from the Internet. How do you differentiate that from a download triggered by some malware? Put in abstract terms, interesting security events are not statistical anomalies. Only a subset of those are interesting. An increase in network traffic might be statistically interesting, but from a security point of view, that rarely ever represents an attack. 
In a somewhat simplified world, we can partition security use-cases into two groups: The problems where machine learning has made a difference and the ones where machine learning has been tried, but will likely never yield usable results. In machine learning lingo, from a supervised perspective, the former category is comprised of all the problems where we have “good”, labeled data. The latter is where we don’t have that. The unsupervised side looks a bit different. There we have to distinguish among the different unsupervised approaches. For this conversation, let’s consider clustering, dimensionality reduction, and association rule learning as the main approaches within unsupervised learning. All of these approaches are useful to make large dataset easier to analyze or understand. They can be used to reduce the number of dimensions or fields of data to look at (dimensionality reduction) or group records together (clustering and association rules). However, these algorithms are of limited use when it comes to identifying anomalies or ‘attacks’. 
The following diagram summarizes this again: 
Diagram 1 — Incomplete view of machine learning algorithms and applications in security. 
Let’s have a quick look at the different groups of machine learning algorithms, starting with the supervised case. This is where machine learning has made the biggest impact in cyber security. The two poster use-cases are malware classification, or the classification of files, and spam detection. The former is the problem of identifying whether a file is benign — we can execute it without having to worry about any ‘side effects’ — or if it is malware that will have a negative impact when we run it. Today’s approaches in this area have greatly benefited from deep learning where it has helped drop false positive rates to very manageable numbers while also reducing the false negative rates at the same time. Malware identification works so well because of the availability of millions of labeled samples (from both malware and benign applications). These samples allow us to train deep belief networks extremely well. The problem of spam identification is very similar in the sense that we have a lot of training data to teach our algorithms right from wrong. 
Where we don’t have great training data is in most other areas. For example, in the realm of detecting attacks from network traffic. We have tried for almost two decades to come up with good training data sets for these problems, but we still do not have a suitable one. The last data set we thought was decent was the MIT LARIAT data set, which turned out to be significantly biased. It’s a really hard, if not impossible, problem to assemble a good training data set. And without one, we cannot train our algorithms. There are other problems like the inability to deterministically label data, the challenges associated with cleaning data, or understanding the semantics of a data record. But those are out of scope for this article. 
On the unsupervised side, let’s start with dimensionality reduction. Applying it to security data works pretty well, but again, it doesn’t really bring us any closer to finding anomalies in our data set. The same is true for association rules. They help us group data records, such as network traffic, but how do we assess anomalies with this information? Clustering could be interesting to find anomalies. Maybe we can find ways to cluster ‘normal’ and ‘abnormal’ entities, such as users or devices? It turns out that the fundamental problems with clustering in security are distance functions and the ‘explainability’ of the clusters. If you are interested in the details of that, you can find more information about the challenge with distance functions and explainability in this blog post. 
The above algorithms are tools that could potentially be useful for detecting attacks if used in the right way. Aside from the challenges mentioned already, there are some other significant ingredients that we are missing. The first one is context. Context is anything that helps us better understand the role of the entities involved in the data, such as information about devices, applications, or users. Context for devices includes things like a device’s role, it’s location, it’s owner, etc. Rather than looking at network traffic logs in isolation, we need to add context to make sense of the data. Is a device supposed to respond to DNS queries? If you know that it is a DNS server, this is absolutely normal behavior, but if it weren’t a DNS server, that kind of behavior could be a sign of an attack. 
In addition to context, we need to build systems with expert knowledge. Ideally systems that help us capture expert knowledge in simple ways. This is very different from throwing an algorithm at the wall and seeing if it yields anything potentially useful. One of the interesting approaches in the area of knowledge capture that I would love to see getting more attention is Bayesian belief networks. Is anyone done anything interesting with those in security? 
We should also consider building systems that do not necessarily solve all of our problems right away, but can help make security analysts more effective by assisting them in their daily routines and in their work. Data visualization is a great candidate in that area. Instead of having analysts look at thousands of rows of data, they can look at visual representations of the data that unlocks a deeper understanding of the data in a very short amount of time. It’s also a great tool to verify and understand the results of machine learning applications. 
In Zen, koans are just a tool or a building block to get to the end goal. Just like machine learning, it’s a tool that you have to know how to apply and use in order to come to new understanding and find attackers in your systems. 
Guest post © by Raffael Marty — Follow him on twitter and check out his blog. 
Upcoming Data Science Virtual Salons 2020: 
- DSS Elevate Virtual | Women, Data, Tech (July 30, 2020 // Online) #DSSElevate 
- DSS Elevate Virtual | Women, Data, Tech (August 27, 2020 // Online) #DSSElevate 
- DSS Virtual Salon | Media, Advertising, Entertainment (September 22–25, 2020) 
- DSS Virtual Salon | Retail & Ecommerce (Nov 17–20, 2020) 
Note from Towards Data Science’s editors: While we allow independent authors to publish articles in accordance with our rules and guidelines, we do not endorse each author’s contribution. You should not rely on an author’s works without seeking professional advice. See our Reader Terms for details. 
Hands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look 
Written by 
Written by",Formulatedby,2020-09-22T16:42:44.398Z
Content-Based Recommendations — Part 3 | by Rakesh4real | Fnplus Club | Medium,"Content-based recommendations are the simplest of all approaches. The main idea is to recommend based on the properties of items instead of using aggregate user behavior.For example, If Bob likes action movies; recommending movies in the same action genre to Bob is called content-based recommendation 
How will we know if two movies have similar content? 
We can know how similar two items are with metrics such as cosine similarity metrics. 
Let us understand how cosine similarity works. We can represent a movie-genre matrix in a two-dimensional space as shown below. The x-axis represents the discrete value for comedy attribute and the y-axis represents adventure attribute. We put discrete value ‘1’ for comedy attribute if is movie is full of comedy and ‘0’ if the movie isn’t that funny; Same is done to adventure attribute. We can represent each movie along with genre in the two-dimensional space as shown below. This matrix can be used to find how similar the movies are. 
Here, we notice that the formula used for cosine similarity is a good start but finding the angles from the kind of data we have is difficult. 
We can solve this problem with the same approach but completely different formula! The below-mentioned formula can be used instead 
One thing to note down is that with the increase in the number of factors depending on which we are computing similarities, our matrix dimensions increase or decrease 
Cosine similarity can be implemented to genre very easily. The code snippet below does the same. 
Other methods( or metrics) are also available which we will discuss in detail in coming sections. Some of them are — 
Note: You may do some string wrangling and get ‘years’ from data set to recommend with it’s help. 
Performing this trick may be termed as one of the Arts for designing a Recommendation Systems. Everything depends on the nature of the data you have. Again we have to decide how far the two movies have to be substantially different? Chose a mathematical function that smoothly scales timeline into range zero to one and implements it in code. Here, the exponential function does the trick. 
Code for Python implementation of recommending based on release years is shown below — 
Note: Test as many functions as possible to get best recommendations 
We have to remember that our recommendation algorithms in surpriselib have just ONE job — Predict rating for a given user for a given movie. 
K Nearest Neighbours[KNN] Algorithm: Fancy name for a simple idea. It selecting ’N’ number of things that are close to the things you are interested in. 
Note: For Top N recommendations, the relative order of predicted ratings matter. Not predicted Rating itself. If you are really striving for fine-tuning prediction accuracy, there are ways to normalize our predicted ratings to get them into the range we want. For example, Log-Quantile Normalisation does the trick. BUT NOBODY CARES IN REAL WORLD. ONLY TOP-N ARE CARED ABOUT!!! 
The main idea of this algorithm is to extract properties from the film itself which are then quantified and analyzed for recommendations. 
Written by 
Written by",Rakesh4real,2019-07-30T05:53:29.802Z
Medium SEO: How To Rank Your Medium Articles In Google Search | by Aamir Kamal 🚀🚀🚀 | Menlo Blogging | Medium,"I got a job offer because one of the most prominent internet marketers in my country finds out my article through Google search. As an internet marketer, it is your job to learn search engine optimization. If you don’t know about SEO as a writer, you will learn everything on this Medium article; how to rank your Medium article, and how to get more traffic to your Medium stories. So, how can use optimize your Medium articles for the search engines? 
In this article you will learn the following things: 
Search Engine Optimization (SEO) is making your article optimize for the search engine so that it easily getting ranked for keywords of your interest. There are some useful techniques through which you could target the right audience, write for the right audience, and get more exposure in the niche of your interest. 
Google is one of the famous search engines with the highest market share. Google has over 200 factors that they consider before ranking a website on Google search. As a writer on Medium, metrics like website loading speed, domain authority, and other minor factors are already compared in this but if you are competing with the writers who have more SEO knowledge then you might miss the chances. 
Read: Don’t Buy A Course Of SEO Instead Read This 
The most important thing to do when you want to optimize your Medium stories for search engines is to use the Medium available SEO setting on your story. While writing the draft, click on the three horizontal dots on the top right of your Medium story. Click on that three horizontal dots and click on the “More settings” option. 
There are two things in SEO setting on your Medium story, you have to optimize it the following way; 
The most important thing in ranking your Medium articles in Google search or through any search engine is by optimizing your Medium articles through On-Page and Off-Page SEO. 
This keyword is one of the highest competitive keywords on the Internet and with the right keyword research, I have got over 133,000 views on this article. As you can see, even though I have just made $20 from this article but I wrote this article just to share my thoughts or opinions and not to make money. But, how you can rank on any keyword through writing on Medium and optimizing your Medium articles for search engines? This On-Page SEO tips will help you. 
If you think that just optimizing your Medium story through SEO will help you then you might be true but to increases your chances of getting your article on the front page of the search engines, you need to do some little work by doing off-page SEO of your Medium stories. This is what I do when I publish an article on Medium. 
Off-page SEO is mostly done by bloggers who are writing on their blogs. They work very hard to rank their blog posts, as Medium.com has a good domain authority so it is far easier to rank your article compared to your self-hosted blog. I was watching a YouTube video where the blogger in the video has deleted all his Medium articles and exported it to her blog, why? because she was getting good traffic on her blog and was making a very low amount of money so she taught maybe just publishing those articles on her blog will generate more money, but this concept is wrong. You might end up with less traffic because of the domain authority and age factor of your site. 
I hope this helps. 
Let’s keep in touch! Sign up for my weekly newsletter (Just Blogging On Medium and Marketing related content directly in your inbox) 
Read more related stories: 
Written by 
Written by",Aamir Kamal 🚀🚀🚀,2020-08-15T09:28:43.022Z
"Master Data Management, how to match and merge records to unify your data | by Julien Kervizic | Hacking Analytics | Medium","In most organizations, it is particularly impactful around entities such as products and customers. To achieve clean results and data, records needs to be associated and merged to form this unified view of the entities. 
There are multiple challenges that data professionals face when dealing with the creation and maintenance of master data entities. From the definition of a matching strategy, the setup of a consolidation strategy and field authority strategy. 
There are different types of matching that can be accomplished from automated rules to match data together, or have it gone through a review process. 
The matching methods used can be set up to handle different matching certainties, with the most certain matches going through an automated process and the more uncertain ones, going through a review process. This type of approach can be useful to get the appropriate samples to train a prediction model. 
There exists a variety of matching types possible, from exact matching, phonetic matching, propensity, fuzzy or negative matching. 
Exact Match: An exact match relies on being able to link two different sources of information based on the existence of a specific key that allows to match the information. This can be, for instance, an EAN code for products, an national identity number, the concatenation of a name, and an address. 
Phonetic Match: There can cases when an exact match might not be sufficient to handle the merging of information. This usually happened when data needs to be inputted by hand based on vocal input. For instance, recording a name or address at a point of sales (POS) system. When this happens, we need to be able to relax the matching algorithm to handle phonetically similar cases. 
In python, the Phonetics library incorporates a few of the most popular phonetic matching algorithms. Generally, these algorithms work well for English but often do not support other languages. 
In the above example, we can see how Julien and Julian, have the same phonetic representation using the Soundex algorithm, explaining why Starbucks Barista often misspell my name to Julian. 
Phonetics algorithms such as Soundex are incorporated in some databases such as Postgres or SQLServer. 
Fuzzy match: A fuzzy match is a match that is not exact. Using fuzzy matching comes with is a trade-off between exactness and coverage of the match. Some of its application relates to allowing for different types of spelling or input misspelling, matching users when other criteria match and their address are within a certain radius of each other. The following tutorial from datacamp shows how we can compare the different strings and generate a score enabling this inexact match. 
Propensity match: Propensity matching provides a different type of in-exact matching, but this time relying on a prediction model to produce the likely value that a match is valid. 
They provide a certain level of confidence that we should consider the match, with a threshold for assignment that can be configured. 
Provided we can obtain some training data; we can leverage supervised learning methods such as logistic regression to generate this propensity score. 
To compare two records containing text inputs for matches, we need to be able to generate features either from their raw, cleansed, or phonetic representation and is to compare the two sets of strings. We can also add additional features to the mix. 
The above will create features based on the edit hamming distance. It will look up pair of column values in a data-frame assuming the same column names are suffixed by _1 or _2 and compare their raw representation and lowercased representation using hamming normalized distance. The following is a sample output: 
Based on that data, we can train a prediction model. This is easily done using sklearn: 
The probability score can then be computed and compared to the threshold value. 
Negative Match: Negative Matching provides additional conditions/exclusion rules, about when not to match two records together. They can be used to exclude an unlikely event. 
An example of when negative matches could be used is showed above. When comparing two sets of user records and figuring out that the distance between the records, two addresses are above a certain threshold. This type of negative match, when implemented within a workflow, could make it so that an automated match is not applied but still flagged for manual review. 
There are multiple considerations to have when looking at the merging of data. Entities can have multiple names, languages can be impactful in the cases of names, places, or addresses, data should be normalized, and data cleansing is usually necessary before matching the different attributes. 
Name frequency: When used for matching, one of the factors to take into consideration is is the frequency of names. Different names and different frequency of occurrences, and common names should need a higher burden of evidence before being matched together. 
The above example shows how unlikely my name is to be found compared to John Smith. Given that it is so unlikely to be found, it should require much less external evidence than for a John Smith. If, for instance, we were to try to match records of a John Smith, we might need to add either an additional phone number of address match to the mix. 
Pseudonyms and Alternative naming: It is worth noting that different entities can have alternative nomenclatures, be them official, stage names, or aliases. 
Take the example of the city of Astana originally called Akmolinsk further renamed to Tselinograd and Akmola, Astana, and now called Nur-Sultan or the city of Madras nowadays named Chennai. Or of some of the historical characters, Alexander the great, Pliny the young, or singers EMINEM, 50 cents, or even actors such as Ben Kingsley whose true name is Krishna Pandit Bhanji. 
Languages: Languages tend to be an essential consideration to have in matching different entities. Take, for example, the matching of various sources in English and Chinese. The entity might be the same, but there are different representations of the same name across languages. 
Take, for example, the Nur-Sultan case mentioned before, the same entity has different name representations, and just one of these representations Nur-Sultan has a diverse language representation in French, English, and Chinese. 
Records merging across languages is heavily reliant on an appropriate multi-lingual data model coupled to having linking data across languages. 
Abbreviations: Abbreviations impact how different text records might need to be matched. From first names shortening, such as Bill for William or Pete for Peter, to title abbreviations such as Dr. for Doctor, they make it harder to find matches without additional processing. 
Data Cleansing & Data normalization: Trying to match differently text fields also requires some degree of data cleansing. From handling potential trimming of spaces, special characters, and punctuations to correcting misspellings, there are numerous steps to take to standardize the input and allow it to find an accurate match. 
For example, phone numbers can be provided in quite different formats, for the same true phone number. They can be provided with a county code information, in + (+44) or 00 format (0044), or without country code, they can be provided as a pure numerical chain (0601020304) or with separators (060–10–20–30–4). To be able to perform a full match, it is crucial to normalize the data in the same format. 
REGEXP is a particularly useful tool in this respect, allowing to match strings and substrings based on different patterns. 
In the example above, the REGEXP matches sequences of digits and sequences of digits preceded by a +. This provides a decent first step towards cleaning and normalizing phone numbers. There are better solutions for this particular use case than relying directly on REGEXP, but it proves useful as soon as you need to do something a little custom. 
An authority strategy help define which fields in the strategy should be considered as the authoritative source of information, when conflicting information is provided. There exist different types of authority strategies, such as authority hierarchy, to time authority, to voting rules. 
An authority hierarchy helps define a preference for taking certain types of information from different sources. The logic behind this type of strategy is that some sources of information should be considered as “trusted sources,” while others have information of different qualities. 
Let’s look at the information contained from a government database of national identity and names. We can consider this as a highly trusted source of information. Compare this to an input form on a website, on a site where typos or bad input can occur both for the identity and names. 
The example shown above shows how an overall hierarchy strategy could work in selecting (in yellow) profile attributes in a consolidated record. 
Having an authority hierarchy of information help in this case to prioritize the information coming from more trusted sources than others. In the example explained above, we would consider the government database the more authoritative source of information and use that information where provided. In case the information from this source is not available, we would be relying on the input-form, but only as a complement for the authoritative source. 
Different fields of information can be “mutable” — i.e., they are liable to change, and it is important how to treat information across time. 
Fields such as names are relatively immutable. They can technically change — for instance, when getting married or requesting a name change with the government. But in the majority of cases, they wouldn’t frequently be changing. 
Other fields, such as an address or a phone number, have a higher tendency to be mutable. You can easily change the address or phone numbers. Fields with a higher mutability tendency tend to benefit more from applying a time-based authority strategy. You would want to have the freshest source of information. 
The table above shows what would happen to the attribute selected in the consolidated profile if we extended the previous example with a time-bound authority strategy for the email and mobile fields. 
When dealing with multiple sources of information for the same fields, it can be beneficial to apply a voting rule authority strategy. The authoritative field would be defined through a majority vote across the different sources of information. 
A voting rule authority strategy can provide a high level of signal when trusted sources of information are inexistent. Still, there exists a high number of available sources that can be correlated. 
Extending the example discussed above with a voting strategy for the address field, besides the fact that more misspelling of my name happens, we can see that the address provided by the government data source is no longer selected for the unified profile. This is due to more than one data-source providing an alternate address. 
The types of authority strategy that needs to be applied should be very much field dependent. 
Multilevel strategy: Depending on the number of data sources available, it might be useful to group the data-sources by classes and apply a multilevel authority strategy on these different fields. 
Time-Dependent: No matter what strategy you use, time will always be a factor to some extent. It is important to place time boundary on most fields to apply the strategy, in the voting strategy example it wouldn’t have been very smart to apply the voting strategy if all the records resulting in the vote for “Rodeo-drive” had been before an authoritative record (in terms of source hierarchy). Likewise, if the last update we got from an “authoritative” source is a few years old, it might still be worth reconsidering whether to use a “fresher” source of information. 
The consolidation strategy dictates how the records are merged. There are generally two different types of consolidation strategies hard merge and soft merge. The hard merge consolidate together multiple records into one single record, while a soft merge strategy creates an association between the different records that, when read, should be grouped. 
Within a hard merge, there are two subtypes of consolidation strategy that can be applied. Records can be hard merged using forward only merging strategy or a backward merging strategy — both subtypes of hard merge results in a consolidated datasets. 
Above is an example of how a hard merge would look like based on a mixed hierarchy/time-bound authority strategy. 
There are benefits to performance a hard merge such as reduction of database size, more efficient queries, and ease of extraction of these authoritative fields. There are however, a few drawbacks the three main ones being 1) data loss 2) Irreversible merging 3) not all authority strategies play well with hard merging. 
The forward only merging strategy, will tie the records together after a matching condition has been met. It will not impact the historical records. 
Let’s take an example where we have two profiles coming from different sources. 
One method to implement the forward merging of associated data is by modifying a redirection table. For the example mentioned above, this can be done in the following way in python: 
At some point, we merge the profiles together and update the redirection table, ie, both ids now refers to the same first profile: 
Pushing additional events after the forward merging of identities: 
This gives us the following events in each profile after the forward only merge happened: 
Besides the id redirection and the merging of attributes, the merging of the profile might incorporate further steps such as the deactivation of the previous profile. 
The backward merging strategy, will tie records back historically and merge together the different records that have been identified with matching conditions. 
In the case of backward merging, the data related to both profiles are merged onto one both historically and as times goes on. In code, this type of strategy can be implemented as an initial merge of events related to both profile and an id redirection. 
This gives us the following events in each profile after the backward merge happened: 
Soft merging strategies rely on an association to be created between the different records. They allow for the different records to be grouped. Like for hard merge, two main strategy subtypes can be applied, a full association or an association with filtering conditions. 
One of the main advantages of using a soft-merge strategy is that the association can always be undone. It does however, have some space and performance disadvantages. Such that all the records would need to be maintained, and queries would need to be created that would need to look up records that are associated and then apply the authority strategy on these records. 
Full Association 
A full association provides an association record for the different entities provided. Think of a table [id1, id2] that allows to associate any record with another one, this table can allow any record to fetch information to any associated user record, and an authority strategy could be applied to the different values onto it. 
An application for this is, for instance, the ability to match an anonymous session to a logged-in user identity. The identity key that was provided before login is a specific cookie value, for instance, a google analytics client id, while once logged in, a new identity is provided that needs to be associated with. 
The full association allows to both extract on-going activities across the different identities, but also allow them to leverage historical data through it. 
To make an association work, we need to be able to tie a given profile to one or more profiles. Taking back the previous code example, we can make a few alterations to the profile class to support this. 
We need to incorporate a way to 1) store associations 2) add associations 3) query the relevant information obtained by association (in this case events). 
Following the same pattern as in the previous example and performing the association. We can see how the full set of events is now captured post association. 
But contrary to the profile merging approach, it is still possible to retrieve the events directly associated with each profile: 
Association with filtering 
An association with filtering provides a little more control as to how the different data-points will be consolidated together. It allows us to implement a forward only kind of merge as an association, useful when you are only able to leverage the joint record if a user has accepted new terms of services, for instance. 
Implementing association with filtering is possible but requires some additional code changes to be able to exclude the unwanted records. 
It is yielding the following results when ingesting the different events. 
Unlike with the forward merging strategy, it is possible to take the viewpoint from both profile 1 and profile 2 perspective. 
There are multiple things to consider when looking at which consolidation strategy to use the volume of data, performance, complexity, and the need to un-match records. 
Volume of data: The overall data volume is a factor that plays in the decision of which strategy to apply. Consider transient identifiers such as website session ids, each able to create a temporary profile. A given user can have hundreds of temporary profiles and potentially one logged-in profile. To fetch the authoritative profile, the application would have to go through all these temporary profiles and apply the authority strategy. Another example where the volume of data can be impactful is when relying on user input. Think of a website letting its users input information related to their favorite holiday destination. This could result in thousands of duplicate records for each destination due to spelling error, different names… Keeping only a soft merge strategy would be quite tricky. 
Uniqueness of matches: The matching strategy may have to deal with non-unique matches. In case of a soft-merge strategy, only an association key would need to be added to properly deal with that information. While in the case of a hard merging, this may lead to information being duplicated across key records. Additional matching rules may be added, such as only merging the first initial record found. Still, the uniqueness of match is a consideration to have when setting up the matching and merging strategies. 
Performance and complexity: Using a soft-merge strategy typically offers lower read performance than a hard merge strategy, and as we saw in our simple code examples above, implementing a soft merge strategy usually requires some additional complexity. 
Uncertain matches: Different types of matching strategies and identifiers. lead to a different risk of matching records that do not belong together. When dealing with strategies that lead to risky matches, it is often better to apply a soft merge strategy. Soft merge makes it easier to accept more un-certain matches are consolidated together as the association can always be undone. 
Regulation: Regulation sometimes plays a role in terms of how records should be merged together. It can, for instance, dictate what data should be available to be used in a consolidated profile or what data could be used for which purpose. Association with filtering is the consolidation strategy that would most easily satisfy different regulations, but also the most complex to integrate. 
There are multiple steps and considerations to have when handling master data management, each of the three strategies that need to be applied when consolidating records has numerous facets and considerations to have. 
Often there needs to be a granular setup not only at the entity level but at the field leve to apply the right strategy to the right piece of information. The question is then more about balancing the different pros and cons of the various (sub)strategies that need to be applied. 
It is often a safe bet to start from a risk-aware strategy to record consolidation and use a soft-merge strategy with review to ensure the best quality and handle the more advanced aspect and more automated consolidated merging at a later stage. 
More from me on Hacking Analytics: 
Written by 
Written by",Julien Kervizic,2020-05-09T15:03:21.949Z
NLP: Word Embedding Techniques for Text Analysis | by Fangyug | SFU Professional Master’s Program in Computer Science | Medium,"Fangyu Gu, Srijeev Sarkar, Yizhou Sun, Hengzhi Wu, Kacy Wu 
This blog is written and maintained by students in the Professional Master’s Program in the School of Computing Science at Simon Fraser University as part of their course credit. To learn more about this unique program, please visit {sfu.ca/computing/pmp}. 
Let’s talk about text data 
It doesn’t matter if you’re spending that lazy Sunday watching shows on Netflix, or filling up that last-minute application to your favorite grad school, you’re surrounded by data. In this article, we will focus on one of the most common forms of data, i.e., “text.” 
News articles, emails, chats, and even blogs such as this one are full of text-based data. Now, with the amount of textual data readily available at our disposal, wouldn’t it make sense to develop a deeper understanding of this data and gain potential revolutionary insights? 
Natural Language Processing, Text Analysis, and its importance 
That’s where Natural Language Processing comes into the picture. It combines our knowledge of linguistics, machine learning, computer science to analyze huge chunks of natural language data, to create useful applications for our society. 
NLP finds applications in several fields. Here we will lay our focus on Text Analysis and ‘word embedding’ in particular. 
Companies use a technique called ‘Text Analysis’ to classify essential emails and texts, understand the general customer sentiment without manually reading reviews and feedback. It helps companies save thousands of hours of human resources, money, and helps automate several mundane processes. 
Word embedding techniques 
Word embedding and its implementations will be the highlight of this article. 
Word embedding helps capture the semantic, syntactic context or a word/term and helps understand how similar/dissimilar it is to other terms in an article, blog, etc. 
Word embedding implements language modeling and feature extraction based techniques to map a word to vectors of real numbers. Some of the popular word embedding methods are: 
We will cover TF-IDF Encoding, Word2Vec Embedding in-depth with explanations, diagrams, and code. 
TF-IDF, short for term frequency-inverse document frequency, can break a word into two parts: TF and IDF. 
TF is the term abbreviation of Term Frequency, defined as the total number of times a term occurs in a document. TF is calculated using the number of times the term occurs in a document divided by the total number of terms. The method is quite intuitive. The more a term occurs in a document, the more significance this term holds against the document. 
However, this is not always the case, depending on the content of documents. From the term frequency that we calculate, we would probably see that terms like “a,” “of,” “the” have the largest value. These words won’t help our analysis much since the word “the” is so common that every article has it. And having these grammatical purpose words is very likely to affect our analysis outcome in the end. One approach to handle this problem is simply removing them. 
We can use the Python package nltk to download all stop words in English. 
From now on, we have just successfully filtered out all stop words, and the remaining are those which hold actual meaning. However, we could probably run into another problem. Suppose we are analyzing news about cats in Canada, and we have calculated the term frequency. In one of these articles, we see words like “Canada,” “animal,” “food,” “cats,” having the same term frequency values. Does it mean these words hold the same amount of importance against the article? The answer is probably No. But the question is, how do we solve this problem? We use IDF. 
Since TF tends to emphasize the wrong term sometimes, IDF is introduced to balance the term weight. IDF, short for inverse document frequency, defined as how frequently a term occurs in the entire document. It is used to balance the weight of terms that occur in the entire document set. In other words, IDF reduces the weight of terms that occur frequently and increases the weight of terms that occur infrequently. 
IDF is calculated using the number of documents containing the term divided by the total number of documents before taking the logarithm. If the term often appears in the entire document set, the IDF result will be close to 0. Otherwise, it will be increasing towards positive infinity. 
To get the final TF-IDF score, we need to multiply the results of TF and IDF. The larger the TF-IDF score is, the more relevant the term is in the documents. From the result, we can see that TF-IDF is proportional to the number of times a word appears in an article, and inverse proportional to the number of times this word appears in the entire domain of articles. 
The mathematical equations for calculating TF-IDF (using the word ‘w’ as an example): 
TF(w)=Frequency of w occurs in the document / the total number of words 
IDF(w)=log(the number of documents containing the term)/ the total number of documents(+1) . 
(Note: the reason to add one in the denominator is to avoid division by zero) 
TF-IDF(w)=TF(w)*IDF(w) 
In this part, we want to show how TF-IDF works, so we write code by ourselves instead of using the function offered by Spark. The code outputs the five words with the highest TF-IDF score in each article. 
The data set we use is a bunch of Fairy tales. The output is shown below: 
Word2vec (word to vector), as the name suggests, is a tool that converts words into vector form. 
In all technicalities, Word2vec is a shallow two-layered neural network, that is used to produce the relative model of word embedding. It works by collecting a large number of vocabulary based datasets as input and outputs a vector space, such that each word in the dictionary maps itself to a unique vector. This allows us to represent the relationship between words. 
Broadly, there are two models CBOW(Continuous Bag of Words) and Skip-Gram. Both neural network architectures essentially help the network learn how to represent a word. This is unsupervised machine learning, and labels are needed to train the model. Either of these two models, can create the labels for the given input and prepare the neural network to train the model and perform the desired task. 
CBOW(continuous bag-of-words) is a model suitable for work with smaller databases, and it does not need much RAM requirement either. In CBOW, we predict a word given its context. The entire vocabulary is used to create a “Bag of Words” model before proceeding to the next task. 
The bag of words model is a simple representation of words that disregards grammar. 
Here is an example for better understanding. 
John likes to watch movies. Mary likes movies too. 
Mary also likes to watch football games. 
A list of words is created by breaking the two sentences. 
We then label each word with the number of occurrences. This is called a bag of words. In computers, this is usually captured as a JSON file. 
The sentences are then combined to get the overall frequency of each unique word. 
Now that the bag of words model is ready, we can use CBOW to predict the probability of a word given these groups of words. 
Each word and its frequency are passed as a unique vector into the input layer of the neural network. Say, if we have X words, the input layer takes in X[1XV] vectors and gives out 1[1XV] in the output layer. 
The input-hidden layer matrix sizes up to [VXN] and the output-hidden layer matrix sizes to [NXV]. In this case, N is the number of dimensions. The layers have no activation function between them. 
To calculate the output, the hidden input layers’ weights are multiplied by the hidden output layers weights. The error between the output and targets is calculated, and the weights are constantly readjusted through backpropagation. The only non-linearity is the softmax calculations in the output layer to generate probabilities for the word vectors. 
Overall, after calculations and readjustments, the weight between the hidden-output layer is taken as the word vector representation. As we can see, this architecture allows the model to predict the current word relying on influence from surrounding words. 
Suppose we have 10000 unique words, and we represent an input word like “ape” as a one-hot vector(A categorical word/variable can be better understood by an ML algorithm when it is one hot encoded to 0’s and 1's). This vector will have 10000 components; each component contains one vocabulary. And we place ‘1"" in one position, which represents the word “ape” and place 0 in the rest of the positions. 
The output of the network is a single vector with 10000 components as well. The probability that a randomly selected nearby word is that vocabulary word. We don’t need to consider the hidden layer neurons since none of them are active. However, the output neurons use softmax. 
When we evaluate the trained network on an input word, the output is a probability distribution instead of a one-hot vector. 
The Hidden Layer 
For example, we use a word vector with 300 features as the input. So the hidden layer should be a 300*10000 matrix, which means there are 10000 rows for words in vocabulary and 300 columns for hidden neurons. 
The one-hot vector we use as input is used to pick out the corresponding row in the matrix. This means that the hidden layers are only used as a lookup table. And the output of the hidden layer is just the word vector for the input word. 
The Output Layer 
As we pick up the word vector for “ape” in the hidden layers, it will be sent to the output layer. The output layer is a softmax regression classifier. 
So what is the softmax regression classifier? Softmax regression is a generalization of logistic regression that we can use for multi-class classification. 
In softmax regression, all of the input components are classified in different classes. And the output is a one-hot vector. 
After the output vector multiple the weight vector from the hidden layer, it then applies the function exp(x) to the result. Finally, to get the outputs to sum up to 1, we need to divide the result by the sum of the result from all 10000 output nodes. 
The words with a similar meaning are brought closer together in the vector space. The skip-gram model uses a word to predict the words surrounding it, and it relies on words with a closer context to function efficiently. 
Although the two models show mirror symmetry, they vary in terms of architecture and performance. 
The choice of model depends on the user’s task. 
To speed up the training of word2vec model, there are two ways you could try: 
Hierarchical Softmax 
Now, the biggest problem is that we have a large amount of calculation from the hidden layer to the output softmax layer, because all words for softmax probability must be calculated, before finding the highest probability value. This model is shown below. And V represents the size of the glossary. 
The first improvement: 
For the mapping from the input layer to the hidden layer, a simple method of summing and averaging all input word vectors is used instead of a linear transformation of the neural network and an activation function. For instance, the input is three 4-dimensional word vectors: 
( 1, 2, 3, 4 ) 
( 9, 6, 11, 8 ) 
( 5, 10, 7, 12 ) 
Then the word vector after our word2vec mapping is 
( 5, 6, 7, 8 ) 
The second improvement: 
It improves the number of calculations from the hidden layer to the output softmax layer. To avoid calculating all words for the softmax probability, word2vec uses the Huffman tree to replace mapping from the hidden layer to the output softmax layer. 
Since we have converted all the probability calculations from the output softmax layer into a binary Huffman tree, our softmax probability calculation only needs to be performed along with the tree structure. As shown below, we can walk along the Huffman tree from the root node to the words of our leaf nodes W2. 
All the internal nodes in our Huffman tree are similar to the neurons in the hidden layer of the neural network, where the word vector of the root node corresponds to our projected word vector, and all leaves nodes are similar to the neurons in the softmax output layer of the neural network. The number of leaf nodes is the size of the vocabulary. In the Huffman tree, the softmax mapping from the hidden layer to the output layer is step by step along the Huffman tree. Therefore, this softmax is named “Hierarchical Softmax.” 
Negative Sampling 
As mentioned in the previous section, millions of weights and tens of millions of training data mean that it is difficult to train this network. 
So what we can do is to modify the optimization objective function. This strategy is called “Negative Sampling” so that each training sample only updates a small part of the weights in the model. It could reduce the number of train calculations but also improve the quality of the final word vector. 
We need to sample common words first. 
For common words like “the,” there will be two issues: 
So word2vec uses a down-sampling strategy. For each word we encounter in the training sample, we have a probability of deleting it. This probability is called the “sampling rate” which is related to the frequency of words.What is the sampling rate? 
We use z(wi) to represent a word, and z(wi) represents the probability (frequency) that it appears in the thesaurus. For example, peanut appears 1,000 times in the 1bilion’s corpus, then z(peanut)=1E-6. Then there is a parameter called ‘sample’ that controls the degree of downsampling, which is generally set to 0.001. The smaller the value, the easier it is to throw away some words. 
It can be seen that if z(wi)<=0.0026 P(wi)=1, then we will not throw these words away. If the occurrence frequency is very high, z(wi)==1(equivalent to nearly every training sample has the word), P(wi)=0.033, you can see that there is still a very low probability that we retain this word. 
And then we do negative sampling. 
Training a neural network means inputting a training sample to adjust the weight so that it predicts this training sample more accurately. In other words, each training sample will affect all weights in the network. As we discussed before, the size of our dictionary means that we have a lot of weight, all of which need to be adjusted slightly. Negative sampling solves this problem, and every time we change a small part of the weight, not all. 
If the vocabulary size is 10000, when the input sample (“fox”, “quick”) is input to the neural network, “fox” is one-hot encoded, and in the output layer we expect the neuron node corresponding to the “quick” word to be output 1, the remaining 9999 should output 0. Here, the words corresponding to the 9999 neuron nodes that we expect to be 0 are negative words. The idea of ​​negative sampling is also very straightforward. A small number of negative words will be randomly selected, such as 10 negative words. Then update the corresponding weight parameters. 
Assume that the original model requires 300 × 10,000 each time (in fact, there is no reduction in the number, but during the operation, the number of loads needs to be reduced.) Now only 300 × (1 + 10) is reduced a lot. 
Selecting negative samples: 
Here comes the question, how to choose 10 negative samples? The negative samples are also selected based on their probability of occurrence, and this probability is related to their frequency of occurrence. Words that appear more often are more likely to be selected as negative samples. 
This probability is expressed by a formula, and each word is given a weight-related to its frequency. The probability formula is: 
You can train your word2vec by genism easily. In the following example, we use a bunch of fairy tales as the data set. This code use word2vec model to find out familiar word for a specific word. 
The output is: 
These words are considered to have some connection with ‘clean’. 
In the above blog post, we introduced and explained two commonly-used word embedding techniques (TF-IDF and Word2vec). Each method includes an introduction, diagram, implementation, and elaboration with code on a real-world dataset. Further, we have also included Training Tricks (improvement methods) for the model. Word Embedding is the foundation of any major text analysis task, and we hope to have done justice to this topic by covering it in-depth. 
Written by 
Written by",Fangyug,2020-02-04T08:21:35.643Z
NLP-BERT model | by Ran | Ran ( AI Deep Learning ) | Medium,"這篇文章，將在 Ubuntu 的 workstation 上，實現 NLP-BERT model。(本文章內容經筆者實現驗證過) 
…… 進行中 
https://medium.com/ran-ai-deep-learning，Email: ran1988mail@gmail.com 
Written by 
Written by",Ran,2019-06-27T02:29:12.564Z
Various types of Distance Metrics in Machine Learning | by Sourodip Kundu | Analytics Vidhya | Medium,"A number of Machine Learning Algorithms — Supervised or Unsupervised, use Distance Metrics to know the input data pattern in order to make any Data-Based decision. A good distance metric helps in improving the performance of Classification, Clustering, and Information Retrieval process significantly. In this article, we will discuss different Distance Metrics and how do they help in Machine Learning Modelling. 
In many real-world applications, we use Machine Learning algorithms for classifying or recognizing images and for retrieving information through an Image’s content. For example — Face recognition, Censored Images online, Retail Catalog, Recommendation Systems, etc. Choosing a good distance metric becomes really important here. The distance metric helps algorithms to recognize similarities between the contents. 
Basic distance function we all know that is Pythagorean Theorem. In order to calculate the distance between two data points A and B Pythagorean theorem considers the length X and Y-axis 
In Machine Learning algorithm we used this formula as a distance function. 
Now we will discuss some of the distance metrics here and implement them in python 
The set of input attributes, for which we want to make a prediction about the resulting output attributes, is called the query, or query point. The first step in making a prediction with MBL(Memory-Based Learning) is to look through the database to find all the data points whose input attributes are similar to the query point. In order to do that, we have to define what is meant by similar. We need to define a distance metric that tells how close two points are. 
Now we will understand the math behind the Distance Metrics and how to Implement them. 
Euclidean distance is the most common use of distance. In most cases when people said about distance, they will refer to Euclidean distance. Euclidean distance is also known as simply distance. When data is dense or continuous, this is the best proximity measure. 
In Cartesian coordinates, if p = (p1, p2,…, pn) and q = (q1, q2,…, qn) are two points in Euclidean n-space, then the distance (d) from p to q, or from q to p is given by the Pythagorean formula: 
Manhattan distance is a metric in which the distance between two points is the sum of the absolute differences of their Cartesian coordinates. In a simple way of saying it is the total sum of the difference between the x-coordinates and y-coordinates. 
Suppose we have two points A and B if we want to find the Manhattan distance between them, just we have, to sum up, the absolute x-axis and y — axis variation means we have to find how these two points A and B are varying in X-axis and Y- axis. In a more mathematical way of saying Manhattan distance between two points measured along axes at right angles. 
In a plane with p1 at (x1, y1) and p2 at (x2, y2). 
Manhattan distance = |x1 — x2| + |y1 — y2| 
This Manhattan distance metric is also known as Manhattan length, rectilinear distance, L1 distance or L1 norm, city block distance, Minkowski’s L1 distance, taxi-cab metric, or city block distance. 
The Minkowski distance is a generalized metric form of Euclidean distance and Manhattan distance. 
The Minkowski distance of order p between two points 
Mostly Cosine distance metric is used to find similarities between different documents. In cosine metric, we measure the degree of angle between two documents/vectors(the term frequencies in different documents collected as metrics). This particular metric is used when the magnitude between vectors does not matter but the orientation. 
Cosine similarity formula can be derived from the equation of dot products:- 
The Jaccard similarity index (sometimes called the Jaccard similarity coefficient) compares members for two sets to see which members are shared and which are distinct. It’s a measure of similarity for the two sets of data, with a range from 0% to 100%. The higher the percentage, the more similar the two populations. Although it’s easy to interpret, it is extremely sensitive to small samples sizes and may give erroneous results, especially with very small samples or data sets with missing observations. 
My git repo which contains the Implementation code:- 
References:- 
Written by 
Written by",Sourodip Kundu,2019-10-28T12:54:59.071Z
Cloud Security Design Principles to Follow in 2018 | by Nutanix | Medium,"Make cloud security your responsibility! Or to put in the words of Werner Vogels, CTO of AWS, “We are responsible for the security of the cloud.” In the age of ever increasing data security concerns, security is everyone’s job. This is more apt for cloud infrastructure security. While cloud cost management may be a concern it is not the priority. 
While cloud comes with the flexibility that gives developers and organizations the freedom to start up experiment and scale with ease it also comes with a great responsibility to tackle the increased security threat surface area. Werner recently stressed on the importance of encrypting and securing your data on cloud at the re:Invent 2017 in Las Vegas. 
Cloud security is a shared responsibility of the cloud provider and customer. Security is also one of the five pillars of a well architected framework for cloud infrastructures, as published by AWS. 
As a responsible customer of cloud, you can follow the given security design principles to effectively safeguard your information, systems and other cloud assets. 
Encrypting data should be your default behaviour. You must follow the principle of least privilege to reduce the threat surface area. Provide only the required necessary permissions to users and groups. You can always give more permissions on a need basis. Having a solid identity and access control is paramount to a secure infrastructure. 
Ensure there is no credential sharing. Each entity/individual in your team should have their own credentials. This will help you to quickly isolate any security incident. 
Also, don’t forget to rotate access credentials regularly! Always follow identity and access management best practices. 
While most public cloud providers have APIs which help you to automate in numerous security best practice checks, cloud management platforms like Botmetric can help you in the quest to automate. Finding and bridging security gaps in your cloud infrastructure must be automated as much as possible. 
Securing the perimeter of your cloud infrastructure is just the tip of the iceberg. You must have a robust security in place at every level from the perimeter to the application. For example: On AWS you must have proper and well defined security controls in edge network, virtual private cloud (VPC), subnet, load balancer, every instance, operating system, and your application logic. 
The detective services refer to enabling all access and flow logs, across all layers of cloud infrastructure. If your cloud provider provides ways to monitor the access to your infrastructure in real time, you must enable such options. For example, on AWS, you can enable CloudTrail. You can also go one step ahead and automate the response to known security alerts. For example, on AWS, you can automatically disable a user’s account on a specified number of continuous failed login attempts using CloudTrail events and AWS Lambda. 
Focusing on cloud data security is the most essential element. It is mandatory to take all the necessary steps to protect your data. From data protection perspective, data can be categorized on a cloud infrastructure as follows: 
Data in transit includes data transmitted between servers within your infrastructure, or between your servers and internet, which may include your end users. You can ensure safety of data in transit by using transmission protocols that implement the latest version of Transport Layer Security (TLS). Consider using HTTPS, or, in fact, force HTTPS usage at places where sensitive information is transmitted. 
Data at rest includes data stored in storage mediums persisting data. It includes block storage, database, and object storage. A common cloud security best practice is to encrypt data at rest, so that even if an intruder gets access to stored data, the real data will still be safe as it is encrypted. You should also check with your cloud provider if it provides built-in encryption mechanisms for various storage mediums. Additionally, check if you can bring your own encryption keys for heightened security. 
In addition to safeguarding your data and protecting it from getting into un-authorized hands, you must have a well defined Data Backup Policy. On incidents when the intruder just deletes data instead of trying to access it, you should be able to recover. At least, your mission critical data must be backed up at proper intervals. 
Despite following all cloud security best practices, you may fail. The best solution is to be ready for anything. Implement a response plan as well as a recovery plan to solve possible security incidents. 
While cloud providers ensure security at their end (physical infrastructure and at other levels based on the service you are using), you as a customer must focus on security at your end. 
It is important to be extremely stringent while designing/defining the security controls of your cloud infrastructure. Follow best practices from day-1, regularly, monitor each of the security layers in your infrastructure, efficiently automate best practice checks, including automating the response to known incidents. Make security your #1 priority in 2018 to stay compliant and let security breaches in your cloud be a thing of the past. Security risks are tough pills to swallow. Remember: Security is everybody’s job! 
The original post is published in the Botmetric blog. Read it Here. 
Written by 
Written by",Nutanix,2018-01-04T18:16:48.302Z
How Transformers Work. Transformers are a type of neural… | by Giuliano Giacaglia | Towards Data Science,"Transformers are a type of neural network architecture that have been gaining popularity. Transformers were recently used by OpenAI in their language models, and also used recently by DeepMind for AlphaStar — their program to defeat a top professional Starcraft player. 
Transformers were developed to solve the problem of sequence transduction, or neural machine translation. That means any task that transforms an input sequence to an output sequence. This includes speech recognition, text-to-speech transformation, etc.. 
For models to perform sequence transduction, it is necessary to have some sort of memory. For example let’s say that we are translating the following sentence to another language (French): 
“The Transformers” are a Japanese [[hardcore punk]] band. The band was formed in 1968, during the height of Japanese music history” 
In this example, the word “the band” in the second sentence refers to the band “The Transformers” introduced in the first sentence. When you read about the band in the second sentence, you know that it is referencing to the “The Transformers” band. That may be important for translation. There are many examples, where words in some sentences refer to words in previous sentences. 
For translating sentences like that, a model needs to figure out these sort of dependencies and connections. Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs) have been used to deal with this problem because of their properties. Let’s go over these two architectures and their drawbacks. 
Recurrent Neural Networks have loops in them, allowing information to persist. 
In the figure above, we see part of the neural network, A, processing some input x_t and outputs h_t. A loop allows information to be passed from one step to the next. 
The loops can be thought in a different way. A Recurrent Neural Network can be thought of as multiple copies of the same network, A, each network passing a message to a successor. Consider what happens if we unroll the loop: 
This chain-like nature shows that recurrent neural networks are clearly related to sequences and lists. In that way, if we want to translate some text, we can set each input as the word in that text. The Recurrent Neural Network passes the information of the previous words to the next network that can use and process that information. 
The following picture shows how usually a sequence to sequence model works using Recurrent Neural Networks. Each word is processed separately, and the resulting sentence is generated by passing a hidden state to the decoding stage that, then, generates the output. 
Consider a language model that is trying to predict the next word based on the previous ones. If we are trying to predict the next word of the sentence “the clouds in the sky”, we don’t need further context. It’s pretty obvious that the next word is going to be sky. 
In this case where the difference between the relevant information and the place that is needed is small, RNNs can learn to use past information and figure out what is the next word for this sentence. 
But there are cases where we need more context. For example, let’s say that you are trying to predict the last word of the text: “I grew up in France… I speak fluent …”. Recent information suggests that the next word is probably a language, but if we want to narrow down which language, we need context of France, that is further back in the text. 
RNNs become very ineffective when the gap between the relevant information and the point where it is needed become very large. That is due to the fact that the information is passed at each step and the longer the chain is, the more probable the information is lost along the chain. 
In theory, RNNs could learn this long-term dependencies. In practice, they don’t seem to learn them. LSTM, a special type of RNN, tries to solve this kind of problem. 
When arranging one’s calendar for the day, we prioritize our appointments. If there is anything important, we can cancel some of the meetings and accommodate what is important. 
RNNs don’t do that. Whenever it adds new information, it transforms existing information completely by applying a function. The entire information is modified, and there is no consideration of what is important and what is not. 
LSTMs make small modifications to the information by multiplications and additions. With LSTMs, the information flows through a mechanism known as cell states. In this way, LSTMs can selectively remember or forget things that are important and not so important. 
Internally, a LSTM looks like the following: 
Each cell takes as inputs x_t (a word in the case of a sentence to sentence translation), the previous cell state and the output of the previous cell. It manipulates these inputs and based on them, it generates a new cell state, and an output. I won’t go into detail on the mechanics of each cell. If you want to understand how each cell works, I recommend Christopher’s blog post: 
With a cell state, the information in a sentence that is important for translating a word may be passed from one word to another, when translating. 
The same problem that happens to RNNs generally, happen with LSTMs, i.e. when sentences are too long LSTMs still don’t do too well. The reason for that is that the probability of keeping the context from a word that is far away from the current word being processed decreases exponentially with the distance from it. 
That means that when sentences are long, the model often forgets the content of distant positions in the sequence. Another problem with RNNs, and LSTMs, is that it’s hard to parallelize the work for processing sentences, since you are have to process word by word. Not only that but there is no model of long and short range dependencies. To summarize, LSTMs and RNNs present 3 problems: 
To solve some of these problems, researchers created a technique for paying attention to specific words. 
When translating a sentence, I pay special attention to the word I’m presently translating. When I’m transcribing an audio recording, I listen carefully to the segment I’m actively writing down. And if you ask me to describe the room I’m sitting in, I’ll glance around at the objects I’m describing as I do so. 
Neural networks can achieve this same behavior using attention, focusing on part of a subset of the information they are given. For example, an RNN can attend over the output of another RNN. At every time step, it focuses on different positions in the other RNN. 
To solve these problems, Attention is a technique that is used in a neural network. For RNNs, instead of only encoding the whole sentence in a hidden state, each word has a corresponding hidden state that is passed all the way to the decoding stage. Then, the hidden states are used at each step of the RNN to decode. The following gif shows how that happens. 
The idea behind it is that there might be relevant information in every word in a sentence. So in order for the decoding to be precise, it needs to take into account every word of the input, using attention. 
For attention to be brought to RNNs in sequence transduction, we divide the encoding and decoding into 2 main steps. One step is represented in green and the other in purple. The green step is called the encoding stage and the purple step is the decoding stage. 
The step in green in charge of creating the hidden states from the input. Instead of passing only one hidden state to the decoders as we did before using attention, we pass all the hidden states generated by every “word” of the sentence to the decoding stage. Each hidden state is used in the decoding stage, to figure out where the network should pay attention to. 
For example, when translating the sentence “Je suis étudiant” to English, requires that the decoding step looks at different words when translating it. 
Or for example, when you translate the sentence “L’accord sur la zone économique européenne a été signé en août 1992.” from French to English, and how much attention it is paid to each input. 
But some of the problems that we discussed, still are not solved with RNNs using attention. For example, processing inputs (words) in parallel is not possible. For a large corpus of text, this increases the time spent translating the text. 
Convolutional Neural Networks help solve these problems. With them we can 
Some of the most popular neural networks for sequence transduction, Wavenet and Bytenet, are Convolutional Neural Networks. 
The reason why Convolutional Neural Networks can work in parallel, is that each word on the input can be processed at the same time and does not necessarily depend on the previous words to be translated. Not only that, but the “distance” between the output word and any input for a CNN is in the order of log(N) — that is the size of the height of the tree generated from the output to the input (you can see it on the GIF above. That is much better than the distance of the output of a RNN and an input, which is on the order of N. 
The problem is that Convolutional Neural Networks do not necessarily help with the problem of figuring out the problem of dependencies when translating sentences. That’s why Transformers were created, they are a combination of both CNNs with attention. 
To solve the problem of parallelization, Transformers try to solve the problem by using Convolutional Neural Networks together with attention models. Attention boosts the speed of how fast the model can translate from one sequence to another. 
Let’s take a look at how Transformer works. Transformer is a model that uses attention to boost the speed. More specifically, it uses self-attention. 
Internally, the Transformer has a similar kind of architecture as the previous models above. But the Transformer consists of six encoders and six decoders. 
Each encoder is very similar to each other. All encoders have the same architecture. Decoders share the same property, i.e. they are also very similar to each other. Each encoder consists of two layers: Self-attention and a feed Forward Neural Network. 
The encoder’s inputs first flow through a self-attention layer. It helps the encoder look at other words in the input sentence as it encodes a specific word. The decoder has both those layers, but between them is an attention layer that helps the decoder focus on relevant parts of the input sentence. 
Note: This section comes from Jay Allamar blog post 
Let’s start to look at the various vectors/tensors and how they flow between these components to turn the input of a trained model into an output. As is the case in NLP applications in general, we begin by turning each input word into a vector using an embedding algorithm. 
Each word is embedded into a vector of size 512. We’ll represent those vectors with these simple boxes. 
The embedding only happens in the bottom-most encoder. The abstraction that is common to all the encoders is that they receive a list of vectors each of the size 512. 
In the bottom encoder that would be the word embeddings, but in other encoders, it would be the output of the encoder that’s directly below. After embedding the words in our input sequence, each of them flows through each of the two layers of the encoder. 
Here we begin to see one key property of the Transformer, which is that the word in each position flows through its own path in the encoder. There are dependencies between these paths in the self-attention layer. The feed-forward layer does not have those dependencies, however, and thus the various paths can be executed in parallel while flowing through the feed-forward layer. 
Next, we’ll switch up the example to a shorter sentence and we’ll look at what happens in each sub-layer of the encoder. 
Let’s first look at how to calculate self-attention using vectors, then proceed to look at how it’s actually implemented — using matrices. 
The first step in calculating self-attention is to create three vectors from each of the encoder’s input vectors (in this case, the embedding of each word). So for each word, we create a Query vector, a Key vector, and a Value vector. These vectors are created by multiplying the embedding by three matrices that we trained during the training process. 
Notice that these new vectors are smaller in dimension than the embedding vector. Their dimensionality is 64, while the embedding and encoder input/output vectors have dimensionality of 512. They don’t HAVE to be smaller, this is an architecture choice to make the computation of multiheaded attention (mostly) constant. 
Multiplying x1 by the WQ weight matrix produces q1, the “query” vector associated with that word. We end up creating a “query”, a “key”, and a “value” projection of each word in the input sentence. 
What are the “query”, “key”, and “value” vectors? 
They’re abstractions that are useful for calculating and thinking about attention. Once you proceed with reading how attention is calculated below, you’ll know pretty much all you need to know about the role each of these vectors plays. 
The second step in calculating self-attention is to calculate a score. Say we’re calculating the self-attention for the first word in this example, “Thinking”. We need to score each word of the input sentence against this word. The score determines how much focus to place on other parts of the input sentence as we encode a word at a certain position. 
The score is calculated by taking the dot product of the query vector with the key vector of the respective word we’re scoring. So if we’re processing the self-attention for the word in position #1, the first score would be the dot product of q1 and k1. The second score would be the dot product of q1 and k2. 
The third and forth steps are to divide the scores by 8 (the square root of the dimension of the key vectors used in the paper — 64. This leads to having more stable gradients. There could be other possible values here, but this is the default), then pass the result through a softmax operation. Softmax normalizes the scores so they’re all positive and add up to 1. 
This softmax score determines how much how much each word will be expressed at this position. Clearly the word at this position will have the highest softmax score, but sometimes it’s useful to attend to another word that is relevant to the current word. 
The fifth step is to multiply each value vector by the softmax score (in preparation to sum them up). The intuition here is to keep intact the values of the word(s) we want to focus on, and drown-out irrelevant words (by multiplying them by tiny numbers like 0.001, for example). 
The sixth step is to sum up the weighted value vectors. This produces the output of the self-attention layer at this position (for the first word). 
That concludes the self-attention calculation. The resulting vector is one we can send along to the feed-forward neural network. In the actual implementation, however, this calculation is done in matrix form for faster processing. So let’s look at that now that we’ve seen the intuition of the calculation on the word level. 
Transformers basically work like that. There are a few other details that make them work better. For example, instead of only paying attention to each other in one dimension, Transformers use the concept of Multihead attention. 
The idea behind it is that whenever you are translating a word, you may pay different attention to each word based on the type of question that you are asking. The images below show what that means. For example, whenever you are translating “kicked” in the sentence “I kicked the ball”, you may ask “Who kicked”. Depending on the answer, the translation of the word to another language can change. Or ask other questions, like “Did what?”, etc… 
Another important step on the Transformer is to add positional encoding when encoding each word. Encoding the position of each word is relevant, since the position of each word is relevant to the translation. 
I gave an overview of how Transformers work and why this is the technique used for sequence transduction. If you want to understand in depth how the model works and all its nuances, I recommend the following posts, articles and videos that I used as a base for summarizing the technique 
Hands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look 
Written by 
Written by",Giuliano Giacaglia,2020-06-28T19:45:29.507Z
Data Science – BerkeleyISchool – Medium,,NA,NA
The Vandalisation of ‘UX’. The original promise of UX — How UX… | by Max Taylor | Spotless Says | Medium,"Etymologists have long been aware of the effect known as semantic drift — how the meanings of phrases can change over time. For example, mouse and bookmark now make sense as concepts in computers as well as their root meaning. 
Semantic drift has happened to the term ‘UX’ (user experience). ‘UX’ in its original textbook definition [1] no longer mirrors what is being blogged about today. Judging from job descriptions, UX practitioners are tasked with increasingly narrow scopes and are asked to ignore the wider matters of concern outlined in the original UX scripture. 
But, this point is somewhat inaccurate. ‘Semantic drift’ sounds like it is a passive actor; as though it were simply linguistic osmosis. In fact, the drift around ‘UX’ is very much active, with people’s agendas playing a key role. Indeed, ‘UX’ has been vandalised. 
To understand why — some quick UX history. 
This is the dark age. User Experience used to be determined by the developers, but was rarely considered at all. As long as it was functional, it was good enough…which was also terrible. 
The human-computer interaction (HCI) renaissance! Early practitioners were very smart and multi-disciplined. They had a variety of skills in cognitive psychology, design, ergonomics and computing. High levels of strategic thinking and a good level of taste in design. 
The original holy UX books are written and Don Norman coins the term ‘User Experience’. Most importantly, UX is defined as the product AND everything around it; including how a business aligns itself to deliver the best experience around the product [1]. For example, not only does UX encompass the interface, but also how it feels to press the keys, plug it in, discover it, look at it, order it online, go through customer service, unbox it — and so on. UX as a practice is meant to consider the full end to end experience; everyone and everything involved in — basically the whole enchilada! 
In summary, UX was defined as an evidence-based human-centered research and design loop, interlacing objectivity and creativity to deliver optimised experiences based on needs. It is a full service approach that considers every level that consequently effects experience. 
The reason for this cocktail of holism and reductionism is because if there are any inconsistencies in quality on any level of the experience journey, it’s going to cause a barrier. You don’t just wash the front of your car and call it a day, the rest of the car is going to look even worse now. UX practitioners need to look at the problem through these different lenses and shift gears where necessary. 
Is there any point in releasing a great product via a poorly designed service, or a great service around a poorly designed product? No, you need both. Your business is only as good as the worst designed element. To avoid this limiting factor, service design, interaction design, product design and interface design need equal attention, and they all come as part of the UX package. In other words: 
Case Study — Destiny (video game service) 
Destiny 1 & 2 are the most expensive video games ever made. To justify the cost, the game was intended to be a live service operation, with continuous updates and new challenges to its player base. The ROI is that the player base will continue to purchase add ons for the game (similar to buying more chapters for your book), which increases the life cycle of the game and creates a prolonged stream of income. 
The game has cutting edge interface, product and interaction design and is highly revered for its rewarding and smooth gameplay. Yet a glance at its Youtube community will reveal that most of the fanbase feel disenfranchised and no longer recommend buying it. So why is there so much fanbase dissent? 
They overlooked the user experience of the game’s service design. The game made the players choose between achieving their goals via gambling their real world money or through time consuming in-game grinds. The service puts monetisation schemes before the player’s desires. Naturally, a service design lens would flip this paradigm; finding a balance between monetisation and the player’s feelings of being manipulated. 
UX practitioners used to be generalists, needing to keep an equal eye on both design and research to understand how they wrap around each other as part of a flowing process. But this paradigm was deliberately shifted. 
What happened next is still going on now. New people from visual design backgrounds started gravitating towards UX to chase pound coins. This isn’t the sole reason — many visual designers sincerely did want to help with design strategy instead of just beautifying interfaces. But the key difference is that this intake was largely disinterested in psychology, ergonomics and computing. So the path of least resistance was to not learn those elements and just carve out a space from UX design called UI/UX. 
A UX career is not out of bounds for anyone. With enough drive, knowledge and spark in the right areas you can make it. But UI/UX seems like a half measure, and simply doesn’t hold the same market value. From what I see in their forums [2], they focus on creating aesthetic yet suboptimal interfaces in design tools. There seems to be some tip of the iceberg knowledge of what UX is — but that’s obviously not the same level of strategic thinking you get from the average UXer, who should have a deeper understanding of how science informs the design. 
After the goldrush, the definition wars really kicked in — everyone wants to plant their flag in the ground and coin the next big design term; to be the next Don Norman. They start blogging incessantly and throwing everything at the wall to see what sticks. All the community blogs follow a pattern of nonsensical reinventions of the wheel: ‘WHAT IS empathetic system design’, ‘WHAT IS person engagement design’, ‘WHAT IS doing and thinking design’ and of course ‘UX is dead’. 
This is the vandalisation of UX — the community actively shifted its definition as everyone attempted to draw out their own territory from it. I am not the first to notice how aesthetic focused “UX” causes design oversight- see, for example, ‘the dribbblisation of design’ [5] and ‘visual design is not a thing’ [6]. 
You might think as a human-centered evidence-based design community, we would put aside our personal branding ambitions and keep a consistent and accessible definition. In the name of retaining market value, you might also think we would set more rigid guidelines outlining prerequisite skills as border control. 
But no. Hiring managers remained confused and bought the most colourful portfolios with their eyes and the most strident and aggrandising design mythos with their ears. Others insisted on ramming untrained graphic designers into UX-type roles. The floodgates were opened, and the rapids scarred the landscape into an unrecognisable form. 
Now the subdisciplines are ostensibly becoming increasingly siloed. Subsequently, they are splitting into different directions. Reacting to the new landscape, there is an industry push for a design and research split, as though they are mutually exclusive skills that aren’t interlaced. Another split appears to be people shifting towards exclusively granular lenses like UI/UX, whilst others are going for a solely holistic approach like Service Design. 
The problem: the field of UX was intentionally responsible for both. 
While the term ‘UX’ has been conceptually narrowed thanks to semantic drift, people turn to Service Design to fulfil the original promise of UX. It’s the new hope. But it’s facing the same battle to define itself sufficiently. 
I have seen blogs from the Service Design community proclaiming that UX has a narrower scope than service design [3]. In other words — they suggest Service Design has drawn a bigger circle around UX (which they erroneously insist is interaction design) and therefore takes the most into account. Based on what they might see from the UI/UX community, it’s no wonder they would make that assertion. 
There is a widespread misunderstanding that UX is exclusively interaction or touchpoint design. A cursory glance at the term ‘user experience’ should reveal the contrary; it’s an inclusive term, concerning anything that consequently affects a user’s experience. 
I have also seen Service Design blogs concluding that there is a big UX shaped hole in the service design process, and the two disciplines must intertwine [4]. It’s like the inevitable collision of the Andromeda and Milky Way galaxies. Again I can’t say that Service Design is outside the scope of real UX, but nonetheless I second their motion. 
Service design is a holistic view, but it also an exclusive view, which comes at the cost of looking at the problem from a 30,000 ft view through frosted glass. Conversely, interface and product design become so focused and reductionist that they lose sight of the rest of the business ecosystem beyond their field of view. The beauty of the true definition of UX is its propensity to shift lenses, encourage lateral thinking and apply its transferable principles across all levels. For example, the poor performance of an interface touchpoint could have its roots in a misunderstanding based on an unmet service level need. But a holistic approach might miss the interface issue whilst a reductionist approach could miss the service level issue. 
‘When my brothers try to draw a circle to exclude me, I shall draw a larger circle to include them’ — Martin Luther King, Jr. 
Practitioners are most valuable when they can cross over research and design but also when they can adjust their lens to see the same problem as part of a wider eco-system and as part of an interface. I would encourage a reunification of subdisciplines and a return to the fundamental definition. Lets help non-practitioners understand our craft without breaking the community into mono-think silos. It’s the projects that determine the right lenses to apply, not the job title. 
The principles of human-centered design will always be the same. Unchain your minds from your job descriptions and proactively shift the application of your skill set to where it is needed most. Realise service, interaction, product and interface are all our responsibility under the flag of user experience. Unity is our strength. 
- Thanks for reading! leave a 👏👏👏 
- Our agency: Spotless — where we leave no UX stone unturned — service, business, interaction, product & interface design + research 
- Special thanks to Dr. Nick Fine, who is much wiser than myself and whose content was an inspiration. For more on this subject, check out this video below👇 
- My Linkedin 
- Art by Arnas Samuolis 
___________________________________________________________________ 
“But everything is too much for one practitioner to consider. In focusing ourselves to specialise on specific levels (UI / product / interaction / service / business) and splitting ourselves in to either researchers or designers we can realise our full potential. T-shaped!” 
Is that worth the cost of overlooking entire affectors of user experience for any product/service? 
Is that worth the cost of hiring 4 expensive practitioners (Interaction designer, service designer, user researcher, UX/UI designer) to range across the same levels as a generalist practitioner? 
Is it true that the underlying principles, skills and methods are sufficiently different between a service designer and an interaction designer? Is it true that someone with a design background can’t learn the relevant psychology (if they are truly interested) and vice versa? 
If anything, the T in T-shape isn’t a deep dive into a specific UX sub discipline, and its not to stay within the confines of either research or design. The skill you most want to T shape is the ability transfer the principles of good design across these factors. This is why the broad UX lens is perfectly positioned to engage the ambiguity that comes with design. After all, why put a cap on your abilities? We all have a ceiling but don’t put one there yourself. 
Its true, we can’t wear every UX lens at once. But consider the visual system where the eye is only looking at a fraction of the environment at any one time, yet the brain creates a workable image of the environment around it. Aim to build the clearest image of the product and everything around it that you can. 
Understandably, we all come into this industry strong in some areas and comparatively weaker in others. But it’s not in anyone’s interest to keep those areas weak. Do not skip leg day at the gym / do not skip service level issues when looking at interactions. 
Written by 
Written by",Max Taylor,2019-09-18T08:26:12.720Z
What is a Transformer?. An Introduction to Transformers and… | by Maxime | Inside Machine learning | Medium,"New deep learning models are introduced at an increasing rate and sometimes it’s hard to keep track of all the novelties. That said, one particular neural network model has proven to be especially effective for common natural language processing tasks. The model is called a Transformer and it makes use of several methods and mechanisms that I’ll introduce here. The papers I refer to in the post offer a more detailed and quantitative description. 
The paper ‘Attention Is All You Need’ describes transformers and what is called a sequence-to-sequence architecture. Sequence-to-Sequence (or Seq2Seq) is a neural net that transforms a given sequence of elements, such as the sequence of words in a sentence, into another sequence. (Well, this might not surprise you considering the name.) 
Seq2Seq models are particularly good at translation, where the sequence of words from one language is transformed into a sequence of different words in another language. A popular choice for this type of model is Long-Short-Term-Memory (LSTM)-based models. With sequence-dependent data, the LSTM modules can give meaning to the sequence while remembering (or forgetting) the parts it finds important (or unimportant). Sentences, for example, are sequence-dependent since the order of the words is crucial for understanding the sentence. LSTM are a natural choice for this type of data. 
Seq2Seq models consist of an Encoder and a Decoder. The Encoder takes the input sequence and maps it into a higher dimensional space (n-dimensional vector). That abstract vector is fed into the Decoder which turns it into an output sequence. The output sequence can be in another language, symbols, a copy of the input, etc. 
Imagine the Encoder and Decoder as human translators who can speak only two languages. Their first language is their mother tongue, which differs between both of them (e.g. German and French) and their second language an imaginary one they have in common. To translate German into French, the Encoder converts the German sentence into the other language it knows, namely the imaginary language. Since the Decoder is able to read that imaginary language, it can now translates from that language into French. Together, the model (consisting of Encoder and Decoder) can translate German into French! 
Suppose that, initially, neither the Encoder or the Decoder is very fluent in the imaginary language. To learn it, we train them (the model) on a lot of examples. 
A very basic choice for the Encoder and the Decoder of the Seq2Seq model is a single LSTM for each of them. 
You’re wondering when the Transformer will finally come into play, aren’t you? 
We need one more technical detail to make Transformers easier to understand: Attention. The attention-mechanism looks at an input sequence and decides at each step which other parts of the sequence are important. It sounds abstract, but let me clarify with an easy example: When reading this text, you always focus on the word you read but at the same time your mind still holds the important keywords of the text in memory in order to provide context. 
An attention-mechanism works similarly for a given sequence. For our example with the human Encoder and Decoder, imagine that instead of only writing down the translation of the sentence in the imaginary language, the Encoder also writes down keywords that are important to the semantics of the sentence, and gives them to the Decoder in addition to the regular translation. Those new keywords make the translation much easier for the Decoder because it knows what parts of the sentence are important and which key terms give the sentence context. 
In other words, for each input that the LSTM (Encoder) reads, the attention-mechanism takes into account several other inputs at the same time and decides which ones are important by attributing different weights to those inputs. The Decoder will then take as input the encoded sentence and the weights provided by the attention-mechanism. To learn more about attention, see this article. And for a more scientific approach than the one provided, read about different attention-based approaches for Sequence-to-Sequence models in this great paper called ‘Effective Approaches to Attention-based Neural Machine Translation’. 
The paper ‘Attention Is All You Need’ introduces a novel architecture called Transformer. As the title indicates, it uses the attention-mechanism we saw earlier. Like LSTM, Transformer is an architecture for transforming one sequence into another one with the help of two parts (Encoder and Decoder), but it differs from the previously described/existing sequence-to-sequence models because it does not imply any Recurrent Networks (GRU, LSTM, etc.). 
Recurrent Networks were, until now, one of the best ways to capture the timely dependencies in sequences. However, the team presenting the paper proved that an architecture with only attention-mechanisms without any RNN (Recurrent Neural Networks) can improve on the results in translation task and other tasks! One improvement on Natural Language Tasks is presented by a team introducing BERT: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. 
So, what exactly is a Transformer? 
An image is worth thousand words, so we will start with that! 
The Encoder is on the left and the Decoder is on the right. Both Encoder and Decoder are composed of modules that can be stacked on top of each other multiple times, which is described by Nx in the figure. We see that the modules consist mainly of Multi-Head Attention and Feed Forward layers. The inputs and outputs (target sentences) are first embedded into an n-dimensional space since we cannot use strings directly. 
One slight but important part of the model is the positional encoding of the different words. Since we have no recurrent networks that can remember how sequences are fed into a model, we need to somehow give every word/part in our sequence a relative position since a sequence depends on the order of its elements. These positions are added to the embedded representation (n-dimensional vector) of each word. 
Let’s have a closer look at these Multi-Head Attention bricks in the model: 
Let’s start with the left description of the attention-mechanism. It’s not very complicated and can be described by the following equation: 
Q is a matrix that contains the query (vector representation of one word in the sequence), K are all the keys (vector representations of all the words in the sequence) and V are the values, which are again the vector representations of all the words in the sequence. For the encoder and decoder, multi-head attention modules, V consists of the same word sequence than Q. However, for the attention module that is taking into account the encoder and the decoder sequences, V is different from the sequence represented by Q. 
To simplify this a little bit, we could say that the values in V are multiplied and summed with some attention-weights a, where our weights are defined by: 
This means that the weights a are defined by how each word of the sequence (represented by Q) is influenced by all the other words in the sequence (represented by K). Additionally, the SoftMax function is applied to the weights a to have a distribution between 0 and 1. Those weights are then applied to all the words in the sequence that are introduced in V (same vectors than Q for encoder and decoder but different for the module that has encoder and decoder inputs). 
The righthand picture describes how this attention-mechanism can be parallelized into multiple mechanisms that can be used side by side. The attention mechanism is repeated multiple times with linear projections of Q, K and V. This allows the system to learn from different representations of Q, K and V, which is beneficial to the model. These linear representations are done by multiplying Q, K and V by weight matrices W that are learned during the training. 
Those matrices Q, K and V are different for each position of the attention modules in the structure depending on whether they are in the encoder, decoder or in-between encoder and decoder. The reason is that we want to attend on either the whole encoder input sequence or a part of the decoder input sequence. The multi-head attention module that connects the encoder and decoder will make sure that the encoder input-sequence is taken into account together with the decoder input-sequence up to a given position. 
After the multi-attention heads in both the encoder and decoder, we have a pointwise feed-forward layer. This little feed-forward network has identical parameters for each position, which can be described as a separate, identical linear transformation of each element from the given sequence. 
How to train such a ‘beast’? Training and inferring on Seq2Seq models is a bit different from the usual classification problem. The same is true for Transformers. 
We know that to train a model for translation tasks we need two sentences in different languages that are translations of each other. Once we have a lot of sentence pairs, we can start training our model. Let’s say we want to translate French to German. Our encoded input will be a French sentence and the input for the decoder will be a German sentence. However, the decoder input will be shifted to the right by one position. ..Wait, why? 
One reason is that we do not want our model to learn how to copy our decoder input during training, but we want to learn that given the encoder sequence and a particular decoder sequence, which has been already seen by the model, we predict the next word/character. 
If we don’t shift the decoder sequence, the model learns to simply ‘copy’ the decoder input, since the target word/character for position i would be the word/character i in the decoder input. Thus, by shifting the decoder input by one position, our model needs to predict the target word/character for position i having only seen the word/characters 1, …, i-1 in the decoder sequence. This prevents our model from learning the copy/paste task. We fill the first position of the decoder input with a start-of-sentence token, since that place would otherwise be empty because of the right-shift. Similarly, we append an end-of-sentence token to the decoder input sequence to mark the end of that sequence and it is also appended to the target output sentence. In a moment, we’ll see how that is useful for inferring the results. 
This is true for Seq2Seq models and for the Transformer. In addition to the right-shifting, the Transformer applies a mask to the input in the first multi-head attention module to avoid seeing potential ‘future’ sequence elements. This is specific to the Transformer architecture because we do not have RNNs where we can input our sequence sequentially. Here, we input everything together and if there were no mask, the multi-head attention would consider the whole decoder input sequence at each position. 
The process of feeding the correct shifted input into the decoder is also called Teacher-Forcing, as described in this blog. 
The target sequence we want for our loss calculations is simply the decoder input (German sentence) without shifting it and with an end-of-sequence token at the end. 
Inferring with those models is different from the training, which makes sense because in the end we want to translate a French sentence without having the German sentence. The trick here is to re-feed our model for each position of the output sequence until we come across an end-of-sentence token. 
A more step by step method would be: 
We see that we need multiple runs through our model to translate our sentence. 
I hope that these descriptions have made the Transformer architecture a little bit clearer for everybody starting with Seq2Seq and encoder-decoder structures. 
We have seen the Transformer architecture and we know from literature and the ‘Attention is All you Need’ authors that the model does extremely well in language tasks. Let’s now test the Transformer in a use case. 
Instead of a translation task, let’s implement a time-series forecast for the hourly flow of electrical power in Texas, provided by the Electric Reliability Council of Texas (ERCOT). You can find the hourly data here. 
A great detailed explanation of the Transformer and its implementation is provided by harvardnlp. If you want to dig deeper into the architecture, I recommend going through that implementation. 
Since we can use LSTM-based sequence-to-sequence models to make multi-step forecast predictions, let’s have a look at the Transformer and its power to make those predictions. However, we first need to make a few changes to the architecture since we are not working with sequences of words but with values. Additionally, we are doing an auto-regression and not a classification of words/characters. 
The available data gives us hourly load for the entire ERCOT control area. I used the data from the years 2003 to 2015 as a training set and the year 2016 as test set. Having only the load value and the timestamp of the load, I expanded the timestamp to other features. From the timestamp, I extracted the weekday to which it corresponds and one-hot encoded it. Additionally, I used the year (2003, 2004, …, 2015) and the corresponding hour (1, 2, 3, …, 24) as the value itself. This gives me 11 features in total for each hour of the day. For convergence purposes, I also normalized the ERCOT load by dividing it by 1000. 
To predict a given sequence, we need a sequence from the past. The size of those windows can vary from use-case to use-case but here in our example I used the hourly data from the previous 24 hours to predict the next 12 hours. It helps that we can adjust the size of those windows depending on our needs. For example, we can change that to daily data instead of hourly data. 
As a first step, we need to remove the embeddings, since we already have numerical values in our input. An embedding usually maps a given integer into an n-dimensional space. Here instead of using the embedding, I simply used a linear transformation to transform the 11-dimensional data into an n-dimensional space. This is similar to the embedding with words. 
We also need to remove the SoftMax layer from the output of the Transformer because our output nodes are not probabilities but real values. 
After those minor changes, the training can begin! 
As mentioned, I used teacher forcing for the training. This means that the encoder gets a window of 24 data points as input and the decoder input is a window of 12 data points where the first one is a ‘start-of-sequence’ value and the following data points are simply the target sequence. Having introduced a ‘start-of-sequence’ value at the beginning, I shifted the decoder input by one position with regard to the target sequence. 
I used an 11-dimensional vector with only -1’s as the ‘start-of-sequence’ values. Of course, this can be changed and perhaps it would be beneficial to use other values depending of the use case but for this example, it works since we never have negative values in either dimension of the input/output sequences. 
The loss function for this example is simply the mean squared error. 
The two plots below show the results. I took the mean value of the hourly values per day and compared it to the correct values. The first plot shows the 12-hour predictions given the 24 previous hours. For the second plot, we predicted one hour given the 24 previous hours. We see that the model is able to catch some of the fluctuations very well. The root mean squared error for the training set is 859 and for the validation set it is 4,106 for the 12-hour predictions and 2,583 for the 1-hour predictions. This corresponds to a mean absolute percentage error of the model prediction of 8.4% for the first plot and 5.1% for the second one. 
The results show that it would be possible to use the Transformer architecture for time-series forecasting. However, during the evaluation, it shows that the more steps we want to forecast the higher the error will become. The first graph (Figure 3) above has been achieved by using the 24 hours to predict the next 12 hours. If we predict only one hour, the results are much better as we see on the second the graph (Figure 4). 
There’s plenty of room to play around with the parameters of the Transformer, such as the number of decoder and encoder layers, etc. This was not intended to be a perfect model and with better tuning and training, the results would probably improve. 
It can be a big help to accelerate the training using GPUs. I used the Watson Studio Local Platform to train my model with GPUs and I let it run there rather than on my local machine. You can also accelerate the training using Watson’s Machine Learning GPUs which are free up to a certain amount of training time! Check out my previous blog to see how that can be integrated easily into your code. 
Thank you very much for reading this and I hope I was able to clarify a few notions to the people who are just starting to get into Deep Learning! 
Written by 
Written by",Maxime,2020-03-05T20:42:48.604Z
What is Hidden in the Hidden Markov Model? | by Vimarsh Karbhari | Acing AI | Medium,"Hidden Markov Models or HMMs are the most common models used for dealing with temporal Data. They also frequently come up in different ways in a Data Science Interview usually without the word HMM written over it. In such a scenario it is necessary to discern the problem as an HMM problem by knowing characteristics of HMMs. 
In the Hidden Markov Model we are constructing an inference model based on the assumptions of a Markov process. 
The Markov process assumption is that the “future is independent of the past given that we know the present”. 
It means that the future state is related to the immediately previous state and not the states before that. These are the first order HMMs. 
What is Hidden? 
With HMMs, we don’t know which state matches which physical events instead each state matches a given output. We observe the output over time to determine the sequence of states. 
Example: If you are staying indoors you will be dressed up a certain way. Lets say you want to step outside. Depending on the weather, your clothing will change. Over time, you will observe the weather and make better judgements on what to wear if you get familiar with the area/climate. In an HMM, we observe the outputs over time to determine the sequence based on how likely they were to produce that output. 
Let us consider the situation where you have no view of the outside world when you are in a building. The only way for you to know if it is raining outside it so see someone carrying an umbrella when they come in. Here, the evidence variable is the Umbrella, while the hidden variable is Rain. See the probabilities in the diagram above. 
Since this is a Markov model, R(t) depends only on R(t-1) 
A number of related tasks ask about the probability of one or more of the latent variables, given the model’s parameters and a sequence of observations which is sequence of umbrella observations in our scenario. Some tasks that related to this example are also similar to those asked in a Data Science Interview(See Questions here): 
It is worth spending time learning HMMs in detail. Above you will see the Matrix based representations for HMM for the same umbrella problem we talked about. Scikit-learn provides the framework to use HMMs in Python. 
Conclusion: 
HMMs allow us to model processes with a hidden state, based on observable parameters. The main problems solved with HMMs include determining how likely it is that a set of observations came from a particular model, and determining the most likely sequence of hidden states. They are a valuable tool in temporal pattern recognition. Within the temporal pattern recognition area, the HMMs find application in speech, handwriting and gesture recognition, musical score following and SONAR detection. 
The next discussion around this topic is the Chinese Room Argument which I have talked about here. 
Source(Please refer these to know HMMs in mode detail): 
Hidden Markov Models (Sean R Eddy) 
Hidden Markov Models — JHU Computer Science Paper 
Subscribe to our newsletter here. We are building a new course to help people ace data science interviews. Sign up below to join the wait-list! 
Written by 
Written by",Vimarsh Karbhari,2020-02-26T05:38:04.243Z
Unsupervised Learning – Data Driven Investor – Medium,"Types of Machine Learning Systems: 
I was quite the geek growing up.",NA,NA
Machine Learning using AWS ML. Machine learning is an application of… | by Janitha Tennakoon | Data Driven Investor | Medium,"Machine learning is an application of artificial intelligence (AI) that provides systems the ability to automatically learn and improve from experience without being explicitly programmed. Amazon Machine Learning (Amazon ML) is a robust, cloud-based service that makes it easy for developers of all skill levels to use machine learning technology. In this post let’s look at how we can use Amazon Machine Learning technologies to create machine learning models. Assuming that you already know machine learning concepts let us dive straight into Amazon Machine Learning. 
Before going into creating a machine learning model in AWS ML let us first understand what are the key concepts that are used in AWS machine learning. 
A data source is an object which contains metadata associated with data inputs to Amazon ML. Amazon ML reads the input data and computes descriptive statistics, gather schema and other information and stores them as a part of the data source object. Currently, AWS ML only support data source creation inputs as Amazon S3 buckets and Amazon RedShift only. One thing to remember when creating a data source is that a data source does not store a copy of your input data. It only stores a reference for the input data. So in an instance where our input data resides on an S3 bucket if we move or change the S3 file Amazon ML will not be able to create an ML model using that input data. 
An ML model is a mathematical model that generates predictions by finding different patterns in your data. Presently, Amazon ML supports three types of ML models. 
Both Binary classification and Multi-class classification comes under supervised learning and Regression comes under unsupervised learning. Supervised learning is where we provide training data to the model and unsupervised learning is where there is no need of training data to be provided. 
Binary classification (logistic loss function + SGD) Predict values that can only have two categories such as true or false. (ex:- whether a person has diabetes, whether a person might get a housing loan etc..) 
Multi-class classification (multinomial logistic loss + SGD)Predict values that belong to limited, predefined categories. (ex:- what type of transportation a person might use (bus/train/car) etc ..) 
Regression (squared loss function + SGD)Predict a numeric value (ex:- number of patients per day, income of a person etc..) 
Evaluations measure the quality of ML models and determine whether the model is performing well. Measurements like AUC, F1-score, Accuracy, Precision, Recall are used in order to determine the quality. 
Asynchronously generate predictions for multiple input data observations. This is useful when there is a huge number of records which need to be predicted. Rather than running them one at a time using batch predictions we can predict them all by only running once. 
Synchronously generate predictions for individual data observations. This is useful in scenarios like interactive web applications where low latency is required when predicting. 
Above are the main key concepts that are used in Amazon ML. Now it is time to get our hands dirty while creating an ML model in AWS ML. For this blog post, let’s create a machine learning model where we will ask the question of whether a specific person will get a loan or not. 
Let’s first head into the Amazon machine Learning service using the console. In services, it should be under Machine Learning category. Inside you will be greeted with the following screen if you are launching this at the first time. 
Here you are provided with resources for getting started. For now, let’s click on Get started and start creating our machine learning model. In the next page click Launch under Standard setup to start creating a data source. 
Before starting to create dat source in AWS ML there are couple of things we need to do beforehand. First, we must save our data in the comma-separated values(.csv) as data sources are only supported by this file format. Then we need to analyze the data and start feature processing on data that we have gathered. Feature processing is the process of transforming the attributes further to make them more meaningful. Examples for common feature processing are 
After transforming our data we need to upload the data to an Amazon S3 bucket in order to hand it over to Amazon machine learning. For this post, let us use the banking datasource that is provided by Amazon themselves. You can download this datasource by this link. Our question for predicting is whether a given person will take a loan or not. So the first thing we need to do is convert yes/no in column loan to 1/0 and rename the column to target because that is our target attribute (make sure to replace unknown values as well). Also, make sure to delete column y. 
After transforming the data we need to upload these data to Amazon S3 in order to create datasource. For that create a new bucket in S3 and upload the csv file. 
Now that we have input data available in S3 bucket we can continue on to creating the data source. After selecting launch from the previous screen in ML next page will land you to create datasource. Here specify the S3 location and provide a datasource name. After that click on Verify. When verifying Amazon ML will ask for permission to access S3 bucket. Choose Yes. 
After a couple of seconds, Amazon ML will verify the datasource and let you know. 
In the next page, we need to define a schema for the datasource. By default, Amazon ML will scan the input data and auto-generate a schema for us. But we can do modifications to the auto-generated schema. First, check yes for radio button for Does the CSV contain the column names. When defining data types for our attributes we must select from below pre-defined categories. 
Binary — choose for an attribute that has only two possible states, such as yes or noCategorical — choose for an attribute that takes on a limited number of unique values Numeric — choose for an attribute that takes quantity as valueText — choose for an attribute that is a string of word. 
In the next page, we need to select our target attribute. In the checkbox asking whether we are creating a ML model select Yes. After that select the target attribute from the schema. (you need to only specify target attribute if you will use the datasource for training and evaluating the ML model) 
On the Row ID page, for Does your data contain an identifier? , make sure that No, the default, is selected. Next, in the review page review the datasource and select Create datasource. 
To train an ML model we need to specify the following parameters. 
Since we have created this ML model using the wizard the datasource is automatically selected with the datasource that we have just created. The name of the target attribute is also automatically selected in this step. For the simplicity of this blog, we are going to use Default(Recommended) settings that are generated by Amazon ML. If we want to specify further data transformations and add training parameters we can select Custom. When we select Review we can review the settings we have provided and settings that are automatically generated by Amazon ML. 
Here under evaluation data, you can see that our data is split 70% as training data and 30% evaluation data. If we select custom we can customize these parameters. In custom mode following training parameters can be configured. 
Maximum model size — the maximum model size in bytes. The default value is 100 MB. You are priced against the model size. 
Maximum number of passes over the data — to discover patterns Amazon ML use multi passes over your data. The default value is 100. The number of patterns may increase with the number of passes and may increase the quality of the model. 
Shuffle type for training data — shuffle type when you splitting the data. By default, Amazon does not shuffle the training data. 
Regularization type and amount — the performance of complex ML models suffers when data contains too many patterns. Regularization helps models from overfitting training data. 
In our post, we are going to use the default values provided by Amazon ML. After reviewing select Create Ml model. In this step, Amazon ML adds your model to the processing queue. When Amazon ML creates your model, it applies the defaults and performs the following actions: 
While your model is in the queue, Amazon ML reports the status as Pending. While Amazon ML creates your model, it reports the status as In Progress. When it has completed all actions, it reports the status as Completed. Wait for the evaluation to complete before proceeding. 
That is it. We have created our first ML model using Amazon ML. But the steps do not stop there. The next step is to evaluate the ML model that we have created. 
In Amazon ML evaluation process it generates industry standard quality metrics. One such metric is AUC (Area Under the Curve). This expresses the performance quality of your ML model. AUC measures the ability of the model to predict a higher score for positive examples as compared to negative examples To review the AUC of the model that we have created, on the ML model summary page in ML model report pane choose Evaluations and then the model. 
We can adjust the score threshold in order to change the accuracy of the model. The ML model generates numeric prediction scores for each record in a prediction datasource, and then applies a threshold to convert these scores into binary labels of 0 (for no) or 1 (for yes). By changing the score threshold. To set the threshold select Adjust score threshold. 
The default threshold provided by Amazon ML is 0.5 . You can fine-tune this value in order to meet your requirements. Adjusting this value changes the level of confidence that the model must have in a prediction before it considers the prediction to be positive. It also changes how many false negatives and false positives you are willing to tolerate in your predictions. If you need only highest likelihood value to be set as positive you can set a higher threshold value like in a scenario where testing positive for a disease a wrong positive result may be critical. 
Amazon ML can generate two types of predictions. 
Let us first look at real-time predictions. Real-time predictions are used when there is low latency required when predicting from the ML model. They are ideal for interactive websites and mobiles. For applications that require real-time predictions, we must create a real-time endpoint for the ML model. For this, we will accrue charges while the end-point is available. But we can try using real-time prediction feature in Tools without creating a real-time endpoint. In ML model reports select Try real-time predictions. 
Here you can paste a record and choose to create a prediction. Then Amazon ML populates the predicted label in the real-time. 
To generate batch predictions we need to select Batch predictions from the Amazon Machine Learning. Choose new batch prediction and on the next page select the ML model. 
To generate a batch prediction we need to have uploaded the batch data to S3 bucket. After uploading batch predictions we can point them in the Locate the input data. After that configure the batch data where you will be again asked for Amazon ML permission to access S3 object. For S3 destination choose an S3 bucket. This will be the location where the results will be uploaded after completing. After completing batch predictions will run and will be uploaded to the specified S3 location. 
Above explained is the basic outline of creating a machine learning model using AWS Machine Learning. More complex machine learning models can be created using following AWS documentation. 
In each issue we share the best stories from the Data-Driven Investor's expert community. Take a look 
Written by 
Written by",Janitha Tennakoon,2019-01-14T15:03:06.509Z
"How Machine Learning is used in Cyber Attacks | by Informer | Sep, 2020 | Medium","Machine learning is not only utilized by security professionals, but by adversaries with malicious intent. How are they using this to improve their cyber attacks? 
In the previous post, we explored the many ways security analysts and organizations are utilizing machine learning in order to prevent cyber crime. The ability of machine learning algorithms to classify previously unseen data and predict future data means they have a wide range of uses in cyber defense. However, the same traits of machine learning can equally be used in malicious contexts. 
In this second post, we will explore the different ways attackers are leveraging machine learning to improve social engineering and hide their malware from antivirus. 
To recap, machine learning is the process of feeding a model sets of data with useful features in order for it to ‘learn’ the underlying concepts and make predictions about future data. It can predict what future data may look like, or classify previously unseen data points. In the context of cyber crimes, it may be used to generate sophisticated and targeted phishing emails. 
Where machine learning was used in cyber security to identify similar malware and malicious links, instead with cyber crime it is used to evade filters, bypass CAPTCHA checks, and generate targeted phishing emails. When comparing the two, cyber security appears to have much more consolidated uses for machine learning. But future trends towards evasive malware and phishing may pose a serious threat to the cyber security industry. 
CAPTCHAs are present in web applications throughout the internet, with the purpose of preventing automated scripts from brute-forcing or mass-signing up many bot accounts for malicious purposes. CAPTCHAs involve completing a simple challenge that a robot may find difficult. However, as machine learning becomes more advanced, models can become extremely efficient at solving these and bypassing the protection. This could lead to brute force attacks which may compromise an account. 
This quickly becomes a game of cat and mouse; the CAPTCHAs become more obscure and difficult to answer, but the machine learning algorithms are improved in order to identify the photos. A paper published in 2018 was able to achieve 100% accuracy on certain CAPTCHA implementations and over 92% in Amazon’s implementation. 
Machine learning algorithms can also be used to generate similar data to a given dataset. This is especially useful in generating passwords out of terms that relate to a user. This method has been showcased in the popular TV show Mr Robot, and many online tools take inspiration from this. The combination of Open Source Intelligence (OSINT) and machine learning can generate password lists much more concise and likely to result in a successful brute force attempt. 13% of passwords generated in one method were actually used in real-world scenarios, which in terms of brute-forcing is highly impressive. 
Machine learning used in the detection of malware could be considered one of the first true applications of artificial intelligence in the cyber security industry. However, recent papers have been published showing how malware can be crafted to evade these detections through the use of machine learning. One paper used a machine learning algorithm to change a particular malware and was able to successfully evade detection in over 45% of cases. The repercussions of this type of malware evolving would be serious and felt around the world. Luckily these techniques have not been viewed successfully in the wild (apart from malware implementing AI phishing scams) and current antivirus software is holding its own. 
Phishing attacks have been around for over 20 years and have continued to be successful even though public knowledge of them has increased. To increase the likelihood of a successful scam, manual information gathering can be done on a target to craft the perfect tweet or email. This can achieve up to 45% clickthrough rates on a malicious link. However, this process is much slower than automated methods. 
A Blackhat published paper has used machine learning to automate this, scraping a Twitter user’s profile and generating targeted tweets from bot accounts. This method was found to be 4 times faster than manual phishing, and maintain a high click-through rate of between 33% and 66%, making it as believable as manual phishing in some scenarios, if not more so. This is a real-world example of how automation and AI can be used to target and scam specific users. 
With social media being a battleground for fake news and the spreading of misinformation, it is inevitable that AI bots are being used for spreading such information in the most believable way. Equally, however, machine learning may hold the key in fact-checking and preventing the spread. 
Cyber attacks using machine learning are only recently being explored and developed. Evasive malware which poses the most serious risk has yet to be seen in a real-world attack. In absolute terms, machine learning in cyber security is more developed and widely implemented than its cyber attack counterpart. However, the next few years may see a shift in malicious activity, with artificial intelligence at the heart of it.‍ 
‍ 
Tyler Sullivan, Security Consultant 
Connect: https://www.linkedin.com/in/tyler-sullivan-9a5882158/ 
Tyler is a Security Consultant at Informer. During his degree in computer science, Tyler’s main focuses were on cyber security and machine learning. This manifested itself in his dissertation topic, looking at how machine learning algorithms can identify infected bots on a network. 
Originally published at https://www.informer.io. 
Written by 
Written by",Informer,2020-09-16T08:18:29.886Z
Intuitive Guide to Latent Dirichlet Allocation | by Thushan Ganegedara | Towards Data Science,"Topic modelling refers to the task of identifying topics that best describes a set of documents. These topics will only emerge during the topic modelling process (therefore called latent). And one popular topic modelling technique is known as Latent Dirichlet Allocation (LDA). Though the name is a mouthful, the concept behind this is very simple. 
To tell briefly, LDA imagines a fixed set of topics. Each topic represents a set of words. And the goal of LDA is to map all the documents to the topics in a way, such that the words in each document are mostly captured by those imaginary topics. We will systematically go through this method by the end which you will be comfortable enough to use this method on your own. 
This is the fourth blog post in the series of light on math machine learning A-Z. You can find the previous blog posts linked to the letter below. 
A B C D* E F G H I J K L M N O P Q R S T U V W X Y Z 
*denotes articles behind Medium Paywall. 
What are some of the real world uses topic modelling has? Historians can use LDA to identify important events in history by analysing text based on year. Web based libraries can use LDA to recommend books based on your past readings. News providers can use topic modelling to understand articles quickly or cluster similar articles. Another interesting application is unsupervised clustering of images, where each image is treated similar to a document. 
The short answer is a big NO! I’ve swept through many different articles out there. And there are many great articles/videos giving the intuition. However most of them stop at answering questions like: 
Which I do talk about but don’t believe we should stop there. The way these models are trained is a key component I find missing in many of the articles I read. So I try to answer few more questions like: 
Once you understand the big idea, I think it helps you to understand why the mechanics in LDA are the way they are. So here goes; 
Each document can be described by a distribution of topics and each topic can be described by a distribution of words 
But why do we use this idea? Let’s imagine it through an example. 
Say you have a set of 1000 words (i.e. most common 1000 words found in all the documents) and you have 1000 documents. Assume that each document on average has 500 of these words appearing in each. How can you understand what category each document belongs to? One way is to connect each document to each word by a thread based on their appearance in the document. Something like below. 
And then when you see that some documents are connected to same set of words. You know they discuss the same topic. Then you can read one of those documents and know what all these documents talk about. But to do this you don’t have enough thread. You’re going to need around 500*1000=500,000 threads for that. But we are living in 2100 and we have exhausted all the resources for manufacturing threads, so they are so expensive and you can only afford 10,000 threads. How can you solve this problem? 
We can solve this problem, by introducing a latent (i.e. hidden) layer. Say we know 10 topics/themes that occur throughout the documents. But these topics are not observed, we only observe words and documents, thus topics are latent. And we want to utilise this information to cut down on the number of threads. Then what you can do is, connect the words to the topics depending on how well that word fall in that topic and then connect the topics to the documents based on what topics each document touch upon. 
Now say you got each document having around 5 topics and each topic relating to 500 words. That is we need 1000*5 threads to connect documents to topics and 10*500 threads to connect topics to words, adding up to 10000. 
Note: The topics I use here (“Animals”, “Sports”, “Tech”) are imaginary. In the real solution, you won’t have such topics but something like (0.3*Cats,0.4*Dogs,0.2*Loyal, 0.1*Evil) representing the topic “Animals”. That is, as mentioned before, each document is a distribution of words. 
To give more context to what’s going on, LDA assumes the following generative process is behind any document you see. For simplicity let us assume we are generating a single documents with 5 words. But the same process is generalisable to M documents with N words in each. The caption pretty well explain what’s going on here. So I won’t reiterate. 
This image is a depiction of what an already-learnt LDA system looks like. But to arrive at this stage you have to answer several questions, such as: 
This will be answered in the next few sections. Additionally, we’re going to get a bit technical this point onwards. So buckle up! 
Note: LDA does not care the order of the words in the document. Usually, LDA use the bag-of-words feature representation to represent a document. It makes sense, because, if I take a document, jumble the words and give it to you, you still can guess what sort of topics are discussed in the document. 
Before diving into the details. Let’s get a few things across like notations and definitions. 
First let’s put the ground based example about generating documents above, to a proper mathematical drawing. 
Let’s decipher what this is saying. We have a single α value (i.e. organiser of ground θ) which defines θ; the topic distribution for documents is going to be like. We have M documents and got some θ distribution for each such document. Now to understand things more clearly, squint your eyes and make that M plate disappear (assuming there’s only a single document), woosh! 
Now that single document has N words and each word is generated by a topic. You generate N topics to be filled in with words. These N words are still placeholders. 
Now the top plate kicks in. Based on η, β has some distribution (i.e. a Dirichlet distribution to be precise — discussed soon) and according to that distribution, β generates k individual words for each topic. Now you fill in a word to each placeholder (in the set of N placeholders), conditioned on the topic it represents. 
Viola, you got a document with N words now! 
α and η are shown as constants in the image above. But it is actually more complex than that. For example α has a topic distribution for each document (θ ground for each document). Ideally, a (M x K) shape matrix. And η has a parameter vector for each topic. η will be of shape (k x V). In the above drawing, the constants actually represent matrices, and are formed by replicating the single value in the matrix to every single cell. 
θ is a random matrix, where θ(i,j) represents the probability of the i th document to containing words belonging to the j th topic. If you take a look at what ground θ looks like in the example above, you can see that balls a nicely laid out in the corners not much in the middle. The advantage of having such a property is that, the words we produce are likely to belong to a single topic as it is normally with real-world documents. This is a property that arise by modelling θ as a Dirichlet distribution. Similarly β(i,j) represents the probability of the i th topic containing the j th word. And β is also a Dirichlet distribution. Below, I’m providing a quick detour to understand the Dirichlet distribution. 
Dirichlet distribution is the multivariate generalisation of the Beta distribution. Here we discuss an example of a 3-dimensional problem, where we have 3 parameters in α that affects the shape of θ (i.e. distribution). For an N-dimensional Dirichlet distribution you have a N length vector as α. You can see how the shape of θ changes with different α values. For example, you can see how the top middle plot shows a similar shape to the θ ground. 
The main take-away is as follows: 
Large α values push the distribution to the middle of the triangle, where smaller α values push the distribution to the corners. 
We still haven’t answered the real problem is, how do we know the exact α and η values? Before that, let me list down the latent (hidden) variable we need to find. 
If I’m to mathematically state what I’m interested in finding it is as below: 
It looks scary but contains a simple message. This is basically saying, 
I have a set of M documents, each document having N words, where each word is generated by a single topic from a set of K topics. I’m looking for the joint posterior probability of: 
given, 
and using parameters, 
But we cannot calculate this nicely, as this entity is intractable. Sow how do we solve this? 
There are many ways to solve this. But for this article, I’m going to focus on variational inference. The probability we discussed above is a very messy intractable posterior (meaning we cannot calculate that on paper and have nice equations). So we’re going to approximate that with some known probability distribution that closely matches the true posterior. That’s the idea behind variational inference. 
The way to do this is to minimise the KL divergence between the approximation and true posterior as an optimisation problem. Again I’m not going to swim through the details as this is out of scope. 
But we’ll take a quick look at the optimization problem 
γ , ϕ and λ represent the free variational parameters we approximate θ,z and β with, respectively. Here D(q||p) represents the KL divergence between q and p. And by changing γ,ϕ and λ, we get different q distributions having different distances from the true posterior p. Our goal is to find the γ* , ϕ* and λ* that minimise the KL divergence between the approximation q and the true posterior p. 
With everything nicely defined, it’s just a matter of iteratively solving the above optimisation problem until the solution converges. Once you have γ* , ϕ* and λ* you have everything you need in the final LDA model. 
In this article we discussed about Latent Dirichlet Allocation (LDA). LDA is a powerful method that allows to identify topics within the documents and map documents to those topics. LDA has many uses to it such as recommending books to customers. 
We looked at how LDA works with an example of connecting threads. Then we saw a different perspective based on how LDA imagine a document is generated. Finally we went into the training of the model. In this we discussed a significant amount of mathematics behind LDA, while keeping the math light. We took a look at what a Dirichlet distribution looks like, what is the probability distribution we’re interested in finding (i.e. posterior) and how do we solve that using variational inference. 
I will post a tutorial on how to use LDA for topic modelling including some cool analysis as another tutorial. Cheers. 
Here are some useful references for understanding LDA is anything wasn’t clear. 
Prof. David Blei’s original paper 
An intuitive video explaining basic idea behind LDA 
Lecture by Prof. David Blei 
Hands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look 
Written by 
Written by",Thushan Ganegedara,2019-03-27T16:52:33.080Z
Cyber Security Best Practices for Work From Home / Teleworking | by IARM Information Security | IARM Information Security | Medium,"With the recent trends worldwide, the Teleworking otherwise termed as remote working or working from home is on the raise. 
Should we consider this option as a threat or an opportunity for an organisation. 
It is definitely an opportunity but be aware to assess the threat involved in extending this option to your employees. 
Everyone will talk about productivity, engagement, motivation, cost savings etc, but all these can prove just the opposite if the Cyber Security Vulnerability and threats are not evaluated prior to extending these facilities to the employees. 
So what do you think one should do before extending the teleworking or remote working options for employees? 
If you have any queries or help please feel free to contact us 
IARM Information Security Pvt. Ltd. | info@iarminfo.com | www.iarminfo.com 
Written by 
Written by",IARM Information Security,2020-03-18T12:36:54.967Z
The Zero Trust Model: Should we be taking information security advice from Congressmen? | by Drawbridge Networks | Medium,"In September of 2016 a Congressional Subcommittee released a report detailing their investigation into the OPM breach, arguably the worst computer security incident in American history. Among the Committee’s recommendations is that Federal information security efforts should be reprioritized toward a Zero Trust Model. Read this: 
The OPM data breaches… illustrate the challenge of securing large, and therefore high-value, data repositories when defenses are geared toward perimeter defenses. In both cases the attackers compromised user credentials to gain initial network access, utilized tactics to elevate their privileges, and once inside the perimeter, were able to move throughout OPM’s network, and ultimately accessed the “crown jewel” data held by OPM. The agency was unable to visualize and log network traffic which led to gaps in knowledge regarding how much data was actually exfiltrated by attackers… 
Agencies should move toward a “zero trust” model of information security and IT architecture. The zero trust model centers on the concept that users inside a network are no more trustworthy than users outside a network… 
In order to effectively implement a zero trust model, organizations must implement measures to visualize and log all network traffic, and implement and enforce strong access controls for federal employees and contractors who access government networks and applications. 
Frankly, we’re somewhat surprised to see such a cogent statement about information security coming from a Congressional report. When Washington policy makers aren’t busy threatening to prevent the global coordination of information about security threats, they’re proposing overly broad backdoor requirements for all kinds of encryption software that law enforcement will never need to access. 
Despite all those missteps, we happen to strongly agree with this particular piece of advice, and not just for Federal agencies, but for any organization that faces the threat of sophisticated attacks. Perhaps it helps that one of the authors the report, Representative Will Hurd, has a background in Computer Science. But more importantly, the Zero Trust Model is an idea that really came out of Forrester Research, who have been promoting it for several years. Forrester outlined their ideas in a public comment to NIST in 2013, which was referenced by the Congressional Report about OPM. 
Forrester provides a perfect metaphor for the typical enterprise network — it’s “like an M&M, with a hard crunchy outside and a soft, chewy center.” They point out that once an attacker gets past the perimeter defenses in most computer networks, they have access to all of the resources in the network. If organizations are going to be resilient against sophisticated attackers, they have to anticipate that compromises are going to happen, and take steps to architect their networks to limit the impact of those compromises. 
However, operating an internal network with a true zero trust footing is easier said than done. We build firewalls for a reason — to limit the complexity of the attack surface that our organizations present to the Internet, so that we can focus our efforts on protecting the remaining surface. Treating every enterprise application as if it is directly connected to the Internet might be a great goal in theory, but in practice security teams are stretched too thin already. The challenge is to find a reasonable middle ground. Where that middle ground is drawn depends greatly on the kinds of tools that we have at our disposal and the reach they give our teams. 
I won’t repeat all the points made in Forrester’s comment — it’s only 18 pages — but two themes in particular stand out for us — the need to log and inspect all internal network traffic, and the need for internal network segmentation based on a least privilege strategy. Better internal segmentation helps reduce the exposure that key applications have to the internal network — doubling down on the value of firewalling rather than pretending it doesn’t exist. The logs make incidents possible to investigate — if you can’t see lateral movement within your network there is little that you can do in midst of an incident to stop it. 
At Drawbridge Networks, our mission is to build new technology that makes the hardening of internal networks easier, more cost effective, and more secure. There are three key capabilities that we bring to the table that we think make it significantly easier for organizations to achieve better internal segmentation and logging. 
Microsegmentation for the Whole Enterprise: Most Microsegmentation technology is designed specifically around protecting cloud workloads. Although protecting the cloud is important, workstations are important too. Many incidents begin by compromising an end user workstation either through spear phishing, infected USB drives, or another vector. Attackers are adept at pivoting from an initial workstation compromise to gaining credentials that provide broader access to the network. Therefore, if you want to protect a whole network, workstations need to be a part of the picture. 
Total Internal Network Visibility: While Forrester argues that many organizations log internal network traffic, we’re not sure we agree. Some internal traffic might be logged at key points, but few organizations today have total endpoint to endpoint visibility. In addition, our product provides process and user context for network traffic that you can only get from an endpoint agent. 
True Least Privilege Segmentation: Building effective network segments is hard work, and doing it with physical switches is expensive. The consequence is that even relatively well segmented networks are not truly locked down to a least privilege level. Our software enables segments to be configured on the fly, and we can build network segments based on user groups rather than IP addresses or physical switch configuration. Two employees with different job functions can be sitting on the same physical network segment and have network level access to different servers based on their job function. That capability affords truly granular, least privilege segments that enable employees to access the systems they need to do their jobs, and nothing more. 
These three capabilities ultimately have to do with amplifying your security team’s reach. If segmentation becomes easier and visibility becomes easier, you can afford to do more to lock down your internal network than what you are doing today. You might not reach an ideal state of operating as if the perimeter doesn’t exist, but you can find a middle ground that is closer to the ideal. In practice this may be the difference between an incident, and a data breach. That is the difference that we want to make. 
- Tom Cross, CTO 
Written by 
Written by",Drawbridge Networks,2016-11-17T00:42:42.654Z
Design APIs like you design User Experience | by Prashant Agrawal | Better Practices | Medium,"Update: This article is now available in Korean as well. Thanks to Jay Yeoul Ahn for the contribution. 
User Experience (UX) is the value that you provide to your users when they are using your product. It is a term that is mostly associated with beautiful looking User Interfaces. But, by principle, User Experience Design (UXD or UED) “is the process of enhancing user satisfaction with a product by improving the usability, accessibility, and pleasure provided in the interaction with the product.” Designers spend a decent amount of time to make sure that they make every interaction, every UI element as delightful as possible. 
Producing the best type of UX is mainly processes — the processes that Design/UX teams use to make sure that their design is valuable, desirable, and usable to the community. It is not just about providing a beautiful UI. They make sure that in the design process all stakeholders are involved so that there are no sudden surprises towards the end. They also do enough research, user testing and prototyping to make sure that all the ideas or designs they are generating, those resonate with the target users. 
Design is not just what it looks like and feels like. Design is how it works. 
— Steve Jobs. 
The way we see an API’s lifecycle at Postman, it contains stages like Designing, Debugging, Automated testing, Documenting the API, Monitoring and Publishing the API so the end users can start consuming it. 
“Designing and Mocking an API” is the first and most important step of this lifecycle. Any good engineer can tell that if you start with a shaky foundation or bad bedrock, you’re going to collapse, and the same is true for APIs. If you haven’t done a good job in designing the API, it will have cascading effects on other steps in the lifecycle. 
When APIs have already been built, changes are difficult, expensive and time intensive. Even worse, the changes to published APIs might break any client using the API. The design allows for planning all the details, iterating over several proposals and perform what-if analysis. Moreover, changes made to the API in the designing stage are easy and cheap to perform. 
API Design helps us to avoid building the wrong API, or API in the wrong way 
Still, you will find developers spending more time on developing and coding their APIs rather than designing them. APIs are considered to be consumed by machines and not a user-facing entity, as a result of which many organizations don’t spend enough attention and resources to set up a design process for APIs. But today, when the microservice architecture is taking the world by storm, APIs are starting to look more like a product rather than a technology. They should be seen as a way to provide the functionality to end user. 
In the API space, we build something on a machine for a machine to use and this is wrong because there are people on the other side of API clients. 
— Ronnie Mitra, Director of Technology at Publicis Sapient 
Many RESTful APIs that are designed today are nothing more than object browsers delivering internal representations of the domain/business objects over HTTP/S as a transport layer. So, how can you get better in designing APIs, how can you make sure all the APIs that you design follow consistent patterns, how can you make sure that new people joining the team do not reinvent the wheel of making design decisions about API that rest of the organization has learned over time. If you read those above questions again what you will find missing is a process of API design similar to the one UX designers employ and these are the kinds of problems that are being solved by them every day. 
A great UX should ideally follow the usability rules defined by Peter Morville known as UX Honeycomb. 
In order to design an API with a great UX, you should answer these questions while designing it: 
The strength of a design lies as much in the steps taken to create it as in the final result. A design process helps you to remain transparent and efficient while designing a solution. It sets a clear expectation and decreases the risk of delivering unwanted results. 
The Double Diamond design process by the British Design Council is one of such many design processes. It allows people to make intentional design decisions by exploring various options (divergent thinking) while validating stronger ones and weeding out the weaker ones (convergent thinking). 
This approach will help you in achieving two of the most important things in any design process: 
This phase is split into Discover/Research and Define/Synthesis. While designing something new it is very easy for people to blow the actual scope of the problem and put all their effort into designing something that no one actually wants or will use. This phase of designing keeps us grounded and makes sure we are designing for the right thing by doing diligent research and defining the right problem statement to solve. 
Discover/Research: When you are building a new Product, Feature or an API you are trying to solve a problem. In order to provide a solution, you need to first understand the problem. Always start by acquiring the Domain Knowledge about the problem. Try to understand what problem you are trying to solve, who is this being built for etc. Clarify on the use-cases and the requirements. Research on existing solutions to understand how they are solving the same problem and what you can learn from their architectural/technical decisions. Talk to all the teams inside/outside your organization that are stakeholders for this new API which might include Product Team, Design Team, Security Team, etc. Their different perspective will give you a broader view to think about your API and the technical decisions that you are planning to take from that point onwards. 
You can also leverage the Postman API Network for your research or reference to find out how different organization’s APIs looks like. Postman’s API Network contains a diverse list of APIs which are published by organizations from a lot of different domains and industries. For example, if you are building the API in FinTech space, for your research you can check some of the APIs published by other organizations in the Financial Services section. 
This step will result in a lot of unstructured information, overlapping and conflicting use-cases/requirements. So in the next step, structure this information so that you can make better design decisions around your API. 
Define/Synthesis: From the raw data, find trends and insights that span across different users and scenarios. Try to pin down exact use-cases and requirements you are planning to cater. This stage of the process is also the right time to flag off any dependency that your API might have on another service, which is quite common in a microservices architecture. Define your Resource Models and relationships between them, which will help you in identifying all the Resources for which you would need to expose APIs. You should also clearly outline the behavior and expectations of the API so that consumers don’t have to make any assumptions while using your APIs. Doing this will help you identify the amount of business logic that will be part of the service or the client. 
This phase is split into Develop/Ideation and Deliver/Implementation. This phase will ensure that whatever you decide to design, it’s designed the right way to ensure that you are not compromising with quality, consistency, security and the best practices outlined by the organization/industry. 
Develop/Ideation: Once you are clear on ‘what needs to achieved’ using APIs, it’s time to design those APIs so that you can communicate on what you as API provider intent to ship. API Description Languages such as OpenAPI/Swagger, RAML, etc are a great way to express important aspects of an API. Especially the frontend design decisions and architectural design decisions, which are visible to the consumer, can be captured by API description languages. But one drawback with these API description languages is that they can get way too technical, verbose and difficult to collaborate upon. So alternatively what you can do is import Swagger, OpenAPI or RAML specification files into Postman Collection. 
If you have worked closely with a UX/Design team before you would have noticed that some of the outcomes at the end of their designing process are documentation around all the features and interactions, representation of various states of UI (error states, empty states, etc) and a prototype which demonstrates the exact interaction with the UI and what should be the expected outcome. 
Using Postman collections you can also achieve the same outcome for your APIs and provide total transparency to all the stakeholders around it. 
Deliver/Implementation: After you have successfully designed your APIs now is the time to start implementation. But before you start coding your APIs you should establish a Contract Test for your API which will ensure that if anything changes during development in the agreed API request/response design, these Contact tests will start failing. Then you can either update your code or inform all the affected teams. Also, it is very easy in Postman to collaborate with your entire team. You can constantly receive feedback using comments while you are actually developing your APIs and any changes that you make will instantly be made available to everyone involved in the process. 
When you are working in a growing team with new people joining in every now and then, after a certain point of time it becomes very difficult to ensure the quality, consistency, and security of the APIs. Imagine what would happen if every new developer will start following their own convention of naming endpoints, using Authorization, headers query parameters, success and error response structure, etc while developing an API. 
Design teams face a similar challenge when creating a new UI. They solve this by building Design Style Guides which make sure that everyone in the team uses the same UI component, colors, typography, etc. Using Design Style Guide it becomes easy to communicate between teams, keep UI consistent in the long term and bring agility in the design and development processes. 
So, why not have a Design Style Guide for APIs itself? Companies like Google, Microsoft, Cisco, Paypal, and many more rely on these API Styleguides to make sure their APIs remain consistent, look cohesive and intuitive. The benefits of consistency accrue in aggregate as well; consistency allows teams to leverage common code, patterns, documentation and design decisions. It will also ensure to make the developer experience as smooth as possible, for both API producer and consumer. Some of the important things that should be part of an API Style Guide are: 
To help people quickly get started, I have published a collection as a template that you can import and start fiddling around. 
Like all other design processes, the API design process also needs to be an iterative process. Publishing an API to your users is not the end of an API’s lifecycle. You have to keep the channel of constant feedback from API consumers always open and make sure that you keep accommodating suggestions using the same design process. 
Written by 
Written by",Prashant Agrawal,2020-04-19T15:35:04.675Z
Top 10 Cyber Security Fundamentals for Small to Medium Businesses | by Robert Smith | Medium,"If you are using the internet and are into online shopping, then by now, you would be aware of words like phishing, hacking, data hacking, etc. The internet has impacted most of the business, not only it has led to the expansion of business, but at the same time, it has also paved the way for certain vulnerabilities, which leads to the demand for cyber security experts who can help in securing the system. The work of the network security engineer is to create a secure digital medium. With the growing demand for cyber security professionals, it has become one of the most sought-after career options. Cyber security is the need of the hour; whether we talk about big businesses or small or medium enterprises, there is a widespread demand for cyber security professionals. 
Whether you are running a small or medium business enterprise or a large business, the security of data is essential. In the U.S. Congressional Small Business Committee, it was found that around 71 percent of cyber-attacks happened at companies with less than 100 employees. What’s even more surprising is that the 2016 State of SMB Cyber Security Report by Ponemon and @Keeper had around 50% cyber security attack in 2015. This whooping number shows that SMB is more vulnerable to cyber-attacks, and they need to find ways that can help them secure the data. The reason small enterprises are more susceptible to cyber-attacks is that they have a lesser secured system and are easy to attack. With the help of the right cyber security experts, you can be assured of having a safe digital system. 
With the growing advancement of computer technology, cyber attackers have also advanced their mode of attack. To combat it, you would need the assistance of network security engineers and cyber security professionals. At Global Tech Council, we have the most advanced cyber security certification online courses. These courses help in preparing a proficient workforce of network security engineers. Whether you are a beginner or already working in this field, you are going to get the benefit of this course. 
Written by 
Written by",Robert Smith,2020-03-16T06:02:32.791Z
12 Useful Things to Know about Machine Learning | by James Le | Cracking The Data Science Interview | Medium,"Machine learning algorithms can figure out how to perform important tasks by generalizing from examples. This is often feasible and cost-effective where manual programming is not. As more data becomes available, more ambitious problems can be tackled. As a result, machine learning is widely used in computer sincere and other fields. However, developing successful machine learning applications requires a substantial amount of “black art” that is hard to find in textbooks. 
I recently read an amazing technical paper by Professor Pedro Domingos of University of Washington titled “A Few Useful Things to Know about Machine Learning.” It summarizes 12 key lessons that machine learning researchers and practitioners have learned include pitfalls to avoid, important issues to focus on, and answers to common questions. I’d like to share these lessons in this article because they are extremely useful when thinking about tackling your next machine learning problems. 
All machine learning algorithms generally consist of combinations of just 3 components: 
The fundamental goal of machine learning is to generalize beyond the examples in the training set. This is because, no matter how much data we have, it is very unlikely that we will see those exact examples again at test time. Doing well on the training set is easy. The most common mistake among machine learning beginners is to test on the training data and have the illusion of success. If the chosen classifier is then tested on new data, it is often no better than random guessing. So, if you hire someone to build a classifier, be sure to keep some of the data to yourself and test the classifier they give you on it. Conversely, if you’ve been hired to build a classifier, set some of the data aside from the beginning, and only use it to test your chosen classifier at the very end, followed by learning your final classifier on the whole data. 
Generalization being the goal has another major consequence: data alone is not enough, no matter how much of it you have. 
This seems like rather depressing news. How then can we ever hope to learn anything? Luckily, the functions we want to learn in the real world are not drawn uniformly from the set of all mathematically possible functions! In fact, very general assumptions — like smoothness, similar examples having similar classes, limited dependencies, or limited complexity — are often enough to do very well, and this is a large part of why machine learning has been so successful. Like deduction, induction (what learners do) is a knowledge lever: it turns a small amount of input knowledge into a large amount of output knowledge. Induction is a vastly more powerful lever than deduction, requiring much less input knowledge to produce useful results, but it still needs more than zero input knowledge to work. And, as with any lever, the more we put in, the more we can get out. 
In retrospect, the need for knowledge in learning should not be surprising. Machine learning is not magic; it can’t get something from nothing. What it does is get more from less. Programming, like all engineering, is a lot of work: we have to build everything from scratch. Learning is a more like farming, which lets nature do most of the work. Farmers combine seeds with nutrients to grow crops. Learners combine knowledge with data to grow programs. 
What if the knowledge and data we have are not sufficient to completely determine the correct classifier? Then we run the risk of just hallucinating a classifier (or parts of it) that is not grounded in reality, and is simply encoding random quirks in the data. This problem is called overfitting, and is the bugbear of machine learning. When your learner outputs a classifier that is 100% accurate on the training data but only 50% accurate on test data, when in fact it could have output one that is 75% accurate on both, it has overfit. 
Everyone in machine learning knows about overfitting, but it comes in many forms that are not immediately obvious. One way to understand overfitting is by decomposing generalization error into bias and variance. Bias is a learner’s tendency to consistently learn the same wrong thing. Variance is the tendency to learn random things irrespective of the real signal. A linear learner has high bias, because when the frontier between two classes is not a hyperplane the learner is unable to induce it. Decision trees don’t have this problem because they can represent any Boolean function, but on the other hand they can suffer from high variance: decision trees learned on different training sets generated by the same phenomenon are often very different, when in fact they should be the same. 
Cross-validation can help to combat overfitting, for example by using it to choose the best size of decision tree to learn. But it’s no panacea, since if we use it to make too many parameter choices it can itself start to overfit. 
Besides cross-validation, there are many methods to combat overfitting. The most popular one is adding a regularization term to the evaluation function. This can, for example, penalize classifiers with more structure, thereby favoring smaller ones with less room to overfit. Another option is to perform a statistical significance test like chi-square before adding new structure, to decide whether the distribution of the class really is different with and without this structure. These techniques are particularly useful when data is very scarce. Nevertheless, you should be skeptical of claims that a particular technique “solves” the overfitting problem. It’s easy to avoid overfitting (variance) by falling into the opposite error of underfitting (bias). Simultaneously avoiding both requires learning a perfect classifier, and short of knowing it in advance there is no single technique that will always do best (no free lunch). 
After overfitting, the biggest problem in machine learning is the curse of dimensionality. This expression was coined by Bellman in 1961 to refer to the fact that many algorithms that work fine in low dimensions become intractable when the input is high-dimensional. But in machine learning it refers to much more. Generalizing correctly becomes exponentially harder as the dimensionality (number of features) of the examples grow, because a fixed-size training set covers a dwindling fraction of the input space. 
The general problem with high dimensions is that our intuitions, which come from a 3-dimensional world, often do not apply in high-dimensional ones. In high dimensions, most of the mass of a multivariate Gaussian distribution is not near the mean, but in an increasingly distant “shell” around it; and most of the volume of a high-dimensional orange is in the skin, not the pulp. If a constant number of examples is distributed uniformly in a high-dimensional hypercube, beyond some dimensionality most examples are closer to a face of the hypercube than to their nearest neighbor. And if we approximate a hypersphere by inscribing it in a hypercube, in high dimensions almost all the volume of the hypercube is outside the hypersphere. This is bad news for machine learning, where shapes of one type are often approximated by shapes of another. 
Building a classifier in 2 or 3 dimensions is easy; we can find a reasonable frontier between examples of different classes just by visual inspection. But in high dimensions it’s hard to understand what is happening. This in turn makes it difficult to design a good classifier. Naively, one might think that gathering more features never hurts, since at worst they provide no new information about the class. But in fact, their benefits may be outweighed by the curse of dimensionality. 
Machine learning papers are full of theoretical guarantees. The most common type is a bound on the number of examples needed to ensure good generalization. What should you make of these guarantees? First of all, it’s remarkable that they are even possible. Induction is traditionally contrasted with deduction: in deduction you can guarantee that the conclusions are correct; in induction all bets are off. Or such was the conventional wisdom for many centuries. One of the major developments of recent decades has been the realization that in fact we can have guarantees on the results of induction, particularly if we’re willing to settle for probabilistic guarantees. 
We have to be careful about what a bound like this means. For instance, it does not say that, if your learner returned a hypothesis consistent with a particular training set, then this hypothesis probably generalizes well. What is says is that, given a large enough training set, with high probability your learner will either return a hypothesis that generalizes well or be unable to find a consistent hypothesis. The bound also says nothing about how to select a good hypothesis space. It only tells us that, if the hypothesis space contains the true classifier, then the probability that the learner outputs a bad classifier decreases with training set size. If we shrink the hypothesis space, the bound improves, but the chances that it contains the true classifier shrink also. 
Another common type of theoretical guarantee is asymptotic: given infinite data, the learner is guaranteed to output the correct classifier. This is reassuring, but it would be rash to choose one learner over another because of its asymptotic guarantees. In practice, we are seldom in the asymptotic regime (also known as “asymptopia”). And, because of the bias-variance tradeoff discussed above, if learner A is better than learner B given infinite data, B is often better than A given finite data. 
The main role of theoretical guarantees in machine learning is not as a criterion for practical decisions, but as a source of understanding and driving force for algorithm design. In this capacity, they are quite useful; indeed, the close interplay of theory and practice is one of the main reasons machine learning has made so much progress over the years. But caveat emptor: learning is a complex phenomenon, and just because a learner has a theoretical justification and works in practice doesn’t mean the former is the reason for the latter. 
At the end of the day, some machine learning projects succeed and some fail. What makes the difference? Easily the most important factor is the features used. If you have many independent features that each correlate well with the class, learning is easy. On the other hand, if the class is a very complex function of the features, you may not be able to learn it. Often, the raw data is not in a form that is amenable to learning, but you can construct features from it that are. This is typically where most of the effort in a machine learning project goes. It is often also one of the most interesting parts, where intuition, creativity and “black art” are as important as the technical stuff. 
First-timers are often surprised by how little time in a machine learning project is spent actually doing machine learning. But it makes sense if you consider how time-consuming it is to gather data, integrate it, clean it and pre-process it, and how much trial and error can go into feature design. Also, machine learning is not a one-shot process of building a dataset and running a learner, but rather an iterative process of running the learner, analyzing the results, modifying the data and/or the learner, and repeating. Learning is often the quickest part of this, but that’s because we’ve already mastered it pretty well! Feature engineering is more difficult because it’s domain-specific, while learners can be largely general-purpose. However, there is no sharp frontier between the two, and this is another reason the most useful learners are those that facilitate incorporating knowledge. 
In most of computer science, the 2 main limited resources are time and memory. In machine learning, there is a third one: training data. Which one is the bottleneck has changed from decade to decade. In the 1980’s, it tended to be data. Today it is often time. Enormous mountains of data are available, but there is not enough time to process it, so it goes unused. This leads to a paradox: even though in principle more data means that more complex classifiers can be learned, in practice simpler classifiers wind up being used, because complex ones take too long to learn. Part of the answer is to come up with fast ways to learn complex classifiers, and indeed there has been remarkable progress in this direction. 
Part of the reason using cleverer algorithms has a smaller payoff than you might expect is that, to a first approximation, they all do the same. This is surprising when you consider representations as different as, say, sets of rules and neural networks. But in fact propositional rules are readily encoded as neural networks, and similar relationships hold between other representations. All learners essentially work by grouping nearby examples into the same class; the key difference is in the meaning of “nearby.” With non-uniformly distributed data, learners can produce widely different frontiers while still making the same predictions in the regions that matter (those with a substantial number of training examples, and therefore also where most text examples are likely to appear). This also helps explain why powerful learns can be unstable but still accurate. 
As a rule, it pays to try the simplest learners first (e.g., naive Bayes before logistic regression, k-nearest neighbor before support vector machines). More sophisticated learners are seductive, but they are usually harder to use, because they have more knobs you need to turn to get good results, and because their internals are more opaque). 
Learners can be divided into 2 major types: those whose representation has a fixed size, like linear classifiers, and those whose representation can grow with the data, like decision trees. Fixed-size learners can only take advantage of so much data. Variable-size learners can in principle learn any function given sufficient data, but in practice they may not, because of limitations of the algorithm or computational cost. Also, because of the curse of dimensionality, no existing amount of data may be enough. For these reasons, clever algorithms — those that make the most of the data and computing resources available — often pay off in the end, provided you’re willing to put in the effort. There is no sharp frontier between designing learners and learning classifiers; rather, any given piece of knowledge could be encoded in the learner or learned from data. So machine learning projects often wind up having a significant component of learner design, and practitioners need to have some expertise in it. 
In the early days of machine learning, everyone had their favorite learner, together with some a priori reasons to believe in its superiority. Most effort went into trying many variations of it and selecting the best one. Then systematic empirical comparisons showed that the best learner varies from application to application, and systems containing many different learners started to appear. Effort now went into trying many variations of many learners, and still selecting just the best one. But then researchers noticed that, if instead of selecting the best variation found, we combine many variations, the results are better — often much better — and at little extra effort for the user. 
Creating such model ensembles is now standard. In the simplest technique, called bagging, we simply generate random variations of the training set by resampling, learn a classifier on each, and combine the results by voting. This works because it greatly reduces variance while only slightly increasing bias. In boosting, training examples have weights, and these are varied so that each new classifier focuses on the examples the previous ones tended to get wrong. In stacking, the outputs of individual classifiers become the inputs of a “higher-level” learner that figures out how best to combine them. 
Many other techniques exist, and the trend is toward larger and larger ensembles. In the Netflix prize, teams from all over the world competed to build the best video recommender system. As the competition progressed, teams found that they obtained the best results by combining their learners with other teams’, and merged into larger and larger teams. The winner and runner-up were both stacked ensembles of over 100 learners, and combining the two ensembles further improved the results. Doubtless we will see even larger ones in the future. 
Occam’s razor famously states that entities should not be multiplied beyond necessity. In machine learning, this is often taken to mean that, given two classifiers with the same training error, the simpler of the two will likely have the lowest test error. Purported proofs of this claim appear regularly in the literature, but in fact there are many counter-examples to it, and the “no free lunch” theorems imply it cannot be true. 
We saw one counterexample in the previous section: model ensembles. The generalization error of a boosted ensemble continues to improve by adding classifiers even after the training error has reached zero. Thus, contrary to intuition, there is no necessary connection between the number of parameters of a model and its tendency to overfit. 
A more sophisticated view instead equates complexity with the size of the hypothesis space, on the basis that smaller spaces allow hypotheses to be represented by shorter codes. Bounds like the one in the section on theoretical guarantees above might then be viewed as implying that shorter hypotheses generalize better. This can be further refined by assigning shorter codes to the hypothesis in the space that we have some a priori preference for. But viewing this as “proof” of a tradeoff between accuracy and simplicity is circular reasoning: we made the hypotheses we prefer simpler by design, and if they are accurate it’s because our preferences are accurate, not because the hypotheses are “simple” in the representation we chose. 
Essentially all representations used in variable-size learners have associated theorems of the form “Every function can be represented, or approximated arbitrarily closely, using this representation.” Reassured by this, fans of the representation often proceed to ignore all others. However, just because a function can be represented does not mean it can be learned. For example, standard decision tree learners cannot learn trees with more leaves than there are training examples. In continuous spaces, representing even simple functions using a fixed set of primitives often requires an infinite number of components. 
Further, if the hypothesis space has many local optima of the evaluation function, as is often the case, the learner may not find the true function even if it is representable. Given finite data, time and memory, standard learners can learn only a tiny subset of all possible functions, and these subsets are different for learners with different representations. Therefore the key question is not “Can it be represented?”, to which the answer is often trivial, but “Can it be learned?” And it pays to try different learners (and possibly combine them). 
The point that correlation does not imply causation is made so often that it is perhaps not worth belaboring. But, even though learners of the kind we have been discussing can only learn correlations, their results are often treated as representing causal relations. Isn’t this wrong? if so, then why do people do it? 
More often than not, the goal of learning predictive models is to use them as guides to action. If we find that beer and diapers are often bought together at the supermarket, then perhaps putting beer next to the diaper section will increase sales. But short of actually doing the experiment it’s difficult to tell. Machine learning is usually applied to observational data, where the predictive variables are not under control of the learner, as opposed to experimental data, where they are. Some learning algorithms can potentially extract causal information from observational data but their applicability is rather restricted. On the other hand, correlation is a sign of a potential causal connection, and we can use it as a guide to further investigation. 
Like any discipline, machine learning has a lot of “folk wisdom” that can be hard to come by, but is crucial for success. Professor Domingos’ paper summarized some of the most salient items that you need to know. 
If you would like to follow my work on Machine Learning, you can check out my Medium and GitHub, as well as other projects at https://jameskle.com/. You can also tweet at me on Twitter, email me directly, or find me on LinkedIn. Or join my mailing list to receive my latest thoughts right at your inbox! 
Written by 
Written by",James Le,2020-05-10T23:14:50.533Z
Introduction to Hidden Markov Models | by Tomer Amit | Towards Data Science,"Markov Chains 
Let us first give a brief introduction to Markov Chains, a type of a random process. We begin with a few “states” for the chain, {S₁,…,Sₖ}; For instance, if our chain represents the daily weather, we can have {Snow,Rain,Sunshine}. The property a process (Xₜ)ₜ should have to be a Markov Chain is: 
In words, the probability of being in a state j depends only on the previous state, and not on what happened before. 
Markov Chains are often described by a graph with transition probabilities, i.e, the probability of moving to state j from state i, which are denoted by pᵢ,ⱼ. Let’s look at the following example: 
The chain has three states; For instance, the transition probability between Snow and Rain is 0.3, that is — if it was snowing yesterday, there is a 30% chance it will rain today. The transition probabilities can be summarized in a matrix: 
Notice that the sum of each row equals 1 (think why). Such a matrix is called a Stochastic Matrix. The (i,j) is defined as pᵢ,ⱼ -the transition probability between i and j. 
Fact: if we take a power of the matrix, Pᵏ, the (i,j) entry represents the probability to arrive from state i to state j at k steps. 
In many cases we are given a vector of initial probabilities q=(q₁,…,qₖ) to be at each state at time t=0. Thus, the probability to be at state i at time t will be equal to the i-th entry of the vector Pᵏq. 
For instance, if today the probabilities of snow, rain and sunshine are 0,0.2,0.8, then the probability it will rain in 100 days is calculated as follows: 
The 2nd entry equals ≈ 0.44. 
Hidden Markov Model 
In a Hidden Markov Model (HMM), we have an invisible Markov chain (which we cannot observe), and each state generates in random one out of k observations, which are visible to us. 
Let’s look at an example. Suppose we have the Markov Chain from above, with three states (snow, rain and sunshine), P - the transition probability matrix and q — the initial probabilities. This is the invisible Markov Chain — suppose we are home and cannot see the weather. We can, however, feel the temperature inside our room, and suppose there are two possible observations: hot and cold, where: 
Basic Example 
As a first example, we apply the HMM to calculate the probability that we feel cold for two consecutive days. In these two days, there are 3*3=9 options for the underlying Markov states. Let us give an example for the probability computation of one of these 9 options: 
Summing up all options gives the desired probability. 
Finding Hidden States — Viterbi Algorithm 
In some cases we are given a series of observations, and want to find the most probable corresponding hidden states. 
A brute force solution would take exponential time (like the calculations above); A more efficient approach is called the Viterbi Algorithm; its main idea is as follows: we are given a sequence of observations o₁,…,oₜ . For each state i and t=1,…,T, we define 
That is, the maximum probability of a path which ends at time t at the state i, given our observations. The main observation here is that by the Markov property, if the most likely path that ends with i at time t equals to some i* at time t−1, then i* is the value of the last state of the most likely path which ends at time t−1. This gives us the following forward recursion: 
here, αⱼ(oₜ) denotes the probability to have oₜ when the hidden Markov state is j . 
Here is an example. Let us generate a sequence of 14 days, in each 1 denotes hot temperature and 0 denotes cold. We will use the algorithm to find the most likely weather forecast of these two weeks. 
We get: 
Now let’s use the algorithm: 
We get: 
Which leads to the output: 
We used the following implementation, based on [2]: 
Learning and the Baum-Welch Algorithm 
A similar approach to the one above can be used for parameter learning of the HMM model. We have some dataset, and we want to find the parameters which fit the HMM model best. The Baum-Welch Algorithm is an iterative process which finds a (local) maximum of the probability of the observations P(O|M), where M denotes the model (with the parameters we want to fit). Since we know P(M|O) by the model, we can use a Bayesian approach to find P(M|O) and converge to an optimum. 
HMM have various applications, from character recognition to financial forecasts (detecting regimes in markets). 
References 
[1] https://cse.buffalo.edu/~jcorso/t/CSE555/files/lecture_hmm.pdf 
[2] http://www.blackarbs.com/blog/introduction-hidden-markov-models-python-networkx-sklearn/2/9/2017 
Hands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look 
Written by 
Written by",Tomer Amit,2019-10-30T16:13:50.143Z
Principal Component Analysis. This article is divided into two… | by Mahesh Kumar 👾 | maheshkkumar | Medium,"This article is divided into two sections, the first section deals with the pictorial explanation of Principal Component Analysis (PCA) and the second section deals with mathematical explanation of PCA. 
The simplest answer would be that it is an analysis of finding the principal components in the given data set. 
It is often useful to measure the data in terms of it’s principal components instead of the normal 2 dimensional plane of x-y axis. The principal components are the underlying structure in the data. They represent the directions in which the data has maximum variance and also the directions in which the data is most spread out. 
Let’s consider the the diagram show below to understand the above statements. 
Imagine that the green circles are the data points of a data set. In order to find the direction of maximum variance, fit a straight line in the data set where the data is most spread out. A vertical line with the data points projected on to it will look like the diagram show below. 
It looks like the line doesn’t show the direction of maximum variance in the data set. It is probably not the principal component. 
A better fit would be a horizontal line and the data points projection is show in the diagram below. 
Oh yeah, this looks much better and the data is way more spread out on this line, it has maximum variance and with more analysis we can conclude that there cannot be any other line that has maximum variance. Thus, we can say that the horizontal line is the principal component for the above data set. 
2. Eigenvectors and Eigenvalues 
In any data set, we can deconstruct the data into eigenvectors and eigenvalues. Eigenvectors and values exist in pairs. Every eigenvector has a corresponding eigenvalue. 
When we consider the example that we previously used, eigenvector was the direction of the line (vertical or horizontal) drawn to find the maximum variance. An eigenvalue is just a number, it tells us how much variance exists in the data in a particular direction and in the previous example an eigenvalue was a number that tells us how the data set is spread out on the line which is an eigenvector. 
Therefore, an eigenvector with the highest eigenvalue is the Principal Component. 
It turns out that there were not many eigenvectors or eigenvalues in the previous example. For a fact the number of eigenvectors and eigenvalues that exist equals the number of dimensions of a data set. 
Say you are measuring the age and attendance of each student in a classroom. So, the two variables are age and attendance of students. Therefore the data set is 2 dimensional and it has 2 eigenvectors and eigenvalues. 
If you were to measure the age, attendance and percentage of each student in a classroom, this leads to a 3 dimensional data set given the 3 variables. Therefore it will have 3 eigenvectors and eigenvalues. 
This might sound confusing and might be complicated, let’s make it clear using some diagrams. 
At the moment the data set which is depicted in oval shape on the x-y axis, where x could be the age and y could be the attendance. These are the 2 dimensions that are currently used to measure the data set. 
As already discussed there is a principal component or a line that will split the data set for maximum variance as show below. 
From the above diagram, we get one eigenvector and the second eigenvector will be perpendicular or orthogonal to the first one. The reason the two eigenvectors are orthogonal to each other is because the eigenvectors should be able to span the whole x-y area. 
The eigenvectors have given us a much more useful axis to frame the data set. We can now re-produce the above data set with the new dimensions. 
Just to make it clear, we didn’t mess with the data set, we just changed the view of angle of the data set with the new x’-y’ axis. These axes are much more intuitive to the shape of the data set. These directions give the most variation and provide us with more information. In case, there was no variation in the data set, then everything would be equal to 1 and it would have been such a boring statistic and the eigenvalue would be zero. 
In this section, let’s consider the mathematics point of view to understand the PCA and learn some basic statistics along the way to achieve our goal of figuring out PCA. 
Mean is the average value of the entire data. Let’s consider a group of students, in order to find the mean of their age, we will add up the age of every student and divide the sum by the total number of students. 
2. Variance 
It is the measure of the spread of the date in the data set using the mean. When we consider the group of students example, variance is the sum of the squared difference between the age of each student and the mean age value, then the sum is divided by the number of students. 
Variance can also be represented as the measure of the deviation from the mean for the points in one dimension. 
3. Covariance 
It is a measure of how much each of the dimensions vary from the mean with respect to each other. 
The covariance is measured between 2 dimensions to see if there is relationship between the 2 dimensions, e.g., relationship between the height and weight of students. 
The covariance of one dimension is variance. 
A positive value of covariance indicates that both the dimensions are directly proportional to each other, where if one dimension increases the other dimension increases accordingly. 
A negative value of covariance indicates that both the dimensions are indirectly proportional to each other, where if one dimension increases then other dimension decreases accordingly. 
If in case the covariance is zero, then the two dimensions are independent of each other. 
One of the major purpose of covariance is to find the relationship between dimensions in high dimensional data set, usually dimensions greater than 3, where the visualization is tricky. 
We can also represent the dimensions of a higher dimensional data set in terms of covariance matrix. Let’s consider the example of a 3 dimensional data set, then the covariance matrix can be represented as follows 
4. Transformation Matrices 
Consider the following 2 dimensional matrix multiplied by a vector. 
Now, assume we take a multiple of (3,2) vector and multiply it with the 2 dimensional matrix. 
From the above two operations, what did we understand? 
5. Eigenvalue Problem 
The eigenvalue problem is any problem of the form: 
Any value of lambda for which the above equation has a solution is called the eigenvalue for A and the vector v which corresponds to this value is called the eigenvector of A. 
Going back to our above example: 
6. Noise 
Noise in any data must be low or — no matter the analysis technique — no information about a system can be extracted. 
There exists no absolute scale for the noise but rather it is measured relative to the measurement, e.g. recorded ball positions. 
A common measure is the signal-to-noise ration (SNR), or a ration of variances. 
A high SNR (>>1) indicates high precision data, while a low SNR indicates noise contaminated data. 
7. Covariance Matrix 
Assuming zero mean data (subtracting the mean from each data value), consider the indexed vectors {x1, x2, …., xm} which are the rows of an m x n matrix X. 
Each row corresponds to all measurements of a particular measurement type (xi). 
Each column of X corresponds to a set of measurements from a particular time instant. 
We can now define our covariance matrix Sx. 
8. Solving PCA: Eigen Vectors of Covariance Matrix 
Let’s derive our first algebraic solution to PCA using linear algebra. This solution is based on the property of eigenvector decomposition. 
The goal is summarized as follows: 
In the next article, let’s take a look at the applications point of view of PCA. 
Originally published at maheshkumar.xyz on December 22, 2016. 
If you liked this article — I’d really appreciate if you hit the like button to recommend it. You can also follow me on Twitter. Peace! 
Written by 
Written by",Mahesh Kumar 👾,2018-07-26T12:30:52.355Z
Automated and Interpretable Machine Learning | by Francesca Lazzeri | Microsoft Azure | Medium,"This blog post is authored by Francesca Lazzeri (@frlazzeri) 
Automated machine learning is based on a breakthrough from Microsoft’s Research Division. The approach combines ideas from collaborative filtering and Bayesian optimization to search an enormous space of possible machine learning pipelines intelligently and efficiently, serving essentially as a recommender system for machine learning pipelines. 
On the other hand, when using an algorithm’s outcomes to make high-stakes decisions, it’s important to know which features it did and did not take into account. Additionally, if a model isn’t highly interpretable, the business might not be legally permitted to use its insights to make changes to processes. 
In this article, I will explain how you can get started and combine automated machine learning and model interpretability to accelerate AI, while still making sure that models are highly interpretable, minimizing model risk and making it easy for any enterprise to comply with regulations and best practices. 
Below are some useful resources that you might want to check out while learning more about automated ML and model interpretability: 
In some of my previous posts, I introduced a few use cases and machine learning capabilities related to Automated Machine Learning. Specifically, I talked about: 
Automated machine mearning is a capability that allows a developer or data scientist to access an automated service that identifies the best machine learning pipelines for their labelled data. It empowers customers, with or without data science expertise, to identify an end-to-end machine learning pipeline for any problem. 
Automated machine learning is designed to not look at the customer’s data. Customer data and execution of the machine learning pipeline both live in the customer’s cloud subscription (or their local machine), which they have complete control of. Only the results of each pipeline run are sent back to the Automated Machine Learning service, which then makes a probabilistic choice of which pipelines should be tried next. 
When it comes to predictive modeling, you have to make a trade-off: Do you just want to know what is predicted? For example, the probability that a customer will churn or how effective some drug will be for a patient. Or do you want to know why the prediction was made and possibly pay for the interpretability with a drop in predictive performance? 
In some cases, you do not care why a decision was made, it is enough to know that the predictive performance on a test dataset was good. But in other cases, knowing the ‘why’ can help you learn more about the problem, the data and the reason why a model might fail. 
In this post, you learn how to explain why your model made the predictions it did with the various interpretability packages of the Azure Machine Learning Python SDK. 
Using the classes and methods in the SDK, you can get: 
During the training phase of the development cycle, model designers and evaluators can use interpretability output of a model to verify hypotheses and build trust with stakeholders. They also use the insights into the model for debugging, validating model behavior matches their objectives, and to check for bias. 
In machine learning, features are the data fields used to predict a target data point. For example, to predict credit risk, data fields for age, account size, and account age might be used. In this case, age, account size, and account age are features. Feature importance tells you how each data field affected the model’s predictions. For example, age may be heavily used in the prediction while account size and age don’t affect the prediction accuracy significantly. This process allows data scientists to explain resulting predictions, so that stakeholders have visibility into what data points are most important in the model. 
Using these tools, you can explain machine learning models globally on all data, or locally on a specific data point using the state-of-art technologies in an easy-to-use and scalable fashion. 
The interpretability classes are made available through multiple SDK packages. Learn how to install SDK packages for Azure Machine Learning. 
You can apply the interpretability classes and methods to understand the model’s global behavior or specific predictions. The former is called global explanation and the latter is called local explanation. 
The methods can be also categorized based on whether the method is model agnostic or model specific. Some methods target certain type of models. For example, SHAP’s tree explainer only applies to tree-based models. Some methods treat the model as a black box, such as mimic explainer or SHAP’s kernel explainer. The explain package leverages these different approaches based on data sets, model types, and use cases. 
The output is a set of information on how a given model makes its prediction, such as: 
There are two sets of explainers: Direct Explainers and Meta Explainers in the SDK. 
Direct explainers come from integrated libraries. The SDK wraps all the explainers so that they expose a common API and output format. If you are more comfortable directly using these explainers, you can directly invoke them instead of using the common API and output format. The following table lists the direct explainers available in the SDK: 
Meta explainers automatically select a suitable direct explainer and generate the best explanation info based on the given model and data sets. The meta explainers leverage all the libraries (SHAP, LIME, Mimic, etc.) that we have integrated or developed. The following are the meta explainers available in the SDK: 
In addition to Meta-selecting of the direct explainers, meta explainers develop additional features on top of the underlying libraries and improve the speed and scalability over the direct explainers. 
Currently TabularExplainer employs the following logic to invoke the Direct SHAP Explainers: 
The intelligence built into TabularExplainer will become more sophisticated as more libraries are integrated into the SDK and we learn about pros and cons of each explainer. 
TabularExplainer has also made significant feature and performance enhancements over the Direct Explainers: 
The following diagram shows the current structure of direct and meta explainers: 
Any models that are trained on datasets in Python numpy.array, pandas.DataFrame, iml.datatypes.DenseData, or scipy.sparse.csr_matrix format are supported by the interpretability explain package of the SDK. 
The explanation functions accept both models and pipelines as input. If a model is provided, the model must implement the prediction function predict or predict_proba that conforms to the Scikit convention. If a pipeline (name of the pipeline script) is provided, the explanation function assumes that the running pipeline script returns a prediction. We support models trained via PyTorch, TensorFlow, and Keras deep learning frameworks. 
The explain package is designed to work with both local and remote compute targets. If run locally, The SDK functions will not contact any Azure services. You can run explanation remotely on Azure Machine Learning Compute and log the explanation info into Azure Machine Learning Run History Services. Once this information is logged, reports and visualizations from the explanation are readily available on Azure Machine Learning Workspace portal for user analysis. 
Automated machine learning allows you to understand feature importance. During the training process, you can get global feature importance for the model. For classification scenarios, you can also get class-level feature importance. You must provide a validation dataset (X_valid) to get feature importance. 
There are two ways to generate feature importance. 
Once done, you can use retrieve_model_explanation method to retrieve feature importance for a specific iteration: 
You can visualize the feature importance chart in your workspace in the Azure portal: 
To see all scripts and tutorials for Automated machine learning and Model Interpretability within Azure ML service, you can visit the GitHub repo: aka.ms/AzureMLServiceGithub — it includes the latest Azure Machine Learning Python SDK samples. 
Written by 
Written by",Francesca Lazzeri,2019-07-15T20:19:56.396Z
5 Concepts You Must Know to Pass AWS Machine Learning Certification Exam | The Startup,"I recently passed the AWS ML specialty exam (here’s my digital badge)and noticed that there are some important concepts that may be missed even by experienced data scientists. While there are many concepts you have to master in order to pass the AWS ML specialty exam, I’m just listing five important ones. 
Before we get into the five concepts it’s important to note that this article is by no means a comprehensive study guide nor it means that understanding these five concepts will guarantee you to pass the exam. Absolutely not, the AWS ML specialty exam is a tough one and needs hours of serious preparation and study. 
Now let’s dive into these five concepts. While reading these concepts I want you to note the common theme among them. Balance & trade-off is the common theme and that’s a very important aspect to keep in mind when building your machine learning model. 
Note: I will use simple language and no-equations to explain the concepts so it can suit beginners as well as experienced readers. 
When your model has high bias this means the model is simple and can’t capture many features during the training (learning) phase. You don’t want that. 
When your model has high variance this means the model is complex and is not only capturing features but also learning anything but those specific training set features, this is also referred to as overfitting. This model will definitely perform badly on new data it never saw before. You don’t want that. 
You could say okay let’s just minimize both bias and variance in our model. The bad news is that you can’t minimize both, the good news is that you can achieve a balance between both. 
As you can see there is a sweet spot in the middle to balance both Bias and Variance. If your model shift to the right side then it’s getting more complicated thus increasing variance and resulting in overfitting. If your model shifts to the left then it’s getting too simple thus increasing bias and results in underfitting. 
A good data scientist knows how to tradeoff bias and variance by tuning the model’s hyperparameters thus achieving optimum model complexity. 
Note that a simple model means a small number of neurons and fewer layers while a complex model means a big number of neurons and several layers. Keep in mind that a complex model needs more resources (CPU or GPU) for learning, whether that’s on-premises or in the cloud this means money spent. The longer you use these resources the more money you pay. 
Logically you want your model to learn (train) fast so it’s ready for deployment and inference. Who wouldn’t want a shorter learning (training) time and accurate models. However, This is not possible in the world of machine learning. Let’s understand why. 
The learning rate is a tuning parameter that determines the step size of each iteration (epoch) during model training. The step size is how fast (or slow) you update your neurons' (model) weights in response to an estimated error. Keep in mind that model weights are updated using the backpropagation error method. So, the input will flow from the input nodes of your model through the neurons to the output nodes then the error is determined and backpropagated to update the neuron's (model) weights. How fast to update those neurons (model) weights is what we are looking into now. 
If the learning rate is high thus the model weights are updated fast and frequent your model will converge fast but it may overshoot the true error minima. This means a faster but erroneous model. 
If the learning rate is low thus the model weights are updated slowly your model will take a long time to converge but will not overshoot the true error minima. This means a slower but more accurate model. 
As you can see in the figure, a high learning rate (green line) caused quick convergence in terms of low error but stayed like that for all epochs. This is probably because the big step size caused the model to overshoot the true error minima. You don’t want your model doing that. 
A low learning rate (blue line) caused a slow convergence in terms of taking long to decrease the error. In fact, you may reach the last epoch before the model fully converges and find the true minima. You don’t want your model to do this either. 
The good learning rate (red line) is a balance between high and low learning rate. It avoids overshooting true minima while converging in a reasonable time. 
A balanced learning rate is a tradeoff between fast convergences and achieving low error. A good data scientist knows how to tune the model’s parameters to get the optimum learning rate. 
First, let’s start by mentioning the difference between batch size and epoch. Epoch is how many times your algorithm works through the entire training set, so an epoch of one means the algorithm works once from the input nodes all the way to the output nodes of the neural network and back again using backward propagation. On the other hand, a batch is the number of samples your algorithm will work before updating the weights. 
So, an epoch can have one or more batches. I know this may sound confusing but think of how your neural network learns by propagating from input to output and count that as one epoch. Then think if your whole training set and divide it into smaller training sets and call each small training set a batch. 
Now the question is what is the good batch size, should I keep my entire dataset as one batch or it’s better to divide it into a number of batches. If a number of batches are better, what is the best number of batches I should use. Again, with everything in machine learning there is no simple answer, let's see why. 
A large batch size means your model will learn faster, however, it may miss the true minima and oscillates. Oscillations are when you see your model weights are updating rapidly with big numbers. A smaller batch size also referred to as mini-batch means your model will learn slower, however, it may not miss the true minima (but get stuck in local minima) and stay stable. We always talk about fast vs. slow learning cause time equals money and the longer your model takes to learn the more you pay for compute resources. 
The graph shown may not be linear but what I want you to get out of it is the relationship between batch size, model learning speed, and model stability. Again, it’s all about balance how to balance between big batch size and mini-batch to achieve acceptable training speed while avoiding missing true minima and keeping your model stable. Knowing the optimum batch size isn’t easy but a good data scientist knows how to balance it. 
Let’s start by defining in simple words what’s gradient descent and what’s an activation function. Think of gradient descent as the weights used to update your neural network during the backpropagation from output to input nodes. Think of Activation as the equation tied to each neuron in your model, this equation decides if this neuron should be activated or not depending on the neuron’s input relevancy to the model prediction. 
In some cases when you have a deep neural network with several layers and based on your choice of the activation function (along with other hyper-parameters), the gradients will become very small and may vanish while backpropagating from the output to input nodes through the layers of the network. The problem here is the weights of the neurons in your model won’t get updated (or get updated with very small values) thus your model won’t learn (or will get minimal learning). This is a clear case of vanishing gradient descent problem. 
In other cases, the opposite happens the gradients become very large and while propagating from output to input nodes they keep getting multiplied thus larger and larger gradient values to update the neurons. This results in an unstable model and sometimes an overflow. 
To avoid both the vanishing and exploding gradient descent you have to balance the hyper-parameters of your deep neural model. I will not get into how to choose the hyper-parameters because that’s a big topic and it’s more of an art than science. However, you should understand the concept of vanishing and exploding gradient descent. 
Confusion Matrix is a method to measure the performance of machine learning classification models. 
It’s always good to give an analogy to explain the confusion matrix. Let’s say you’re into football and your favorite team is playing tonight. Your team has been practicing all season and is in great shape, you bet your friend that your team will win. Let’s see how a confusion matrix can apply to your team in tonight’s football game. 
True Positive (TP): You predicted your team will win tonight’s game and they actually win. 
False Positive (FP): You predicted your team will win tonight’s game but they lose. That’s called type 1 error. 
False Negative (FN): You predicted your team will lose but they win. That’s called type 2 error. 
True Negative (TN): You predicted your team will lose and they lose. 
For classification models, you can easily draw a confusion matrix after you run your model through the test set. You want to see how the model performed, luckily, there are several ways to measure your model. Each has pros and cons, let’s see what they are. 
Accuracy: It literally means what it’s called, how many samples we classified accurately. 
Although it sounds good it doesn’t give you much information on the times your model missed. 
Precision: This is the answer for: out of all the times my model said positive, how many were really positive. You care about precision when False Positive is important to your output. Let’s say you’re a small company and you send samples to potential customers who might buy your product. You don’t want to send samples to customers that will never buy your product no matter what. The customer who gets a sample but doesn’t buy your product is false positive because you predicted they will buy your product (Predicted = 1) but actually they never will (Actual = 0). In such cases, you want to decrease the FP as much as you can in order to have high precision. 
Recall: This is the answer for: out of the actual positives, how many were classified correctly. You care about the recall when False Negative is important to your output. Let’s take an example of your credit card, someone stole your credit card number and used to purchase stuff online from a sketchy website that you never visit. That’s clearly a fraudulent transaction but unfortunately, your banks’ algorithm didn’t catch it. What happened here is that your bank predicted it’s not a fraud (predicted = 0) but it was actually a fraud (actual =1). In such a case, your bank should develop a fraud detection algorithm that decreases the FN thus increases the recall. 
F1: This is a balance between both precision and recall since both will equally affect your F1 score. Think of it this way, a higher F1 value means the more predicted positives were actually positives and more predicted negatives were actually negative. 
If you haven’t read anything above just read this part it will help you, I promise. 
The common theme between all these 5 concepts is balance. Machine Learning is about balance and trade-offs. It’s not a zero-sum game, you can’t have a fast learning model and yet very accurate one. You need to learn how to balance your model according to the requirements of the problem you’re trying to solve. All the above trade-offs mentioned (bias-variance, learning rate, batch size, gradient descent) are interrelated and have a cause & effect on each other. They are not separate silos. 
Only by experience, you will learn how to build an optimum model for the problem you’re trying to solve. Below is a summary of fixes to some of these issues. You will probably see some of them in your AWS ML specialty exam. 
Finally, good luck if you’re writing your AWS ML specialty exam soon. If you’re just interested in machine learning I hope this article got you more excited and more interested in machine learning. I meant for this article to be easy to read with no math so people at different expertise levels can enjoy it. 
Written by 
Written by",Mo Daoud,2020-05-08T09:57:19.437Z
NLP – Towards Data Science,,NA,NA
Azure Sentinel Resources. List of resources available for Azure… | by Adrian Grigorof | Medium,"List of resources available for Azure Sentinel SIEM cloud-based solution. The list is a work in progress and it is updated on regular basis. 
Last update: June 12, 2019 
People (last name alphabetical order)Tim Burrell, MicrosoftIan Hellen, Principal Software Engineer at MicrosoftPaul Huijbregts, Threat Management (THREATMAN) Specialist at MicrosoftMaarten Goet, Microsoft MVP & Microsoft Regional DirectorEliav Levi, Director of Product Management, Microsoft Azure SentinelLiza Mash Levin, Senior Program Manager Lead, Azure Sentinel teamIrek Romaniuk, Senior Network Security Engineer 
Microsoft Reference Websites Microsoft’s Azure Sentinel DocumentationAzure Sentinel Git RepositoryAzure Log Analytics output plugin for LogstashAzure MonitorKusto Query Language (KQL) Reference 
ArticlesIntroducing Microsoft Azure Sentinel by Eliav Levi, Director of Product Management, Microsoft Azure SentinelAzure Sentinel: design considerations by Maarten Goet, Microsoft MVP & Microsoft Regional DirectorSyslog to Azure Sentinel by Irek Romaniuk, Senior Network Security EngineerAzure Sentinel custom logs: Getting your MDATP alerts into your workspace by Paul HuijbregtsMachine Learning powered detections with Kusto query language in Azure Sentinel by Liza Mash LevinSecurity Investigation with Azure Sentinel and Jupyter Notebooks — Part 1 by Ian HellenSecurity Investigation with Azure Sentinel and Jupyter Notebooks — Part 2 by Ian HellenSecurity Investigation with Azure Sentinel and Jupyter Notebooks — Part 3 by Ian HellenIntegrating Azure Security Center with Azure Sentinel by Yuri DiogenesAzure Sentinel: Microsoft’s thoroughly modern SIEM by Simon BissonCloud SIEM: Why all the fuss? by Rocky DeStefanoTime series analysis applied in a security hunting context by Tim Burrell (MSTIC)Audit Scheduled tasks using Azure Sentinel by Mattias BorgAzure Sentinel Insecure Protocols Dashboard Implementation Guide by Jon Shectman and Brian Delaney — SecurityPFEAzure Sentinel — MineMeld. Bring Your Own Threat Intelligence feeds by Antonio FormatoSecuring the hybrid cloud with Azure Security Center and Azure Sentinel by Gilad Elyashar Principal Group PM Manager, Azure Security CenterThe Journey to Azure Sentinel (Deploy Azure Sentinel) by Eli Shlomo 
GroupsSentinel Hackers (Linkedin group and meetup)Microsoft Communities Azure Sentinel Group 
3rd Party ProductsGrafana — Visualization Grafana Github RepositoryMicrosoft Graph Security 
Other links (parsers, use-cases, queries)Notable Events in Azure Security CenterpfSense Logstash parser and Kibana dashboardsPalo Alto Common Event Format (CEF) Configuration GuidesAzure Log Analytics output plugin for LogstashSending IOCs to the Microsoft Graph Security API using MineMeldMineMeld Microsoft Graph Security API Extension (Git) 
Written by 
Written by",Adrian Grigorof,2019-06-12T21:49:33.465Z
Cloud Security & The Shared Responsibility Model: | by Daniel Simmons | Beanfield Metroconnect | Medium,"2018 has been a landmark year for cloud services with cloud consumption revenue surpassing server sales for the first time in history; analysts predict that by the end of this year companies will have relocated 60% of all server workloads from on-premise infrastructure to Public Cloud. We will likely look back at 2018 as the inflection point in the technology adoption lifecycle that cloud services “crossed the chasm” between the early adopter and early majority phase. We did it! 
Now that we are here, it’s about time we get down to brass tacks and talk security. With companies around the world sprinting to get to the cloud, I have conversations every day that remind me — most companies don’t understand cloud security. The number one fear I encounter from C suite teams at the onset of large digital transformation journeys — “the cloud isn’t as secure as our on-premise infrastructure.” The truth is that the security of your cloud infrastructure lies mostly on the shoulders of your company and not the cloud service provider. If this is new information for you, let me introduce you to the Shared Responsibility Model [1][2]. 
With companies around the world sprinting to get to the cloud, I have conversations every day that reminds me — most companies don’t understand cloud security… [t] he truth is that the security of your cloud infrastructure lies mostly on the shoulders of your company and not the cloud service provider. 
The shared responsibility model is nearly identical across the service provider landscape, and to break it down as simply as possible there are two distinct cloud security roles and responsibilities: 
1. Security “OF” the cloud (CSPs) 
2. Security “IN” the cloud (Your Company) 
Microsoft, Amazon, Google, Oracle, etc. — all are responsible for the security “OF” the cloud. The scope is pretty limited and includes the physical security of data centres, and the infrastructure inside (CPU, Storage, Network). Your responsibility as a consumer of cloud services is the security “IN” the cloud, which is most of the work. This dichotomy is best represented as an iceberg, with the vast majority of responsibility falling on the customer and small scope of work for CSPs represented by the portion floating above the surface of the ocean. 
Try and name a few recent cloud security breaches; major global news stories like Equifax, Sony, and Uber, were all incidents where customers failed in the fulfillment of their responsibilities for security “IN” the cloud. It is why although the IaaS (Infrastructure-as-a-Service) workloads for those incidents were hosted in Azure, and AWS respectively, Amazon and Microsoft were seldom named in reports and were never attributed responsibility. If we are honest with ourselves; companies with information security issues in the cloud have information security issues outside of the cloud. If your managed IT service provider tells you that the cloud isn’t as secure as your on-premise servers, it’s time for a third-party audit. 
The customer scope in the shared responsibility model is significant, so it’s important to critically evaluate internal capabilities and determine whether or not these are duties your organization is honestly able to deliver in-house. Whether you have adopted a multi-cloud strategy or a single vendor solution, the customer responsibilities for security will include but may not be limited to these roles: 
To backfill IT bench depth most Canadian enterprises are turning to cloud integration partners like Sourced Group, Scalar, Pythian, and Arctiq for cloud security and digital transformation expertise. The IT service industry is undergoing a radical transformation, with cloud service adoption driving IT service outsourcing market growth. The cloud integration service provider market is already undergoing a period of consolidation; legacy managed IT service providers and internal IT resources will become less likely sources of the required experience or technical depth for cutting edge digital transformation initiatives. Companies should consider funding retraining initiatives for internal IT resources and be vigilant about evaluating whether or not their managed IT services providers are making those investments themselves. 
Whether your company is planning a digital transformation initiative, or you have been in the cloud for sometime, a routine check-up never hurts. Have a frank discussion with your team about the shared responsibility model. With the bulk of the work on your shoulders, it is important you know your role in keeping your cloud secure. In the infamous words of the Notorious B.I.G — 
And if you don’t know, now you know. 
Daniel Simmons is the Director of Cloud Strategy at Beanfield Metroconnect, a cloud product manager, cloud evangelist, and a solution architect for cloud and data centre services. Daniel has spent the past year overseeing the launch of the Beanfield and Megaport partnership with the goal of bringing multi-cloud connectivity to every office building in the city. He lives in Toronto with his partner and dog. 
Written by 
Written by",Daniel Simmons,2018-10-11T13:53:36.758Z
What is anomaly detection?. The director of security for a large… | by Ricky Thomas | Avora | Medium,"The director of security for a large regional retailer looked over the employee behaviour analysis charts. Something wasn’t right — shrinkage had increased 2 percent in the last 24 weeks alone. Anomaly detection analytics indicated the same four employees in one city clocked in extra-early on the fourth Saturday of each month, the day of the monthly company-wide super sale. 
Cross-checking shrinkage reports eventually revealed these employees conspired with outsiders to steal hundreds of thousands of dollars of TVs, electronics and computer equipment. By examining anomalies in employee data, the company was able to prevent further losses. 
An anomaly is a pattern that deviates from the expected or normal result. Anomaly detection is locating patterns that do not behave as expected — it looks at clues and compares attributes to discover out-of-the-ordinary patterns. Many times, there are multiple anomalies in groups, not just single occurrences. 
Take a look at some examples of real-world anomalies to get a better idea of how detection is used: 
Anomaly detection programs use algorithms to determine when a metric is acting in a way that is a departure from historical trends. The algorithms are on constant alert for irregular behavior. 
There are several different types of anomalies: 
Simple anomaly detection flags data points that fall outside regular patterns in a distribution — whether the median, mode, mean or quantiles. 
Machine learning anomaly detection methods include: 
Modern anomaly detection programs are complex, but you can do simple anomaly detection using a low-pass filter with a moving average. 
Anomaly detection can help organizations realize significant cost savings. For example, a McKinsey Global study revealed that data-driven predictive maintenance will save manufacturer’s up to $630 billion in 2025. Similarly, the Deloitte University Press report “Industry 4.0 and Manufacturing Ecosystems” shows predictive maintenance and detecting anomalies related to machine failure could save companies like Caterpillar millions of dollars and reduce equipment downtime. 
Anomaly detection helps stakeholders throughout the organization. Marketing managers can better predict future consumer trends, operations managers are able to predict equipment purchasing needs and maintenance failure, and C-suite leaders can better manage employee compensation, training and incentive packages. On every front, anomaly detection combined with predictive analytics changes the face of business. 
Avora provides anomaly detection with intelligent alerting to find the unknowns in your business 
Written by 
Written by",Ricky Thomas,2017-04-17T18:31:26.065Z
Word2Vec. Intro | by Long | Medium,"word2vec is a method that use a vector to represent a word, because calculate word in the network directly is difficult. 
how to use number vector to represent a word? 
one-hot coding is a very start method to make word digitized, it just represent a word but no meaningful, u can get it from nothing. 
word embedding matrix is(vocab_num * feature_num). 
The destination of Wrod2Vec is to train to embedding model,not predict some result. 
Step 1:get inputs and targets 
the below example shows some of the training samples ,word pairs(input, target), we would take from the sentence “The quick brown fox jumps over the lazy dog.” I’ve used a small window size of 2 just for the example. The word highlighted in blue is the input word. 
every word represented by word pair,same word pair frequency of occurrence is high, nearby word probability is high, vice versa. 
embedding model establish process is use statistic frequency to back adjust the embedding matrix. 
So, if two words have similar contexts, then our network is motivated to learn similar word vectors for these two words! 
There have some tricks in real train process: 
full_code 
Reference: 
Written by 
Written by",Long,2017-07-04T03:45:04.888Z
Time-series Anomaly Detection with Twitter’s ESD test | by Elisha Shrestha | Medium,"Why Anomaly Detection? 
Working on time-series data from different IoT sensors, we usually face a problem of missing data, and mostly getting unusual data patterns. This is usually due to not receiving data on time, network latency, and a plethora of other reasons. We had two major problems — one, the order of Time-Series sequence is important and these anomalies were degrading our data for other tasks like forecasting and data analysis. The other one was not getting alerts on time. We mostly knew it after the clients complain about it. So we somehow had to come up with an approach to detect these anomalies and integrate it into our system. And this is how it started. 
Anomaly detection refers to the process of identifying events that do not conform to an expected pattern or data points that deviate from a dataset’s normal behavior. While time-series data is comprised of a sequence of values over time. Each data point is a pair of the moment in time the metric was observed and the value of that metric at that point in time. Time series data provides us with the required knowledge to make guesses about what can be reasonably expected in the near future. Anomaly detection techniques use those expectations to detect actionable signals within data. 
In this short blog post, among various anomaly detection techniques we will discuss Twitter’s Anomaly Detection package implemented in python. We need to start from the basics to understand the mechanism behind Twitter’s Anomaly Detection. Generalized ESD is an extension of Grubb’s test, which is a hypothesis testing, so it has test statistics!. Grubbs’ test is used to find a single outlier in a univariate data set assumed to come from a normally distributed dataset. It’s deceptively simple to run. It checks for outliers by looking for the maximum of the absolute deviation between the values and the mean, and then decides whether this deviation is an outlier based on its own home-made G statistic. 
Grubbs’s test is defined for the hypothesis: 
H(null): There are no outliers in the data set 
H(alt): There is exactly one outlier in the data set 
The G statistic is simply the maximum deviation (z-score) from the mean, while the G Critical Value can be found via table or formula. It depends on t-distribution so the value depends on the alpha level and number of samples. 
Find the G test statistic >> Find the G Critical Value >> Reject the point as an outlier if the test statistic is greater than the critical value. 
The Generalized Extreme Studentized Deviate (ESD) Test is a generalization of Grubbs’ Test. It can handle more than one outlier. To do this, first we need to provide an upper bound on the number of potential outliers. Generalized ESD then performs Grubb’s test for the provided number of potential anomalies. In each iteration, a single point will be removed regardless of whether it’s an outlier. Any point before the last test where G > Gcritical (ie. h(null) rejected, it’s significant) will be considered an outlier. 
The problem with the ESD test on its own is that it assumes a normal data distribution, while real-world data can have a multimodal distribution. To circumvent this, Twitter proposes Seasonal ESD (S-ESD) which employs seasonal decomposition to remove the seasonal and trend components from the time series, leaving the residual component, similar to STL decomposition. Any time series can be decomposed with STL decomposition into a seasonal, trend, and a residual component. The key is that the residual has a unimodal distribution that ESD can test. However, there is still the problem that extreme, spurious anomalies can corrupt the residual component. To fix it, the paper proposes to use the median to represent the “stable” trend, instead of the trend found by means of STL decomposition. Finally, for data sets that have a high percentage of anomalies, the research papers propose Seasonal Hybrid ESD (S-H-ESD) to use the median and Median Absolute Deviate (MAD) instead of the mean and standard deviation to compute the z-score since mean and standard deviation are highly sensitive to large numbers anomalies. Using MAD enables a more consistent measure of the central tendency of a time series with a high percentage of anomalies. 
Now let’s generate a time-series data with a weekly seasonality where weekends have a relatively higher value than weekdays to check if S-H-ESD is able to capture the anomalies present. We introduced 4 anomalies; 2 on weekdays and 2 on weekends. Let’s see how it performs. 
We can see that S-H-ESD was able to identify the same 4 anomalies we introduced when the number of potential anomalies provided was 5. It was able to detect both global as well as local anomalies in the time series data by taking seasonality and trend into account. 
References: 
Note: Full source available at https://github.com/Gritfeat-Solutions/time-series-data-analysis 
Written by 
Written by",Elisha Shrestha,2020-01-08T05:26:17.488Z
Overview Of The Hidden Markov Model (HMM)— What it can do for you in Machine Learning | by Victor Irechukwu | Medium,"The Hidden Markov Model (HMM) is a relatively simple way to model sequential data. It is a statistical Markov model in which the system being modelled is assumed to be a Markov process with unobserved (i.e. hidden) states. You only know observational data and not information about the states. 
In order to uncover the Hidden Markov Model, we need to understand what a Markov Model is in the first place. A Markov model describes the probabilistic relationship between different states. Suppose we have a system with three states: ‘happy’, ‘sad’, and ‘angry’. Over the course of a day (let’s say every hour) a person will be in a particular state, e.g the one that corresponds to their emotion. A person can stay in one state for several hours and then potentially change to a different state. This can be represented as a Markov Model where we think of a sequence of states, and where each state depends on the previous state at a particular instant in time (e.g a certain hour of the day) with some probabilistic measures. 
In Markov models, the state is directly visible to the observer, and therefore the state transition probabilities are the only parameters, while in the hidden Markov model, the state is not directly visible, but the output, dependent on the state, is visible. That is, in real life we will not have the sequence of states our system went through, instead we will have sequences of observations that are dependent on each state. 
A Concrete Example of HMMReference: Hidden_Markov_Model 
Now let’s expatiate more on HMM by using the following example. Consider two friends, Alice and Bob, who live far apart from each other and who talk together daily over the telephone about what they did that day. Bob is only interested in three activities: walking in the park, shopping, and cleaning his apartment. The choice of what to do is determined exclusively by the weather on a given day. Alice has no definite information about the weather, but she knows general trends. Based on what Bob tells her he did each day, Alice tries to guess what the weather must have been like. 
Alice believes that the weather operates as a discrete Markov chain. There are two states, “Rainy” and “Sunny”, but she cannot observe them directly, that is, they are hidden from her. On each day, there is a certain chance that Bob will perform one of the following activities, depending on the weather: “walk”, “shop”, or “clean”. Since Bob tells Alice about his activities, those are the observations. The entire system is that of a hidden Markov model (HMM). 
Alice knows the general weather trends in the area, and what Bob likes to do on average. In other words, the parameters of the HMM are known. They can be represented as follows in Python: 
In this piece of code, start_probability represents Alice's belief about which state the HMM is in when Bob first calls her (all she knows is that it tends to be rainy on average). The particular probability distribution used here is not the equilibrium one, which is (given the transition probabilities) approximately {'Rainy': 0.57, 'Sunny': 0.43}. The transition_probability represents the change of the weather in the underlying Markov chain. In this example, there is only a 30% chance that tomorrow will be sunny if today is rainy. The emission_probability represents how likely Bob is to perform a certain activity on each day. If it is rainy, there is a 50% chance that he is cleaning his apartment; if it is sunny, there is a 60% chance that he is outside for a walk. 
Hidden Markov models are especially known for their application in: 
Reinforcement Learning, an area of machine learning concerned with how software agents ought to take actions in an environment so as to maximise some notion of cumulative reward. 
Speech recognition, an inter-disciplinary sub-field of computational linguistics that develops methodologies and technologies that enables the recognition and translation of spoken language into text by computers. 
Pattern recognition, an automated recognition of patterns and regularities in data in machine learning. 
Other applications include Analysis of biological sequences (e.g. proteins and DNA), Happy Customer Insights, Handwriting recognition, Fraud Detection etc. 
If there’s need for you to apply a probabilistic approach to a machine learning problem that typically follows a sequential state, you can always feel free to explore the Hidden Markov Model. 
See https://hmmlearn.readthedocs.io/en/latest/tutorial.html for insights on Python implementation. There are other implemented HMM libraries available in other languages. 
That’s it for now. I hope you learned one thing or two about the Hidden Markov Model. For more information about this statistical model and mathematical description, please feel free to go through the articles on my reference. 
REFERENCE 
http://www.math.chalmers.se/~olleh/Markov_Karlsson.pdf 
Hidden Markov Model 
Hidden Markov Models Simplified 
Understanding Hidden Markov Models 
From “What is a Markov Model” to “Here is how Markov Models Work” 
Written by 
Written by",Victor Irechukwu,2018-12-17T16:15:21.437Z
Topic Modeling Open Source Tool. A tool built with python and streamlit… | by Bamigbade Opeyemi | Towards AI — Multidisciplinary Science Journal | Medium,"A large amount of data is being generated and collected in different forms every second. Obtaining the right, relevant, and desired information from data is an interesting and difficult task to achieve within a time frame. Technologies and Algorithms have been developed to fetch the information that we are looking for of which some are as simple as the use of a dictionary and search engines. still, the results of these methods need to be narrowed down or summarised with a particular topic. In-text mining, one of the techniques used to extract the right information is Topic Modeling which is a process to automatically identify topics present in a given text and to derive latent patterns. 
Access the tool via Topic Modeling Open Source Tool link 
At the end of this article, you will be able to understand what topic modeling is, existing methods and approaches, some evaluating methods in topic modeling, understand how the “topic modeling open-source tool” has been built, deployment to Heroku, utilization of the tool and probably contribute to the open-source project. 
Topic modeling methods and algorithms are different from the use of rule-based text mining which uses keywords in a dictionary or regular expressions in search techniques but rather an unsupervised approach to finding and observing a bunch of words known as topics in large clusters of text. with topic modeling, we can easily understand, organize, and summarize a large collection of textual information. These bunch of words (topics) and patterns are hidden in across document but get discovered and noticed after training a topic model on it. 
The assumptions of topic models are that each document consist of a mixture of topics and each topic consists of a collection of words. which means the end goal is to ascertain the assumptions which in turn lead to the discovery of the latent bunch of words (topics). 
With Topic Modelling, we can do the following: 
There are many algorithms and methods for building and training a topic model of which some are Latent Semantic Analysis or Indexing (LSA), Probabilistic Latent Semantic Analysts (PLSA), Latent Dirichlet Allocation (LSA), Hierarchical Dirichlet Process (HDP) and others. Among all these, the most common and popular is LDA which is also the first algorithm that has been implemented in this open-source tool. Others will be included soonest. 
LDA is a probabilistic topic modeling. i.e, it uses conditional probabilities to discover the Latent topic structure. It typically the choice here because it generalizes to new documents easily. you can read more on this via the references at the end of this article. 
Evaluation of unsupervised machine learning as always been challenging since there is no ground truth. Topic modeling, which is also an example of this category of machine learning is not an exception. Nevertheless, we still need to make sense of the model output and determine if to keep it or not. The common approach to evaluating a topic model include but not limited to 
As seen in (Wallach et al., 2009), LDA is typically evaluated by either measuring performance on some secondary task, such as document classification or information retrieval, or by estimating the probability of unseen held-out documents given some training documents. A better model will give rise to a higher probability of held-out documents, on average. 
Data science is an iterative process that could involve lots of repetitive steps with the aim to get the best out of a given data. Some tasks even follow the same modeling pattern with just little changes. this can be tiring sometimes especially when you have to code all-action points in the process. With this open-source tool, the repetitive process in topic modeling can be done at the click of a button or selection of an action bar without the need to do any coding. The topic model can also be evaluated with visualization which is a better way of communicating insights from data. With this tool, reports can be generated and downloaded after modeling and evaluations have been done. 
The project folders, modules, and scripts have been arranged in such a way to ease contribution and emphasize on clean coding best practices. 
A new Python environment can be created on the local PC with all necessary packages, frameworks, and libraries installed with the right version. the libraries can found in the requirements.txt file 
Each script is written to contain codes that handle or work together to achieve a common goal. The following are a high-level explanation for each script. for more details, see Gensim library documentation or google search terms 
Streamlit is an open-source app framework that provides a relatively easy way for data scientists and machine learning engineers to create beautiful, performant apps in only a few hours! It combines three ideals around embracing Python scripting, weave in interactions, and instant deployment. With streamlit, coupling the backend with the frontend wasn't a difficult task but keeping track of user session state was a bit challenging due to the way streamlit was built to run top-down at every little alteration made. The session state keeping for the project is explained in the next sub-heading. We divided the user interface into the side panel and main panel whereby side panel handles the majority of the alterations needed and the major panel displays the effect of each side panel alteration and little main panel action buttons. 
The headers in the project interface designed with HTML but rendered as markdown with streamlit. These functions are invoked before any major task in the app logic. 
In the app.py, parameters are collected from the user just as seen in the code snippet below: 
One major problem using streamlit is keeping the session state. This app is designed to accept changes whenever the user needs to perform a task and not just the predefined method. Streamlit provides a caching mechanism that allows apps to stay performant even when loading data from the web, manipulating large datasets, or performing expensive computations. This can be done by the use of the “@st.cache” function decorator it tells Streamlit that whenever the function is called it needs to check a few things: 
The problem with caching here is that we are working at the class level and not function level, therefore, we need a mechanism that can operate at the class level. This led to the creation and use of the session state class just as described in the script below. The original code snippet adopted and modified was from this Git gist: https://gist.github.com/tvst/faf057abbedaccaa70b48216a1866cdd 
With the session state class, we can keep track of variables states and value for every alteration made by the user thereby resulting in expected outputs. Any variable that gets updated by upon each rerun of the script is added to session state variables just as described in the code snippet below: 
There are two files that can download after the evaluation of the model. These are: 
The report chart is generated as PDF file, encoded, and decoded in base64 which can be downloaded via a generated link as a return of the function when invoked. 
The App is hosted on Heroku which have explained a couple of time in previous articles. links to these articles can be found in the reference section of this article. The only difference is the addition of the “NLTK.txt” requirement file which was used to handle NLTK dependencies. 
Do find time to contribute to this tool, check out my other articles, and further readings in the reference section. Kindly remember to follow me so as to get notified of my publications. 
Connect with me on Twitter and LinkedIn 
Project repository: 
References: 
Towards AI publishes the best of tech, science, and engineering. Subscribe with us to receive our newsletter right on your inbox. For sponsorship opportunities, please email us at pub@towardsai.net Take a look 
Written by 
Written by",Bamigbade Opeyemi,2020-08-26T06:41:54.841Z
Calculate Similarity — the most relevant Metrics in a Nutshell | by Marvin Lüthe | Towards Data Science,"Many data science techniques are based on measuring similarity and dissimilarity between objects. For example, K-Nearest-Neighbors uses similarity to classify new data objects. In Unsupervised Learning, K-Means is a clustering method which uses Euclidean distance to compute the distance between the cluster centroids and it’s assigned data points. Recommendation engines use neighborhood based collaborative filtering methods which identify an individual’s neighbor based on the similarity/dissimilarity to the other users. 
In this blog post I will take a look at the most relevant similarity metrics in practice. Measuring similarity between objects can be performed in a number of ways. 
Generally we can divide similarity metrics into two different groups: 
2. Distance Based Metrics: 
Similarity based methods determine the most similar objects with the highest values as it implies they live in closer neighborhoods. 
Correlation is a technique for investigating the relationship between two quantitative, continuous variables, for example, age and blood pressure. Pearson’s correlation coefficient is a measure related to the strength and direction of a linear relationship. We calculate this metric for the vectors x and y in the following way: 
where 
The Pearson’s correlation can take a range of values from -1 to +1. Only having an increase or decrease that are directly related will not lead to a Pearson’s correlation of 1 or -1. 
Implementation in Python: 
Pearsons correlation: 0.810 
Spearman’s correlation is what is known as non-parametric statistic, which is a statistic who’s distribution does not depend on parameters (statistics that follow normal distributions or binomial distributions are examples of parametric statistics). Very often, non-parametric statistics rank the data instead of taking the originial values. This is true for Spearman’s correlation coefficient, which is calculated similarly to Pearson’s correlation. The difference between these metrics is that Spearman’s correlation uses the rank of each value. 
To calculate Spearman’s correlation we first need to map each of our data to ranked data values: 
If the raw data are [0, -5, 4, 7], the ranked values will be [2, 1, 3, 4]. We can calculate Spearman’s correlation in the following way: 
where 
Spearman’s correlation benchmarks monotonic relationships, therefore it can have perfect relationships that are not linear. It can take a range of values from -1 to +1. The following plot clarifies the difference between Pearson’s and Spearman’s correlation. 
For data exploration, I recommend calculating both Pearson’s and Spearman’s correlation. The comparison of both can result in interesting findings. If S>P (as shown above), it means that we have a monotonic relationship, not a linear relationship. Since linearity simplifies the process of fitting a regression algorithm to the dataset, we might want to modify the non-linear, monotonic data using log-transformation to appear linear. 
Implementation in Python: 
Spearmans correlation: 0.836 
Kendall’s tau is quite similar to Spearman’s correlation coefficient. Both of these measures are non-parametric measures of a relationship. Specifically both Spearman and Kendall’s coefficients are calculated based on ranking data and not the raw data. 
Similar to Pearson’s and Spearman’s correlation, Kendall’s Tau is always between -1 and +1 , where -1 suggests a strong, negative relationship between two variables and 1 suggests a strong, positive relationship between two variables. 
Although Spearman’s and Kendall’s measures are very similar, there are statistical advantages to choosing Kendall’s measure in that Kendall’s Tau has smaller variability when using larger sample sizes. However, Spearman’s measure is more computationally efficient, as Kendall’s Tau is O(n²) and Spearman’s correlation is O(nLog(n)). 
The first step is to rank the data to ranks: 
Then we calculate Kendall’s Tau as: 
where sgn takes the sign associated with the differences in the ranked values. We could write 
as follows: 
The possible results of 
are -1, 0, +1. These are summed to give an idea of the proportion of times the ranks of x and y are pointed in the same direction. 
Implementation in Python 
Kendalls tau: 0.695 
The cosine similarity calculates the cosine of the angle between two vectors. In order to calculate the cosine similarity we use the following formula: 
Recall the cosine function: on the left the red vectors point at different angles and the graph on the right shows the resulting function. 
Accordingly, the cosine similarity can take on values between -1 and +1. If the vectors point in the exact same direction, the cosine similarity is +1. If the vectors point in opposite directions, the cosine similarity is -1. 
The cosine similarity is very popular in text analysis. It is used to determine how similar documents are to one another irrespective of their size. The TF-IDF text analysis technique helps converting the documents into vectors where each value in the vector corresponds to the TF-IDF score of a word in the document. Each word has its own axis, the cosine similarity then determines how similar the documents are. 
Implementation in Python 
We need to reshape the vectors x and y using .reshape(1, -1) to compute the cosine similarity for a single sample. 
Cosine similarity: 0.773 
Cosine similarity is for comparing two real-valued vectors, but Jaccard similarity is for comparing two binary vectors (sets). 
In set theory it is often helpful to see a visualization of the formula: 
We can see that the Jaccard similarity divides the size of the intersection by the size of the union of the sample sets. 
Both Cosine similarity and Jaccard similarity are common metrics for calculating text similarity. Calculating the Jaccard similarity is computationally more expensive as it matches all the terms of one document to another document. The Jaccard similarity turns out to be useful by detecting duplicates. 
Implementation in Python 
Jaccard similarity: 0.500 
Distance based methods prioritize objects with the lowest values to detect similarity amongst them. 
The Euclidean distance is a straight-line distance between two vectors. 
For the two vectors x and y, this can be computed as follows: 
Compared to the Cosine and Jaccard similarity, Euclidean distance is not used very often in the context of NLP applications. It is appropriate for continuous numerical variables. Euclidean distance is not scale invariant, therefore scaling the data prior to computing the distance is recommended. Additionally, Euclidean distance multiplies the effect of redundant information in the dataset. If I had five variables which are heavily correlated and we take all five variables as input, then we would weight this redundancy effect by five. 
Implementation in Python 
Euclidean distance: 3.273 
Different from Euclidean distance is the Manhattan distance, also called ‘cityblock’, distance from one vector to another. You can imagine this metric as a way to compute the distance between two points when you are not able to go through buildings. 
We calculate the Manhattan distance as follows: 
The green line gives you the Euclidean distance, while the purple line gives you the Manhattan distance. 
In many ML applications Euclidean distance is the metric of choice. However, for high dimensional data Manhattan distance is preferable as it yields more robust results. 
Implementation in Python 
Manhattan distance: 10.468 
This blog post provided an overview of the most relevant similarity metrics used in practice. There is no simple if-then flowchart for choosing the appropriate similarity metric. We will first need to understand and study the data. Then, it is always a case by case decision to find the right way to quantify similarity for a given data science problem. 
https://bib.dbvis.de/uploadedFiles/155.pdf 
http://text2vec.org/similarity.html 
http://www.ijaret.org/2.10/COMPARATIVE%20ANALYSIS%20OF%20JACCARD%20COEFFICIENT%20AND%20COSINE%20SIMILARITY%20FOR%20WEB%20DOCUMENT%20SIMILARITY%20MEASURE.pdf 
Hands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look 
Written by 
Written by",Marvin Lüthe,2019-11-18T16:56:09.438Z
"3 Powerful Application of AI in Cyber Security | by Praveen Pareek | Data Driven Investor | Aug, 2020 | Medium","The year 2020 is overcome with the COVID-19. But the virus isn’t the only threat to our security. 
2020 is also set to revolutionize the world with advancements that will shape the future of lives and businesses, alike. 
We now have 5G and IoT to Artificial Intelligence, Cloud technology, and Machine Learning. 
These technologies will become an integral part of our daily lives in creating efficiency, saving time, reducing costs, and unlocking new opportunities. 
The more the world transforms towards a digital future, the higher the rise in threats of Cyber Attacks. 
Modern technology is set to increase the amount of data we create online, and protecting this data will be one of the defining arcs of this decade. 
From system security to network security, businesses will face challenges in optimizing their cybersecurity to prevent malicious attacks from being successful. 
It is hard to prevent malicious attacks because these technologies are new, vulnerabilities are less known, scalability harder due to a lack of familiarity, thereby making all of these ambiguities an excellent target for bad actors to exploit. 
So let’s take a look at some of these technologies, modern regulations in place, and what businesses can do to combat this threat with regards to their cybersecurity. 
One of the biggest concerns in cybersecurity is the sheer volume of data organizations have to tackle daily. 
This is probably one of the reasons why Gmail chose the AI route to block 100 million extra spam messages every day. 
CTO of Seedcamp, David Mytton’s explanation of how AI is disrupting the cybersecurity space is on-point. 
In CIO Mytton says, “As more and more systems become instrumented, the problem shifts from knowing that ‘something’ has happened. This is to highlight that “something unusual” has happened.” 
Instrumented system refers to as — who has logged in and when? What was downloaded and when? What was accessed and when? 
AI intelligence, in conjunction with threat intelligence, can detect new security issues and resolve them, offering a threat detection rate of 95% as opposed to traditional antivirus software, “where the detection rate is only about 90%, meaning 10% of malicious samples are being missed.” 
The hope is that these systems will minimize false alarms and insubstantial issues, leaving a much smaller set of ‘real’ threats to review and address. 
One of the most innovative and effective applications of AI in thwarting security breaches is the use of bio-metric authentication. 
Tech giant, Apple, uses this method — commonly known as “Face ID.” 
The powerful face ID technology combines built-in infra-red sensors and neural engines to recognize the user. 
If Apple’s claims are to be believed, the benefits are vast, with “only a one-in-a-million chance of fooling the AI to open a device with another face.” 
Apart from providing additional layers of security, AI is making teams more efficient, according to 70% of security professionals, and it is eliminating as much as 55% of employees’ manual tasks. 
With the additional layer of security and helping the team pivot their energies towards solving more important tasks, AI also powers productivity. Helping with productivity reduces overall stress levels for everyone involved. 
Though technology is evolving rapidly in the digital landscape, cybersecurity experts will have to deal with phishing attacks. 
These attacks are often targeted to penetrate a network or infect the users of the network itself. 
Though phishing is a generally well-known attack, hackers and malicious actors are becoming smarter (thanks to technological evolution), and their attacks are becoming more and more sophisticated. 
So like 2019, security measures against Phishing will also be necessary for 2020 as well. 
Exploits such as email phishing are hard to eliminate as a problem since you can’t really disable emails altogether, and hackers know that. 
Phishing is also an easier way to get inside a network as opposed to other modern hacks, such as exploiting a zero-day vulnerability. 
Companies today have to always beware of these phishing emails since they only take one wrong click by someone with access to admin credentials on a network to open a backdoor that allows malicious actors to get in, take control, and corrupt the company’s network. 
The problem that most experts face is that there is no one solution to stop phishing attacks from succeeding. 
At the end of the day, these attacks can boil down to a reckless click, human error, and lack of knowledge. 
Blocking downloads without confirmation, assessing the email before opening any links directly, and using anti-malware and anti-spyware software to block or monitor potential malicious activities could help you mitigate the harm but not necessarily prevent it entirely. 
As the Machine Learning and Artificial Intelligence market grows, their application in different business operations, systems, and infrastructure will be a challenge to overcome. 
These technologies are incredibly resource-intensive and will require significant efforts to make them secure against potential attacks. 
AI and ML-based devices and software have to be trained with the help of data, and experts will have to keep a keen eye on the kind of data that is being used. 
Data duping to corrupt the learning process of the Machine Learning algorithm can be injected to hamper the training process. 
This can lead to the algorithm working seemingly fine but producing wrong results, which could, in the case of analytical products and applications, cost businesses millions of dollars. 
How experts monitor and analyze the data will play a crucial part in the future of A.I and ML since the data set being used can be a security vulnerability that will have to be dealt with. 
In the current climate, this is a less severe issue due to A.I and ML operating in specialized environments, but once businesses begin to scale these processes, there are bound to be vulnerabilities. 
When processes such as threat analysis and data review become completely automated, malicious actors could exploit these processes to misguide companies and manipulate results without any obviously apparent problems. 
Furthermore, the technology itself can be used to discover new vulnerabilities, breakthrough security measures, and tools, and penetrate systems through the same algorithm that is being used to protect networks. 
The future is digital, there is no denying it but simply focusing on the possible benefits isn’t going to cut it. 
For businesses, it is crucial to realize their responsibility towards consumers and take the necessary steps to ensure data protection and other cybersecurity avenues. 
It is also vital for them to focus strongly on the security of their own platforms, services, and products to ensure that the adoption of modern technology drives positive results. 
The technologies we’ve talked about have great potential, but the journey into the world of technology requires avid preparation to ensure security and safety. 
Businesses today have to invest more into optimizing their security, create new strategies, implement new infrastructure, and leverage modern tools to ensure that they are ahead of the and ready to fight any cyber-threats that may come their way. 
If you enjoyed this article, it would really help if you hit recommend below!Follow me on Twitter, LinkedIn, and Medium 
Read my all posts/articles here: Praveen Pareek 
In each issue we share the best stories from the Data-Driven Investor's expert community. Take a look 
Written by 
Written by",Praveen Pareek,2020-09-07T03:06:16.270Z
Optimization Algorithms: Part 1. This article covers the content… | by Parveen Khurana | Medium,"This article covers the content discussed in the Optimization Algorithms module of the Deep Learning course and all the images are taken from the same module. 
Limitation of Gradient Descent: 
We have seen the Gradient Descent update rule so far and even when we are doing backpropagation, at the core of it is this update rule where we take a weight and update its value based on the partial derivative of the loss function with respect to this weight: 
The question that we look at now is that How do we compute this gradient or in other words, what data to use when computing this gradient: should we use the entire data or should we use a small part of the data or should we use one data point. This decision will result in Batch Gradient Descent, Mini Batch Gradient Descent or Stochastic Gradient Descent. 
The other question we look at is: once we have computed the gradients, how to use it, meaning how to use the gradient when updating the parameter value. Instead of just subtracting the gradient value, shall we look at all the past gradients and the current gradient and then take a decision based on that or shall we just take into consideration only the current gradient value? 
The code for the Gradient Descent is: 
We have randomly initialized the parameters w, b(blue point in the below image represent the parameters w, b): 
The red dot in the above image is the loss value corresponding to this particular value of w, b(which is randomly initialized). 
Now we will run the gradient descent algorithm and see how the loss value changes: 
After a few iterations, the situation is like the below: 
From above, we can say that in every successive iteration/epoch, w and b are changing very little and when we are moving from one value of w, b to another value of w, b, they are almost connected to each other, we are not taking larger steps. So, in this flat region, the values are changing very slowly and when we come at the slope region(in the valley), w and b values are changing very drastically 
And again as we enter the flat region, we can see very small changes in w, b values. 
At the top red portion(light red portion) where we started off with this algorithm, the loss surface had a very gentle slope, it was almost flat and once we are entering this blue region(there is a valley at the bottom), so there was a sudden slope, the slope was very high and at that point, we are seeing larger changes in w, b and again once we reach the bottom portion, that portion is again flat, there also the values are not getting updated very fast, the moment is very slow and two successive values of w, b are very close to each other. 
So, in the gentle areas, the moment is very slow and in the steep areas, the moment is good. 
A deeper look into the limitation of Gradient Descent: 
Let’s take a function and we will see what does the slope of the function means and how it is related to derivative and how does it explain the behavior(in certain regions the moment was steep and in certain regions the moment was fast) that we discussed above 
The derivative at any point is the change in the y value(in yellow in the above image) divided by the change in the x value(blue underlined in the above image). So, it tells how does the ‘y’ changes when ‘x’ changes. 
In the region(in elliptical boundary in above image), the slope is very steep, if we put a ball at the upper point, it will come down very fast, here for a change of value of 1 in x, the value of y is changing by 3, the derivative is larger whereas in the elliptical region in the below image, the slope is gentle. 
and a change of 1 in the x corresponds to a change of 1 in y, so the derivative is smaller as compared to the above region where the slope is steep. 
So, we see that if the slope is very steep the derivative is high and if the slope is gentle the derivative is small. 
The value of the parameters are changing in the proportion of the derivatives so, in the regions where the derivative is small, ‘w’ is also going to change by a small quantity and in the regions where the derivative is large, the parameters are going to change by a large quantity. 
Now taking the above scenario, let’s consider when the random initialization corresponds to the green marked point in the below image: 
The entire region is flat from the initialization point up to(underlined in pink in below image) the start of the red line, so it will take a lot of time to travel up to this red point i.e the start of the valley and then once it goes into the valley, it will start moving faster. 
It might take 100–200 epochs just to reach the edge of the valley and then jump into the valley and that’s not acceptable as we have to run many epochs just to make a small change in the loss value which is not a favorable situation. 
We should try to change gradient descent in a way so that on the gentle slope surfaces, the moment becomes faster. 
This bothers us as we are initializing the parameters randomly and if so happens that we have initialized these parameters at a place where the surface is very flat, then we need to have many many epochs just to come out of that flat surface and enter into a slightly steep region and from there on we can see slight improvement in the loss function. 
Introducing Contour Maps: 
If we see the loss/error surface for the above case from the front and plot it in 2D, we will get: 
If we make two horizontal slices on this surface, each horizontal slice would result in a plane cutting through the 3D surface and wherever the plane is intersecting with the 3D surface or the portion it is cutting through, around that entire perimeter on the 3D surface, the loss function/value is the same. 
Now after making these 2 slices, if we look at it from the top, it would appear to be: 
The ellipse with yellow in it would correspond to the first slice and the other ellipse with green in it would correspond to the second slice through the surface. 
The loss value is the same at the entire boundary of the ellipse. So, the first point of reading contour maps is that whenever we see a boundary that means the entire boundary has the same value for the loss function. 
Now we have shown the distance between the two ellipsed from two ends. One of the distance is small compared to the other one. Reason for this is that the portion circled in the below image 
corresponds to the difference in the green in the below image and there the surface is very steep. 
So, whenever the slope is very steep between two contours, then the distance between the two contours would be small. 
Similarly the region in blue in the below image 
corresponds to the difference in blue in the below image: 
and the slope was a bit gentle here as compared to the red slope and therefore, whenever the slope is gentle when going from one contour to the other contour, then the distance between two contours is going to be high. 
Visualizing Gradient Descent on a 2D Contour Map: 
We have the 3D surface as: 
And the same in 2D looks like: 
The moment of the loss values on this 2D would look like as: 
Since we started off in the flat region, the values would move very slowly along this region. 
Now it has reached the edge of the valley where the slope is steep, we can see larger moments: 
Momentum Based Gradient Descent: 
If the gradients are continuously pointing in the same direction, then it would be good if we take large steps(update) in that direction. 
It’s like taking an average of all the gradients that we have computed so far instead of just relying on the current gradient and the average does not give equal weightage to all the gradients, it’s exponentially decaying weightage to all the previous gradients. 
Code for Momentum Based GD: 
Visualizing Momentum Based GD: 
A disadvantage of Momentum based GD: 
Let’s plot the 2D error surface for another scenario: 
The intuition behind Nesterov Accelerated Gradient Descent: 
We want an algorithm that converges faster than Vanilla GD but does not makes a lot of u-turns as Momentum GD. So, we discuss Nesterov Accelerated Gradient Descent in which the no. of oscillations(u-turns) are reduced compared to Momentum based GD. 
In Momentum GD, the update would look like: 
In the above equation, we have one term for history(underlined in red) and one term for the current gradient(underlined in blue). 
We know that in any case, we need to move by the history component, then why not move by that much first and then compute the derivative at that point and we should then move in the direction of that gradient. 
Code for NAG: 
In case of Momentum GD, we have: 
To start with, both Momentum GD and NAG would have the same curve/line and only in the valley having a steep slope, we would see the effect of NAG as it would take shorter u-turns. 
Written by 
Written by",Parveen Khurana,2020-01-26T17:17:51.232Z
"NLP and Deep Learning All-in-One Part II: Word2vec, GloVe, and fastText | by Bruce Yang | Medium","Written by 
Written by",Bruce Yang,2020-08-18T05:34:44.784Z
Machine Learning | Anomaly detection | by Pierre Portal | Medium,"In this article I will explain how a basic anomaly detection algorithm works. For educational purpose I will use a simple n dimensional training dataset : 
There is no correlation between the features x 1,…,x n here. In a future post I will show how to deal with correlations and multivariate gaussian distributions, that will involve matrix inverses. I will also give an example of application in python. 
We can create a probabilistic model that fit our training data. In this simple example we can use a gaussian (or normal) distribution because it is really often that a dataset is normally distributed. 
To do so, we need the following parameters : the mean and the variance corresponding to the distribution of our training dataset. We can calculate the mean (mu) and the variance (sigma squared) of each feature : 
Now that we have a model that can fit the training data, we can use it to determine the probability of happening for new unseen test data. Our goal is to classify new data point with low value of p(x test) as anomalous. p(x test) is the product of each feature’s probability given our model’s parameters : 
By defining a threshold we can give more or less flexibility to classify new data as normal (y=0) or anomalous (y=1) : 
So our algorithm looks like : 
Written by 
Written by",Pierre Portal,2020-04-08T10:21:30.095Z
How I Prepared For The AWS Certified Machine Learning Exam (July 2020) | by ___ | Towards AI — Multidisciplinary Science Journal | Medium,"In this article, I will describe how I prepared for the AWS Certified Machine Learning — Specialty exam. I will share the materials I used and my thoughts about their usefulness. 
Note that I sat for the exam in July 2020. At that time, the exam guide was at version 1.2 MLS-C01. 
The first thing I did was to read the exam guide to figure out things like examinable content, exam format, passing marks, etc. This document was very helpful in identifying gaps in my knowledge. 
I was curious to find out how I would fare in the exam given my current knowledge of the various AWS services. So I purchased the practice exam to find out. This was a bad decision in hindsight. 
The practice exam will tell you your final score and its breakdown by domain but it will not give you feedback on questions you got wrong. Furthermore, you cannot redo or review it at a later date. 
You will need to look elsewhere if you intend to use the practice exam as a learning tool. 
I was counting on O’Reilly’s online learning platform to help me get ready for the exam because it has a solution to prepare for the exam which consists of a practice exam and a 6-hour video course. Imagine having only to spend 6 hours to prepare for certification! 
It turns out that the video course was more of a guideline on what to study. It gives an excellent overview of the relevant AWS services you should know and pointers to resources you are expected to study at your own time. Watching the video course alone will not prepare you adequately for the exam! 
The thing I like most about this solution is that the practice exams come with feedback and you have infinite attempts. 
I only wish that the solution’s provider kept it current. The video course was based on version 1.1 of the exam guide. While I don’t expect a redo every time the exam guide is updated, it would be useful if there is a note highlighting this and how it affects the relevance of the topics covered in the course. 
I decided to work through the “Machine Learning Path: Data Scientist” learning path on the AWS Training & Certification site. The path ends with the certification exam and it is hosted on AWS so I was hoping that the content is kept current in line with the certification requirements. 
This is what the path looked like: 
I went through all the modules that could be done digitally. 
This path will give you a thorough grounding on how to do machine learning on AWS and do it securely and effectively. I wish I had done it when I got started with AWS. It would have saved a lot of time from building and troubleshooting sub-par solutions. 
In terms of exam preparation, I found the “Exam Readiness: AWS Certified Machine Learning — Specialty” module to be invaluable. It covers exam techniques you can apply to answer questions as well as a practice exam (which comes with answer feedback and unlimited attempts). 
What I noticed while attempting the practice exams is that I tend to struggle with questions that require knowledge of specific AWS service e.g. when to use service X over Y. This was because I use only a very small subset of the AWS ML-related services and an even smaller subset of the broader services in the AWS ecosystem in my day job as a data scientist. 
I found the Learning Library in the AWS Certification & Training library a good resource to quickly gain a basic familiarity of any AWS service. All I had to do was to search for the service and watch its introductory video. 
I spent the remaining study time I had browsing the AWS Machine Learning blog. 
I mainly focused on case study posts to understand how one can mix and match various AWS services to achieve a given outcome. This is a point worth emphasizing because according to the exam guide: 
This exam validates an examinee’s ability to build, train, tune, and deploy machine learning (ML) models using the AWS Cloud 
The case studies are a good way to broaden your knowledge of ML applications in other domains which are important not only for professional growth but for the exam too since the exam is not restricted to any particular use-case. 
I will conclude this article by offering a study plan that I think is good enough to help people get through the exam: 
I hope this has been useful. Let me know in the comments if you have any questions. 
Towards AI publishes the best of tech, science, and engineering. Subscribe with us to receive our newsletter right on your inbox. For sponsorship opportunities, please email us at pub@towardsai.net Take a look 
Written by 
Written by",___,2020-07-10T18:01:00.907Z
The Business Value of Cybersecurity | by JC Gaillard | Security Transformation Leadership | Medium,"Cybersecurity is rising as a key issue on the radar of virtually all organisations. According to a recent AT Kearney report, cyber-attacks have been topping executives’ lists of business risks for three straight years. This concern is also driven by security and privacy becoming increasingly valued by customers, and by regulators stepping into the topic (GDPR in Europe, California Consumer Privacy Act of 2018). 
Beyond this, it is now becoming crystal clear that cybersecurity — beyond good practice and good ethics — is quite simply good business. As a recent Cisco study made clear, cybersecurity will help fuel (and protect) an estimated $5.3trillion in private sector digital Value at Stake in the next 10 years. This is the kind of numbers boards cannot afford to overlook. 
Tangible estimates like this one, however, are painfully rare in the cyber security space. Indeed, concepts relating to cybersecurity are both multi-facetted and very elusive — making them notoriously hard to measure. Furthermore, good cybersecurity is defined by the absence of breaches or losses. Observing what is not happening is a challenging — if interesting — endeavour. 
A stringent example of this measurement problem can be found in a recent BCG research on Total Societal Impact. To their credit, cybersecurity is mentioned fairly extensively throughout the report as a key component of a firms’ ESG (Environmental, Social & Governance) strategy — although not consistently across industry sectors. 
The issue arises when it comes to quantifying that intuition. The BCG, for example, reports finding a significant link between “Securing business and personal data” and a firm’s valuation. Looking into the appendix of the report, the problem lies in the fact that this concept seems to be operationalized through a series of somewhat vague dummy (0/1) variables. Examples of such metrics include whether “measures to ensure customer security” have been taken, or whether an information security management system has been implemented. 
This is not only overly-simplistic — hiding key nuances in levels of cybersecurity maturity across firms — but it also encourages “tick-in-the-box” approaches to cybersecurity which have plagued the field for ages. Tellingly, no quantitative results are actually presented for cybersecurity in the report. 
This lack of details around the quantification of the tangible value of following cybersecurity best practices is a problem. In fact, we believe it is an important reason why the issue is still shifting in and out of most boards’ radars. Gut feeling alone does not make for a strong-enough case: Top executives are increasingly asking “Show me the data”. 
Beyond the fact that measuring success in the cybersecurity is very hard, another issue is the stringent lack of meaningful data. 
This is a really big problem in the field of cyber insurance, for example, which struggles to fit its traditional actuarial models around the scarce data they can get a hold of. The reason for that is quite simple: most organizations are still very reluctant to share what they perceive as highly sensitive cybersecurity data (assuming they even have them to start with). 
We also talked about this problem in the context of training defensive AI for cybersecurity, but this scarcity of reliable InfoSec data hinders generally much-needed research and results. 
Being able to show key stakeholders in business terms what exactly is the tangible value-added of cybersecurity will be key in finally anchoring the topic at the right level of organizations. 
Money — and data ­– talk. And boards usually listen. But we’re not there yet and cybersecurity looks definitely like a promising path for data-driven research. 
Click here to join our newsletter for more Cyber Security Leadership insights. 
Contact Corix Partners to find out more about developing a successful Cyber Security Practice for your business. 
Corix Partners is a Boutique Management Consultancy Firm, focused on assisting CIOs and other C-level executives in resolving Cyber Security Strategy, Organisation & Governance challenges. 
This article was written in collaboration with Vincent Viers. 
Delivering a challenge and an alternative view on common practices in the CyberSecurity space to help the Industry move forward Take a look 
Written by 
Written by",JC Gaillard,2019-02-10T08:20:57.613Z
Accelerate Computing of Anomaly Detection using GPU Platform | by Manish Harsh | Medium,"Anomaly detection is a technique often associated with machine learning (ML) that is used to identify Outliers and Data points that do not agree with a pattern. Few common anomalies: 
Point anomaly is a single instance of data that is an outlier. Like, purchase amount in credit card fraud 
Contextual anomalies do not make sense within the time-frame they are in. Like, individual spending during holiday time. 
Collective anomaly is a set of data that points to an issue Like, someone copying large amounts of data is a cybersecurity flag. 
Due to the existence of these anomalies, it is crucial to develop mechanisms and technologies that can be used to detect issues throughout various industries. 
Statistical methods make up the simplest (non-machine learning) approach to anomaly detection. In this case, the machine flags data points that deviate from common statistical properties of a distribution, such as mean, median, mode, or range. Often, a moving average is used to highlight long-term changes while purposefully diminishing short-term ones. One drawback of this method is the often-imprecise boundary between values considered normal and abnormal. 
Density-based anomaly detection attempts to address this drawback by involving machine learning. It is based on the assumption that normal data points occur around a dense neighborhood with outliers being distant. This is known as the k-nearest neighbors algorithm, with the set of data points evaluated using Euclidean distance or a similar metric. 
Automatic Clustering is a very popular machine learning anomaly detection approaches in network data. The assumption that must be made is that similar data points have a tendency to belong in similar clusters. Instances outside these clusters are the anomalies, as determined by centroid distance. Finally, a support vector machine (SVM) is often a supervised learning tool but recently has been harnessed for machine learning and anomaly detection purposes. Specifically, the algorithm learns a soft boundary through a training data set and then locates the outliers from the official data that are outside learned bounds. Further, both clustering and the use of an SVM are closely tied to the use of a Graphics Processing Unit, or GPU. 
The overall anomaly detection framework can be applied with either a CPU or GPU. However, multiple studies have shown that the GPU is better able to locate outliers in very large data sets using a system of Parallel Algorithms. The framework begins with the raw dataset, which is processed using a feature learning algorithm composed of recurrent neural network (RNN) and autoencoders (AE), both of which are particularly suited to GPU. 
The anomaly detection is composed of both unsupervised learning in the form of multivariate-Gaussian models and supervised learning as logistic regression. Moreover, univariate analysis occurs post-processing to refine the raw dataset. Another advantage of using a GPU for anomaly detection is the fact that it takes less time to train on a GPU than on a CPU, in addition to the data collection rate being faster. Multi-GPU support elements are more effective than both automatic feature creation using a digital library (DL) and manual feature creation at reshaping the model. 
GPUs can be used with anomaly detection techniques to process data in the form of visualization, model training, and inferencing. 
The process of detecting outliers or anomalies using deep learning starts with the data platform. This platform serves as the basis for both the GPU cloud and the artificial intelligence (AI) framework. We can then use algorithms programmed into the GPU cloud to effectively and efficiently detect outlier and form a better model. 
While GPU is often used with deep learning and is highly effective in that role, it can serve a more extensive purpose as related to anomaly detection in the form of acceleration. Specifically, this would start with building GPU databases to ingest the data to be processed. 
The applications of this method are wide-ranging, from use with dashboards and data pipelines to stream processing and building better models faster. Considering the numerous functions of a GPU, we will now move into examining the wide range of network anomaly detection algorithms and machines that have been developed to link with the use of GPUs. 
It is crucial to not only include anomaly detection models that are compatible with GPUs, but also to explore the ways in which a machine learning approach to detecting anomalies integrates with GPUs in a beneficial way. Machine learning involves algorithms that are able to adapt to relevant changes to remain effective. Two of the mechanisms this involves are sophisticated pattern recognition and making intelligent decisions. Specifically, this pattern recognition consists of determining the type and quantity of relationships along with simply noting their existence. This occurs with not only primary and secondary variables but instead with every possible interaction in the pattern. Along with the capability to note irrelevant data and rank variables by importance, algorithms can make decisions with or without the aid of a user. 
Another key component of machine learning anomaly detection algorithms is the ability to self modify, or better typify the data without input from a user. This saves time and also increases the accuracy of programs. Additionally, this modification occurs in multiple iterations, with each cycle producing a more refined model. The purpose of this structure is to find the best fit to the data and focus on high accuracy by isolating or eliminating anomalies. Power and flexibility are two more major advantages of machine learning and anomaly detection in the cloud. This format allows you to have all the power you need when you need it, as large volumes of data can be run at a time. It is also flexible when it comes to working with modeling and data mining projects of different types and sizes. 
When anomaly detection occurs using a GPU, the cloud, and machine learning techniques, there is no need for a set infrastructure that it must work within. This means that high-performance computing power is available at all times. Additionally, this setup is highly cost-effective and feasible for small businesses with large data sets. This flexibility lends itself to the use of support vector machines (SVMs), which are widely thought to be the largest recent advancement in machine learning as they have many powerful applications. Kernel-based learning can be used to transform a data set into a higher dimension, meaning that the space contains all possible combinations of predictive values. While SVMs are currently difficult to use, due to the amount of power they require, they are the future of the anomaly detection industry, particularly due to their close link with GPUs. 
Machine learning algorithms are crucial to use with GPUs; when a learning algorithm is being developed by choosing specific features, the importance of real-number evaluation comes into play. Making these decisions is made much simpler when we have a method to evaluate the learning algorithm before developing the detection system. We assume some anomalous and non-anomalous data examples to form the training and testing sets and use cross-validation or classification to choose the relevant parameters. 
To develop algorithms associated with anomaly detection, we must first choose the features that form the basis of the code. If the data is non-Gaussian, the preferred method is to transform it into a form that fits the normal distribution. The method to come up with features is error analysis, which entails running a training set and making adjustments depending on what the program gets wrong. From here, features are chosen by selecting those values that are unusually large or small in the instance of an anomaly. 
In using a multivariate Gaussian (normal) distribution, we generally have a covariance matrix formed by varying two elements, along the primary diagonal. However, varying one element, still along the diagonal, is also an option that is employed at times. A second option with this type of distribution is using a mean matrix, observing how the mean parameter shifts the center of the distribution. Once we choose and fit the parameters, anomalies can be flagged, also to get rid of linearly dependent and duplicate features. This method is favored for automatically locating correspondences between features, but it is also computationally more expensive. 
Be the anomaly and inspire your surrounding system to evolve for better — Manish Harsh :) 
Works Cited 
1. https://www.datascience.com/blog/python-anomaly-detection?hs_amp=true 
2. http://on-demand.gputechconf.com/gtcdc/2017/presentation/dc7111-joshua-patterson-accelerating-cyberthreat-detection-with-gpus.pdf 
3. https://www.semanticscholar.org/paper/Fast-outlier-detection-using-a-GPU-Angiulli-Basta/38c664ae0ceb62586cc8d06eda957c9affd00796 
4. https://www.researchgate.net/publication/303302092_Big_data-driven_optimization_for_mobile_networks_toward_5G 
5. https://www.usenix.org/legacy/event/sysml07/tech/full_papers/ahmed/ahmed_html/sysml07CR_07.html 
6. https://docs.aws.amazon.com/machine-learning/latest/dg/types-of-ml-models.html 
7. https://www.ritchieng.com/machine-learning-anomaly-detection/ 
8. https://yottamine.com/machine-learning-svm 
9. https://arxiv.org/pdf/1803.04311.pdf 
10. https://www.comsoc.org/publications/journals/ieee-tccn/cfp/deep-reinforcement-learning-future-wireless-communication 
11. https://www.researchgate.net/publication/324517832_Machine_Learning_Aided_Orchestration_in_Multi-tenant_Networks 
12. https://arxiv.org/pdf/1712.05929.pdf 
13. https://arxiv.org/abs/1902.00985 
14. https://ngc.nvidia.com/ 
Written by 
Written by",Manish Harsh,2019-09-01T14:36:10.806Z
Topic Modeling and Latent Dirichlet Allocation (LDA) in Python | by Susan Li | Towards Data Science,"Topic modeling is a type of statistical modeling for discovering the abstract “topics” that occur in a collection of documents. Latent Dirichlet Allocation (LDA) is an example of topic model and is used to classify text in a document to a particular topic. It builds a topic per document model and words per topic model, modeled as Dirichlet distributions. 
Here we are going to apply LDA to a set of documents and split them into topics. Let’s get started! 
The data set we’ll use is a list of over one million news headlines published over a period of 15 years and can be downloaded from Kaggle. 
Take a peek of the data. 
1048575 
We will perform the following steps: 
Loading gensim and nltk libraries 
[nltk_data] Downloading package wordnet to[nltk_data] C:\Users\SusanLi\AppData\Roaming\nltk_data…[nltk_data] Package wordnet is already up-to-date! 
True 
Write a function to perform lemmatize and stem preprocessing steps on the data set. 
Select a document to preview after preprocessing. 
original document: 
[‘rain’, ‘helps’, ‘dampen’, ‘bushfires’] 
tokenized and lemmatized document: 
[‘rain’, ‘help’, ‘dampen’, ‘bushfir’] 
It worked! 
Preprocess the headline text, saving the results as ‘processed_docs’ 
Create a dictionary from ‘processed_docs’ containing the number of times a word appears in the training set. 
0 broadcast 
1 communiti 
2 decid 
3 licenc 
4 awar 
5 defam 
6 wit 
7 call 
8 infrastructur 
9 protect 
10 summit 
Gensim filter_extremes 
Filter out tokens that appear in 
Gensim doc2bow 
For each document we create a dictionary reporting how manywords and how many times those words appear. Save this to ‘bow_corpus’, then check our selected document earlier. 
[(76, 1), (112, 1), (483, 1), (3998, 1)] 
Preview Bag Of Words for our sample preprocessed document. 
Word 76 (“bushfir”) appears 1 time. 
Word 112 (“help”) appears 1 time. 
Word 483 (“rain”) appears 1 time. 
Word 3998 (“dampen”) appears 1 time. 
Create tf-idf model object using models.TfidfModel on ‘bow_corpus’ and save it to ‘tfidf’, then apply transformation to the entire corpus and call it ‘corpus_tfidf’. Finally we preview TF-IDF scores for our first document. 
[(0, 0.5907943557842693), 
(1, 0.3900924708457926), 
(2, 0.49514546614015836), 
(3, 0.5036078441840635)] 
Train our lda model using gensim.models.LdaMulticore and save it to ‘lda_model’ 
For each topic, we will explore the words occuring in that topic and its relative weight. 
Can you distinguish different topics using the words in each topic and their corresponding weights? 
Again, can you distinguish different topics using the words in each topic and their corresponding weights? 
We will check where our test document would be classified. 
[‘rain’, ‘help’, ‘dampen’, ‘bushfir’] 
Our test document has the highest probability to be part of the topic that our model assigned, which is the accurate classification. 
Our test document has the highest probability to be part of the topic that our model assigned, which is the accurate classification. 
Source code can be found on Github. I look forward to hearing any feedback or questions. 
Reference: 
Udacity — NLP 
Hands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look 
Written by 
Written by",Susan Li,2018-06-01T01:30:14.343Z
Active Learning. I am sure when you think the words… | by Shivani Kohli | Medium,"I am sure when you think the words, ‘active learning’ this the first image that comes to mind. In reality, it is slightly different. Active learning is a machine learning technique used when we have a large pool of unlabeled data. 
By now we have all heard, there is A LOT of data available but most of it is unlabeled and labeled data is required for machine learning algorithms. 
As we can see in the above tweet by Richard, it is much easier to create a model with supervised learning as compared to unsupervised learning. Additionally, we need labeled data to validate our models. By labeled data I mean, if I have a sentence, “Snakes scare me”, this sentence is marked as a negative sentence. 
However, anyone who has worked with large bodies of data can attest that labeling millions of rows of data using excel or Mechanical Turk is inefficient and time-consuming, noticing that researcher’s thought, there has to be a better way. That’s how active learning came to existence. Before, I get into what active learning is let’s talk about passive learning. 
Passive learning is the form we are familiar with. In this method, we have a large quantity of data that an expert labels and these labeled data are passed to the algorithm which uses it for classification purposes. As seen in the diagram below. 
However, in active learning we have a large quantity of unlabeled data that are passed to the algorithm. 
The algorithm then chooses pieces of data it wants labeled, it sends these data points to the expert who labels it and sends it back. The basic goal of the algorithm is “If I could only have 1000 labeled pieces of data, which are the most informative I could pick?” This process reduces the number of labeled data points required for learning. It chooses the data points based on which data points it is most uncertain about while classifying. 
This process is also extremely beneficial when we are continuously getting more data points. The active learning algorithm, the learner, can keep on classifying this data. 
Active learning is based on the idea, “are all labeled examples equally important?” 
In the above graph, classifying a will be extremely difficult as compared to b. Implying, b is not that important of a data point and does not necessarily have to be classified. 
In conclusion, active learning is an active research area which aims to drastically cut costs associated with getting labeled data by letting the algorithm decide what pieces of data it needs labeled. 
Written by 
Written by",Shivani Kohli,2018-07-17T14:21:37.999Z
An Introduction to Active Learning | by ODSC - Open Data Science | Medium,"The current utility and accessibility of machine learning is in part due to the exponential increase in the availability of data over time. While data is abundant, labels that are required for specific supervised machine learning tasks can be difficult to obtain. At ODSC West in 2018, Dr. Jennifer Prendki gave an introduction to active learning, a technique which can be used to minimize the time and cost required to build a suitable dataset for supervised learning. Dr. Prendki is currently the VP of machine learning at Figure Eight and has a wealth of experience from a variety of data science roles. 
[Related Article: An Overview of Proxy-label Approaches for Semi-supervised Learning] 
Labeling all the data available can be cost prohibitive despite the multitude of services that offer human labeling. Dr. Prendki offers two solutions; label faster using machine learning and label smarter to maximize the accuracy gain per label. 
It may seem like circular logic to use machine learning models to label data for machine learning, but Dr. Prendki explains that a human-model partnership can be used to develop an effective cycle. A variety of models and services exist to label images rapidly, but their accuracy rate is far from perfect. To insure accuracy, humans then review the labels rapidly and correct any erroneous records. The model can then be retrained with the newly labeled data. This process starts with enough human generated labels from which a model can be trained. The model is then used to label an additional subset of the remaining data, imitating the human labeler. A percentage of the model-labeled data points will be incorrect, so a human will be required to relabel a portion of the automated labeled data. The loop continues with retraining the model on the labeled data and correcting erroneous labels until the model reaches sufficient accuracy or all the data is labeled. 
Dr. Prendki introduces the idea of labeling smarter by maximizing the information to data volume ratio. Selective training can reduce the cost of labeling by processing less data and increasing model accuracy by insuring that the model learns from data points key to generalizing beyond the training data. Randomly sampling training data is often the best practice, but in some cases, selectively sampling data will ultimately provide the best results and will reduce the number of data points needed to be labeled. For example, in the figure below, a model trained by selectively sampling and labeling the blue data points among others, one can build a better model than simply randomly sampling data. 
The concept of smart labeling is intuitive if one is familiar with how to deal with imbalanced classes in machine learning. If the predictive accuracy of undersampled classes is of importance, it is key to focus labeling effort to those classes. However, sample imbalance is not the only way to determine which data points offer the most information gain during training. Dr. Prendki suggested that the change in entropy, and the model’s confidence in the prediction for each data point are key sources of information to determine the most informative records. 
[Related Article: Trends in AI: Towards Learning Systems That Require Less Annotation] 
Key takeaways: 
Original post here. 
Read more data science articles on OpenDataScience.com, including tutorials and guides from beginner to advanced levels! Subscribe to our weekly newsletter here and receive the latest news every Thursday. 
Written by 
Written by",ODSC - Open Data Science,2019-08-09T17:01:04.493Z
NLP — Word Embedding & GloVe. BERT is a major milestone in creating… | by Jonathan Hui | Medium,"BERT is a major milestone in creating vector representations for sentences. But instead of telling the exact design of BERT right away, we will start with word embedding that eventually leads us to the beauty of BERT. If we know the journey, we understand the intuitions better and help us to replicate the success in solving other problems. Since word embedding is a cornerstone for deep learning (DL) NLP, our first article will focus on it first. 
Some words often come in pairs, like nice and easy or pros and cons. So the co-occurrence of words in a corpus can teach us something about its meaning. Sometimes, it means they are similar or sometimes it means they are opposite. 
In word embedding, who you associated with tell you who you are. 
We can describe the word “hen” as: 
Intuitively, we can create a list of properties in describing a word. However, it will be impossible to define a universal set of properties manually that accommodates all the words in the vocabulary. Word Embedding is a Deep Learning DL method in deriving vector representations for words. For example, the word “hen” can be represented by a 512D vector, say (0.3, 0.2, 1.3, …). Conceptually, if two words are similar, they should have similar values in this projected vector space. 
If we encode a word with a one-hot vector, a vocabulary of 40K words requires a 40,000-D vector. In this vector, only one component equals one while others are all zero. This non-zero component identifies a unique word. That is very un-efficient but it is a good start to find a denser representation. 
Let’s enforce another constraint. We project this one-hot vector into this denser representation using linear transformation. i.e. to create the vector h, we multiply the one-hot vector with a matrix. The vector h is in a much lower dimension and we are projecting the one-hot vector into this vector space. 
Given two words wᵢ and wₒ below, we want them to be as close as possible in the projected space if they are similar. 
We can conceptualize the problem slightly differently. Let’s start with the word wᵢ. We first compute its vector representation with an embedding matrix. Then, we multiply it with another matrix to predict another word similar to it, say wₒ. The output will not be a one-hot vector. But we can run a softmax to find the most likely word. This creates a nice concept in linking words that are related to wᵢ. 
What does it buy us? Manual labeling of related words is expensive. Instead, we parse a text corpus and use the co-occurrence words within a context window (say within 2 words range) as our training data. Our key focus is to learn the first embedding matrix to create a dense vector representation for a word. 
But there are two possible ways to do it. 
Skip-gram model 
The first one is the skip-gram model. Given a word, can we predict its neighboring words in a text corpus? Say, we use a 5-grams model (5 consecutive words). Given the word “Patriots”, can we predict the neighbor words with the training data like: 
New England Patriots win 14th straight regular-season game at home in Gillette stadium. 
In the diagram below, we fit the one-hot vector of the word “Patriots” in the word embedding model. It produces 4 predictions about its possible neighbors. 
The log-likelihood for the predicted words given the target word t (“Patriots”) will be: 
For this 5-gram model, we want to predict these 4 words on the right. 
To calculate the probability p(wₒ | wᵢ), we locate the corresponding row and column entries related to wᵢ and wₒ in the corresponding matrix. Then, the conditional probability can be computed as: 
The numerator measures the similarity using a dot product. We train the model such that two similar words should produce the maximum dot product value. The denominator adds up all scores together to renormalize the numerator to a probability value. In a nutshell, for similar words, we move their vector representation closer. We want this pair to have the largest similarity over other combinational pairs involving wI. 
Continuous Bag-of-Words (CBOW) 
The second probability is CBOW. Given the context, we want to predict the target word instead. For example, given “New”, “England”, “win” and “14th”, we want to predict the target word “Patriots”. 
Vector Arithmetic 
Let’s delay the discussion on the training for a second and examine these trained vectors first. Since it is too hard to visualize vectors in high dimensional space, we use PCA to project it into a 2-D space. The diagram plots some of the words in this 2-D space. One important observation is that this process can discover word relations with simple linear algebra! 
For example, 
This is the charm of word embedding because we create a simple linear mathematical model to manipulate words semantically. If we know the vector representations of Poland, Beijing, and China, we can answer questions like what is the capital of Poland. This linear behavior is mainly contributed by the use of matrix (a linear model) in projecting words into a dense space. 
Next, we will examine the cost function for the training in detail. 
Cross-Entropy 
Assume that we are using a bigram (2-gram) model, the cross-entropy between the ground truth and the predictions will be. 
As shown in the equation above, the term in the middle wants to maximize the score between the word pair we observe (the numerator) while minimizing the scores between other pairs involving wI (the denominator). The gradient of the loss function is: 
where we can draw samples from distribution Q (i.e. with distribution p(wi|wI)) to estimate the second term. This is good news because we find an estimation method instead of computing the exact value for all possible word pairs with wI. 
Noise Contrastive Estimation (NCE) 
If we treat the training as a logistic regression problem, the loss function of the word embedding is: 
i.e. we want the ground truth to be classified as true while the others to be false. This is similar to the cross-entropy and the loss function becomes: 
(We will not overwhelm you with the proof in this or the next section. But the proofs can be found here if you are interested.) 
Sampling from Q (p(wi|wI)) is not that simple. For some less common word pairs, we need a large corpus to make the estimation more accurate. There is even a chance that a legitimate word pair may not exist in the training data set. But in the equations above, we can simplify Q to q where q is the word distribution of a single word according to its occurrence ranking in the corpus. Since it depends on a single word only, it is easier to estimate using fewer corpus data. 
Negative Sampling (NEG) 
NEG is a variant of NCE where we apply a logistic function to the relevancy score. So instead of handling it as a regression problem (estimating the conditional probability), we treat it as a classification problem. 
The corresponding objective function becomes: 
This is the function used in the word embedding training. In the next few sections, we will discuss a few implementation details. 
Subsampling of Frequent Words 
To choose the word wI in the training set as the next training data, we can pick sample data using the equation below: 
Obviously, we pick words with higher frequency. 
Design tradeoffs 
Here are different tradeoffs and decision choices for the word embeddings. For example, should we use skip-gram or CBOW? Here are some suggestions from the Google team. 
GloVe is another word embedding method. But it uses a different mechanism and equations to create the embedding matrix. To study GloVe, let’s define the following terms first. 
And the ratio of co-occurrence probabilities as: 
This ratio gives us some insight on the co-relation of the probe word wk with the word wᵢ and wⱼ. 
Given a probe word, the ratio can be small, large or equal to 1 depends on their correlations. For example, if the ratio is large, the probe word is related to wᵢ but not wⱼ. This ratio gives us hints on the relations between three different words. Intuitively, this is somewhere between a bi-gram and a 3-gram. 
Now, we want to develop a model for F given some desirable behavior we want for the embedding vector w. As discussed before, linearity is important in the word embedding concept. So if a system is trained on this principle, we should expect that F can be reformulated as: 
where we just need to compute the difference and the similarity of word embedding for the parameters in F. 
In addition, their relation is symmetrical. (a.k.a. relation(a, b) = relation(b, a)). To enforce such symmetry, we can have 
Intuitively, we are maintaining the linear relationship among all these embedding vectors. 
To fulfill this relation, F(x) would be an exponential function, i.e. F(x) = exp(x). Combine the last two equations, we get 
Since F(x) = exp(x), 
We can absorb log(Xᵢ) as a constant bias term since it is invariant of k. But to maintain the symmetrical requirement between i and k, we will split it into two bias terms above. This w and b form the embedding matrix. Therefore, the dot product of two embedding matrices predicts the log co-occurrence count. 
Intuition 
Let’s understand the concept through matrix factorization in a recommender system. The vertical axis below represents different users and the horizontal axis represents different movies. Each entry shows the movie rating a user gives. 
This can be solved as a matrix factorization problem. We want to discover the hidden factors for the users and movies. This factor describes what a user likes or what the hidden features (like the genre) a movie will be. If their factors match, the movie rating will be high. For example, if a user likes romantic and old movies, they will match well with the movie “When Harry Met Sally” (a romantic movie in the 80s). The vector representations for the user and the movie should produce high value for their dot product. 
Therefore the rating matrix holding all users and movies can be approximated as the multiplication of the users' hidden features and the movies’ hidden features (matrix Z holding the hidden factors of all users and w hold the hidden factors for all movies). 
In GloVe, we measure the similarity of the hidden factors between words to predict their co-occurrence count. Viewed from this perspective, we do not predict the co-occurrence words only. We want to create vector representations that can predict their co-occurrence counts in the corpus also. 
Cost function 
Next, we will define the cost function. We will use the Mean Square Error to calculate the error in the ground truth and the predicted co-occurrence counts. But since word pair have different occurrence frequency in the corpus, we need a weight to readjust the cost for each word pair. This is the function f below. When the co-occurrence count is higher or equal a threshold, say 100, the weight will be 1. Otherwise, the weight will be smaller, subject to the co-occurrence count. Here is the objective function in training the GloVe model. 
Now, we are done with the word embedding. 
Word embedding encodes words. But it does not account for its word context. Next, we will look at vector representations for sentences that can be used for many NLP tasks. BERT is used by Google in its search and good for many NLP tasks. If you want to learn deep learning for NLP, it is one of the most important technology. 
Distributed Representations of Words and Phrases and their Compositionality 
Efficient Estimation of Word Representations in Vector Space 
GloVe: Global Vectors for Word Representation 
Learning Word Embedding 
Attention Is All You Need 
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding 
The Annotated Transformer 
tensor2tensor 
Pre-trained model and code for BERT 
BERT presentation 
Written by 
Written by",Jonathan Hui,2020-07-01T01:24:45.067Z
What is zero-trust security?. Everything you need to know about… | by Keyless Technologies | KeylessTech | Medium,"Everything you need to know about zero-trust security, and why you need zero-trust passwordless authentication like Keyless 
Zero-trust models are a radically different approach to network security that can help companies strengthen security and eliminate cyberthreats. 
At Keyless we’re helping enterprises transition to zero-trust security architectures with our breakthrough biometric authentication technology. 
In this piece we’ll cover: 
Perimeter-based network security models, like firewalls and VPNs, automatically trust users who are inside the network. 
Unfortunately, this approach leaves organizations susceptible to threats launched from within the network; while also failing to protect against incoming threats when systems are being accessed remotely. 
Recent work-from-home orders are highlighting security flaws with the perimeter-based network security approach. 
With the rapid rise of users accessing an organization’s systems remotely, (from outside the security perimeters of corporate firewalls), the chances of a successful breach have increased sharply. 
Since legacy security systems rely on trust, once a hacker gains access to a network, they’re then able to freely move throughout the network until finding sensitive data. 
Malicious attacks aren’t the only issue with legacy security systems. Trust-based models leave organizations susceptible to insider-orchestrated attacks and data leaks. 
Thus the assumption of trust is fundamentally flawed, leaving systems vulnerable to an ever-increasing number of sophisticated attacks. As the classic saying goes “if it can get hacked, it will”. 
With the growing threat of attacks, this is essentially true for all systems that store sensitive data and fail to adequately protect it. Zero-trust models can help organizations restore security and privacy. 
Read more about why traditional security models are ineffective here. 
First coined a decade ago by an analyst at Forrester Research, zero-trust security models assume that all devices and users can’t be trusted. This assumption carries through after a user has initially gained access to the network. 
Based on this assumption that a user cannot be trusted, zero-trust models continuously verify and authenticate users no matter where, when and how they access a system. 
This protects organizations by preventing unauthorized movement within a network’s systems. As such, zero-trust security models protect organizations from the reputational, legal and operational costs associated with large-scale data breaches. 
With cyberthreats increasing in sophistication and scale, zero-trust models can help transform security architectures and protect enterprises by offering new levels of protection, helping to ease the transition into a digital future where remote-work is commonplace. 
As the global workforce moves online, enterprises need authentication solutions that are not only secure, but dynamic and user-friendly at the same time. 
We believe that modern access management is about the right people, having the right level of access at the right time, with the least amount of friction as possible. 
The first step towards implementing a zero-trust security architecture should be to adopt secure passwordless authentication. The second would be to implement access controls at every entry point to an organization’s private systems and databases. 
Access controls can be used to prevent lateral movement throughout the network, while ensuring that only privileged users have access to sensitive databases and private resources. 
By establishing trust, (via re-authenticating), as the user moves through the network, zero-trust authentication prevents malicious actors from being able to launch large-scale attacks. 
The zero-trust model also prevents unauthorized users or employees from accessing data that they shouldn’t have access to. 
At Keyless, we combine multi-modal biometrics with privacy-enhancing cryptography with biometrics and state-of-the-art anti-spoofing technology to enable a passwordless, phishing-proof way to authenticate users, leveraging a zero-trust framework. 
In doing so, we are able to offer seamless, ‘onelook’, multi-factor authentication for end-users and employees, across all platforms and devices. 
Our solution offers strong multi-factor security, by design: 
In other words, users seamlessly authenticate simply by looking into the camera of their registered device. Our network verifies users in less than 100 milliseconds, less time than it takes to type out an email address and password. 
By providing a secure, frictionless way to establish access controls at multiple entry-points, Keyless prevents unauthorized movement through private corporate systems. 
This protects organizations from a range of threats inside the network, like malicious takeovers, insider attacks and data leaks. 
To protect end-users and organizations from other kinds of malicious attacks, like fraudulent attempts to replicate a user’s biometrics, Keyless uses advanced liveness detection and anti-spoofing techniques to ensure that the user is in fact real. 
If you’re interested in how Keyless™ authentication can help deliver secure and seamless digital experiences, whether for your end-users or for an ever more important and dynamic digital workplace, or if you’d simply like to learn more about our platform, then please feel free to get in touch with our team. 
You can email us at info@keyless.io 
We’re always keen to have a chat about how we can help businesses on their journeys towards a complete zero-trust security model. 
Written by 
Written by",Keyless Technologies,2020-06-23T08:08:50.463Z
The External Data Imperative. “First we have to get our internal data… | by Demyst | Medium,"“First we have to get our internal data in order”. It’s something we hear frequently. Why invest externally when there is an underutilized resource on hand? Today’s article breaks down the dimensions of when to focus on external data and why, and discusses why the demand for external data is expanding fast. 
Contents hide 
1 Executive summary 
2 Diminishing returns — Rows 
3 Modeling fatigue — AI 
4 The impact of external data — Columns 
5 Take-aways 
Commercial impact from analytics, i.e. by improvement over the status quo, is driven foundationally by data and algorithms. Within data, one can simplistically think of rows and columns. More rows (observations internally), more columns (features — typically through external data), and a better analytical algorithm multiplicatively combine to drive value, as depicted below. 
In today’s competitive landscape, extracting information content from data is crucial to customer acquisition and better risk selection. Doing so via internal data and improved AI alone yields rapidly diminishing returns. Internal data and advanced algorithms are table stakes. The source of analytical improvement and the basis of competition is shifting to features; to columns; to external data. Enterprises seeking a step change in model quality have an imperative to understand, assess, and capture value from the emerging external data ecosystem. 
And those that build the muscle to harness external data are achieving outsized returns. A sample of Demyst platform users demonstrates leading organizations not just exploring but actually consuming and deriving value from over 9 times the number of external data sources versus the market average. 
The last decade has delivered a renaissance in big data technologies that amass huge amounts of data. Drilling deeper, most applications of these technologies involve “tall and skinny” datasets that have many rows but relatively few columns. Examples include ad-tech applications, that have massive quantities of cookies, but relatively little data about each. Cyber and weblog ingestion platforms, sitting in spark/hadoop clusters, contain billions of observations but limited information. 
There is value in this to be sure, however, the challenge is that most analytical applications and algorithms plateau at scale. I.e. orders of magnitude more data are required in order to deliver lift in modeling algorithms. This can present practical challenges : 
Enterprises that haven’t leveraged and modeled internal data will benefit from doing so to be sure, but this isn’t enough. 
Non-parametric modeling techniques have been used for a long time — KNN, Neural Nets, Decision trees and forests, Gradient boosting techniques; all offer the unique opportunity to discover and leverage deep interaction effects and tease more signal from big data noise to deliver outsized impacts. More recently however advances in compute capacity, frontend tools like DataRobot and H20, and frameworks like TensorFlow and sci-kit-learn, non-parametric and ensemble techniques have become mainstream. 
Coase famously said, “if you torture data long enough, it will confess”. A sample only contains a limited representation of the population statistics, no matter how sophisticated the modeling algorithm. Furthermore, brute force modeling platforms can exacerbate overfit against biased or limited input data. I love a good Kaggle or DataRobot leaderboard as much as the next person — there are very few things more satisfying — but how often does that ensemble Xgboost make its way into production vs a tried and tested logistic regression? Infrequently; because, while marginally better, they are often exponentially more complex, unstable, and prone to deployment error. 
Organizations are adopting AI in a big way, however like with observations, there are diminishing returns as orders of magnitude more capacity, sophistication and effort are required to extract marginal gains. 
It’s not an either/or, but rather and. One needs observations, attributes, and AI to capture model value. 
While data lakes and AI operate at massive scale, there remains major low hanging fruit through adding columns. Going from 50 attributes to 500 can be a lot more impactful and practical than going from 100k records to 1m. 
For example, if we are predicting compliance risks within a bank, expanding to hundreds of millions of daily transactions daily but still mining only basic signals such as transaction to/from, amount vs norms, and static lists are unlikely to yield much. However running every name and company against news articles, blogs, running emails against social media presence, triangulating PII across third party sources to identify inconsistencies, looking up employer profiles, and triangulating device locations, can all yield major lift even without going beyond the current sample sizes. 
Harnessing more features is typically but not always an external data challenge. Most organizations know only a small fraction of information about their own customers versus what is available. 
What’s more, capturing data externally unlocks opportunities that aren’t solely about better predictions. Pre-fill and customer journey optimization is only possible through external data — i.e. without creating undue friction for the customer. Protecting against adverse selection in risk use cases is best done through dimensions, not rows. 
Why now? And why has this opportunity slid down the priority list? We are at an inflection point where disruptors creating better digital journeys are capturing customers more effectively, and big platform players are entering every vertical. The fight for the customer has never been more intense, and with it, the need to optimize every customer interaction with data. However, until now the market for external data has not made the life of the data scientist particularly easy. It’s fragmented. Noisy. Collinear. The value is unpredictable. Compliance and information security are hard, increasingly so. It’s impossible to know what matters until it’s tested. So perhaps not surprising that this is left until later. 
Demyst is investing heavily in solving this. Our vision is that expanding the cube to the right — learning more about what attributes matter and harnessing them within your analytical workflows — is just a few lines of code in a seamless API. We abstract away the discovery, data compliance, contracting, types and schema, and other mechanical aspects of getting the data to where it needs to be to capture value. 
Investing in lift against all 3 dimensions is no longer optional. In the search for competitive edge and model quality, fueling your next-gen engines with more columns is the logical area of strategic focus for analytical leaders and CDOs. Doubling down on the same data with the same models isn’t going to delight your customers, but delivering exactly the right products in the right way will — that’s the external data imperative. 
Want to learn more about Demyst? Schedule a consultation with Demyst to speak with one of our industry-leading data experts. 
Written by 
Written by",Demyst,2020-02-25T14:40:48.953Z
AI & Cyber Security. What this technology revolution may… | by Alexis Vander Wilt | Query.AI | Medium,"The term ‘Artificial Intelligence (AI)’ was coined in 1950, but since then it has grown in unimaginable ways. 
Borrowing from a recent team members blog post “Artificial Intelligence & Everyday Life” by Shaswat Anand, AI can be defined as “an area of computer science that emphasizes the creation of intelligent machines that work and react like humans.” 
In recent years it has found its way into many aspects of our lives including education, healthcare, and manufacturing. We use AI assistants such as Siri, Alexa, Google Assistant, and Cortana every day. They can recognize patterns, use the past to predict the future, and alert us when something unusual is happening. 
Artificial Intelligence also has much potential in the area of cyber security. 
For commercial use, it’s being applied to remove ‘white-noise’ and filter out unwanted data in an attempt to make real threats more obvious. It can be helpful in implementing decoys to trap attackers and making it easier to identify who’s attempting to access your systems. 
In our personal lives, its used for things like biometrics in 2-factor authentication, protecting our Internet-Of-Things(IOT) devices, and notifying us when unusual activity is occurring in our bank or credit card accounts. 
The main focuses of applied AI are deception, detection, prediction, remediation, accessibility, and even adversarial AI. 
In this blog series, we will explore the different applications of AI in cyber security, examining both the good and the bad of this technology. To get us started, this article will provide a brief introduction to each of these topic areas. 
Artificial Intelligence and Machine Learning can be used in the process of cyber Threat Detection and Response(TDR). In fact, over the past several years this has been the primary form of AI in cyber security. Certain AI algorithms are very good at recognizing when events are not matching expected results and can even assist in taking action to repair them. 
It’s ability to detect threats as they are happening means it could potentially prevent attackers from gaining complete access to the system. Two common examples of this are detecting spam e-mails and combating malware through identification and blocking of attacks. 
These are two examples of AI noticing when things have gone wrong, and attempting to prevent incidents before they happen. Certain implementations can be used to analyze user’s password habits and detect when bad passwords are being created, a method for preventing attacks before targeting has even occurred. 
Another way AI has impacted the world of cyber security is its ability to simplify tasks that used to be done by hand. 
A few such uses for automation are: 
One of the largest benefits of machine automation is consistency. AI can dramatically increase the consistency with which we investigate and respond to potential threats. This inherently reduces organizational risk by reducing the potential for human error & mistakes. 
A newer and less well known application of AI geared toward simplification and accessibility is in the world of Natural Language Processing (NLP), which is the processing of human speech or text in a natural language, e.g. English, to remove complexity. This form of AI could help differently-abled people who have accessibility issues with their vision, hearing, or speech. 
All of these are examples of ways in which the introduction of AI and Machine Learning have helped the cyber security workforce to become faster, smarter, and more efficient about their work. 
AI technology is extremely powerful, but it’s important to keep in mind that if we have access to it, so do the attackers. 
Black hat hackers can use AI to make their work faster and smarter too. Even companies with huge cyber security efforts can be bested by AI trained to target its weaknesses. Malicious AI can be a variety of different things from automated attacks that are designed to run and adapt without human interaction, to those which use false data to mislead other Machine Learning algorithms into making incorrect conclusions. 
This AI, like that used for more benevolent purposes, is still in it’s developing stages, but it’s dangerous potential could change the way we view cyber security. It is very important that we learn how to leverage and adapt the tools we use to detect, prevent, and respond to attacks like these in the future. 
These forms of applied AI, specifically when combined with enhancements in low cost computing power, have enabled significant progress since the introduction of AI in the 1950’s. Despite the progress we’ve made, there is no doubt that there is still plenty of work to be done as we move into the next decade and beyond. 
Please stay tuned for the follow up articles as we dive further into AI and its impact on cyber security. 
Thanks for reading! 
If you like this content or have suggestions for other topics you’d like us to cover please let us know, we’d love to hear from you. 
You can reach us at contact@query.ai. 
You’re also free to follow us on linkedin and visit www.query.ai to subscribe to our updates. 
I am a senior Computer Science and Mathematics student, with a passion for understanding Data Analysis and its impacts. I work as part of the team at Query.AI where we are using Natural Language Processing to allow users to “talk to your data” reducing the security learning curve and working to make security more accessible to all. 
Written by 
Written by",Alexis Vander Wilt,2019-11-19T17:36:23.842Z
"Where Does Machine Learning Stand in Cyber Security? | by Christopher Dossman | AI³ | Theory, Practice, Business | Medium","Cyber-security is a critical area in which machine learning(ML) is increasingly becoming significant. But ML in cyber-security extends far beyond merely applying established algorithms to cyber entities. 
The ML community may be unaware, but cyber-security with ML has long-standing challenges that require methodological and theoretical handling. According to recently published research work, some scholars have presented the existing cyber-security problems and provided the AI and deep learning community with related datasets to help dive deeper into ML applications in cybersecurity. 
One of the significant challenges that researchers and the entire ML community need to deal with if they are going to apply ML in cybersecurity successfully is malware classification and detection. 
It is not easy to identify malicious programs as attackers use complicated techniques such as polymorphism, impersonation, compression, and obfuscation to evade detection. Other challenges include limited domain experts which lead to lack of labeled samples and numerous labeling errors, imbalanced data sets, the attacker-defender games, difficulty in identifying malicious sources, the tragedy of metrics, and more. 
Since one of the key hindrances to investigating cyber-security problems is a lack of appropriate data sets, researchers have provided access to datasets that can enable the academic community to investigate the aforementioned challenges and suggest methods that can help minimize or eliminate them. They also present a methodology to help generate labels via pivoting and in so doing provide a solution to common problems such as lack of labels in cyber-security. 
Researchers behind this work are of the idea that the use of ML in cybersecurity should change. They also believe that the cyber community has a duty to help the ML community to become more active in the field. Tell you what? I think so too! 
Currently, there’s a lack of enough qualified and experienced cybersecurity analysts to help minimize the skyrocketing global cyber-attacks. And, there already exists an overabundance of big data that can be used in several algorithms to improve the current state of cybersecurity with ML. Let’s all hope that these research developments will help drive new methods that will boost current state-of-the-art in both ML and cybersecurity. 
To gain access to available datasets, you can contact data-sets@paloaltonetworks.com with ‘Access to data request’ as the subject title. 
Read more: https://arxiv.org/abs/1812.07858v3 
Thanks for reading. Please comment, share and remember to subscribe to our weekly AI scholar Newsletter! Also, follow me on Twitter and LinkedIn. Remember to 👏 if you enjoyed this article. Cheers 
Written by 
Written by",Christopher Dossman,2019-08-20T09:06:45.648Z
"Topic Modeling with LSA, PLSA, LDA & lda2Vec | by Joyce Xu | NanoNets | Medium","This article is a comprehensive overview of Topic Modeling and its associated techniques. 
In natural language understanding (NLU) tasks, there is a hierarchy of lenses through which we can extract meaning — from words to sentences to paragraphs to documents. At the document level, one of the most useful ways to understand text is by analyzing its topics. The process of learning, recognizing, and extracting these topics across a collection of documents is called topic modeling. 
In this post, we will explore topic modeling through 4 of the most popular techniques today: LSA, pLSA, LDA, and the newer, deep learning-based lda2vec. 
All topic models are based on the same basic assumption: 
In other words, topic models are built around the idea that the semantics of our document are actually being governed by some hidden, or “latent,” variables that we are not observing. As a result, the goal of topic modeling is to uncover these latent variables — topics — that shape the meaning of our document and corpus. The rest of this blog post will build up an understanding of how different topic models uncover these latent topics. 
Latent Semantic Analysis, or LSA, is one of the foundational techniques in topic modeling. The core idea is to take a matrix of what we have — documents and terms — and decompose it into a separate document-topic matrix and a topic-term matrix. 
The first step is generating our document-term matrix. Given m documents and n words in our vocabulary, we can construct an m × n matrix A in which each row represents a document and each column represents a word. In the simplest version of LSA, each entry can simply be a raw count of the number of times the j-th word appeared in the i-th document. In practice, however, raw counts do not work particularly well because they do not account for the significance of each word in the document. For example, the word “nuclear” probably informs us more about the topic(s) of a given document than the word “test.” 
Consequently, LSA models typically replace raw counts in the document-term matrix with a tf-idf score. Tf-idf, or term frequency-inverse document frequency, assigns a weight for term j in document i as follows: 
Intuitively, a term has a large weight when it occurs frequently across the document but infrequently across the corpus. The word “build” might appear often in a document, but because it’s likely fairly common in the rest of the corpus, it will not have a high tf-idf score. However, if the word “gentrification” appears often in a document, because it is rarer in the rest of the corpus, it will have a higher tf-idf score. 
Once we have our document-term matrix A, we can start thinking about our latent topics. Here’s the thing: in all likelihood, A is very sparse, very noisy, and very redundant across its many dimensions. As a result, to find the few latent topics that capture the relationships among the words and documents, we want to perform dimensionality reduction on A. 
This dimensionality reduction can be performed using truncated SVD. SVD, or singular value decomposition, is a technique in linear algebra that factorizes any matrix M into the product of 3 separate matrices: M=U*S*V, where S is a diagonal matrix of the singular values of M. Critically, truncated SVD reduces dimensionality by selecting only the t largest singular values, and only keeping the first t columns of U and V. In this case, t is a hyperparameter we can select and adjust to reflect the number of topics we want to find. 
Intuitively, think of this as only keeping the t most significant dimensions in our transformed space. 
In this case, U ∈ ℝ^(m ⨉ t) emerges as our document-topic matrix, and V ∈ ℝ^(n ⨉ t) becomes our term-topic matrix. In both U and V, the columns correspond to one of our t topics. In U, rows represent document vectors expressed in terms of topics; in V, rows represent term vectors expressed in terms of topics. 
With these document vectors and term vectors, we can now easily apply measures such as cosine similarity to evaluate: 
In sklearn, a simple implementation of LSA might look something like this: 
LSA is quick and efficient to use, but it does have a few primary drawbacks: 
pLSA, or Probabilistic Latent Semantic Analysis, uses a probabilistic method instead of SVD to tackle the problem. The core idea is to find a probabilistic model with latent topics that can generate the data we observe in our document-term matrix. In particular, we want a model P(D,W) such that for any document d and word w, P(d,w) corresponds to that entry in the document-term matrix. 
Recall the basic assumption of topic models: each document consists of a mixture of topics, and each topic consists of a collection of words. pLSA adds a probabilistic spin to these assumptions: 
Formally, the joint probability of seeing a given document and word together is: 
Intuitively, the right-hand side of this equation is telling us how likely it is see some document, and then based upon the distribution of topics of that document, how likely it is to find a certain word within that document. 
In this case, P(D), P(Z|D), and P(W|Z) are the parameters of our model. P(D) can be determined directly from our corpus. P(Z|D) and P(W|Z) are modeled as multinomial distributions, and can be trained using the expectation-maximization algorithm (EM). Without going into a full mathematical treatment of the algorithm, EM is a method of finding the likeliest parameter estimates for a model which depends on unobserved, latent variables (in our case, the topics). 
Interestingly, P(D,W) can be equivalently parameterized using a different set of 3 parameters: 
We can understand this equivalency by looking at the model as a generative process. In our first parameterization, we were starting with the document with P(d), and then generating the topic with P(z|d), and then generating the word with P(w|z). In this parameterization, we are starting with the topic with P(z), and then independently generating the document with P(d|z) and the word with P(w|z). 
The reason this new parameterization is so interesting is because we can see a direct parallel between our pLSA model our LSA model: 
where the probability of our topic P(Z) corresponds to the diagonal matrix of our singular topic probabilities, the probability of our document given the topic P(D|Z) corresponds to our document-topic matrix U, and the probability of our word given the topic P(W|Z) corresponds to our term-topic matrix V. 
So what does that tell us? Although it looks quite different and approaches the problem in a very different way, pLSA really just adds a probabilistic treatment of topics and words on top of LSA. It is a far more flexible model, but still has a few problems. In particular: 
We will not look at any code for pLSA because it is rarely used on its own. In general, when people are looking for a topic model beyond the baseline performance LSA gives, they turn to LDA. LDA, the most common type of topic model, extends PLSA to address these issues. 
LDA stands for Latent Dirichlet Allocation. LDA is a Bayesian version of pLSA. In particular, it uses dirichlet priors for the document-topic and word-topic distributions, lending itself to better generalization. 
I am not going to into an in-depth treatment of dirichlet distributions, since there are very good intuitive explanations here and here. As a brief overview, however, we can think of dirichlet as a “distribution over distributions.” In essence, it answers the question: “given this type of distribution, what are some actual probability distributions I am likely to see?” 
Consider the very relevant example of comparing probability distributions of topic mixtures. Let’s say the corpus we are looking at has documents from 3 very different subject areas. If we want to model this, the type of distribution we want will be one that very heavily weights one specific topic, and doesn’t give much weight to the rest at all. If we have 3 topics, then some specific probability distributions we’d likely see are: 
If we draw a random probability distribution from this dirichlet distribution, parameterized by large weights on a single topic, we would likely get a distribution that strongly resembles either mixture X, mixture Y, or mixture Z. It would be very unlikely for us to sample a distribution that is 33% topic A, 33% topic B, and 33% topic C. 
That’s essentially what a dirichlet distribution provides: a way of sampling probability distributions of a specific type. Recall the model for pLSA: 
In pLSA, we sample a document, then a topic based on that document, then a word based on that topic. Here is the model for LDA: 
From a dirichlet distribution Dir(α), we draw a random sample representing the topic distribution, or topic mixture, of a particular document. This topic distribution is θ. From θ, we select a particular topic Z based on the distribution. 
Next, from another dirichlet distribution Dir(𝛽), we select a random sample representing the word distribution of the topic Z. This word distribution is φ. From φ, we choose the word w. 
Formally, the process for generating each word from a document is as follows (beware this algorithm uses c instead of z to represent the topic): 
LDA typically works better than pLSA because it can generalize to new documents easily. In pLSA, the document probability is a fixed point in the dataset. If we haven’t seen a document, we don’t have that data point. In LDA, the dataset serves as training data for the dirichlet distribution of document-topic distributions. If we haven’t seen a document, we can easily sample from the dirichlet distribution and move forward from there. 
LDA is easily the most popular (and typically most effective) topic modeling technique out there. It’s available in gensim for easy use: 
With LDA, we can extract human-interpretable topics from a document corpus, where each topic is characterized by the words they are most strongly associated with. For example, topic 2 could be characterized by terms such as “oil, gas, drilling, pipes, Keystone, energy,” etc. Furthermore, given a new document, we can obtain a vector representing its topic mixture, e.g. 5% topic 1, 70% topic 2, 10% topic 3, etc. These vectors are often very useful for downstream applications. 
So where do these topic models factor in to more complex natural language processing problems? 
At the beginning of this post, we talked about how important it is to be able to extract meaning from text at every level — word, paragraph, document. At the document level, we now know how to represent the text as mixtures of topics. At the word level, we typically use something like word2vec to obtain vector representations. lda2vec is an extension of word2vec and LDA that jointly learns word, document, and topic vectors. 
Here’s how it works. 
lda2vec specifically builds on top of the skip-gram model of word2vec to generate word vectors. If you’re not familiar with skip-gram and word2vec, you can read up on it here, but essentially it’s a neural net that learns a word embedding by trying to use the input word to predict surrounding context words. 
With lda2vec, instead of using the word vector directly to predict context words, we leverage a context vector to make the predictions. This context vector is created as the sum of two other vectors: the word vector and the document vector. 
The word vector is generated by the same skip-gram word2vec model discussed earlier. The document vector is more interesting. It is really a weighted combination of two other components: 
Together, the document vector and the word vector generate “context” vectors for each word in the document. The power of lda2vec lies in the fact that it not only learns word embeddings (and context vector embeddings) for words, it simultaneously learns topic representations and document representations as well. 
For a more detailed overview of the model, check out Chris Moody’s original blog post (Moody created lda2vec in 2016). Code can be found at Moody’s github repository and this Jupyter Notebook example. 
All too often, we treat topic models as black-box algorithms that “just work.” Fortunately, unlike many neural nets, topic models are actually quite interpretable and much more straightforward to diagnose, tune, and evaluate. Hopefully this blog post has been able to explain the underlying math, motivations, and intuition you need, and leave you enough high-level code to get started. Please leave your thoughts in the comments, and happy hacking! 
Nanonets makes it super easy to use Deep Learning. 
You can build a model with your own data to achieve high accuracy & use our APIs to integrate the same in your application. 
For further details visit us here or reach out to us at info@nanonets.com 
Written by 
Written by",Joyce Xu,2018-12-20T06:46:41.859Z
Hidden Markov Model- A Statespace Probabilistic Forecasting Approach in Quantitative Finance | by Sarit Maitra | Analytics Vidhya | Medium,"Hidden Markov Models (HMM) are proven for their ability to predict and analyze time-based phenomena and this makes them quite useful in financial market prediction. HMM can be considered mix of Brownian movements consisting of hidden layers and observed layers and comprising of sequence of events. In quantitative finance, the states of a system can be modeled as a Markov chain in which each state depends on the previous state in a non-deterministic way. In HMM these states are invisible, while observations which are the inputs of the model and depend on the visible states. HMM is typically used to predict the hidden regimes of observation data. The mathematical foundations of HMM were developed by Baum and Petrie in 1966. 
The big question here is that, can we use the performances of stocks in the past to predict their future performances? The data with index seems to have similar behaviors on the same regimes. Thus, it is a natural instinct to analyze the stock behavioral pattern on a similar environment in the past to forecast its future outcomes. Based on this motivation, we will discuss how HMM can be used effectively to predict stock movements 
Markov process is a process for which we can make predictions for the future based solely on its present state just as well as we could knowing the process’s full history [1]. 
We have seen the experimentation of a number of machine learning algorithms on stock prediction with varying degrees of success. We are also aware of the fact that, the price of the stock depends upon a multitude of factors and these factors mostly are hidden variables. The asset returns are comprised of multiple distributions. Time varying means and volatility normally comes from different distributions, regimes or states . Each regime has its own parameters e.g. regime can be stable with low volatility or regime could be risky with high volatility. 
We have discussed earlier, how Statespace model & Kalman filter can be applied to predict stock movement. Here, in this article, we will see how Gaussian Mixture Model (GMM) can be used for regime selection. The selection will involve both the covariance type and the number of components in the model. 
Historical NASDAQ data has been collected since 1970. When we call through API, the entire historical data from 1970 with 12341 rows and 5 columns. We will collect data from 1999 for our experiment. If you have better computing power, may use the entire data-set. 
In the current state, we have few features available for each day in the original data-set. These are the popular opening price, closing price, highest price, lowest price and volume. So, we will use them to compute the future price. In order to get more sequences and, more importantly, get a better understanding of the market’s behavior, we need to break-up the data into samples of sequences leading to different price patterns. 
Therefore, instead of directly using the opening, closing, low, and high prices of a stock, we are going to extract some features using O,C,L,H,V and use them as predictor which will have all the logic to predict the price during a given day. to train our HMM. We can add more relevant features to add a robust model. Here, we will use below features to experiment and show how HMM works. 
We will create a simple model with reasonable predictive power. However, this is not for commercial use purpose. 
Here, each predictor above will be treated as a sequence and will be used to train our model. So, we will have 4 chain of events as stated above on a particular day and we will use them to predict future ‘open’ price. Our goal here is to predict (t+1) value based on n (previous) days information. Therefore, defining the output value as forecast, which is a binary variable storing up or down values. We will predict ‘open’ price is up or down next day using the randomly sequence of events. 
Let us write a function to create 100,00 randomly sequences looking at 10–30 trading days as event. Hence, we have taken 10–30 days as look-back period, which also can be modified to 50–60 days or so. So, it will be a binary classification problem and we will use each of events to train our HMM. The parameters are defined in below program. 
The above concept was taken from Manuel Amunategui who has provided an excellent idea of binning the data and doing binary classification by creating two transition matrices — a positive one and a negative one. 
It takes some time to run considering 100,000 sequences and amount of data points we feed into the system. Here, our network architecture will look through the available data 100,000 times and sequence of 10–30 events to predict ‘open’ price up or down next day. We also have created our forecast variable which will see today’s price to predict tomorrow’s price. 
In simpler Markov Chain, the state is directly visible to the observer, and therefore the state transition probabilities are the only parameters, while in the HMM, the state is not directly visible, but the output, dependent on the state, is visible. Each state has a probability distribution over the possible output. Therefore, the sequence generated by an HMM gives some information about the sequence of states…wikipedia 
So, we collect all details in a data-frame as shown below. We concatenate all newly created columns in a data-frame and remove the null values. 
We drop here all the null values and print msno matrix to visualize that, our data-frame has no null values. 
So, after having observations, we can compute using Bayesian network because out network knows now all the transition probabilities from one state to another. This is kind of similar to neural network here. Our HMM can know below three important things through our observations: 
Now, our first approach is to, using HMM to find regimes for the new variables. We use historical data of each variable to calibrate HMM parameters and to find the corresponding regimes. Let us convert the columns to numpy array and scale down the data. 
We will use historical data of each variable to calibrate HMM parameters and to find the corresponding regimes. The components are regimes here. Let us find out the optimum number of components using some information criteria. We will initiate a Gaussian Mixture model (GMM) to fit our time-series data with a range of 1–15 BIC to find the optimal number for our model. GMM employs an Expectation-Maximization (EM) algorithm to estimate regime and the likelihood sequence of regimes. 
Gaussian mixture model is a probabilistic model that assumes all the data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters- ‘scikit-learn’ 
Choosing a number of hidden states for the HMM is a critical task. We will use standard criteria: the AIC and the BIC to examine the performances of HMM with different numbers of states. The two measures are suitable for HMM because, in the model training algorithm, the Baum–Welch algorithm, the EM method was used to maximize the log-likelihood of the model. We will limit numbers of states from two to four to keep the model simple and feasible for stock prediction. The AIC and BIC are calculated using the following formulas, respectively: 
where L is the likelihood function for the model, M is the number of observation points, and k is the number of estimated parameters in the model. In this paper, we assume that the distribution corresponding to each hidden state is a Gaussian distribution. Therefore, the number of parameters, k, is formulated as k = N2 + 2N − 1, where N is numbers of states used in the HMM. 
Both methods allow us to compare the relative suitability of different models. When choosing among a set of models we want to choose the AIC or BIC with the smallest information criterion value. 
AIC rewards goodness of fit, but it also includes a penalty that is an increasing function of the number of estimated parameters. The penalty discourages over-fitting, because increasing the number of parameters in the model almost always improves the goodness of the fit — wikipedia 
Both metrics implement a penalty; compared to AIC, BIC penalizes additional parameters more heavily and results in selecting fewer parameters. Gaussian Mixture comes with different options to constrain the covariance of the difference classes estimated e.g. spherical, diagonal, tied or full covariance. Here, we will do a grid search taking a range of components and try all the classes on the fitted GMM to identify number of optimal number of components based BIC score. 
Here x-axis is number of Gaussian, y-axis is BIC; we can see that the BIC is minimized for 3 Gaussian, so the best model according to this method also has 3 components.We can see that, above function has identified 6 regimes. 
Time series exhibit temporary periods where the expected means and variances are stable through time. These periods or regimes can be likened to hidden states. 
Now, using the optimum number of 3 regimes, we will use the GMM to fit a model that estimates these regimes. We will initiate and fit a GMM to predict the hidden states using mean and variances. Depends on your computing speed, this might take some time to run. 
The algorithm keeps track of the state with the highest probability at each stage. At the end of the sequence, the it will iterate backwards selecting the state that “won” each time step, and thus creating the most likely sequence of hidden states that led to the sequence of observations. 
We see here that out model is based on Dynamic Bayesian Networks. The AIC & BIC values are quite close here. Finally, we will summarize the mean and variance of each observed variable on the corresponding regime. The performance of the indicators on each regime (in terms of center and spread) is given below. 
We see here that, each regime is characterized by means and covariances of the hidden states (regimes). Covarinaces part is the volatility indicator. The highlighted ones are the mean and variance of foretasted ‘open’ price. Let us go through each state to analyse the output. 
We can also assume that opening price will transition between these regimes based on probability. Our goal is to minimize the variance of return and then maximize the mean price. 
An interesting visualization of Markov chain process that hop from one state to another, can be found here. Although, the mean-variance portfolio provides a foundation of modern finance theory and has numerous extensions and applications; however, it attracts maximum returns under a given risk. Optimal asset allocation across many assets is important and difficult. 
Moreover, from the observations we can see that, that the stock market performs significantly different across different regimes. The momentum of a stock depends on many factors which includes corporate financial condition, management, overall economic condition and industry conditions, besides volatility other macroeconomic variables such as inflation (consumer price index), industrial production index etc. These factors and corresponding stock returns vary widely over different regimes. Moreover, long-term investments depend on the trends of these economic factors. 
Bayesian network model is a statistical model and not a structural model and through this network, we general approach is to look back in time and measure relationship from there without taking into account what could potentially happen. However, the biggest advantage is that, we could encode assumptions through Bayesian network and statistical measures to take care of rarest of rare event could happen and possible could have an impact on forecasting. 
Setting up optimization parameters and constraints to maximize the model performance is an important aspect of any forecasting model. I will discuss about optimization algorithm in a separate article. 
I can be reached here. 
Notice: The programs described here are experimental and should be used with caution. All such use at your own risk. 
References: 
Written by 
Written by",Sarit Maitra,2020-05-22T02:59:38.950Z
Topic Modeling of New York Times Articles | by Susan Li | SwiftWorld | Medium,"(This article first appeared on my website) 
In machine learning and natural language processing, A “topic” consists of a cluster of words that frequently occur together. A topic model is a type of statistical model for discovering the abstract “topics” that occur in a collection of documents. Topic modeling is a frequently used as a text-mining tool for the discovery of hidden semantic structures in a text body. Topic models can connect words with similar meanings and distinguish between the uses of words with multiple meanings. 
For this analysis, I downloaded 22 recent articles from business and technology sections in the New York Times. I am using the collection of these 22 articles as my corpus for the topic modeling exercise. Therefore, each article is a document, with an unknown topic structure. 
Latent Dirichlet allocation (LDA) is one of the most common algorithms for topic modeling. LDA assumes that each document in a corpus contains a mix of topics that are found throughout the entire corpus. The topic structure is unknown — we can only observe the documents and words, not the topics themselves. Because the structure is unknown (also known as latent), this method seeks to infer the topic structure given the known words and documents. 
This has turned the model into a one-topic-per-term-per-row format. For each combination the model has beta — the probability of that term being generated from that topic. For example, the term “adapt” has a 3.204101e-04 probability of being generated from topic 1, but a 8.591570e-103 probability of being generated from topic 2. 
The 4 topics generally describe: 
Let’s set k = 9, see how do our results change? 
From a quick view of the visualization it appears that the algorithm has done a decent job. The most common words in topic 9 include “uber” and “khosrowshahi”, which suggests it is about the new Uber CEO Dara Khosrowshahi. The most common words in topic 5 include “insurance”, “houston”, and “corporate”, suggesting that this topic represents insurance related matters after Houston’s Hurrican Harvey. One interesting observation is that the word “company” is common in 6 of the 9 topics. 
In the interest of space, I fit a model with 9 topics to this data set. I encourage you to try a range of different values of k (topic) to find the optimal number of topics, to see whether the model’s performance can be improved. 
Besides estimating each topic as a mixture of words, topic modeling also models each document as a mixture of topics like so: 
Each of these values (gamma) is an estimated proportion of words from that document that are generated from that topic. For example, the model estimates that about 0.008% of the words in document 1 were generated from topic 1. To confirm this result, we checked what the most common words in document 1 were: 
This appears to be an article about teenage driving. Topic 1 does not have driving related topics, which means that the algorithm was right not to place this document in topic 1. 
Topic modeling can provide ways to get from raw text to a deeper understanding of unstructured data. However, we always need to examine the results carefully to ensure that they make sense. 
So, try yourself, have fun, and start practicing those topic modeling skills! 
Written by 
Written by",Susan Li,2017-09-14T17:13:25.160Z
Hidden Markov Models Simplified. Sanjay Dorairaj | by Sanjay Dorairaj | Medium,"Hidden Markov Models (HMMs) are a class of probabilistic graphical model that allow us to predict a sequence of unknown (hidden) variables from a set of observed variables. A simple example of an HMM is predicting the weather (hidden variable) based on the type of clothes that someone wears (observed). An HMM can be viewed as a Bayes Net unrolled through time with observations made at a sequence of time steps being used to predict the best sequence of hidden states. 
The below diagram from Wikipedia shows an HMM and its transitions. The scenario is a room that contains urns X1, X2 and X3, each of which contains a known mix of balls, each ball labeled y1, y2, y3 and y4. A sequence of four balls is randomly drawn. In this particular case, the user observes a sequence of balls y1,y2,y3 and y4 and is attempting to discern the hidden state which is the right sequence of three urns that these four balls were pulled from. 
Figure 1: HMM hidden and observed states 
Source: https://en.wikipedia.org/wiki/Hidden_Markov_model#/media/File:HiddenMarkovModel.svg 
In this post, we focus on the use of Hidden Markov Models for Parts of Speech (POS) diagram and walk through the intuition and the code for POS tagging using HMMs. 
Complete source code for this post is available in Github at https://github.com/dorairajsanjay/hmm_tutorial 
The reason it is called a Hidden Markov Model is because we are constructing an inference model based on the assumptions of a Markov process. The Markov process assumption is simply that the “future is independent of the past given the present”. In other words, assuming we know our present state, we do not need any other historical information to predict the future state. 
To make this point clear, let us consider the scenario below where the weather, the hidden variable, can be hot, mild or cold and the observed variables are the type of clothing worn. The arrows represent transitions from a hidden state to another hidden state or from a hidden state to an observed variable. 
Notice that, true to the Markov assumption, each state only depends on the previous state and not on any other prior states. 
Figure 2: HMM State Transitions 
HMMs are probabilistic models. They allow us to compute the joint probability of a set of hidden states given a set of observed states. The hidden states are also referred to as latent states. Once we know the joint probability of a sequence of hidden states, we determine the best possible sequence i.e. the sequence with the highest probability and choose that sequence as the best sequence of hidden states. 
The ratio of hidden states to observed states is not necessarily 1 is to 1 as is evidenced by Figure 1 above. The key idea is that one or more observations allow us to make an inference about a sequence of hidden states. 
In order to compute the joint probability of a sequence of hidden states, we need to assemble three types of information. 
Generally, the term “states” are used to refer to the hidden states and “observations” are used to refer to the observed states. 
The above information can be computed directly from our training data. For example, in the case of our weather example in Figure 2, our training data would consist of the hidden state and observations for a number of days. We could build our transition matrices of transitions, emissions and initial state probabilities directly from this training data. 
The example tables show a set of possible values that could be derived for the weather/clothing scenario. 
Figure 3: HMM State Transitions — Weather Example 
Once this information is known, then the joint probability of the sequence, by the conditional probability chain rule and by Markov assumption, can be shown to be proportional to P(Y) below 
Figure 4: HMM — Basic Math (HMM lectures) 
Note that as the number of observed states and hidden states gets large the computation gets more computationally intractable. If there are k possible values for each hidden sequence and we have a sequence length of n, there there are n^k total possible sequences that must be all scored and ranked in order to determine a winning candidate. 
The probability distributions of hidden states is not always known. In this case, we use Expectation Maximization (EM) models in order to determine hidden state distributions. A popular algorithm is the Baum-Welch algorithm (https://en.wikipedia.org/wiki/Baum%E2%80%93Welch_algorithm) 
As seen in the above sections on HMM, the computations become intractable as the sequence length and possible values of hidden states become large. It has been found that the problem of scoring an HMM sequence can be solved efficiently using dynamic programming, which is nothing but cached recursion. 
Shown below is an image of the recursive computation of a fibonnaci series 
Figure 5: Fibonnacci Series — Tree 
One of the things that becomes obvious when looking at this picture is that several results (fib(x) values) are reused in the computation. By caching these results, we can greatly speed up our operations 
Notice the significant improvement in performance when we move to dynamic programming or cached recursion. We use this same idea when trying to score HMM sequences as well using an algorithm called the Forward-Backward algorithm which we will talk about later 
Here we look at an idea that will be leveraged in the forward backward algorithm. This is idea that double summations of terms can be rearrangeed as a product of each of the individual summation. 
The example below explains this idea further. 
Figure 6: HMM — Manipulation Summations 
The code below demonstrates this equivalency relationship 
Similar to manipulating double summations, the max of a double maxation can be viewed as the product of each of the individual maxations. 
Figure — 7: HMM — Manipulation Maxes 
In this section, we will consider the toy example below and use the information from that example to train our simple HMM model 
Figure — 8: HMM — Toy Example — Graph 
Figure — 9: HMM — Toy Example — Transition Tables 
In this example, we score a known sequence given some text 
Let us consider the below sequence 
Figure — 10: HMM — Toy Example — Graph 
The score for this sequence can be computed as 
Figure — 11: HMM — Toy Example — Scoring Known Sequence 
The joint probability for our unknown sequence is therefore 
P(A,B,A,Red,Green,Red) = [P(y_0=A) P(x_0=Red/y_0=A)] [P(y_1=B|y_0=A|) 
P(x_1=Green/y_1=B)] [P(y_2=A|y_1=B) P(x_2=Red/y_2=A)] 
=(1∗1)∗(1∗0.75)∗(1∗1)(1)(1)=(1∗1)∗(1∗0.75)∗(1∗1) 
=0.75(2)(2)=0.75 
Assuming that we need to determine the parts of speech tags (hidden state) given some sentence (the observed values), we will need to first score every possible sequence of hidden states and then pick the best sequence to determine the parts of speech for this sentence. 
We will score this using the below steps 
Note that selecting the best scoring sequence is also known as the Viterbi score. The alternative approach is the Minimum Bayes Risk approach which selects the highest scoring position across all sequence scores. 
Let us consider the below graph 
Figure — 12: HMM — Toy Example — Scoring an Unknown Sequence 
In the example below, we look at Parts of Speech tagging for a simple sentence. The sequence of words in the sentence are the observations and the Parts of Speech are the hidden states. Given a sentence, we are looking to predict the corresponding POS tags. 
Let us consider the below graph, where the states are known and represent the POS tags and the red/green circles represent the observations or the sequence of words. 
Figure — 13: HMM — Toy Example — Scoring an Unknown Sequence 
The code below initializes probability distributions for our priors, hidden states and observations. It then generates a set of all possible sequences for the hidden states. We will use this later to compute the score for each possible sequence. 
Here, we try to find out the best possible value for a particular y location location where y represents our hidden states starting from y=0,1…n-1, where n is the sequence length 
For example, if we need to first pick the position we are interested in, let’s say we are in the second position of the hidden sequence i.e. y1. We examine the set of sequences and their scores, only this time, we group sequences by possible values of y1 and compute the total scores within each group. The group with the highest score is the forward/backward score 
This is demonstrated in the code block below 
Since predicting the optimal sequence using HMM can become computational tedious as the number of the sequence length increases, we resort to dynamic programming (cached recursion) in order to improve its performance. 
In this section, we will increase our sequence length to a much longer sentence and examine the impact on computation time. The same performance issues will also be encountered if the number of states is large, although in this case, we will only tweak the sequence length. 
Notice that the time taken get very large even for small increases in sequence length and for a very a small state count. 
Dynamic programming is implemented using cached recursion. Formulating a problem recursively and caching intermediate values allows for exponential improvements in performance compared to other methods of computation. 
The MBR solution can be computed using dynamic programming. MBR allows us to compute the sum over all sequences conditioned on keeping one of the hidden states at a particular position fixed. This in turn allows us to determine the best score for a given state at a given position. We do this by computing the best score for every state at that position and pick the state that has the highest score. 
Consider the example of a sequence of four words — “Bob ate the fruit”. Let us assume that we would like to compute the MBR score conditioned on the hidden state at position 1 (y1) being a Noun (N). 
This computation can be mathematically shown to be equivalent to 
Figure — 14: HMM — Dynamic Programming — Finding the MBR Score Source: UC Berkeley lectures 
We break up the computation into two parts. In the first part, we compute alpha, the sum of all possible ways that the sequence can end up as a Noun in position 1 and in the second part, we compute beta, the sum of all possible ways that the sequence can start as a Noun. 
The below code computes our alpha and beta values. We make dynamic caching an argument in order to demonstrate performance differences with and without caching. 
We will now test out the dynamic programming algorithm with and without caching enabled to look at performance improvements. 
For our dataset, we will use a much longer sequence since we have a much more efficient algorithm. 
Notice the significant improvements in time when we use the version with cached recursion. 
HMMs are used in a variety of scenarios including Natural Language Processing, Robotics and Bio-genetics. In this post, we saw some of the basics of HMMs, especially in the context of NLP and Parts of Speech tagging. In later posts, I hope to elaborate on other HMM concepts based on Expectation Maximization and related algorithms. 
I want to acknowledge my gratitude to James Kunz and Ian Tenney, lecturers at the UC Berkeley Information and Data Science program, for their help and support. 
Written by 
Written by",Sanjay Dorairaj,2018-03-20T13:56:34.118Z
The Strategic Seventeen: Zero Trust | by Logan Daley | The Startup | Medium,"The long-held misconception of keeping “the bad guys” out gave us the false sense of security that inside of our business we were safe from the nefarious entities that lurked in the deep, dark corners of the Internet. We spent ridiculous amounts of time and money building our castles, ever focused on the “before” of cybersecurity incidents and how to prevent them without a second thought of what to do during and after one if it, goodness forbid, ever came to be. 
We sat like kings and queens on our swivel-chair thrones, smugly believing bad things only happened to other businesses and people; never to us. Our castles were invulnerable to the evil masses that gathered outside. We think that all those shiny boxes and blinking lights protect us and we’ve built a veritable fortress out of our security budget. Well, under your crown of smugness, you’re no longer a king or queen; you’re a joker. 
So, Zero Trust, then? 
What Is It? 
Developed a decade ago, the Zero Trust framework has recently gained more attention due to the collective castle walls of many organisations crumbling and the owners of information systems and data becoming usurped by malicious entities. There is plenty of proof and anecdotal evidence to assure us that cybersecurity incidents are a matter of “when” and not “if”. When you look at it, threat actors tend to come in three varieties: Malicious Outsiders, Malicious Insiders, and Well-Intended Insiders. 
It should be worth noting that these three are not absolutes. For example, if a Malicious Outsider gains access through a compromised perimeter or stolen credentials, they effectively become a Malicious Insider. Even the Well-Intended Insiders can become Malicious Insiders or Malicious Outsiders under the right circumstances. 
The commonality of the three is that they’re all threats but worryingly, two of them are “insiders”. The traditional security model of “inside is good, outside is bad” falls on its face. Depending on what you read and who you talk to, the majority of threats are internal, so why we continue to focus so much on the “before” and keeping the bad guys out is beyond me. Enter the Zero Trust framework. 
A false assumption I hear from people when discussing the Zero Trust approach is they liken it to conspiracy theories and basement-dwellers with tinfoil hats. While at the outset, it sounds like Dr. No (just to throw in a James Bond 007 reference), it should be thought of more like “Yes, but.” Instead of universally saying no to everything, it becomes yes ONLY WHEN conditions have been met. Everything gets verified, inside and out. Never trust, always verify is another way to look at it. 
Where Do I Start? 
I would suggest starting with a consultative approach from an agnostic perspective to fully understand why you need Zero Trust and how it can work for you. There are no shortage of products masquerading as “solutions” that can have effects from a negligible impact to “bricking” your entire network. Get the right people involved from the start and ask the right questions. You need to figure out why Zero Trust before you can approach the “how”. 
You will probably discover that not every aspect of your environment needs a Zero Trust approach, but some surely does. You might have a public Wi-Fi network for internet access only that is segregated from the rest of your systems. On the other hand, the corporate Wi-Fi network should be secured with connected devices and users verified with certificates and multi-factor authentication logons, for example. A good starting point is understanding what are your critical and important systems and data. 
Your source of truth for authentication and authorisation should be squeaky clean, clearly defined, and well maintained. Perhaps a clean-up of your Active Directory (if you use Microsoft) is a good place to start by reviewing roles and responsibilities before defining what they can actually access. After this, a full inventory of devices and services is helpful to understand what will be accessing the systems and data. Being able to identify authorised devices in addition to authorised users is going to be critical, especially when trying to avoid the threat of spoofing. Oh yeah — PLEASE get rid of generic accounts and ban any type of credential sharing. 
The approach actually reminds me a bit of Application Whitelisting because in that case, I also recommend getting everything in order and understand exactly what you are trying to do and why before beginning. 
I would also make sure you have complete support across the business, management and executive buy-in (having a champion of this at the C-level is gold), and that there is clear communication to all stakeholders to ensure they know not just WHAT you are doing, but clearly WHY and HOW it will benefit them. 
How do I make It Work? 
Rather than a one-size fits all whizbang application or appliance, Zero Trust relies on a more strategic approach using a number of technologies and controls, both technical and administrative. At the core of it, as I mentioned above, you should have your house in order and visibility and control of all objects in the infrastructure, from users to computers and all points in between. Review file shares and permissions. Review Group Policy Objects. Apply the principal of least privilege. 
In terms of technology, Multi-Factor Authentication (MFA) is a big one. Applied to important systems and data, it provides a great layer of defence although there may be a little resistance from users if it involves using their personal mobile device. Not everyone wants to install an authenticator app, and SMS only isn’t the most secure, but it will depend on your accepted level of risk. 
If you’re curious why I suggest getting the house in order, it will help if you decide to look at Identity and Access Management (IAM) solutions that rely heavily on your source of truth. There are plenty of other solutions you can look to like orchestration, analytics (especially behavioural from user and computer actions) and never overlook one of the longest-established controls: encryption. Encrypted data in transit, data in use, and data at rest as well as using certificates for verification is invaluable. 
The actual execution of implementing zero trust should be done on a case by case basis to ensure that the solution chosen works best for the organisation it’s intended to protect. After reading all this, though, you’re probably thinking, “Oh good grief. Now I have to buy a whole lot of stuff to make this work!” Ah, but probably not! 
As part of the consulting phase, I recommend understanding what you have via an inventory and you will probably find you already have a lot of the building blocks you need. Remember the big box of Lego we had as kids that had hundreds or thousands of pieces in it and you could build almost everything you wanted? That’s likely your information systems — just don’t go putting Lego wheels on Lego boats — be sure what you have IS, in fact, what you need. 
If you’re heavily invested in the Microsoft space, for example, you likely have a lot of controls available. MFA, IAM, Microsoft System Centre Orchestrator, Microsoft Analytics, Sentinel, Microsoft Cloud App Security, Advanced Threat Protection, and on and on. It’s also a great way to leverage the Microsoft Ecosystem to implement Just In Time (JIT), and Just Enough Administration (JEA) to knuckle down on that “Never Trust, Always Verify” approach. 
Depending on your systems, you may have other vendors and tools, but just be aware you have options and to make sure you get the most out of your existing investments before you spend more money. Ask questions, get answers, and then decide. After all, these are YOUR systems and it is YOUR data. 
I would also suggest reviewing the governance around your information systems and data to ensure that you have policies and procedures that articulate what a Zero Trust framework is, why you are using it, and how it will be used. A regular review of your documentation to this effect is always a good idea; doubly so when rolling out Zero Trust. 
Pitfalls? 
One of the bigger pitfalls I have encountered to date with the application of Zero Trust has been the overzealousness of those implementing it to the point where systems availability, performance, and productivity has been hindered. In some cases, almost like a self-imposed Denial Of Service. Your information systems and data may not be available to untrusted entities, but if they’re not available to trusted ones, then it’s not much good at all, is it? Be cautious about the application of controls and mindful that the systems you’re protecting are there to enable your business. 
This is why we suggest a consultative approach in implementing Zero Trust to clearly define requirements and objectives and to have a clear vision of desired outcomes and measures of success. It needs to be simple and sustainable or else you’ll find shadow IT popping up as users create work-arounds to just get their work done. 
Technical controls, in this manner, are not the be-all and end-all of a Zero Trust project. Administrative controls, enforced governance, clear policies and procedures, and management buy-in and support are crucial to a successful engagement. 
Ghosts in The Machine? 
No matter how many controls you put in place, there will always be ghosts in the machine in the form of people. Be mindful of those you trust and the level of access they have because being human, we can and do make mistakes and do stupid things, even if we have been fully verified. People can be exploited through manipulation and social engineering, coerced to take malicious action, or become disgruntled and abuse their privilege. I once consulted to an organisation where their main administrator had a domestic situation and ended up abusing his privileged access to key systems to take it to whole new level of ugliness. Sometimes additional checks and balances are needed, and sometimes you can only sort it out after the fact. 
Technology, being technology, is subject to failure and errors, so the mechanisms used to authenticate can fail, rendering the whole system unusable because Zero Trust worked too well. Ensure this is planned for to avoid those “oh no” moments. Sometimes this ghost in the machine is more like a demon. Forget the Ghostbusters; call in the Exorcist! 
Anything Missing? 
It’s easy to overlook something in the beginning stages of a Zero Trust implementation, but a clear understanding of the objective up front, an inventory of your systems and data, and a review of existing controls you can leverage can fill in a lot of gaps. You should also try to break it down into short-term tactical actions that lead towards long-term strategic objectives so that you gain benefit at every stage of a Zero Trust implementation rather than waiting until the end. 
Stay safe out there! 
Disclaimer: The thoughts and opinions presented on this blog are my own and not those of any associated third party. The content is provided for general information, educational, and entertainment purposes and does not constitute legal advice or recommendations; it must not be relied upon as such. Appropriate legal advice should be obtained in actual situations. All images, unless otherwise credited, are licensed through ShutterStock 
Written by 
Written by",Logan Daley,2020-01-06T07:51:34.390Z
An Insight to Genetic Algorithms — Part I | by Chathurangi Shyalika | Data Driven Investor | Medium,"Let’s start with the famous quote by Charles Darwin: 
“It is not the strongest of the species that survives, nor the most intelligent, but the one who is most adaptable to change.” 
You must be wondering what this quote has to do with genetic algorithms? Actually, the entire concept of genetic algorithms lies on just the above quote! 
Genetic Algorithms are a search-based optimization technique based on Darwin’s principle of Natural Selection. This is a novel AI technique introduced in 1970’s. This has the ability to solve questions that cannot be solved through any other techniques like Artificial Neural Networks. 
Nature has always been a great source of inspiration to all mankind. Genetic Algorithms are search based algorithms based on the biological concepts of natural selection and genetics. Genetic Algorithms are a subset of a much deeper branch of computation known as Evolutionary Computation. 
Genetic Algorithms were initially developed by John Holland and his students and colleagues at the University of Michigan. Remarkably David E. Goldberg has since been tried on various optimization problems with a high degree of success for Genetic Algorithms. 
In genetic algorithms, we have a pool or a population of possible solutions to a given problem. These solutions then undergo some genetic operations, producing new children and the process is repeated over various generations. Each individual (or candidate solution) is assigned a fitness value based on an evaluation function value and the fitter individuals are given a higher chance to mate and yield more “fitter” individuals. This is in line with the Darwinian Theory of “Survival of the Fittest”. 
In this way, we keep “evolving” better individuals or solutions over generations, till we reach a termination criterion. 
Optimization is the process of making something better. 
Optimization refers to finding the values of inputs in such a way that we get the “best” output values. The definition of “best” varies from problem to problem, but in mathematical terms, it refers to maximizing or minimizing one or more objective functions, by varying the input parameters. 
The set of all possible solutions or values which the inputs can take make up the search space. In this search space, lies a point or a set of points which gives the optimal solution. The aim of optimization is to find that point or set of points in the search space. 
The main task performed by evolutionary algorithms is optimization. The difference between traditional algorithms and evolutionary algorithms is that evolutionary algorithms are dynamic and thus they can evolve over time and can be efficiently be used to represent frequently changing information. 
Evolutionary algorithms have three main characteristics: 
1. Population-Based: Evolutionary algorithms are to optimize a process in which the current set of solutions are bad/not optimal to generate new/ better/optimal solutions. The set of current solutions which is referred here is termed the population. 
2. Fitness-Oriented: There is a fitness value associated with each individual solution which is calculated from a fitness function. This fitness value reflects what extent the solution is good. 
3. Variation-Driven: If there is no acceptable solution in the current population according to the fitness function calculated from each individual, we should make adaptation to generate new better solutions. As a result, individual solutions will undergo a number of variations, simply iterations to generate new solutions. 
Genetic Algorithms have the amazing ability to provide “good enough” and “fast-enough” solutions. This makes genetic algorithms attractive for use in solving real-world problems. The reasons why Genetic Algorithms are needed are as follows − 
1)Solving difficult problems 
Think of a situation where there are a complex and large set of problems to be solved. Even the most powerful computing systems take a very long time (even years) to come across such difficult problems. In such a situation, Genetic algorithms prove to be an effective tool to provide usable near-optimal solutions in a short amount of time. 
2) Getting a better/optimal solution fast 
Some real-world problems may have a number of solutions. In real life, we meet conditions where we need only the most suitable answer in hand. For examples storing of containers of different sizes in a ship dock, arranging rooms in a house map such that at the end same house area is obtaining, storing of goods in a rack such that there is minimal space wasting can be taken. Pathfinding problems like the Travelling Salesmen Problem and VLSI Design also can be addressed by Genetic Algorithms. 
3) Processing Dynamic information 
Recall that Artificial Neural Network requires collected data of a long time and that data should have the ability to represent every state that the system tries to predict in future. The network is unable to work on strange information which was unavailable when the model was being developed. Genetic Algorithms address this issue by being able to provide solutions for dynamically changing data. Since Genetic Algorithms can be effectively used for Traffic light controlling systems, Weather forecasting and for predicting trends and patterns in the stock market. 
4) Obtaining similar solutions 
Have you ever thought to win lottery numbers by analyzing and finding a pattern in previously won lottery numbers? Genetic algorithms are able to provide solutions for such kind of situations where we need to discover some matching similar solutions. Magical! Isn’t it? 
5) Discovering unforeseen solutions 
We have come across situations where children have almost different characteristics from that of their parents. Following this genetic phenomenon, it is clear that Genetic algorithms can be utilized to discover unforeseen solutions. This feature can be adopted for situations like designing interior architecture of a house or a company. 
6) Checking whether there is a solution 
Genetic algorithms can be used in systems to check whether there is a solution for a given problem. For example, there are problems which were unable to solve by linear programming were able to be solved by genetic algorithms. Apart from that genetic algorithms has been used in determining the shape of the turbines in Boeing 747 Jumbo Jet. Earlier it has been calculated that it would approximately take 10 years for that purpose if only supercomputers, calculus, and linear programming was used. 
7) Scientific solutions for random experiments 
When there are a number of possible solutions for a given problem, we tend to use a random answer as the solution. But there is an uncertainty in that random answer not being the most optimal solution for the problem. Genetic algorithms can be used in providing more assured answers for this kind of random experiments. For example, think of a university lecturer preparing exam papers by selecting the questions from a large list of questions. If genetic algorithms are used he can prepare exam papers that contain questions of high quality. 
Genetic Algorithms lie within the following basic principle terms, where you need to be familiar with. 
*Population — It is a subset of all the possible (encoded) solutions for a given problem. Simply, this is the set of individuals and each individual is a solution to the problem we want to solve. 
*Chromosomes − A chromosome is one such solution to the given problem. 
*Gene − A gene is one element position of a chromosome. The individuals in a population are characterized by a set of parameters (variables) and they are particular, termed as Genes. Basically, Genes are joined into a string to form a Chromosome (solution). 
*Allele − It is the value a gene takes for a particular chromosome. 
*Genotype − Genotype is the population in the computation space. In the computation space, the solutions are represented in a way which can be easily understood and manipulated using a computing system. 
*Phenotype − Phenotype is the population in the actual real world solution space in which solutions are represented in a way they are represented in real-world situations. 
*Decoding and Encoding − Decoding is a process of transforming a solution from the genotype to the phenotype space, while encoding is a process of transforming from the phenotype to genotype space. 
*Fitness Function / Evaluation Function –Fitness Function evaluates how close a given solution is to the optimum solution of the desired problem. It determines how fitting a particular solution is. 
*Genetic Operators − These alter the genetic composition of the offspring. These include crossover, mutation, selection, etc. 
The idea of the selection phase is to select the fittest individuals and let them pass their genes to the next generation. 
Two pairs of individuals (parents) are selected based on their fitness scores. Individuals with high fitness have more chance to be selected for mating. 
Fertilization occurs by the process of exchanging the genes in chromosomes. Crossover is such gene-exchanging mechanism that has been identified so far. It is one of the most significant phases in a genetic algorithm. Crossover is the mostly occurring genetic operation among chromosomes. Here for each pair of parents to be mated, a crossover point is chosen at random from within the genes. 
In the crossover, Offspring are created by exchanging the genes of parents among themselves until the crossover point is reached. The new offspring are added to the population through this process of exchanging genes among parents. 
For example, consider the crossover point chosen between gene 2 and 3 of parent chromosomes as shown below in (i). Here (i) shows the parent chromosomes, (ii) crossover operation and (iii) Resulted child chromosomes /off springs respectively. 
Basically, there are 4 types of crossover as follows. 
Single point crossover — One crossover point is selected, binary string from beginning of chromosome to the crossover point is copied from one parent, the rest is copied from the second parent. 
Example:- 1 1 0 0 1 0 1 1 + 1 1 0 1 1 1 1 1 = 1 1 0 0 1 1 1 1 
Two point crossover —Two crossover point are selected, binary string from beginning of chromosome to the first crossover point is copied from one parent, the part from the first to the second crossover point is copied from the second parent and the rest is copied from the first parent. 
Example:- 1 1 0 0 1 0 1 1 + 1 1 0 1 1 1 1 1 = 1 1 0 1 1 1 1 1 
Uniform crossover — Bits are randomly copied from the first or from the second parent. 
Example:- 1 1 0 0 1 0 1 1 + 1 1 0 1 1 1 0 1 = 1 1 0 1 1 1 1 1 
Arithmetic crossover — Some arithmetic operation is performed to make a new offspring 
Example:-1 1 0 0 1 0 1 1 + 1 1 0 1 1 1 1 1 = 1 1 0 0 1 0 0 1 (AND) 
Note that in the crossover, parent chromosomes get directly transferred to offsprings. Thus offsprings have almost or some similar characteristics to that of their parents. 
In some situations, we see children that have almost different characteristics when compared with their parents. This occurs as a result of a mutation that happens between chromosomes. Here some of the genes in a chromosome get flipped and get a completely opposite value. 
Note that crossover is a frequent phenomenon occurs within chromosomes whereas mutation occurs very rarely and so it has a low random probability. Crossover happens between 2 chromosomes whereas mutation occurs in a gene/genes within a single chromosome. 
Since the off-springs hand on completely different features; Mutation occurs to maintain diversity within the population and prevent premature convergence. Thus mutation always adds a disturbance / randomness / noise to the population. So mutation is also known as a randomness added / noise added operation in genetics. 
The basic structure of a Genetic Algorithm is as follows. 
The process should be started with an initial population, which may be generated at random or seeded by other heuristics. Next, using evaluation function, parents are selected from this population for mating. Then crossover and mutation operators are applied to the parents to generate new off-springs. Finally, these off-springs replace the existing individuals in the population and the process repeats. This is really how genetic algorithms actually mimic the human evolution. The whole process can be combined with a flowchart as follows. 
Genetic Algorithms are primarily used in optimization problems, then again they are frequently used in other application areas as well. 
Following are some of the areas in which Genetic Algorithms are frequently used. These are − 
-Optimization − Genetic Algorithms are most commonly used in optimization problems where we have to maximize or minimize a given evaluation function value under a given set of constraints. 
-Artificial Neural Networks − To train neural networks, mainly recurrent neural networks. 
-Financial Sector — In the financial markets, genetic algorithms are most commonly used to find the best combination values of parameters in a trading rule and they can be built into ANN models designed to pick stocks and identify trades. 
-Economics − To characterize various economic models like the cobweb model, asset pricing, game theory equilibrium resolution, etc. 
-Parallelization − Genetic algorithms have very good parallel capabilities and prove to be very effective means in solving certain problems and also provide a good area for research. 
-Image Processing − Genetic algorithms are used for various digital image processing applications like dense pixel matching. 
-Agricultural Sector- Precision agriculture is a new trend in the agricultural sector which is primarily based on Decision Support System (DSS) for farm management with the goal of optimizing returns on inputs while preserving resources. Genetic algorithms are used in precision agriculture systems to determine the best nutrition combinations in greenhouses, determine optimal cropping patterns and in developing agriculture extension agents etc. 
-Scheduling applications −Genetic algorithms are used to solve various scheduling problems, for instance, the timetabling problem. 
-Robot Trajectory Generation − To plan the path which a robot arm takes by moving from one point to another. 
-Parametric Design of Aircrafts − To design aircraft by varying the parameters and evolving better solutions. 
-DNA Analysis −In determining the structure of DNA using spectrometric data about the sample. 
· Multi-modal Optimization − Genetic algorithms are obviously very good approaches for multi-modal optimization in which we have to find multiple optimum solutions. 
· Traveling Salesman Problem and its applications − Genetic algorithms have been used to solve the Traveling salesman problem, which is a well-known combinatorial problem using novel crossover and packing strategies. 
Like any scientific technique, Genetic Algorithms also suffer from a few limitations. These include − 
i) Not suited for all problems, especially problems which are simple and for which derivative information is available. 
ii) The fitness value is calculated repeatedly which might be computationally expensive for some problems. 
iii) The requirement of computers with high processing power and capacity. Since a lot of space is needed to store the increasing population when a genetic algorithm runs. 
iv) Take more time to produce a result if there is not adequate processing power and computer capacity. 
v) Being stochastic, there are no guarantees on the optimal or the quality of the solution. 
vi) If not implemented properly, the Genetic Algorithms may not converge to the optimal solution. 
This section includes a Demo Application developed in Python to demonstrate how genetic algorithms work. 
This example uses the decimal representation for genes, one point crossover, and uniform mutation. 
The objective of the demo is to maximize an equation. Here genetic algorithm has been used to get the best possible values after a number of generations. 
The solution is available at:- 
I will come with more details on this implementation in my next blog post. 
Hope you got a clear understanding through this initiative blog post. If you have any issues or comments on this blog post please leave a comment below. 
Cheers! 
[1] Genetic Algorithms in Search, Optimization and Machine Learning by David E. Goldberg. 
[2] Artificial Intelligence by Prof. AS Karunananda 
[3] https://www.linkedin.com/pulse/introduction-optimization-genetic-algorithm-ahmed-gad/ 
[4] https://www.kdnuggets.com/2018/03/introduction-optimization-with-genetic-algorithm.html 
[5] https://www.analyticsvidhya.com/blog/2017/07/introduction-to-genetic-algorithm/ 
[6] http://mathgifs.blogspot.com/2014/03/the-traveling-salesman.html 
[7] http://www.obitko.com/tutorials/genetic-algorithms/crossover-mutation.php 
In each issue we share the best stories from the Data-Driven Investor's expert community. Take a look 
Written by 
Written by",Chathurangi Shyalika,2019-02-19T04:20:27.123Z
A Comprehensive Guide to Natural Language Generation | by Sciforce | Sciforce | Medium,"As long as Artificial Intelligence helps us to get more out of the natural language, we see more tasks and fields mushrooming at the intersection of AI and linguistics. In one of our previous articles, we discussed the difference between Natural Language Processing and Natural Language Understanding. Both fields, however, have natural languages as input. At the same time, the urge to establish two-way communication with computers has lead to the emergence of a separate subcategory of tasks dealing with producing (quasi)-natural speech. This subcategory, called Natural Language Generation will be the focus of this blog post. 
Natural Language Generation, as defined by Artificial Intelligence: Natural Language Processing Fundamentals, is the “process of producing meaningful phrases and sentences in the form of natural language.” In its essence, it automatically generates narratives that describe, summarize or explain input structured data in a human-like manner at the speed of thousands of pages per second. 
However, while NLG software can write, it can’t read. The part of NLP that reads human language and turns its unstructured data into structured data understandable to computers is called Natural Language Understanding. 
In general terms, NLG (Natural Language Generation) and NLU (Natural Language Understanding) are subsections of a more general NLP domain that encompasses all software which interprets or produces human language, in either spoken or written form: 
NLG makes data universally understandable making the writing of data-driven financial reports, product descriptions, meeting memos, and more much easier and faster. Ideally, it can take the burden of summarizing the data from analysts to automatically write reports that would be tailored to the audience.The main practical present-day applications of NLG are, therefore, connected with writing analysis or communicating necessary information to customers: 
At the same time, NLG has more theoretical applications that make it a valuable tool not only in Computer Science and Engineering, but also in Cognitive Science and Psycholinguistics. These include: 
In the attempts to mimic human speech, NLG systems used different methods and tricks to adapt their writing style, tone and structure according to the audience, the context and purpose of the narrative. In 2000 Reiter and Dale pipelined NLG architecture distinguishing three stages in the NLG process: 
1. Document planning: deciding what is to be said and creating an abstract document that outlines the structure of the information to be presented. 
2. Microplanning: generation of referring expressions, word choice, and aggregation to flesh out the document specifications. 
3.Realisation: converting the abstract document specifications to a real text, using domain knowledge about syntax, morphology, etc. 
This pipeline shows the milestones of natural language generation, however, specific steps and approaches, as well as the models used, can vary significantly with the technology development. 
There are two major approaches to language generation: using templates and dynamic creation of documents. While only the latter is considered to be “real” NLG, there was a long and multistage way from basic straightforward templates to the state-of-the-art and each new approach expanded functionality and added linguistic capacities: 
One of the oldest approaches is a simple fill-in-the-gap template system. In texts that have a predefined structure and need just a small amount of data to be filled in, this approach can automatically fill in such gaps with data retrieved from a spreadsheet row, database table entry, etc. In principle, you can vary certain aspects of the text: for example, you can decide whether to spell numbers or leave them as is, this approach is quite limited in its use and is not considered to be “real” NLG. 
Basic gap-filling systems were expanded with general-purpose programming constructs via a scripting language or by using business rules. The scripting approach, such as using web templating languages, embeds a template inside a general-purpose scripting language, so it allows for complex conditionals, loops, access to code libraries, etc. Business rule approaches, which are adopted by most document composition tools, work similarly, but focus on writing business rules rather than scripts. Though more powerful than straightforward gap filling, such systems still lack linguistic capabilities and cannot reliably generate complex high-quality texts. 
A logical development of template-based systems was adding word-level grammatical functions to deal with morphology, morphophonology, and orthography as well as to handle possible exceptions. These functions made it easier to generate grammatically correct texts and to write complex template systems. 
Finally taking a step from template-based approaches to dynamic NLG, this approach dynamically creates sentences from representations of the meaning to be conveyed by the sentence and/or its desired linguistic structure. Dynamic creation means that the system can do sensible things in unusual cases, without needing the developer to explicitly write code for every boundary case. It also allows the system to linguistically “optimise” sentences in a number of ways, including reference, aggregation, ordering, and connectives. 
While dynamic sentence generation works at a certain “micro-level”, the “macro-writing” task produces a document which is relevant and useful to its readers, and also well-structured as a narrative. How it is done depends on the goal of the text. For example, a piece of persuasive writing may be based on models of argumentation and behavior change to mimic human rhetoric; and a text that summarizes data for business intelligence may be based on an analysis of key factors that influence the decision. 
Even after NLG shifted from templates to dynamic generation of sentences, it took the technology years of experimenting to achieve satisfactory results. As a part of NLP and, more generally, AI, natural language generation relies on a number of algorithms that address certain problems of creating human-like texts: 
The Markov chain was one of the first algorithms used for language generation. This model predicts the next word in the sentence by using the current word and considering the relationship between each unique word to calculate the probability of the next word. In fact, you have seen them a lot in earlier versions of the smartphone keyboard where they were used to generate suggestions for the next word in the sentence. 
Neural networks are models that try to mimic the operation of the human brain. RNNs pass each item of the sequence through a feedforward network and use the output of the model as input to the next item in the sequence, allowing the information in the previous step to be stored. In each iteration, the model stores the previous words encountered in its memory and calculates the probability of the next word. For each word in the dictionary, the model assigns a probability based on the previous word, selects the word with the highest probability and stores it in memory. RNN’s “memory” makes this model ideal for language generation because it can remember the background of the conversation at any time. However, as the length of the sequence increases, RNNs cannot store words that were encountered remotely in the sentence and makes predictions based on only the most recent word. Due to this limitation, RNNs are unable to produce coherent long sentences. 
To address the problem of long-range dependencies, a variant of RNN called Long short-term memory (LSTM) was introduced. Though similar to RNN, LSTM models include a four-layer neural network. The LSTM consists of four parts: the unit, the input door, the output door and the forgotten door. These allow the RNN to remember or forget words at any time interval by adjusting the information flow of the unit. When a period is encountered, the Forgotten Gate recognizes that the context of the sentence may change and can ignore the current unit state information. This allows the network to selectively track only relevant information while also minimizing the disappearing gradient problem, which allows the model to remember information over a longer period of time. 
Still, the capacity of the LSTM memory is limited to a few hundred words due to their inherently complex sequential paths from the previous unit to the current unit. The same complexity results in high computational requirements that make LSTM difficult to train or parallelize. 
A relatively new model was first introduced in the 2017 Google paper “Attention is all you need”, which proposes a new method called “self-attention mechanism.” The Transformer consists of a stack of encoders for processing inputs of any length and another set of decoders to output the generated sentences. In contrast to LSTM, the Transformer performs only a small, constant number of steps, while applying a self-attention mechanism that directly simulates the relationship between all words in a sentence. Unlike previous models, the Transformer uses the representation of all words in context without having to compress all the information into a single fixed-length representation that allows the system to handle longer sentences without the skyrocketing of computational requirements. 
One of the most famous examples of the Transformer for language generation is OpenAI, their GPT-2 language model. The model learns to predict the next word in a sentence by focusing on words that were previously seen in the model and related to predicting the next word. A more recent upgrade by Google, the Transformers two-way encoder representation (BERT) provides the most advanced results for various NLP tasks. 
You can see that natural language generation is a complicated task that needs to take into account multiple aspects of language, including its structure, grammar, word usage and perception. Luckily, you probably won’t build the whole NLG system from scratch as the market offers multiple ready-to-use tools, both commercial and open-source. 
Arria NLG PLC is believed to be one of the global leaders in NLG technologies and tools and can boast the most advanced NLG engine and reports generated by NLG narratives. The company has patented NLG technologies available for use via Arria NLG platform. 
AX Semantics: offers eCommerce, journalistic and data reporting (e.g. BI or financial reporting) NLG services for over 100 languages. It is a developer-friendly product that uses AI and machine learning to train the platform’s NLP engine. 
Yseop is known for its smart customer experience across platforms like mobile, online or face-to-face. From the NLG perspective, it offers Compose that can be consumed on-premises, in the cloud or as a service, and offers Savvy, a plug-in for Excel and other analytics platforms.Quill by Narrative Science is an NLG platform powered by advanced NLG. Quill converts data to human-intelligent narratives by developing a story, analysing it and extracting the required amount of data from it. 
Wordsmith by Automated Insights is an NLG engine that works chiefly in the sphere of advanced template-based approaches. It allows users to convert data into text in any format or scale. Wordsmith also provides a plethora of language options for data conversion. 
Simplenlg is probably the most widely used open-source realiser, especially by system-builders. It is an open-source Java API for NLG written by the founder of Arria. It has the least functionality but also is the easiest to use and best documented. 
NaturalOWL is an open-source toolkit which can be used to generate descriptions of OWL classes and individuals to configure an NLG framework to specific needs, without doing much programming. 
NLG capabilities have become the de facto option as analytical platforms try to democratize data analytics and help anyone understand their data. Close to human narratives automatically explain insights that otherwise could be lost in tables, charts, and graphs via natural language and act as a companion throughout the data discovery process. Besides, NLG coupled with NLP are the core of chatbots and other automated chats and assistants that provide us with everyday support. 
As NLG continues to evolve, it will become more diversified and will provide effective communication between us and computers in a natural fashion that many SciFi writers dreamed of in their books. 
Written by 
Written by",Sciforce,2019-07-04T15:15:43.544Z
"The State of Applied AI. Surveying the landscape: present and… | by Luke Posey | Aug, 2020 | Towards Data Science","Applied AI continues to accelerate, largely fueled by the maturation of tooling and infrastructure. Couple this infrastructure with a strong pool of talent and enthusiasm, readily accessible capital, and high customer willingness to adopt AI/ML and you’ve got something special. We’re turning the corner into a new decade where AI/ML will create real value for both the consumer and the enterprise at an accelerating pace. 
Applied AI: anything to do with taking AI research from the lab to a use-case and everything in-between. From the infrastructure and tooling, to the hardware, to the deployment surfaces in industry, to the models themselves, it takes a village to get a bleeding edge advance in AI research to a use-case. One great test for maturation in our field is the time it takes for a new advance to get from paper to production. Even just a few years ago you could skim some of the major advances in the field and struggle to find real use-cases; this is quickly starting to change. 
Some choice examples: 
To get from research into production takes far more than just a model. It takes a village of both research and engineering efforts in tandem to make these things work. It takes hardware, it takes scalable hosting, it takes DevOps, it takes great data science, and much more. Thankfully, more and more startups are building solutions for each building block, and big players (Uber and Netflix come to mind) are joining in as they open source more and more of their tooling. 
We’ll remember the all-stars who came up with new models, but the engineers turning it all into production code, the labelers creating your next dataset, and the mob vocally opposing the next breach of security and human rights should all be remembered for their contributions to our field. 
We’re seeing huge opportunities for AI use-cases popping up in all industries. As tooling and infrastructure mature, new opportunities are becoming accessible to anyone that can write a few lines of code. Both disruption of existing markets and creation of new markets are being driven by adoption. 
We’ve already seen the proliferation of Machine Learning into your search engine, fraud detection on your credit card, the camera in your smartphone, and modern marketplaces. And we’re starting to see enterprise adoption as legacy companies invest in the tools and teams necessary to augment their products and processes with ML. 
In this essay we’ll cover not only the ways Applied AI has enabled some of our favorite products and features in the digital world, but we’ll also explore how Applied AI is changing workflows, creating new opportunities, and freeing up labor in fields like manufacturing, construction, supply chains, and commerce. We’ll go in depth on current trends in our field, while also taking some stances on where things are going. 
We can typically identify waves of innovation as enabled by some new technology or event. And in the past decade we have seen the inflection point for AI take us from a bundle of hype to real use-cases driving value across industry. 
So why is now the inflection point for a new wave of value in AI/ML? 
As best practices, tooling, and infrastructure start to mature, accessibility is dramatically increasing. In infrastructure and tooling an advanced team or large open source effort remain the norm. While in practical applications we are seeing successful startups built by junior engineers, budding statisticians, and entrepreneurs willing to sift through the mud to make their application work. And say hello to the flood of MBAs interested in taking part in this wave of opportunity. 
Additionally, an influx of talent, better coursework and training programs, and overall massive hype behind the movement has made hiring a good Data Scientist or Machine Learning Engineer less of a mission to outer space. Because of better tooling, Data Scientists and ML Engineers can go more narrow + deep and be very effective. And most MVPs can be built with either an off-the-shelf model or using one of the beautiful and highly accessible libraries like Scikit-Learn or Keras. We can make all the jokes we want about clf.fit(), but the fact actual models with real value are getting built in just a few lines is a good thing. When senior members of a field start to gripe about all the ‘fake engineers’ and ‘fake data scientists,’ what they really mean to say is, “I’m annoyed that junior people are doing in a few hours what used to take me a few weeks.” 
And access to hardware is no longer a blocker. There are plenty of free compute hours lying around for the enterprising individual. Where early MVPs may have previously required a bit of bootstrapping or help from an angel, most non-research ideas can get off the ground with the primary blocker being access to data. This is a VERY GOOD THING. It’s fair to celebrate primary blockers to training models no longer being a niche skillset or access to expensive infrastructure. 
We’re seeing general consolidation in infrastructure around a handful of core products. AWS, GCP, and Azure remain the clear winners in this wave, with hardware from Nvidia and Intel dominating the data center. We’re also seeing companies pop into the space that take on more niche approaches like cleaner training + deployment (see Paperspace and FloydHub). 
We’re obviously all intimately familiar with TensorFlow, PyTorch, Scikit-Learn and the other major modeling tools. Across industry we’re seeing the continued dominance of Jupyter and various clones for most modeling workflows. There’s also a clear split between more Data Science heavy workflows and ML Engineering workflows, where ML Engineers spend their time in their IDE of choice, while modelers spend more time in Jupyter and projects like Colab, Deepnote, Count, and others with their specific advantages. 
And these tools remain core to the ecosystem. But perhaps the biggest enabler in the last 5 years has been around deployments and serving. Docker and Kubernetes remain core to the ecosystem, while a number of tools have jumped in with their own unique value props. Kubeflow is quickly gaining steam, while TensorFlow Serving, MLFlow, BentoML, Cortex and others vie for similar chunks of the market by trying to enable all modelers to get their model into production with minimal effort. “Deploy models in just a few lines of code” is the tagline of numerous projects. Ease of deployment is great for customer acquisition; scaling and maintaining is what keeps customers in the long-term. 
This innovation was to be expected, as the average Data Scientist and less engineering heavy ML Engineer likely isn’t terribly interested in spending too much time on DevOps, container orchestration, scaling, etc. And lots of teams are skipping out on hiring much engineering talent when building their initial core team. Mileage may vary. 
I tend to look at Machine Learning efforts broadly in the following ladder. In the past we were forced to build many of these rungs ourselves or neglect certain steps entirely (messy versioning, nonexistent CI, manual scaling, only maintaining when the model is clearly broken). Thankfully plenty of teams are working to simplify our lives at almost every step: 
In some cases the above efforts are very separate. In others, the same tool handles multiple steps in the process. For example, we might see a tool for serving also easily handle serialization. In other cases the library for training might be tightly integrated with serializing (pickle, joblib, dill, onnx, etc.). The interesting part of the ecosystem is how tooling is maturing to the point where you can have a full-service tool like BentoML, but you also have plenty of other options with additional customization if needed. More engineering heavy teams might not spend any time using Bento, Cortex, or other services that are intended for less technical audiences. Whereas I personally love tools like BentoML and Cortex because they save tons of time for our small team. MLOps is coming a long way. 
It seems like the piece we’re missing the most in our space is monitoring and maintaining. 
Christopher Samiullah very nicely summarizes this here. 
Obviously this list is incredibly biased towards tools I’ve used in the past or actively am using. Some tools which aren’t ML specific are excluded. For example, Airflow is a key part of many workflows but was exempted in this case. You’ll additionally see a clear bias for the Python ecosystem, perhaps to the chagrin of some. We also exclude databases, versioning, etc. Obviously data collection and cleaning are core to our workflows, but much of this process is not new to software engineering and is covered to far more depth elsewhere than I could ever cover here.. We mostly covered tooling excluded to modelers and ML Engineers, not Data Engineers, analysts, or BI heavy Data Science workflows. 
Let’s talk about the hype of GPT-3. I’m perhaps less excited about the outcomes of GPT-3 than I am about the approach as a model for the rest of the industry. 
It seems likely that we’re gearing up for an arms race for the biggest and best (general purpose) models. Compute at that scale isn’t realistic for smaller companies and startups. Smaller efforts will have to favor clever optimizations and research overthrowing more and more compute at problems. A combination of the two seems to be the obvious winner here, and I’m expecting a general consolidation of the leading modeling efforts around a small group of companies with massive war chests that can afford the compute and fund the research. We will then see a few dominant players serving those models which are public and don’t require highly specialized data to work. These use-cases can be consumed by all sorts of products globally. Let’s visualize this. 
GPT-3 is a great example of where this trend is likely headed. In just a few short weeks the Open AI API is already being accessed by dozens of great use-cases. 
And across the ecosystem we’re seeing similar efforts. This model for development isn’t limited to NLP. Over in vision, a handful of autonomous driving startups with a focus on the software/hardware will likely enable the incumbents who don’t wish to do their own R&D to keep up with those who do. The general enablement of a company to tap into these massive efforts without having to perform R&D is a major win. Expect to see all sorts of models offered as a service. The large-scale models will fuel the bulk of innovation, and smaller and smaller pieces of the pie will get cut out by more niche players. As models get better and better at generalizing, expect less reliance on custom modeling efforts. And those business-specific use-cases aren’t in optimizing models as much as gathering specialized application-specific datasets. Data rules everything around me. 
Many walled gardens will be opposed to lack of security guarantees presented by semi-private APIs. There’s a massive opportunity here for companies that can optimize large models, compress models, and make growing data lakes manageable. It’s hard to believe that legacy corporations will all demand on-premises deployment of the next-generation of 175 billion+ parameter models. But don’t put it past them. 
Things start to get especially interesting when we introduce PII into the mix. Don’t be surprised by companies that laugh in your face at the mention of sending their data off their internal network to some new and trendy API. Companies that can compress models and get similar results from smaller models will remain relevant as long as compute and storage remains an expense. Cost of training and serving continues to get slashed, but deployment costs can still be pretty heavy. AI companies continue to have inferior margins to traditional SaaS companies, largely for this reason. 
“Anecdotally, we have seen a surprisingly consistent pattern in the financial data of AI companies, with gross margins often in the 50–60% range — well below the 60–80%+ benchmark for comparable SaaS businesses.” — a16z 
Big models with billions of parameters will continue to get tons of love. And massive datasets will continue to drive the hyped models. In the reality of industry, smaller models are essential in many use-cases. You’re presented with two core decisions when building in edge scenarios: 
We can deploy to edge devices using solutions like TensorFlow lite. And better hardware for edge and consumer devices is coming out of companies like Hailo, Kneron, and Perceive. The pace of innovation in hardware might outpace the need for small models. 
When remote connectivity is an option, we can always consider performing compute off-chip, though there are plenty of blockers and common constraints like connectivity concerns and time to compute. In environments like manufacturing this may be preferable as connectivity may have higher guarantees due to the stationary nature of the process. We’re already seeing 5G factories pop up where remote control systems are getting installed. Wireless sensors communicate back to the control system wirelessly. It stands to reason remote inference will be part of this transition. There are also plenty of use-cases where we can submit our data, complete some other task, and use our results downstream. Think of manufacturing where you might take a picture of a product upstream, perform transformations, and then downstream match the quality check to the product. This obviously isn’t an option in real-time scenarios like autonomous driving. 
Small data is also incredibly attractive. To perform a successful Proof of Concept we may be tolerant of a liberal amount of Type 1 Errors, depending on the industry and use-case. Sensors can also oftentimes be invasive and the less time we need to gather the data the better. An example of a company with this stuff in mind is Instrumental, looking to solve manufacturing quality problems with minimal examples. 
Don’t underestimate small data! 
Risk capital, particularly VCs remain a primary gatekeeper to the future of innovation. And thankfully the tap is wide open on funding AI businesses. Tangentially, enterprise data businesses are also getting healthy rounds, measured by both size of rounds and pure volume of rounds. 
For the common builder, bootstrapping a Machine Learning business continues to become easier and easier. A rough landing page, access to GPT-3 (or any other pre-trained model), a few cloud compute credits, and a clever tweet or two will get you everything you need to build and test your proof of concept. 
All that said, any halfway decent PoC will quickly enable access to VC money, so most will quickly forfeit their ambitions of bootstrapping to profitability. And for good reason. Rounds are closing quicker, an increasing number of active angels and micro funds is enabling faster movement in pre-seed and seed rounds. 
Corporate VCs (Google Ventures, Salesforce Ventures, Samsung Ventures, Intel Capital, etc.) are also especially active in Applied AI and general Data Science businesses. And it makes sense. Developing this stuff in-house is hard. Corporate VCs can help the mothership find synergies with the AI startups they invest in. And some executives still view AI as a risky bet and not worth building an organization around. If they change their mind, these investments in AI startups present both a potential way to onboard new technologies they’re missing out on, but also as a healthy source of talent in an industry where talent acquisition isn’t always the easiest. Check the recent investments of a Corporate VC like Intel Capital and you’ll see AI and general enterprise data companies up and down their deal flow. 
Advances in vision enabled the self-driving revolution, manufacturing breakthroughs, and much more. Advances in NLP have improved search, translation, knowledge understanding, and more. And we’ve only recently started to realize the possibilities of Reinforcement Learning, the potential of GANs, and much more. 
Let’s explore some of the opportunities in a technology-specific approach. Afterwards, we will explore opportunities in an industry-specific approach. It’s interesting to observe the choices startups may make to create a broad technical solution vs taking their technical solution to a specific industry. 
These are by no means exhaustive lists or even scratch the surface. They should, however, serve as inspiration and give you a high level view of the landscape. We intentionally skip RNNs, autoencoders, and certain other models for brevity’s sake. 
Key Technologies & Buzzwords: Convolutional Neural Networks, Dropout, Object Detection (classification + location) 
Frontier Uses: Classification, Scene Understanding, Tracking, Motion, Estimation, Reconstruction 
Dominant Industries: Automotive, Medicine, Military, Photography 
Sample Companies: Cruise, Cognex 
Key Technologies & Buzzwords: GPT, BERT, DistilBERT, XLNet, RoBERTa, Transformer-XL 
Frontier Uses: Speech Recognition, Text Generation, Language Understanding, Translation, Question Answering 
Dominant Industries: Hard to imagine industries where NLP can’t play a role of some kind. (Though I’m not an NLP maximalist!) 
Sample Companies: Open AI, HuggingFace 
Key Technologies & Buzzwords: Markov Decision Processes, Temporal Difference Learning, Monte Carlo, Deep RL, Q-Learning 
Frontier Uses: Games, Markets, Controls, Scheduling 
Dominant Industries (relatively unused): Robotics, Markets & Economics, Industrial Automation (primary use-case for robotics) 
Sample Companies: DeepMind, Open AI, Covariant 
Key Technologies & Buzzwords: Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), CycleGAN, DCGAN, cGAN, StyleGAN, Generator, Discriminator, Game Theory 
Frontier Uses: Photo generation, deepfakes, super resolution, image-to-image translation 
Dominant Industries: Creative & media, modeling, photography, video 
Sample Companies: RunwayML, Rosebud.ai, Generative.photos 
Every industry stands to gain from Applied ML. Finance has largely tackled issues of fraud, manufacturing has solved some of the looming questions in automation that traditional controls couldn’t solve, e-commerce continues to evolve from recommendation systems. All fields are ripe for disruption. Here are some interesting use-cases and companies in sample industries. 
Key Technologies: Computer Vision, Reinforcement Learning, Process Optimization 
Use-Cases: Quality Assurance, Industrial Automation, Process Improvement, Predictive Maintenance 
Sample Companies: Covariant, Instrumental, FogHorn Systems (additionally the incumbents like Siemens and Rockwell, Cognex, and others are actively investing in and performing their own R&D to play defense) 
Key Technologies: Recommendation Systems, Fraud Detection, Order Matching, Personalization 
Use-Cases: Quality Assurance, Industrial Automation, Process Improvement 
Sample Companies: Amazon’s recommendation kingdom is the most obvious sample, massive live marketplaces like Uber optimize live matching with dynamic pricing and routing, payment processors like Stripe and Square rely on Fraud Detection 
Key Technologies: Computer Vision, Sequencing, RNNs & LSTMs, Reinforcement Learning 
Use-Cases: Classification of X-Rays and other imaging, Drug Discovery, Genomics, Mapping the Brain (and much more!) 
Sample Companies: Insitro, Sophia Genetics, Flatiron Health, Allen Institute (non-profit), 
Key Technologies: Computer Vision, Object Detection, Semantic Segmentation/Scene Understanding 
Use-Cases: Autonomous Driving 
Sample Companies: Tesla, Waymo, Cruise, and many others 
Key Technologies: Computer Vision 
Use-Cases: Safety, Mapping, Visualizing, Autonomy in Machinery 
Sample Companies: Intsite, Kwant, Buildot 
Key Technologies: NLP, GANs, Computer Vision 
Use-Cases: Text Generation, Video Generation, Song and Story Writing, Assistants, Speech Generation, Modeling, Deepfakes 
Sample Companies: RunwayML, Rosebud, Persado 
Key Technologies: Let’s not encourage an AI arms race. 
Use-Cases: Let’s not encourage an AI arms race. 
Sample Companies: Let’s not encourage an AI arms race. 
Key Technologies: Computer Vision, Reinforcement Learning, Process Optimization 
Use-Cases: Predictive Maintenance, Optimizing the Grid 
Sample Companies: Stem, Origami, Infinite Uptime 
Key Technologies: NLP, Anomaly Detection, Traditional ML 
Use-Cases: Automated Banking Experiences, Fraud Detection, Personalization, Risk Management, Wealth Management, Trading 
Sample Companies: Ravelin, Tala, Verifi, Suplari, every major bank and their service providers, Quantopian 
Let’s continue the conversation on Twitter. 
I write about all things data, AI/ML, and other topics in my weekly newsletter. 
Hands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look 
Written by 
Written by",Luke Posey,2020-08-05T20:18:52.715Z
The basics of investigating an Office 365 breach | by Pete Bryan | Positive Security Thinking | Medium,"Organisations of all sizes use Office 365 for email, data storage, and collaboration services making it a tempting target for attackers, and a key resource for organisations to secure. Larger organisations may have security operations with a range of tools capable of effectively monitoring Office 365. However, there are several tools and features within the basic Office 365 offering that you can use simply and easily to monitor or investigate Office 365 activity regardless of your organisational size or security capabilities. 
As with many Software as a Service (SaaS) applications identity is they key security control plane for organisations to protect and monitor. Office 365 is no exception, with every security incident involving identity in some form. Identity within Office 365 is managed by Azure Active Directory (Azure AD) and this service provides a wealth of data and features to aid in investigating and should be a primary resource in any investigation. Whilst Office 365 provides some user identity management via https://admin.microsoft.com to access they key features we are going to look at here we need to access the AAD service directly via https://portal.azure.com. 
Which flavour of Azure AD licence you have will determine which of the following features you will have access to in the Azure portal. If you have Azure AD P1 or P2 you will have access to a Risk Based sign-in features, these provide automatic detection of suspicious or risky sign-ins and can be used to quickly identity potential account compromises. Navigate to the Azure AD service within the Azure portal and scroll down until you see two options — Users flagged for risk, and Risk events. 
Users flagged for risk will show you a list of users who meet a criterion for them to be considered suspicious. This can include things such as impossible travel situations, use of anonymisation service, or the appearance of that account in a data breach. Clicking on a user will present details about these risk events why they were flagged for risk, and a risk level of either low, medium, or high. 
Risk events will show you the individual sign-in events that were deemed risky and that are linked to the Users flagged for risk. Each event shows you the status of the logon and the risk level, as well as details of the reason for applying the risk level. This feature can be useful for tracking Conditional Access policies you may have applied Azure AD. 
These intelligent risk-based services provide a quick and easy way to identify user accounts that may have been compromised, however they require additional licencing and may not provide the full picture. To expand on them we can use detailed logging provided by Azure AD which is broken down into two key elements, Sign-in logs which provide details of all sign-ins, and Audit logs which provide details of all AAD activity. Whilst Sign-in logs are another Azure AD P1 &P2 feature the Audit log is available for all Azure AD subscriptions. 
Sign-ins is gives you a simplified view of each sign-in that has occurred and gives you an overview of the sign-in characteristics. However, it does not natively provide an easy way to summarise or visualise the data. We can do that by using another Office 365 tool, Power BI, and handily the Sign-ins page provides one click integration with Power BI. 
Clicking this button will open the Power BI service, import your Azure AD data and load the Azure Activity Directory Activity Logs app which provides a range of summarisations and visualisations of the data. 
Using this app we can quickly get an idea of things such as sign-in location, number of sign-ins, ratio of failed to successful sign-ins, and much more. This allows us to easily identify potentially suspicious patterns. On top of this Power BI’s natural language query engine allows us to ask questions of the data to investigate further. For example in the dashboard above we can see we had some sign-in activities from South Africa, to see how many sign-ins we saw we can simply type “How many signin activities came from South Africa?” into the query box and we will be presented with an answer. 
PowerBI Pro is only available as part of Office 365 Enterprise, but Power BI Desktop (https://www.microsoft.com/store/productId/9NTXR16HNW1T) is a completely free to use application and is often overlooked in favour of Excel. I would highly recommend learning some of its capabilities. It is extremely powerful and its native integration with data from many Microsoft services makes it a very quick and easy to dig into data. You can still export the Azure AD data in CSV or JSON format if you want to interrogate it with other tools, but the simplicity and capability of Power BI makes it my first choice. 
Moving back to the Azure AD service within the Azure Portal we have on more data source to look at, the Audit log. This log provides details of all changes made to Azure AD, and whilst it’s not as immediately useful for triaging a potential account breach the logs do provide a useful source of data if you want to investigate further and see if any changes have been made to the Directory that shouldn’t have been. 
There is no native Power BI integration with these logs but they can be downloaded for interrogation. To do this in Power BI Desktop download the audit log in CSV format, then using Power BI Desktop click ‘Get Data’, select ‘Text/CSV’ and select the audit log you downloaded. Once imported you can select, query, and visualise the data in a range of ways to suit your investigation. 
Another common thing to check early in an investigation is whether any of your users have been targeted by malware or phishing. As this investigation is focussed on Office 365, we are not going to look at endpoint security, but we can still investigate attempted malware delivery by email or document sharing. As standard Office 365 comes with Exchange Online Protection (EOP) malware protection for mailboxes, and SharePoint Online and OneDrive for Business and the best place to access and investigate this activity is via the Office 365 Security & Compliance Portal (https://protection.office.com). Within the Threat management tab of this portal you can access data regarding malware detections, items quarantined, and emails reported by users. The licencing you have will determine the type and granularity you will see, with Office 365 E5 adding additional ATP capabilities to this tab, however for the purpose of this article we will focus on the basic, native capabilities of Office 365. 
The first section of the Threat management tab is the Dashboard, which provides an overview of threats observed targeting your organisation, whilst it is likely to always show some threats it can be useful to identity any specific changes such a sudden spike that might suggest an increased threat. 
The next section of value for our investigation is Review. Here you can review items that have been quarantined both in email or SharePoint and OneDrive to see if someone is trying to share malicious files within your organisation. In addition, if you have enabled the Office 365 message reporting add in (https://docs.microsoft.com/en-us/office365/securitycompliance/enable-the-report-message-add-in) you can review the volume of emails reported by users as potentially being phishing, allowing you to get a better idea of if your organisation is being targeted. 
The Threat management tab provides a useful high-level overview to identify trends or anomalies that might be useful for your investigation, however if you want to dig deeper into the data a number of other tools are required. The first of these are the Office 365 Reports, these pre-generated reports provide a wealth of information on a range of areas including malware threats. Opening the reports Dashboard will show you a large number of graphs that can be drilled down on to get more detail, down to the level of the raw data. Looking at the screenshots below we can see how we go from a graph showing malware detection trends into the raw data regard what was seen and when. 
If you need to get more granular detail on a specific email you are investigating, you can also run a Message trace from the Office 365 Security & Compliance portal. Found under the Mail flow tab, this feature allows you to request granular details about a specific email or set of emails. In most cases this level of detail isn’t needed in the initial investigation phase, but it can be useful if you are starting an investigation from the point of a user reported email, or other specific email threat. 
One tool that is not really considered a security investigation tool in Office 365 is the Content search feature. Primarily aimed at compliance users it can be useful for tracking the spread of a file or email within your estate. For example, we have an email received by several of our users, each comes from a different recipient and has a different subject line, but the content is the same each time. We can create a content search to find all the emails within the environment that have this content. Whilst this tool requires you to be quite targeted in your search it can be extremely powerful. This means that as well as being useful for you it is often abused by attackers to find sensitive information so it’s worth creating alert policies (see below) to monitor for content search activity. 
If you have gotten to a point where you believe your Office 365 environment might have been compromised one of the things you may want to do is look at what actions have been conducted by the attacker. The best source of data for this stage of your investigation is your Audit log, which provides full details of all activity within Office 365. This log is accessed via the Office 365 Security & Compliance portal under the Search tab, however it is not enabled by default and the first time you access the Audit log search page you will be prompted to Turn on auditing. Make sure you do this before you need the log for investigation purposes! 
When searching the audit log you can scope your search based on time, activity, and user, meaning you can look at a specific users activity or across all users for specific activity types. You can of course search all data from all users, however the detail in the audit log means you will get a very large number of log entries so this isn’t recommended. 
When searching for specific entries it can be difficult to find the audit log name of the activity you are after, so use the search box under the activity picker and try out a few different keywords to find the activity you are looking for. You can also select multiple activities so if one doesn’t meet you needs try selecting a few. In the below search I am looking for all mailbox rule creation or modification activity to see if I can find any potential mailbox forwarding activity. I can see from the results that a new mailbox rule was created, when, by whom, and what the rule created does. 
The audit log results can be exported in CSV format and imported into Power BI for further interrogation. 
Once you have finished you investigation you may want to regularly check for certain activity with Office 365 to identify the next potential compromise. One easy way to do this is to set up an alert for that key activity. Alerts are accessed via the Alerts tab of the Office 365 Security & Compliance portal. Here you can see alerts generated, review alert policies, and create your own policies. Alerts generated appear in the portal and can also be emailed to key individuals and provides an easy way to continuously monitor for risky behaviour within your Office 365 environment. 
We have seen that just using the built-in tools of Office 365 we can quickly and easily investigate identity or email-based compromises, monitor malware within the environment, and track attacker activity post breach. Its worth spending some time with these tools to make sure you are familiar with them (and that audit logging is enabled) before you need to use them. If you have other security tooling it is also worth looking at the data available via the Office 365 Management Activity API (https://docs.microsoft.com/en-us/office/office-365-management-api/office-365-management-activity-api-reference ) and Microsoft Security Graph API (https://www.microsoft.com/en-us/security/partnerships/graph-security-api ) for integration and monitoring purposes. 
It’s also worth bearing in mind there are many advanced security features offered by Microsoft to enhance the security and monitoring of Office 365 such as DLP, Microsoft Office 365 ATP, and Microsoft Cloud App Security, which I have not covered here and that you should look at. 
Written by 
Written by",Pete Bryan,2019-04-09T16:52:30.440Z
Your Guide to Latent Dirichlet Allocation | by Lettier | Medium,"Before we get started, I made a tool (here’s the source) that runs LDA right inside your browser (it’s pretty neat). Be sure to have that open as I go along, it will make things a lot clearer. 
All of the emoji I use come from this page. Just select any one emoji and copy it into the input boxes. If you don’t see the emoji, try using Firefox version 50 or greater. 
Latent Dirichlet Allocation (LDA) is a “generative probabilistic model” of a collection of composites made up of parts. Its uses include Natural Language Processing (NLP) and topic modelling, among others. 
In terms of topic modelling, the composites are documents and the parts are words and/or phrases (phrases n words in length are referred to as n-grams). 
But you could apply LDA to DNA and nucleotides, pizzas and toppings, molecules and atoms, employees and skills, or keyboards and crumbs. 
The probabilistic topic model estimated by LDA consists of two tables (matrices). The first table describes the probability or chance of selecting a particular part when sampling a particular topic (category). 
The second table describes the chance of selecting a particular topic when sampling a particular document or composite. 
Take the image up above for example. We have four emoji sequences (the ‘composites’) and three types of emoji (the ‘parts’). 
The first three documents have ten of only one type of emoji, while the last document has ten of each type. 
After we run our emoji composites through LDA, we end up with the probabilistic topic model you see above. 
The left table has ‘emoji-versus-topics’, and the right table shows ‘documents-versus-topics’. Each column in the left table and each row in the right table sums to one (allowing for some truncation and precision loss). 
So if I were to sample (draw an emoji out of a bag) Topic 0, I’d almost certainly get the avocado emoji. If I sampled Document 3, there’s an equal (or ‘uniform’) probability I’d get either Topic 0, 1, or 2. 
The LDA algorithm assumes your composites were generated like so: 
Now we know this algorithm (or generative procedure/process) is not how documents (such as articles) are written, but this — for better or worse — is the simplified model LDA assumes. 
A bit about the Dirichlet distribution before we move on. 
What you see in the animation above are iterations of taking 1000 samples from a Dirichlet distribution using an increasing alpha value. 
The Dirichlet distribution takes a number (called alpha in most places) for each topic (or category). In the GIF (and for our purposes), every topic is given the same alpha value you see displayed. Each dot represents some distribution or mixture of the three topics like (1.0, 0.0, 0.0) or (0.4, 0.3, 0.3). Remember that each sample has to add up to one. 
At low alpha values (less than one), most of the topic distribution samples are in the corners (near the topics). For really low alpha values, it’s likely you’ll end up sampling (1.0, 0.0, 0.0), (0.0, 1.0, 0.0), or (0.0, 0.0, 1.0). This would mean that a document would only ever have one topic, if we were building a three topic probabilistic topic model from scratch. 
At alpha equal to one, any space on the surface of the triangle (or if you prefer, ‘2-simplex’) is fair game (in other words, uniformly distributed). You could equally likely end up with a sample favoring only one topic, a sample that gives an even mixture of all the topics, or something in between. 
For alpha values greater than one, the samples start to congregate in the center of the triangle. This means that as alpha gets bigger, your samples will more likely be uniform — that is, represent an even mixture of all the topics. 
The GIF demonstrates the sampling of topic mixtures for the documents, but the Dirichlet distribution is also assumed the source (the Bayesian prior) for the mixture of parts per topic. 
Three topics were used, as that works well when plotting in three dimensions. But typically, it is better to use more than three topics, depending on the number of documents you have. 
If you look at the LDA tool mentioned earlier, you’ll see an alpha and beta slider. 
These two hyperparameters are required by LDA. The alpha controls the mixture of topics for any given document. Turn it down, and the documents will likely have less of a mixture of topics. Turn it up, and the documents will likely have more of a mixture of topics. 
The beta hyperparameter controls the distribution of words per topic. Turn it down, and the topics will likely have less words. Turn it up, and the topics will likely have more words. 
Ideally, we want our composites to be made up of only a few topics and our parts to belong to only some of the topics. With this in mind, alpha and beta are typically set below one. 
If you view the number of topics as a number of clusters and the probabilities as the proportion of cluster membership, then using LDA is a way of soft-clustering your composites and parts. 
Contrast this with say, k-means, where each entity can only belong to one cluster (hard-clustering). LDA allows for ‘fuzzy’ memberships. This provides a more nuanced way of recommending similar items, finding duplicates, or discovering user profiles/personas. 
You could analyze every GitHub repository’s topics/tags and infer themes like native desktop client, back-end web service, single-paged app, or flappy bird clone. 
If you choose the number of topics to be less than the documents, using LDA is a way of reducing the dimensionality (the number of rows and columns) of the original composite versus part data set. 
With the documents now mapped to a lower dimensional latent/hidden topic/category space, you can now apply other machine learning algorithms which will benefit from the smaller number of dimensions. For example, you could run your documents through LDA, and then hard-cluster them using DBSCAN. 
Of course, the main reason you’d use LDA is to uncover the themes lurking in your data. By using LDA on pizza orders, you might infer pizza topping themes like ‘spicy’, ‘salty’, ‘savory’, and ‘sweet’. 
You could analyze every GitHub repository’s topics/tags, and infer themes like ‘native desktop client’, ‘back-end web service’, ‘single paged app’, or ‘flappy bird clone’. 
There are a few ways of implementing LDA. Still, like most — if not all — machine learning algorithms, it comes down to estimating one or more parameters. 
For LDA, those parameters are phi and theta (although sometimes they’re called something else). Phi is the ‘parts-versus-topics’ matrix, and theta is the ‘composites-versus-topics’ matrix. 
To learn how it works, let’s walk through a concrete example. 
The documents and emojis are shown in the image above. 
Our hyperparameters are: 
The following manual run-through is based on the academic paper ‘Probabilistic Topic Models’ by Steyvers & Griffiths. 
Instead of estimating phi and theta directly, we will estimate the topic assignments for each of the four emoji using the Gibbs sampling algorithm. Once we have the estimated topic assignments, we can then estimate phi and theta. 
To start, we need to randomly assign a topic to each emoji. Using a fair coin (sampling from a uniform distribution), we randomly assign to the first cat emoji ‘Topic 0’, the second cat ‘Topic 1’, the first dog emoji ‘Topic 1’, and the second dog ‘Topic 0’. 
This is our current topic assignment per each emoji. 
This is our current emoji versus topic counts. 
This our current document versus topic counts. 
Now we need to update the topic assignment for the first cat. We subtract one from the emoji versus topic counts for Cat 0, subtract one from the document versus topic counts for Cat 0, calculate the probability of Topic 0 and 1 for Cat 0, flip a biased coin (sample from a categorical distribution), and then update the assignment and counts. 
After flipping the biased coin, we surprisingly get the same Topic 0 for Cat 0 so our tables before updating Cat 0 remain the same. 
Next we do for Cat 1 what we did for Cat 0. After the flipping the biased coin, we get Topic 0 so now our tables look like so. 
This is our current topic assignment per each emoji. 
This is our current emoji versus topic counts. 
This our current document versus topic counts. 
What we did for the two cat emoji we now do for the dog emoji. After flipping the biased coins, we end up assigning Topic 1 to Dog 0 and Topic 1 to Dog 1. 
This is our current topic assignment per each emoji. 
This is our current emoji versus topic counts. 
This our current document versus topic counts. 
Since we’ve completed one iteration, we are now done with the Gibbs sampling algorithm and we have our topic assignment estimates. 
To estimate phi, we use the following equation for each row-column cell in the ‘emoji-versus-topic’ count matrix. 
And for estimating theta, we use the following equation for each row-column cell in the document versus topic count matrix. 
At long last we are done. The image above contains the estimated phi and theta parameters of our probabilistic topic model. 
The matrix on the left shows us that the cat emoji is much, much more likely to represent Topic 0 than Topic 1, and that the dog emoji is much more likely to represent Topic 1 than Topic 0. 
The matrix on the right shows us that Document 0 is more likely to be about Topic 0 than Topic 1, and that Document 1 is more likely to be about Topic 1. 
This is how LDA can be used to classify documents (composites) and words/n-grams (parts) into topics. 
If you enjoyed this article, you might also like Boost your skills, how to easily write K-Means, k-Nearest Neighbors from Scratch, and Let’s make a Linear Regression Calculator with PureScript. 
How have you used LDA? Do you prefer Gensim or MALLET? How cool would it be if Elasticsearch did topic modeling out of the box? Let me know in the responses below. Thanks! 
Written by 
Written by",Lettier,2019-05-31T22:07:32.122Z
Optimization – The Opex Analytics Blog – Medium,"by Larry Snyder 
by Larry Snyder 
The puzzle below comes from The Opex Analytics Weekly Puzzle: A Collection of… 
I just came across an article in Inbound Logistics on the six steps you should go through in a network model. This article serves as a good reminder… 
Last week, Dan Gilmore of Supply Chain Digest asked us to provide predictions for what would happen in the field of network design. Here is his full article with predictions in other…",NA,NA
Implementation of Page rank algorithm in python using Random walk method | by ANJALI CHACHRA | Medium,"Have you ever asked yourself how google ranks the pages when you search something on google.com?If yes, have a look at PageRank algorithm definition. 
PageRank (PR) is an algorithm used by Google Search to rank sites in their web crawler results. 
I’ll not go into much details here, but to give you an idea, the World Wide Web can be seen as a large graph, nodes represents websites and edge represents links. The graph looks something like this. 
The webpage relevance(label/rank on each node) is measured by the number of incoming links that lead to it. Furthermore, the relevance increases if the incoming link to the page comes from a well known page. 
Let’s quickly dive into implementation of Page rank using random walk method 
6.1 If yes, pick node from set of nodes randomly and increment it’s rank 
6.2 Otherwise select node from list obtained in step 4 randomly and increment it’s rank 
This process is repeated until the resulting vector, with rank for every single page, converges. 
Now, verify page rank values obtained by random walk method with the pagerank method of networkx library. 
The output shows which page is most important. 
Conclusion : Thus we see that after some iterations page rank obtained from random walk method matches with values obtained from built in function of python. 
Enjoy the source and have fun. Cheers! 
Written by 
Written by",ANJALI CHACHRA,2019-02-23T09:32:18.655Z
Product Discovery Tips. Product discovery refers to the… | by Roman Pichler | Medium,"Product discovery refers to the activities required to determine if and why a product should be developed. Carrying out this work makes it more likely to create a product users actually want and need. In this article, I share my recommendations to help you reflect on and improve your product discovery work. 
Product discovery is a team sport. You should therefore involve the right people in the discovery work and secure enough of their time. I find it helpful to form a product discovery team that consists of: 
Involving others in the discovery work allows you to leverage their knowledge and expertise, builds rapport, and creates support for key product decisions including the selection of a specific market segment and stand-out features. 
A UX designer, for example, might help you observe and interview users; and a developer and tester might advise you on technical feasibility, identify technical risks, and build prototypes; a sales rep might get you in touch with target users and help you with competitor research; a ScrumMaster might facilitate collaboration and advise on process issues — for instance, which process and tools should be best used to visualize and track the discovery work. (I prefer to use a Kanban-based process and a Kanban board, as I discuss in my book Strategize in more detail.) 
Your product discovery work should focus on nailing the value proposition, target group, business goals, business model, and stand-out features of the product — not on writing user stories, designing the user interface, or building the actual solution. Your goal should be to mitigate the risk of building a product nobody wants and needs, not to figure out the product details. 
Having said that, it’s ok to address key UX and technology risks and evaluate important user interaction and architecture options as part of the discovery work. But the bulk of the UX design, user story writing, and technology work should be done after you have successfully validated the problem. 
To get your focus right, consider using a tool like my Product Vision Board to capture your idea, and identify assumptions and risks. 
Minimise the amount of time you spend on product discovery in order to accelerate time-to-market. You should aim to get a good enough product out as fast as possible, and then adapt it to the market feedback. There is no way to guarantee that the product strategy and business model are spot on, and that the new product or next version will be a success. 
But don’t rush the necessary work, and resist the temptation to jump start building the actual product. Explore the key assumptions and risks in your product strategy and business model by systematically testing and addressing them in an iterative fashion using, for example, observations, interviews, surveys, and prototypes. 
If you can’t confidently state why people are going to use your product, who those individuals are, what makes your product stand out from the crowd, and why it’s worthwhile for your business to develop and provide the product, then you are not in a position to build the actual solution. Instead, continue the discovery work (persevere or pivot), or stop and move on to another product idea. 
With all the product discovery work that needs to happen, it can be easy to lose sight of the most important success factor — the people who are going to use the product. There is no point in coming up with a smart business model and sophisticated user interactions, if you don’t know who the users are, what they may be struggling with, and what works well for them, and what doesn’t. If the users don’t need and like your product, you will find it hard to monetise it and achieve product success. 
Therefore, get out of the building (as Steve Blank says) and meet target users face-to-face as part of the discovery work. I know that’s not always easy to do. But to succeed with product discovery, I find it paramount that you — the person in charge of the product — carry out user research yourself and develop a deep understanding of the user needs. 
When talking to users, take a genuine interest in the people you meet. Have an open mind and let go of preconceived notions about the problem you think the users have or the solution you believe they need. This allows you to discover what people really want and need thereby maximising the chances of creating a successful product. 
The people involved in product discovery should also participate in building the actual solution. The UX designer, developer, tester should work on the development team. The stakeholders should regularly attend product strategy and roadmap reviews, as well as sprint review meetings. This speeds up the product innovation process, creates a smooth transition from discovery to delivery activities, and mitigates the risk of loss of knowledge. 
Handing off the discovery work is ineffective and wasteful: It requires detailing the knowledge acquired in form reports and other artefacts. This is time-consuming, and it is likely to lead to misunderstandings and loss of knowledge — it is hard to precisely capture the discovery learnings, and written information is open to misinterpretation. 
Therefore, refrain from using separate discovery and delivery teams. I find it is usually better to involve more people from the development team rather than opting for two distinct teams. Remember that quickly learning what to do (and what not) and quickly delivering the right solution are more important to product success than cost efficiency and saving money — at least at the early life cycle stages. 
Knowing upfront how much time and effort will be required to carry out the necessary discovery work is tricky. New risks and work items often emerge as part of the validation work. Luckily, there is a straightforward solution: timebox your product discovery work. This technique is particularly useful when you work on a new product or when you make a bigger change to an existing one, for example, to take the product to a new market in order to extend its life cycle. 
A time box is a fixed period of time, such as one week, that cannot be extended. At the end of the time box, the work stops, and you reflect on what has been achieved. If the work has not been completed, and your product strategy and business model still contain significant risks, you have two options: add another time box — and possibly pivot — or stop the work. 
Consider holding weekly review meetings that involve the people carrying out the validation work as well as the management sponsor. Use the meetings to reflect on the risks that are being addressed and the learnings that you have achieved; consider the risks that still exist in your product strategy and business model; and decide if and how to continue. 
While the traditional usage of the term suggests that product discovery is the first stage in a new-product development process (see Cooper’s Stage-Gate™ process), it would be a mistake to limit product discovery to new products. 
As your product grows and matures, its value proposition, market, stand-out features, and business model will change. To make and keep your product successful, it is paramount that you proactively review and adjust your product strategy, roadmap, and business model. These adjustments sometimes require little or no product discovery work. 
But when you find that bigger changes are required, such as, taking your product to a new market or adding features and redesigning the user experience to sustain growth, you are likely to find that you have to carry out specific discovery work, as the picture below shows. If that’s the case, then I recommend anticipating the work and stating it on your product roadmap. 
Modern product discovery is hence best understood as a set of activities rather than a clear-cut phase that is separated by a milestone or gate from building the actual solution. 
You can learn more about product discovery with the following: 
Written by 
Written by",Roman Pichler,2018-01-15T13:48:02.494Z
Cyber Security for Beginners. In recent years cybersecurity is the… | by Tarun N | Medium,"In recent years cybersecurity is the buzz word that we hear every other day through some or the other means. There is a drastic increase in the opportunities as well as the people interested in this field. But the problem that many of them face is that they do not have a complete picture of what “CYBERSECURITY” actually is and how they could be the right fit. There are always people asking or googling around regarding cybersecurity like what is it, what programming languages are required, what tools are used, etc. So in this blog, I would try to explain what actually cybersecurity is in the broader sense and how would you actually fit in if you are willing to change or take up cybersecurity as your career. 
No more boring stuff. Let’s jump into for what you are here. 
“Cyber” “Security”. As it tells, securing or providing security to the cyber world. This includes everything, right from the smartphone from your hand, the laptop that you use, the Wi-Fi router that you’re connected to, the website you’re browsing, all your online shopping, etc.There’s a huge gap between the advancements in this virtual world and the security that is in place currently. I hope most of you already know the figures for the impact it had in past and what it would cost if the gap is not filled in time. 
This is a very vast domain which includes multiple subdomains which existed since years and a few new ones added to it. 
Image Credits: https://www.linkedin.com/pulse/map-cybersecurity-domains-version-20-henry-jiang-ciso-cissp 
I will be explaining the major subdomains briefly so that it would not be too geeky and at the same time it would convey you the overall picture of each of them. 
Security Engineering 
Let’s begin with Security Engineering. This mostly consists of geeky stuff for developing robust and secure systems which include network designing, security architecture design and review, cloud security, secure application development, and access controls. 
Network designing comprises of designing of network such as placing of routers, switches and firewalls efficiently. It also includes proper configuration of security controls in different network devices so as to keep a check on the data traffic that flows to and fro. Networking is the backbone of any organization which involves IT. Thus, this is the first step where security should be implemented properly and efficiently to cut down the outsider’s threat. Anyone who has expertise in networking, network device configurations would fit in. If you are interested in this field but do not have any knowledge then I would recommend you to take CCNA Routing & Switching training. 
Security architecture is something each and every organization would have irrespective of what level of and how IT is used. This is the bird view of the whole security that is implemented in an organization. It includes what all and where the controls are needed to be implemented and how they correlate to the overall system architecture. This job is mostly done by the security professionals who got a good amount of experience in this field and probably a CISO(Chief Information Security Officer). To reach this level you need to have at least 10–15 years of experience and some relevant certifications such as CISSP. 
Cloud security is something which has come into the picture recently with the rise in organizations moving to the cloud. Security on the cloud can be handled from two different aspects. One is the cloud provider and another is the customer hosting his/her resources on the cloud.As a cloud provider, they make sure the infrastructure is configured properly such that each of their customers is isolated completely from each other so that attack on one customer may not give nightmare to the other customers and at the same time maintaining availability as per SLA. To be part of this you need to have a thorough knowledge of cloud architecture and it’s working. There are several vendor related training and certification programs that you can take up such AWS by Amazon.As a customer, they have to make sure that their resources are properly configured and developed. Here secure application development comes into picture which is another major subdomain of security engineering. This involves secure coding and implementation of Secure Development Lifecycle. This job is done mostly by developers who are trained with knowledge in recent developments in security such as new vulnerabilities and how they can be fixed. 
Access controls are allowing/disallowing of various rights to different people in the organization. Like employees may only have the access to enter their own company and a particular employee of a company may not have access to various blocks in the building where he has no work as the hub room, etc. It not only involves restriction of physical access but also while accessing systems. It restricts a user certain functionalities in his/her assigned desktop/laptop at workplace according to his job role such as the installation of any software, inserting flash drives, connecting to private networks, etc. It also allows/disallows the information that you can view, the websites you can view, etc. To be part of this you need to have knowledge of different security controls and their implementation. Knowledge of tools such as SailPoint is a big plus. 
Security Operations 
This is the actual part where all the action takes place. There are SOC’s (Security Operations Center) in every organization monitoring each and every event to prevent any cyber attack. It is the heart of security in any organization for preventing and defending against any advanced cyber attack. SOCs and SIEMs are responsible to notify and manage responsible teams in event of any security incident. 
This domain mainly includes all the operations right from the prevention of cyber attack to dealing with as well as eradicating it. It also includes forensics to study how, why and what lead the cyber attack to be successful as well as identify who has done it, why and what did they gain from it. It also includes what all resources are affected. 
Vulnerability management is done for prevention of cyber attack. It ensures that all the products or resources being used are patched to avoid exploitation of all known vulnerabilities. 
To be a part of Security Operations you need to have immense knowledge about vulnerabilities, incident response, detection, and prevention. CCNA CyberOps training will be of great use if you wish to take up a career in this domain. 
Threat Intelligence 
Before I explain what threat intelligence is, let me explain the two common threats to any organization. They are insiders and outsiders threat. While outsiders threat is mainly the threat over the internet. This threat could be reduced by restricting the inflow of traffic and isolation of internet facing resources from critical resources. Insiders threat is mostly the insiders' job like the frustrated employees. They may be doing it consciously or can also be completely unaware. In either case, the effect is exorbitant. 
To keep a check to these threats, threat intelligence comes into the picture. It is the analysis of data gathered from various existing threats(inside & outside) to prevent any attack from happening. The people working in this domain are cyber threat analysts and to be one you need to have immense knowledge of information security as well as knowledge in networking administration. 
Risk Assessment 
This is the part the whole world actually thinks cybersecurity is limited to. Risk assessment is where all the blue teaming, red teaming takes place. Let me explain what actually these blue and red teaming actually mean. 
Consider there is some random website say www.myzon.com which happens to be a famous e-commerce website. So there’s always a group of people who are always interested in breaking things, say “Breakers”. On one fine day, MyZon has hosted a big online shopping event on its website featuring all types of attractive deals. Due to its heavy advertising, it caught the attention of Breakers and they decided to bring it down by some or the other means. And they leave no stone unturned to launch a massive successful attack on this website. At the same time, there is another group of people from the MyZon company’s end called “Fixers”. These people see that the website is suffering from some type of attack and they try their best to keep it up such that it doesn’t affect any of their end users. 
Here breakers are none but the red team and fixers are the blue team. 
In an organization itself, there are red teams and blue teams which carry out assessments regularly so that their resources are not affected by any means. Red team plays the role of real hackers and the blue team mostly comprising of developers with expertise in secure application development try to improve and make their resources more secure. 
Apart from red and blue teaming, various other vulnerability assessments are done such as black box, white box, grey box, etc., 
In this domain, various risks are assessed and fixed according to their criticality. 
To be a part of this domain you need to be either a good breaker or fixer. Good fixers are the ones with good coding skills. Breakers are the ones who can place themselves in the shoes of hackers. For this lateral thinking is the most important quality required. To be a breaker you need to have coding skills(at least should be able to interpret the flow of the application, not necessarily be able to code) and keep yourself updated with recent vulnerabilities uncovered and also how existing vulnerabilities can be exploited in various other ways. 
Governance 
This is the domain that doesn’t get your hands dirty with geek stuff. It’s basically like the government which sets and controls laws, administration, auditing, etc. However, few of these subdomains do require prior experience in information security and some minimum knowledge in this domain. This is the place where people with no prior IT experience can also enter. For example, cyber law is the need of the hour in which the lawyers can take an extra step to learn about cybersecurity and cybercrime. 
Governance mainly comprises of auditing, laws, policies and procedures, compliances, etc. All the standards and checklists stuff come under this. The people working in this are responsible to check if their organization and its employees are following and maintaining all the industry standards set by well-known organizations such as ISO, OWASP, etc. 
This is just a drop from the huge ocean of cybersecurity. Every domain is like a vast ocean in itself and has a lot to offer. Happy learning :) 
Keep hacking ;) 
Written by 
Written by",Tarun N,2019-03-06T18:21:42.962Z
How to get started with machine learning on graphs | by David Mack | Octavian | Medium,"Since our talk at Connected Data London, I’ve spoken to a lot of research teams who have graph data and want to perform machine learning on it, but are not sure where to start. 
In this article, I’ll share resources and approaches to get started with machine learning on graphs. 
From talking with research teams, it’s really clear how broad and pervasive graph data is — from disease detection, genetics and healthcare to banking and engineering, graphs are emerging as a powerful analysis paradigm for hard problems. 
Simply put, a graph is a collection of nodes (e.g. people) and relationships between them (e.g. Fatima is a friend of Jacob). Often those nodes have properties (e.g. Fatima is age 23). 
It’s common to store this data in a database. One popular database is Neo4j, in their own words “[the] world’s leading graph database, with native graph storage and processing.” 
Neo4j allows you to query your database with Cypher, the graph equivalent of SQL. In our toy example above, we can get a list of Fatima’s friends like so: 
Graphs are a really flexible and powerful way to represent data. Traditional relational databases, with their fixed schemas, make it hard to store connections between different entities, yet such connections are an abundant and vital part of the real world. In a graph database, these connections are easy to store and query. Furthermore, often the relationships between many things (e.g. the connections between family members) collectively provide vital information, and graph databases make this easy to analyze. 
The term “relationship” and “edge” are used interchangeably in this document. Neo4j uses the former, much of graph theory uses the latter. 
First of all, why use machine learning? A great article on this question is “Ways to think about machine Learning” by Benedict Evans, covering the ways that companies are starting to think about and actually use ML. 
Distilling Ben’s argument to focus on graph ML, there are two major ways that it is useful: 
ML can automate functions that are easy for a human to do, but hard to describe to a computer 
Real world data is noisy and has many complex sub structures. Tasks such as “outline the people in this image” are easy for a human, but very hard to turn into a discrete algorithm. 
Deep learning allows us to transform large pools of example data into effective functions to automate that specific task. 
This is doubly true with graphs — they can differ in exponentially more ways than an image or vector thanks to the open-ended relationship structure. Using graph ML we can create functions to spot recurring patterns 
ML can transform information on a scale humans cannot 
The double-edged sword of computers is that they will do exactly what we tell them — no more, no less (the occasional bug excepted!). 
This means that they will perform our exact instructions, with no deviation of improvisation. And they will keep performing them, no matter how long we let them run. 
Therefore, computers can process data on a scale no human could (due to the time or attention required). This makes new analysis possible, such as analyzing billions of transaction webs to fingerprint fraud. 
Our definition is simply “applying machine learning to graph data”. This is intentionally broad and inclusive. In this article I’ll tend to focus on neural network and deep learning approaches as they’re our own focus, however where possible I’ll include links to other approaches. 
In this article I’m not going to cover “traditional” graph analysis — that’s the well known algorithmic techniques like PageRank, clique identification, shortest path, etc. These are very powerful and should be considered a first port-of-call due to their well understood nature and plentiful implementation in public libraries. 
Whilst an exciting field full of promise, machine learning on graphs is still a nascent technology. 
In mainstream areas of ML the community has discovered widely applicable techniques (e.g. transfer learning using ResNet for images or BERT for text) and made them accessible to developers (e.g. TensorFlow, PyTorch, FastAI). However, there are no equivalently easy, universal techniques, nor do any of the popular ML libraries have support for graph data. 
Similarly, graph databases such as Neo4j do not offer ways to run machine learning algorithms on their data (although Neo4j are thinking a lot about how to make that possible!) 
One of the reasons for the lack of graph support in deep learning libraries is that the flexibility of the data-structure (e.g. any node can have any number of relationships to other nodes) doesn’t fit the fixed computation-graph, fixed sized tensor paradigm popular with deep learning libraries and GPU manufacturers. 
Put more simply, it’s hard to represent and manipulate a sparse graph as a matrix. Not impossible, but definitely harder than working with vectors, text and images. 
However despite this, there is a strong surge of interest in graph ML. My personal prediction is that this area will become both mainstream and a foundational tool to how we analyze data in many industries. 
Note that, just like any other ML technique, most graph ML requires a large volume of training data. 
Whilst the answer to this question can vary a lot with the task and dataset, it’s helpful to outline what you, the keen adventurer, should expect to encounter. 
It’s almost certain that you’ll be coding up this system yourself — high-level tools for graph ML do not yet exist. It’s likely you’ll be building the system using Python and an ML library like TensorFlow or PyTorch. Depending on your scale, you may be training your model on a single machine, or using a distributed cluster (interestingly, many graph learning approaches naturally distribute quite well). 
You will probably start by extracting your data from a graph — likely stored in CSV files, a graph database like Neo4j, or another format. 
Then you’ll ingest that data into a machine learning library. In my current work (involving millions of small graphs) I precompile each graph into a TFRecord, with feature vectors storing the nodes, relationships and the adjacency matrix. All node properties and text is tokenized using a common dictionary. 
This works for small graphs, but for larger graphs you’ll need some sort of scheme to partition the graph into smaller training examples (e.g. you could train on patches, or on individual node-edge-node triples). 
Note that some approaches tabularize the data before it reaches the machine learning library. Node2Vec is a good example of this, where random walks are used to transform each node into a vector. These vectors are then fed into the machine learning model as a list. 
Once the data is ingested, the actual modeling and learning begins. There is a wide variety of possible approaches here. 
Finally, the model needs to be used or served somehow. In some cases a new node/edge/graph property is computed by the model and this can be added to the original data-store. 
In other cases a model is produced for online prediction. In this setup, one needs to build a system to feed the model with any graph data it requires to perform its predictions (possibly once again ingesting it from a graph database) and then finally that prediction can be served up to a user or follow-on system (e.g. an answer spoken by Alexa to a user). 
Alright, so let’s look at some of the approaches you can take to perform machine learning on graphs. 
I’ll outline methods here, point out some of their pros and cons, and link to fuller descriptions. For the sake of time and space, I’ll have to sacrifice some detail here. 
Despite being a young field, researchers have come up with a dazzling array of approaches and variations. Whilst I’ve tried to cover the major areas in this article, there is sadly no way to make this list fully exhaustive. 
If there is an approach you’d like to see added to this article, please let us know, so we can add it. 
There is a wide variety of starting points and overall approaches for graph ML. Therefore, it’s helpful to narrow those down by thinking about what is the general task you’re trying to achieve. 
As with any learning system, your success and development effort will be hugely helped by narrowing down what you want to achieve. By coming up with a minimal, well-specified goal, your model and dataset can be reduced down to something more tractable. 
On the last point, graph databases are particularly sirenous, encouraging us towards grand “omnipotent” goals: as the database can represent almost anything, it’s tempting to try to build overly general intelligence. You are warned :) 
Types of task we’ll cover: 
This article is intended as a launch-pad for your own research. Just as with any data science, methods will need to be adapted to your individual circumstances. As a lot of graph ML is still in early research, you should expect to try many approaches before finding one that works. 
Before jumping into building a graph ML system (potentially requiring a high investment in infrastructure), it’s important to consider if simpler methods could suffice. 
There are a few ways to simplify the problem: 
I’ll refer back to some of these approaches in the following sections where particularly applicable. 
Some graph ML approaches can be used for multiple tasks. I’ve included their full description here. In the later sections I’ll make reference to this section and highlight some task-specific details. 
Once again, it is not possible to do justice to these large fields of work, the best we can do here is to give you pointers to explore further. 
Feel free to skip this section and proceed to the task you’re interested in solving, then refer back here for detail. 
Node embeddings were one of the early developments in graph ML, and have remained popular due to their simplicity, robustness and computational efficiency. 
A node embedding simply means calculating a vector for each node in the graph. The vector is calculated to have useful properties, for example the dot product of any two nodes’ embeddings could tell you if they’re from the same community. 
In this way, the embedding simplifies the graph data down to something much more manageable: a vector. 
Node embeddings are often calculated by incorporating lots of graph structure together (more on this in a moment). 
Their tradeoff is that necessarily information is discarded. A fixed-length vector can rarely represent all of the graph structure around a node. It may or may not incorporate node and relationship properties. 
However, with a bit of creativity a node embedding can be used in combination with other graph ML approaches. In this setting, the embedding becomes a node property and can be used as a booster for other techniques, which perhaps don’t penetrate as far into the graph structure as the embedding generation did. 
Here I’ll highlight some of the main embedding approaches. For a comprehensive survey of graph embedding techniques and their comparison, checkout these two recent papers. 
Random walks 
Random walks are a surprisingly powerful and simple graph analysis technique, backed up by a long lineage of mathematical theory. 
A random walk is where one starts at a node in the graph, randomly chooses an edge, then traverses it. And then repeats the process, until a sufficiently long path is produced. 
The genius of a random walk is it turns a many dimensional irregular thing (the graph) into a simple matrix (the list of fixed length paths, each path composed of its nodes). 
In large enough volume, it’s theoretically possible to reconstruct the basic graph structure from random walks. And random walks play to machine learning’s great strength: learning from large volumes of data. 
There are many ways to harness random walks to calculate node embeddings. In what follows I’ll highlight some of the main approaches. 
Node2Vec 
Node2Vec is a popular and fairly generalized embedding technique using random walks. 
The way to turn these random walks into an embedding is with a clever optimization objective. First assign each node a random embedding (e.g. gaussian vector of length N). Then for each pair of source-neighbor nodes in each walk, we want to maximize the dot-product of their embeddings by adjusting those embeddings. Finally, we simultaneously minimize the dot-product of random node pairings. The effect of this is that we learn a set of embeddings that tend to have high dot products for nodes in the same walks, e.g. in the same community/structures. 
The final bit of Node2Vec is that it has parameters to shape the random walks. Using the “In-out” hyper-parameter you can prioritise whether the walks focus on small local areas (e.g. are these nodes in the same small community?) or whether the walks wander broadly across the graph (e.g. are these nodes in the same type of structure?). 
Node2Vec extensions 
Node2Vec’s strength is its simplicity, but that is also its downfall. The standard algorithm does not incorporate node properties or edge properties, amongst other desirable pieces of information. 
However, it’s quite straightforward to extend Node2Vec to incorporate more information. Simply alter the loss function. For example: 
For inspiration, check out the large number of papers citing Node2Vec. 
Collaborative filtering using random walks 
A very simple example use of random walks is to solve the collaborative filtering problem e.g. given users’ reviews of products, which other products will a user like?. 
This broadly follows the same scheme as node2vec, although has been simplified even further. You can see the entire implementation and an explanation in our article. 
Graph Networks are a rich and important area of graph ML. The basic premise is to embed a neural network into the graph structure itself: 
Typically this involves storing states for each node and using an adjacency matrix to propagate those states to the nodes’ neighbors. 
A good overview paper is “Relational inductive biases, deep learning, and graph networks” from DeepMind which both surveys the history of this sub-field and also proposes a unifying approach for comparing different Graph Networks and neural networks in general. 
In the above paper, graph networks are thought of as a collection of functions, for propagating state and for aggregating state across nodes, edges and entire graphs. In this way, many different architectures from the literature are comparable. Here’s an extract showing these functions in action: 
A graph network has many possible outputs: 
These can then be used like embeddings for tasks like classification, scoring and link-prediction. 
Graph Networks are very general and powerful — they’ve been used to analyze many things from natural language, 3d scenes to biology. Our recent work has been showing that they can implement many common traditional graph algorithms. 
Some starting points to understand more about graph networks and their capabilities: 
Our extensions to graph networks 
Under our MacGraph research project, we’ve been experimenting with a number of extensions to graph networks. Whilst we’re still refining the approaches, they’ve shown enough promise that they’re worth including here for other researchers. 
We’re attempting to learn different reasoning algorithms, extracting and transforming data from the graph. To do this we’ve added to the aforementioned network a series of components that make it akin to a graph-based Turing machine: 
This is a common task and quite well studied. The basic formulation is: 
What is the probability p(A,R,B) that node A has relationship R to node B? 
Examples are knowledge graph completion (e.g. if Michelangelo is a painter born in Tuscany, is he Italian?) and predicting protein interaction. This can be used both for trying to predict new, unknown things (e.g. what drugs might be effective?) as well as improving existing imperfect data (e.g. which project does this task belong to?). 
More information on many of the approaches can be found in the earlier section “General graph ML approaches” 
Node embeddings (often generated using random walks) are frequently used for link prediction. 
Embeddings are often generated such that nearby nodes in the graph have similar embedding tensors. Therefore a comparison between (e.g. dot product or euclidean distance) provides a likelihood of linkage. Some methods like Node2Vec actually directly train the embedding on the presence/absence of links. 
Graph networks can be used to generate node embeddings for link prediction. In this case, incorporate link prediction capability into the network’s loss function. 
This means to tabularize the graph data, then run a traditional feed-forward network on it. 
For example, each node could be represented by its properties (concatenated into a tensor). Each training example has two nodes and relationship type as features and existence of edge as label. Remember to balance the label classes. 
This simple approach can work well when a lot of the graph structure is reflected in the properties (e.g. a graph of streets and each node has its GPS location) 
This interesting approach from Stanford essentially memorizes the graph into tensors and matrices. “Our model outperforms previous models and can classify unseen relationships in WordNet and FreeBase with an accuracy of 86.2% and 90.0%, respectively.” 
Reinforcement learning can also be used for link prediction. In this approach, the network learns to extract a series of facts from the graph which it can combine to produce a link prediction. 
One example of this approach is “Multi-hop knowledge graph reasoning with reward shaping” in which the network learns to walk the graph and use that information to produce a link prediction. 
Another common task is to try to classify or score part of the graph. For example, trying to find how relevant proteins are to a certain gene. Or trying to cluster students into their schools based on their friendship groups. 
Classification means to output a probability distribution across potential labels, scoring means to output a scalar that might be used for comparison with others. Both are conceptually similar, classification involves more dimensions. 
Formally, the task is to define one of the following functions, where Output is the set of possible output class distributions or the set of possible output scores: 
f(n:Node) → r ∈ Output 
g(e:Edge) → r ∈ Output 
h(g:Graph) → r ∈ Output 
Most approaches to performing this have two steps: 
Step 1. can be performed using many different approaches, which I’ll list below. 
Step 2. is often performed using a feed-forward neural network (FFN). The extraction and aggregation are either done using hand-crafted functions (e.g. read out specific nodes, sum together specific edges) or learned functions (e.g. attention for extraction, convolution for aggregation). 
The choice of both steps is a matter of data science and experimentation, there has yet to emerge any clear “one size fits all” solution. 
More information on many of the approaches can be found in the earlier section “General graph ML approaches” 
Node embeddings provide a rich source of node-state for classification and scoring. 
When using embeddings, often the node(s) under inspection will have their embeddings passed through a small FFN to produce the desired output. Depending on the use case, the node’s properties can also be included in the FFN’s input. 
If the node embeddings are created using random-walks (e.g. using Node2Vec) they will incorporate local structural information (for example, which community the node is in, or what super-structure it is part of) that may be relevant to the classification or scoring being performed (e.g. clustering different graph sub-communities). 
Graph Networks are a versatile, generalized approach to embedding a neural network into the graph itself. 
A graph network computes node, edge and graph states (although some of these can be omitted depending on the application). 
These states can then be transformed to produce the final output. For instance, the graph state could be passed through a FFN to create an overall graph categorization. 
There are many different examples in the literature of graph networks, see the introductory section for a brief survey of them. 
An interesting approach is outlined in “Graph classification using Structural Attention”. In this work, the graph is repeatedly read from using attention to extract nodes. 
The center of the network is an LSTM cell which controls which nodes are read from, incorporates the attention read data into its internal state, then outputs a prediction of the graph’s classification. 
This is similar to the approach (although not on graph data) used in Compositional Attention Networks for Machine Reasoning, where a central RNN cell guides attentional reading and composition of the read data. 
Simplifying the problem down to a tabular dataset opens up many better-researched approaches (e.g. feed-forward and convolutional neural networks). 
One way to do this is to treat each node with its properties as a training example. This may involve hand-crafting additional properties that you believe will assist in the classification/scoring. 
Another way to tabularise the graph is to extract fixed sized patches. In this model, a node, its edges, and potentially its neighbors are extracted into a fixed sized table. The fixed size means that a maximum number of edges and nodes can be stored, and if more exist in the table, they must be randomly sampled. Furthermore, if there are fewer nodes and edges than the fixed table can store, it will need padded out with a designated null value. Finally, one must choose how to select patches — a simple model is to extract one per node or edge. 
Tabularization discards potentially valuable network information, but simplifies engineering and model research. 
Finally, a non machine learning approach is worth consideration. If nodes are being scored on some sort of linkage, content or textual basis, a search-engine/document retrieval inspired approach may work. 
This ranges from simple fuzzy text matching, through to PageRank, phrase and concept matching. 
This is a very mature field of computer science. For more pointers, check out Wikipedia’s Information retrieval article. 
Throughout this article I’ve linked to many resources, they are all listed here for convenience. Additionally, I’ve added further items of interest not listed earlier. 
Octavian’s mission is to develop systems with human-level reasoning capabilities. We believe that graph data and deep learning are key ingredients to making this possible. If you interested in learning more about our research or contributing, get in touch. 
Written by 
Written by",David Mack,2018-12-29T17:18:03.689Z
Eli Cyber Security – Medium,Knowledge belongs to the world,NA,NA
Don Miller – Medium,"IT Pro embracing DevOps; interested in business informatics, tech startups, agile, infosec, data, design thinking, digital, product management 
See more",NA,NA
Understanding Principle Component Analysis(PCA) step by step. | by The Nobles | Analytics Vidhya | Medium,"Principal component analysis (PCA) is a statistical procedure that is used to reduce the dimensionality. It uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components. It is often used as a dimensionality reduction technique. 
Step 1: Standardize the dataset. 
Step 2: Calculate the covariance matrix for the features in the dataset. 
Step 3: Calculate the eigenvalues and eigenvectors for the covariance matrix. 
Step 4: Sort eigenvalues and their corresponding eigenvectors. 
Step 5: Pick k eigenvalues and form a matrix of eigenvectors. 
Step 6: Transform the original matrix. 
Let's go to each step one by one. 
Assume we have the below dataset which has 4 features and a total of 5 training examples. 
First, we need to standardize the dataset and for that, we need to calculate the mean and standard deviation for each feature. 
After applying the formula for each feature in the dataset is transformed as below: 
The formula to calculate the covariance matrix: 
the covariance matrix for the given dataset will be calculated as below 
Since we have standardized the dataset, so the mean for each feature is 0 and the standard deviation is 1. 
var(f1) = ((-1.0-0)² + (0.33-0)² + (-1.0-0)² +(0.33–0)² +(1.33–0)²)/5 var (f1) = 0.8 
cov(f1,f2) = ((-1.0–0)*(-0.632456-0) + (0.33–0)*(1.264911-0) + (-1.0–0)* (0.632456-0)+(0.33–0)*(0.000000 -0)+(1.33–0)*(-1.264911–0))/5cov(f1,f2 = -0.25298 
In the similar way be can calculate the other covariances and which will result in the below covariance matrix 
An eigenvector is a nonzero vector that changes at most by a scalar factor when that linear transformation is applied to it. The corresponding eigenvalue is the factor by which the eigenvector is scaled. 
Let A be a square matrix (in our case the covariance matrix), ν a vector and λ a scalar that satisfies Aν = λν, then λ is called eigenvalue associated with eigenvector ν of A.Rearranging the above equation, 
Aν-λν =0 ; (A-λI)ν = 0 
Since we have already know ν is a non- zero vector, only way this equation can be equal to zero, if 
det(A-λI) = 0 
Solving the above equation = 0 
λ = 2.51579324 , 1.0652885 , 0.39388704 , 0.02503121 
Eigenvectors: 
Solving the (A-λI)ν = 0 equation for ν vector with different λ values: 
For λ = 2.51579324, solving the above equation using Cramer's rule, the values for v vector are 
v1 = 0.16195986 v2 = -0.52404813 v3 = -0.58589647 v4 = -0.59654663 
Going by the same approach, we can calculate the eigen vectors for the other eigen values. We can from a matrix using the eigen vectors. 
Since eigenvalues are already sorted in this case so no need to sort them again. 
If we choose the top 2 eigenvectors, the matrix will look like this: 
Feature matrix * top k eigenvectors = Transformed Data 
The results are the same, only change in the direction of the PC1, which according to me, doesn’t make any difference as mentioned here also. So we have successfully converted our data from 4 dimensional to 2 dimensional. PCA is mostly useful when data features are highly correlated. 
Since this is my first blog, I am open to suggestions and please do check out the code version of above on my GitHub. 
Written by 
Written by",The Nobles,2020-01-16T12:34:51.628Z
An Intuitive Introduction of Word2Vec by Building a Word2Vec From Scratch | by Manish Nayak | Towards AI — Multidisciplinary Science Journal | Medium,"In this article, I will try to explain Word2Vec vector representation, an unsupervised model that learns word embedding from raw text and I will also try to provide a comparison between the classical approach One-hot encoding and Word2Vec. 
The classical approach of solving text-related problems is one-hot encode the word. This approach has multiple drawbacks. 
Let’s consider the following two sentences. 
I like watching a movie. 
I enjoy watching a movie. 
In one-hot encode vector representation of the words looks as follows 
Intuitively we know that enjoy and like are kind of similar words. The Euclidean distance between movie and enjoy is the same as the Euclidean distance between enjoy and like. This is a major drawback. 
Word2Vec is an approach that helps us to achieve similar vectors for similar words. Words that are related to each other are mapped to points that are closer to each other in a high dimensional space. Word2Vec approach has the following advantage. 
Word2vec is a three-layer neural network, In which the first is the input layer and the last layers are the output layer. The middle layer builds a latent representation so the input words transformed into the output vector representation. 
In the Word2vec vector representation of words, we can find interesting mathematical relationships between word vectors. 
king — man = queen — woman 
In our previous example of two sentences. 
If we follow the CBOW approach and take the surrounding(context) words as input and try to predict the target word then the output will be as follow. 
A vectored form of the input and output looks like below. 
In the neural network for our current example, there will be three neurons in the hidden layer and output will have five neurons with softmax function so that it will give the probability of words. 
Accompanied jupyter notebook for this post can be found on Github. 
The drawback of the classical approach is, it does not consider the order in which the words occur in the sentence and context is lost. it assumes that words in the document are independent of one another. Vector representation in this classical approach leads to data sparsity. This drawback can be overcome by Word2vec vector representation. 
I hope this article helped you to get an understanding of Word2vec and why we prefer Word2vec over one-hot-encoding. It will at least provides a good explanation and a high-level comparison between one-hot-encoding and Word2vec vector representation. 
Towards AI publishes the best of tech, science, and engineering. Subscribe with us to receive our newsletter right on your inbox. For sponsorship opportunities, please email us at pub@towardsai.net Take a look 
Written by 
Written by",Manish Nayak,2020-06-11T17:29:22.030Z
Dimensionality reduction and visualization using PCA(Principal Component Analysis) | by Ashwin Singh | Medium,"LinkedIn : www.linkedin.com/in/ashwin-singh- 403116173 
In this blog we are going to discover what is dimensionality reduction ? How it is achieved using PCA ? What is standardization ? What is Covariance of a data matrix? How PCA is used in visualization ? Geometrical and mathematical interpretation of PCA. How PCA can be interpreted using Matrix Factorization? We ll go through all these things, so fasten your seat belts and get ready for the journey. 
Dimensionality reduction is the process of reducing the number of random variables under consideration, via obtaining a set of principal variables .It can be divided into feature extraction and feature selection . 
Feature Selection : In this method we select the best features from a set of features provided in the data set. We donot create new features , instead we choose the most important features from the given set of features and hence we remain with lesser dimensions or lesser features to work with. 
Feature Extraction : In this method we create new features , and these features are not present in our original feature set . These features are not interpretable . 
Following are the essential points which explain why we need Dimensionality reduction : 
Standardization is a process of centering and scaling of our data. We can do row and column standardization but we perform column standardization in general. Standardization helps in dealing with the different scale issues present in the dataset generally i.e. let’s say in the dataset of students of a classroom , one feature is weight and the other feature is height so the weight is measured in kgs and the height is measured in cms , these two are of different scales which creates problem in further computation and interpretability so we standardize the data to remove these scale issues . In column standardization we standardize our column values (features) by making the mean of the column equal to zero and variance equal to 1. Let for the given matrix below: 
Mean of the new points of the feature f_j is 0 and the variance is 1. So the standardizaton moves mean vector to origin and squishes/expands the variance of the feature to 1. Following are three important properties of standardization: 
Covariance tells how two features vary together. 
As we can see from the above formula we can calculate the covariance between two features of any matrix. Whatif the matrix is standardized ? In that case our mean will become 0 and and the formula will become just the summation of multiplication of the corresponding elements of the features. Which can further be generalized as the dot product of the features. Which can be further written as the dot product of the matrices. This can be understood from below: 
S is a square symmetric matrix. The diagonal of the covariance matrix is the covariance of the feature with itself which is nothing but variance , and if the matrix was standardized then the diagonal values of the covariance matrix become 1. The dXd shape of the matrix is arrived as follows: 
PCA is essentially the rotation of coordinate axes , chosen such that each successful axis captures or preserves as much variance as possible. It is the simplest and the most fundamental technique used in dimensionality reduction. PCA is a feature extraction technique of Dimensionality reduction. PCA is an unsupervised technique which doesn’t take the class labels into consideration . 
Let’s take another example of a dataset with features f1 and f2. The data is standardized i.e. mean centered and variance is 1. 
In PCA we are not changing the points present in space. We change the axes , then coordinates of each point will change but the actual location of the points remains unchanged. 
Given a data matrix , Eigen vectors represent the directions in which most of the data is spread. Eigen values represent the amount of spread. We calculate the covariance matrix from our data matrix, and using that covariance matrix we calculate the eigen vectors and their corresponding eigen values. 
Geometrically , we rotate our axes after doing standardization. Since there are d dimensions so we’ll get d eigen vectors and d eigen values. For each eigen vector we project points on the vector and measure the variance or spread i.e. eigen value and in the end the vector with highest eigen values our principal component. Now that we have new dimensions or features the values of these features are the projection of points on these vectors.Now if we like we can decide to ignore the components of lesser significance. We do loose some information, but if the eigen values are small, we don’t loose much. If we leave out some components, the final dataset will have less dimensions than the original. 
We can calculate the eigen value and eigen vector using the covariance matrix .Follow up this link . 
eigen values of S(covariance matrix) = λ1, λ2 , λ3 ­­­­. . . . λ_d 
eigen vectors of S(covariance matrix) = v1, v2, v3. . . . v_d 
Every pair of eigen vector are perpendicular to each other. 
Following are the steps followed in calculating eigen vectors: 
1. Column standardization of the data matrix. 
2. Calculating the covariance matrix S. 
3. Calculating the eigen values and eigen vectors of S . 
4. u_1 = v_1 
We calculate the percentage of variance explained using the eigen vectors can be given by: 
Once we have found our eigen vectors which are of the shape d x 1 we now convert our standardized data matrix into the new form i.e. project the standardized points in the new feature space. Let us say out of d eigen vectors we choose k top eigen vectors. Each eigen vector is of shape d x 1, so when we put all the eigen vectors in one matrix , our matrix becomes of the shape d x k. Now we perform matrix multiplication of the X and the matrix of eigen vectors. We end up having a new matrix of shape n x k with the points projected on the new feature space. 
The above depiction shows that the newer matrix has lesser dimensions. This data can be visualized easily . As we know that k<d , but how do we decide the best k when we are performing PCA only for dimensionality reduction because when we perform PCA for visualization we choose k to be either 2 or 3 but what should be k when we perform PCA for reducing the dimensions not necessarily to 2 or 3. 
To resolve this issue we check the cumulative variance explained by the number of vectors selected(k) . This can also be called as percentage of variance explained which is discussed above. We can check the information retained for as many number of vectors we select. 
The covariance matrix can be decomposed using matrix factorization to perform PCA. Following depiction shows how PCA can be performed using matrix factorization: 
Its been enough of theory , so now let’s jump to the code. We ll be dealing with MNIST dataset . You can download the dataset from here. MNIST dataset is a simple computer vision dataset . It consists of 28 X 28 pixel images of handwritten digits . The dataset has 784 dimensions. There are 10 class labels from 0 to 9. 
Now we have got our eigen vectors stored in the variable vectors. Now we find our new points. 
https://www.appliedaicourse.com 
https://www.scss.tcd.ie/~dahyotr/CS1BA1/SolutionEigen 
https://www.utdallas.edu/~herve/Abdi-EVD2007-pretty 
Written by 
Written by",Ashwin Singh,2019-09-15T17:19:37.866Z
The most insightful stories about Cybersecurity – Medium,,NA,NA
Industrial applications of topic model | by Fatma Fatma | Medium,"In short, topic modeling is a text-mining technique for discovering topics in documents . A topic contains a cluster of words that frequently occur together, and topic modeling can connect words that have similar meanings and can distinguish between uses of words with multiple meanings . Given that text documents are composed of words, a topic covered in more than one document can be expressed by a combination of strongly related words, and any given document can be associated with more than one topic . Thus, topic modeling is a technique that can be used to infer hidden topics in a collection of text documents . The two key outputs from generating a topic model on a collection of documents are: 1) a list of topics (i.e., groups of words that frequently occur together) and 2) lists of the documents that are strongly associated with each of the topics. Ideally, each topic should be distinguishable from other topics. In the more, scholars leverage the topic model in different research. 
Figure 1. topic model applications 
Figure 1. shows the fields which are used the topic model to improve the related results, In the other words, these are some current industrial applications of topic models. The following are abstracts of selected papers per application. 
Living lab: This study applies topic modeling analysis on a corpus of 86 publications in the Technology Innovation Management Review (TIM Review) to understand how the phenomenon of living labs has been approached in the recent innovation management literature. Although the analysis is performed on a corpus collected from only one journal, the TIM Review has published the largest number of special issues on living labs to date, thus it reflects the advancement of the area in the scholarly literature. According to the analysis, research approaches to living labs can be categorized under seven broad topics: 1) Design, 2) Ecosystem, 3) City, 4) University, 5) Innovation, 6) User, and 7) Living lab. Moreover, each topic includes a set of characteristic subtopics. A trend analysis suggests that the emphasis of research on living labs is moving away from a conceptual focus on what living labs are and who is involved in their ecosystems to practical applications of how to design and manage living labs, their processes, and participants, especially users, as key stakeholders and in novel application areas such as the urban city context [[1]] 
Bio-informatics: Topic modeling is a useful method (in contrast to the traditional means of data reduction in bioinformatics) and enhances researchers’ ability to interpret biological information. Nevertheless, due to the lack of topic models optimized for specific biological data, the studies on topic modeling in biological data still have a long and challenging road ahead. In recent years, we have been witnessing exponential growth of biological data, such as microarray datasets. This situation also poses a great challenge, namely, how to extract hidden knowledge and relations from these data. As mentioned above, topic models have emerged as an effective method for discovering useful structure in collections. Therefore, a growing number of researchers are beginning to integrate topic models into various biological data, not only document collections. In these studies, we find that topic models act as more than a classification or clustering approach. They can model a biological object in terms of hidden “topics” that can reflect the underlying biological meaning more comprehensively. Therefore, topic models were recently shown to be a powerful tool for bioinformatics [[2]] 
Summarization: 
Opinion summarization: summarizing the newly discovered opinions is important for governments to improve their services and companies to improve their products . Because no queries are posed beforehand, detecting opinions is similar to the task of topic detection on sentence level. Besides telling which opinions are positive or negative, identifying which events correlated with such opinions are also important [[3]] 
Meeting summarization: Producing meeting documents requires an instantaneous recorder during meetings, which costs extra human resources and takes time to amend the file. However, a high-quality meeting document can enable users to recall the meeting content efficiently. The paper aims to discuss these issues. An application based on this framework is developed to help the users find topics and obtain summarizations of meeting contents without extra effort. This app uses the Bluemix speech recognizer to obtain speech transcripts. It then combines latent Dirichlet allocation and a TextTiling algorithm with the speech script of meetings to detect boundaries between different topics and evaluate the topics in each segment. TextTeaser, an open API based on a feature-based approach, is then used to summarize the speech transcripts “[[4]] 
Sentiment: With the expansion and acceptance of Word Wide Web, sentiment analysis has become progressively popular research area in information retrieval and web data analysis. Due to the huge amount of user-generated contents over blogs, forums, social media, etc., sentiment analysis has attracted researchers both in academia and industry, since it deals with the extraction of opinions and sentiments. In this paper, we have presented a review of topic modeling, especially LDA-based techniques, in sentiment analysis. We have presented a detailed analysis of diverse approaches and techniques, and compared the accuracy of different systems among them. The results of different approaches have been summarized, analyzed and presented in a sophisticated fashion. This is the really effort to explore different topic modeling techniques in the capacity of sentiment analysis and imparting a comprehensive comparison among them [[5]] 
Chatbot: this papir said “Dialog evaluation is a challenging problem, especially for non task-oriented dialogs where conversational success is not well-defined. We propose to evaluate dialog quality using topic-based metrics that describe the ability of a conversational bot to sustain coherent and engaging conversations on a topic, and the diversity of topics that a bot can handle. To detect conversation topics per utterance, we adopt Deep Average Networks (DAN) and train a topic classifier on a variety of question and query data categorized into multiple topics. We propose a novel extension to DAN by adding a topic-word attention table that allows the system to jointly capture topic keywords in an utterance and perform topic classification. We compare our proposed topic based metrics with the ratings provided by users and show that our metrics both correlate with and complement human judgment. Our analysis is performed on tens of thousands of real human-bot dialogs from the Alexa Prize competition and highlights user expectations for conversational bots.“[[6]] 
Topic tracking: Discovering and tracking topics in a text stream has attracted the interests of many researchers. A limitation of most existing methods is that they organize topics in flat structures. Topic hierarchy could reveal the potential relations between topics, which can help to find high quality topics when analyzing the text stream. In this paper, a hierarchical online non-negative matrix factorization method (HONMF) is proposed to generate topic hierarchies from text streams. The proposed method can dynamically adjust the topic hierarchy to adapt to the emerging, evolving, and fading processes of the topics. In the experiment, HONMF is evaluated under a variety of metrics. Compared with the baseline methods, our method can achieve better performance with competitive time efficiency[[7]] 
Question and answer: There is increasing interest in text analysis based on unstructured data such as articles and comments, questions and answers. This is because they can be used to identify, evaluate, predict, and recommend features from unstructured text data, which is the opinion of people. The same holds true for TEL, where the MOOC service has evolved to automate debating, questioning and answering services based on the teaching-learning support system in order to generate question topics and to automatically classify the topics relevant to new questions based on question and answer data accumulated in the system. To that end, the present study proposes an LDA-based topic modeling. The proposed method enables the generation of a dictionary of question topics and the automatic classification of topics relevant to new questions [[8]] 
Text categorization: topic detection is defined as the task of finding out different themes from the collection of documents. One of topic detection approach is about finding a topic for every document in the corpus. Any word or group of words which tells what the document is about is defined as the topic of the document.[[9]] 
Similarity: Reputation management experts have to monitor — among others — Twitter constantly and decide, at any given time, what is being said about the entity of interest (a company, organization, personality…). Solving this reputation monitoring problem automatically as a topic detection task is both essential — manual processing of data is either costly or prohibitive — and challenging — topics of interest for reputation monitoring are usually fine-grained and suffer from data sparsity. We focus on a solution for the problem that (i) learns a pairwise tweet similarity function from previously annotated data, using all kinds of content-based and Twitter-based features; (ii) applies a clustering algorithm on the previously learned similarity function. Our experiments indicate that (i) Twitter signals can be used to improve the topic detection process with respect to using content signals only; (ii) learning a similarity function is a flexible and efficient way of introducing supervision in the topic detection clustering process. The performance of our best system is substantially better than state-of-the-art approaches and gets close to the inter-annotator agreement rate. A detailed qualitative inspection of the data further reveals two types of topics detected by reputation experts: reputation alerts / issues (which usually spike in time) and organizational topics (which are usually stable across time) [[10]] 
Spam filter: At present, content-based methods are regard as the more effective in the task of Short Message Service (SMS) spam filtering. However, they usually use traditional text classification technologies, which are more suitable to deal with normal long texts; therefore, it often faces some serious challenges, such as the sparse data problem and noise data in the SMS message. In addition, the existing SMS spam filtering methods usually consider the SMS spam task as a binary-class problem, which could not provide for different categories for multi-grain SMS spam filtering. In this paper, the authors propose a message topic model (MTM) for multi-grain SMS spam filtering. The MTM derives from the famous probability topic model, and is improved in this paper to make it more suitable for SMS spam filtering. Finally, the authors compare the MTM with the SVM and the standard LDA on the public SMS spam corpus. The experimental results show that the MTM is more effective for the task of SMS spam filtering. [[11],[12]] 
Classification: With the overflowing of Short Message Service (SMS) spam nowadays, many traditional text classification algorithms are used for SMS spam filtering. Nevertheless, because the content of SMS spam messages are miscellaneous and distinct from general text files, such as more shorter, usually including mass of abbreviations, symbols, variant words and distort or deform sentences, the traditional classifiers aren’t fit for the task of SMS spam filtering. In this paper, the authors propose a Short Message Biterm Topic Model (SM-BTM) which can be used to automatically learn latent semantic features from SMS spam corpus for the task of SMS spam filtering. The SM-BTM is based on the probability of topic model theory and Biterm Topic Model (BTM). The experiments in this work show the proposed model SM-BTM can acquire higher quality of topic features than the original BTM, and is more suitable for identifying the miscellaneous SMS spam[[13]] 
Recommender System: suggest using technologies of TDT to group news items instead of common item-based clustering technologies[[14]] 
Chemical Topic Modeling: this paper said “we adopted a probabilistic framework called “topic modeling” from the text-mining field. Here we present the first chemistry-related implementation of this method, which allows large molecule sets to be assigned to “chemical topics” and investigating the relationships between those. In this first study, we thoroughly evaluate this novel method in different experiments and discuss both its disadvantages and advantages. We show very promising results in reproducing human-assigned concepts using the approach to identify and retrieve chemical series from sets of molecules. We have also created an intuitive visualization of the chemical topics output by the algorithm. This is a huge benefit compared to other unsupervised machine-learning methods, like clustering, which are commonly used to group sets of molecules. Finally, we applied the new method to the 1.6 million molecules of the ChEMBL22 data set to test its robustness and efficiency. In about 1 h we built a 100-topic model of this large data set in which we could identify interesting topics like “proteins”, “DNA”, or “steroids”. Along with this publication we provide our data sets and an open-source implementation of the new method (CheTo) which will be part of an upcoming version of the open-source cheminformatics toolkit RDKit.”[[15]] 
IOT+health-care : The purpose of this study is to unravel key themes latent in the sparse but growing academic literature on the application of IoTs in healthcare. Specifically, we performed topic modeling and identified five dominant clusters of research, namely, privacy and security, wireless network technologies, applications, data, and smart health and cloud. Our results show that research in healthcare IoT has mainly focused on the technical aspects with little attention to social concerns. In addition to categorizing and discussing the topics identified, the paper provides directions for future research.[[16]] 
HR: Human-machine teaming aims to meld human cognitive strengths with the unique capabilities of smart machines. An issue within human-machine teaming is a lack of communication skills on the part of the machine such as the inability to know when to interrupt human teammates. A proposed solution to this issue is an intelligent interruption system that monitors the spoken communication of human teammates and predicts appropriate times to interrupt without disrupting the teaming interaction. The current research expands on a prosody-only task boundary model as an intelligent interruption system with a topic-only task boundary model. The topic-only task boundary model outperforms the prosody-only model with a 9.5% increase in the F1 score, but is limited in its ability to process topical data in real-time, a previous benefit of the prosody-only task boundary model.[[17]] 
IOT: this paper said “The Internet of Things (IoT) provide intelligence for the communication between people and physical objects. An important and critical issue in the IoT service applications is how to match the suitable IoT services with service requests. To solve this problem, researchers use semantic modeling methods to make service matching. Semantic modeling methods in IoT extract meta-data from text using rule-based approaches or machine learning techniques often suffer from the scalability and sparseness since text provided by sensors is short and unstructured. In recent years, topic modeling has been used in IoT service matchmaking. However, most topic modeling methods do not perform well in IoT service matchmaking since the text is too short. In order to address the issues, this paper proposes a new topic modeling method to extract topic signatures provided by intelligent devices. The method extends the classical knowledge representation framework and improves the qualities of service information extraction, and this process is able to improve the effectiveness of service matchmaking in IoT service. The framework incorporates human cognition to improve the effectiveness of the algorithm and make the algorithm more robust in heterogeneous systems in the IoT. The usefulness of the method is illustrated via experiments using real datasets[[18]]. 
Healthcare: this paper said “Pharmacovigilance, and generally applications of natural language processing models to healthcare, have attracted growing attention over the recent years. In particular, drug reactions can be extracted from user reviews posted on the Web, and automated processing of this information represents a novel and exciting approach to personalized medicine and wide-scale drug tests. In medical applications, demographic information regarding the authors of these reviews such as age and gender is of primary importance; however, existing studies usually either assume that this information is available or overlook the issue entirely. In this work, we propose and compare several approaches to automated mining of demographic information from user-generated texts. We compare modern natural language processing techniques, including extensions of topic models and convolutional neural networks (CNN). We apply single-task and multi-task learning approaches to this problem. Based on a real-world dataset mined from a health-related web site, we conclude that while CNNs perform best in terms of predicting demographic information by jointly learning different user attributes, topic models provide additional information and reflect gender-specific and age-specific symptom profiles that may be of interest for a researcher“[[19]] 
Blockchain: this paper said”Cognitive manufacturing has brought about an innovative change to the 4th industrial revolution based technology in combination with blockchain distributed ledger, which guarantees reliability, safety, and security, and mining-based intelligence information technology. In addition, artificial intelligence, machine learning, and deep learning technologies are combined in processes for logistics, equipment, distribution, manufacturing, and quality management, so that an optimized intelligent manufacturing system is developed. This study proposes a topic mining process in blockchain-network-based cognitive manufacturing. The proposed method exploits the highly universal Fourier transform algorithm in order to analyze the context information of equipment and human body motion based on a variety of sensor input information in the cognitive manufacturing process. An accelerometer is used to analyze the movement of a worker in the manufacturing process and to measure the state energy of work, movement, rest, and others. Time is split in a certain unit and then a frequency domain is analyzed in real time. For the vulnerable security of smart devices, a side-chain-based distributed consensus blockchain network is utilized. If an event occurs, it is processed according to rules and the blocking of a transaction is saved in a distributed database. In the blockchain network, latent Dirichlet allocation (LDA) based topic encapsulation is used for the mining process. The improved blockchain distributed ledger is applied to the manufacturing process to distribute the ledger of information in a peer-to-peer blockchain network in order to jointly record and manage the information. Further, topic encapsulation, a formatted statistical inference method to analyze a semantic environment, is designed. Through data mining, the time-series-based sequential pattern continuously appearing in the manufacturing process and the correlations between items in the process are found. In the cognitive manufacturing, an equalization-based LDA method is used for associate-clustering the items with high frequency. In the blockchain network, a meaningful item in the manufacturing step is extracted as a representative topic. In a cognitive manufacturing process, through data mining, potential information is extracted and hidden rules are found. Accordingly, in the cognitive manufacturing process, the mining process makes decision-making possible, especially advanced decision-making, such as potential risk, quality prediction, trend prediction, production monitoring, fault diagnosis, and data distortion.”[[20]] 
*****************************References**************************** 
[1] Westerlund, M., Leminen, S., & Rajahonka, M. (2018). A topic modelling analysis of living labs research. Technology Innovation Management Review, 8(7).‏ 
[2] Liu, L., Tang, L., Dong, W., Yao, S., & Zhou, W. (2016). An overview of topic modeling and its current applications in bioinformatics. SpringerPlus, 5(1), 1608.‏ 
[3] Ku, L. W., Lee, L. Y., Wu, T. H., & Chen, H. H. (2005, August). Major topic detection and its application to opinion summarization. In Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval (pp. 627–628). ACM.‏ 
[4] Huang, T. C., Hsieh, C. H., & Wang, H. C. (2018). Automatic meeting summarization and topic detection system. Data Technologies and Applications, 52(3), 351–365.‏ 
[5] Rana, T. A., Cheah, Y. N., & Letchmunan, S. (2016). Topic modeling in sentiment analysis: a systematic review. Journal of ICT Research and Applications, 10(1), 76–93.‏ 
[6] Guo, F., Metallinou, A., Khatri, C., Raju, A., Venkatesh, A., & Ram, A. (2018). Topic-based evaluation for conversational bots. arXiv preprint arXiv:1801.03622.‏ 
[7] Tu, D., Chen, L., Lv, M., Shi, H., & Chen, G. (2018). Hierarchical online NMF for detecting and tracking topic hierarchies in a text stream. Pattern Recognition, 76, 203–214.‏ 
[8] Kim, K., Song, H. J., & Moon, N. (2017). Topic Modeling for Learner Question and Answer Analytics. In Advanced Multimedia and Ubiquitous Engineering (pp. 652–655). Springer, Singapore.‏ 
[9] Haribhakta, Y., Malgaonkar, A., & Kulkarni, P. (2012, September). Unsupervised topic detection model and its application in text categorization. In Proceedings of the CUBE International Information Technology Conference (pp. 314–319). ACM.‏ 
[10] Spina, D., Gonzalo, J., & Amigó, E. (2014, July). Learning similarity functions for topic detection in online reputation monitoring. In Proceedings of the 37th international ACM SIGIR conference on Research & development in information retrieval(pp. 527–536). ACM.‏ 
[11] Ma, J., Zhang, Y., Wang, Z., & Yu, K. (2016). A message topic model for multi-grain SMS spam filtering. International Journal of Technology and Human Interaction (IJTHI), 12(2), 83–95.‏ 
[12] Al Moubayed, N., Breckon, T., Matthews, P., & McGough, A. S. (2016, September). Sms spam filtering using probabilistic topic modelling and stacked denoising autoencoder. In International Conference on Artificial Neural Networks (pp. 423–430). Springer, Cham.‏ 
[13] Ma, J., Zhang, Y., Zhang, L., Yu, K., & Liu, J. (2017). Bi-Term Topic Model for SMS Classification. International Journal of Business Data Communications and Networking (IJBDCN), 13(2), 28–40.‏ 
[14] Qiu, J., Liao, L., & Li, P. (2009, July). News recommender system based on topic detection and tracking. In International Conference on Rough Sets and Knowledge Technology (pp. 690–697). Springer, Berlin, Heidelberg.‏ 
[15] Schneider, N., Fechner, N., Landrum, G. A., & Stiefl, N. (2017). Chemical Topic Modeling: Exploring Molecular Data Sets Using a Common Text-Mining Approach. Journal of chemical information and modeling, 57(8), 1816–1831.‏ 
[16] Dantu, R., Dissanayake, I., & Nerur, S. (2019, January). Exploratory Analysis of Internet of Things (IoT) in Healthcare: A Topic Modeling Approach. In Proceedings of the 52nd Hawaii International Conference on System Sciences. 
[17] Peters, N. S., Bradley, G. C., & Marshall-Bradley, T. (2019, February). Task Boundary Inference via Topic Modeling to Predict Interruption Timings for Human-Machine Teaming. In International Conference on Intelligent Human Systems Integration (pp. 783–788). Springer, Cham.‏ 
[18] Liu, Y., Du, F., Sun, J., Jiang, Y., He, J., Zhu, T., & Sun, C. (2018). A crowdsourcing-based topic model for service matchmaking in Internet of Things. Future Generation Computer Systems, 87, 186–197.‏ 
[19] Tutubalina, E., & Nikolenko, S. (2018). Exploring convolutional neural networks and topic models for user profiling from drug reviews. Multimedia Tools and Applications, 77(4), 4791–4809.‏ 
[20] Chung, K., Yoo, H., Choe, D., & Jung, H. (2018). Blockchain Network Based Topic Mining Process for Cognitive Manufacturing. Wireless Personal Communications, 1–15.‏ 
Written by 
Written by",Fatma Fatma,2019-04-04T22:58:17.512Z
How Artificial Intelligence is Changing Cyber Security | by Charu Mitra Dubey | The Startup | Medium,"With over 15% of the total enterprises leveraging AI, it has become a matter of great debate whether AI is good or bad. Though AI was originally coined in 1950, it has seen an exponential growth in the past few years and people are concerned about how is it going to affect the human life. Rumours are hovering all around regarding the aspects of Artificial Intelligence. From Sophia, the bot to Alexa has caught the eyes of people making them wonder how is this field going to turn around. 
Currently, AI has already entered fields like healthcare, manufacturing, education and cybersecurity. Cybersecurity is the main concern for today’s digital world, there are still uncertainties about the impact of AI. Not just the corporates but also the government sectors are also trying to master AI and Machine Learning for the protection of data and creating more opportunities in the respective field. 
With the advancements in AI, many companies have started to use it as a powerful weapon against the puissant cyber attacks and trespasses. AI allows you to automate the detection of threat and combat even without the involvement of the humans. Powering your data to stay more secure than ever. Since AI is totally machine language driven, it assures you complete error-free cyber-security services. Moreover, companies have also started to put more resources than ever for boosting AI driven technologies. 
To leverage AI to the fullest, collaborating it with the right security-intelligence personnel is necessary against all the types of attacks. Malware and virus attacks are common in the cyber world. Highly skilled hackers know how to trigger the right attacks, leaving the company’s cyber cell no clues about what happened. And here comes AI to rescue. AI let the defenders protect and stay strong even against the series of attacks. 
The other area of cybersecurity that can be affected by AI is the password protection and authenticity detection systems. Since passwords are much vulnerable, AI is implemented a lot over this section. The term for such security systems is biometric logins. AI is being used for the detection of physical characteristics like fingerprints, retina scans, etc making the system more safe and secure than ever. 
But that is not all about AI and Cybersecurity. 
As most of the features of AI are still uncovered, vulnerabilities also exist regarding its usage. It’s not only the white hat hackers who know how to put AI as an armour but there are even the black ones who are using it as their weapons. 
The black hat people have also started to explore for how AI can be a panacea for them as well. That means that the people with the wrong intentions have also started to gain authority over AI, making them more powerful and skilled to get their things done. They have started to develop the hacks and methodologies in order to break against the cyber securities. 
Although companies have specific cybersecurity cells still the sophisticated attackers somehow find their ways towards breaking the breaches. 
As AI is adding values to the security sectors of the corporations and individuals as well, it is also spreading more power in the wrong hands. In order to give AI more authority in the near future for the security purposes, we need to stay sure that it stays with the white hat people only. 
Though AI is still in the developing stage, there is a lot more to explore about it. With time we will be able to classify it as a boon or a bane. 
Written by 
Written by",Charu Mitra Dubey,2018-09-13T08:50:22.773Z
How DevOps Increases System Security | by Adam Hawkins | slashdeploy | Medium,"The perception of DevOps and its role in the IT industry has changed over the last five years due to research, adoption, and experimentation. Accelerate: The Science of Lean Software and DevOps by Gene Kim, Jez Humble, and Nicole Forsgren makes data-backed predictions about how DevOps principles and practices yield better software in almost any measurable way and more successful businesses. Their research, along with others such as James Wickett and Josh Corman, former CTO of Sonartype and respected information security researcher, has centered around the concept of incorporating information security objectives into DevOps (a set of practices and principles they termed “Rugged DevOps”). Dr. Tapabrata Pal, Director and Platform Engineering Technical Fellow at Capital One, came up with similar ideas and described their processes as DevOpsSec, having dispelled the myth that DevOps and system security are orthogonal. 
In fact, it’s the opposite. DevOps practices done right increases system security in the same way that continuous delivery increases stability. 
The Three Ways of DevOps describe continuous delivery, production to development feedback, and constant learning. Continuous delivery requires developing software in incrementally small changes and verifying each change with automated tests across a deployment pipeline. The computerized pipeline offers teams multiple ways to improve security when compared to software development without an automated deployment pipeline. 
Security issues are like any other software regression. They may be tested for so that they don’t occur in production. There are multiple ways to apply automated testing to InfoSec: 
Adding these tests to the deployment pipeline dramatically increases security since it’s automated: this is known as a “shift left“. It ensures software is secure from the start, automatically, and throughout the pipeline. 
Organizations often do not have enough InfoSec engineers to go around. That creates negative consequences since InfoSec checks are pushed to the end of the process and may only happen when there’s enough capacity. Consider for a moment just running your existing automated test suite when there was an extra engineer in the team. Accepting that proposition for automated functional testing is ludicrous in modern IT, why allow the same for InfoSec testing? Adding InfoSec tests to the pipeline verifies each change and scales out with the organization. The deployment pipeline is a bigger force for change than a few engineers. More importantly, adding tests exposes issues to everyone and shifts responsibility to the code author to patch the regression. 
Automated tests ensure known regressions do not enter production. However, they do not guard against attacks and other malicious activity in production. Teams need to track and alert on telemetry data that indicates malicious activity or other red flags in production. This is the second way of DevOps that establishes feedback from production to development. Teams already have production telemetry for latency, request count, and active users, and so on, so InfoSec telemetry should be integrated as well. Examples include: 
This kind of telemetry data is critical to understanding how the system is being used in production. Based on this insight, teams can action by adding regression tests to the pipeline having identified potential problems, resulting in an increased security posture for production. More importantly, it increases visibility. Security changes are more likely to occur when a team realizes they’re under attack. 
Nick Galberth from Etsy echoes this sentiment1after graphing security telemetry: 
“One of the results of showing this graph was that developers realized that they were being attacked all the time! And that was awesome because it changed how developers thought about the security of their code as they were writing the code.” 
This practice also aids scenarios where pre-production testing and compliance checks are not enough. Accelerate includes a troubling case study of an ATM vendor that demonstrates production InfoSec telemetry’s value. The company noticed their ATMs were put into maintenance mode at unscheduled times. This allowed the attacker to physically extract cash from the machine. A developer installed the backdoor years ago. Apparently, backdoors of this type are difficult or near impossible to detect beforehand. However, the production telemetry detected the anomaly and alerted the team. The team proactively found the fraud and resolved the issue before the scheduled cash audit process. 
These examples demonstrate how DevOps practices improve system security. First, like any other aspect of software, add automated tests to the deployment pipeline. Second, add production telemetry to production to direct development changes. The third way calls for learning and experimentation to further improve the software development process. Unfortunately, sometimes teams will miss this aspect. DevOps establishes feedback loops, and the third way continuously improves them to reduce toil, reduce bugs, and/or adapt to changing conditions. 
Compliance and auditing is a common pain point. It slows down the process since documentation has to be produced and manual reviews are required. This doesn’t have to halt the process. Automation can drastically improve the compliance and auditing process by removing toil. The Google SRE Book defines toil as “the kind of work tied to running a production service that tends to be manual, repetitive, automatable, tactical, devoid of enduring value, and that scales linearly as a service grows.” Accelerate includes a case study of 18F and Cloud.gov. 
The case study demonstrates a government organization implementing an automated process for writing system security plans (SSP) and obtaining a right to operate from the designated authority. The SSP plans must be reviewed. They’re often a hundred of pages and highly detailed. Creating and maintaining them manually is impossible in a dynamic cloud environment. 18F created a tool that automatically generates an SSP into YML which can be transformed into PDFs or published as GitBooks for internal and external review saving immeasurable amounts of man-hours (and increasing happiness in the process). Private sector IT companies tend to have a more relaxed level of regulation. Regardless, the same compliance and auditing techniques can be and should be leveraged to reduce ongoing effort and toil. 
Similar approaches may be used in downstream auditing and compliance processes. Given the production telemetry systems contain InfoSec data, they may be exposed to auditors in a self-service way during reviews. Auditors can check control like appropriate logging or specific event handling. The deployment pipeline also provides a complete change history for the application in production. It’s possible to generate compliance reports using the code, the deployment pipeline, and other automation. This approach again reduces toil for all involved, increases accuracy, and ideally leads to more completed audits. 
DevOps is the best way for modern IT to build, test, and ship software. The Three Ways provide a framework for understanding how and why to approach software development problems. Changing and improving InfoSec is not so different than what the cloud and continuous delivery did to software. Everything stems from the idea that increasing frequency decreases difficulty. It saw teams go from deploying quarterly to measuring deploys-per-day per developer. That’s an astonishing velocity improvement. It can affect the same change by applying the three ways to InfoSec outcomes: automated testing, production telemetry, and continuous learning and improvement. Applying all three builds a culture of continuous verification that ultimately raises the security floor across the industry. That sounds like a textbook case of increasing security today and in the future. 
Originally published at cloudacademy.com on March 21, 2019. 
Written by 
Written by",Adam Hawkins,2019-04-11T07:07:45.106Z
Real-time User Signal Serving for Feature Engineering | by Pinterest Engineering | Pinterest Engineering Blog | Medium,"Del Bao, Vikas Kumar: Software Engineer, Ads ML InfraZack Drach: Engineer Manager, Ads ML Infra 
More than 320 million unique visitors come to Pinterest every month to discover inspiring content, which also gives advertisers a unique opportunity to get their products in front of people with commercial intent. 
Curation of this personalized catalog requires accurate and real-time mapping of users’ short-term interests. Mapping long term interests is equally important for showing relevant content, as short-term interests can be noisy. 
Some typical use scenarios for user features: 
To help machine learning engineers capturing user propensity, we’ve developed a real-time user signal service that acts as a platform for consuming user engagement data, as well as computing and serving user features. With this new platform, developers spend a minimum amount of effort to build, test and experiment new user signals with ML algorithms and provide personalized on-site and in-app user signals to our serving systems. 
Since the launch, the platform garnered popularity across the company and enabled several use cases for creators engagement, ads retrieval, shoppable in retrieval, and search ranking. The system reliably produces and serves user signals in real-time at Pinterest scale with minimum infra cost. 
Here we’ll share more on how we designed the system to achieve these goals. 
The five pillars of a user signal platform are: 
There are several design considerations to make the service performant and speed up developer velocity. 
A Materializer is responsible for joining external data against user events. We steer towards designing a generic materializer container to ease developers’ coding efforts. Another goal is to reduce data fetching costs so that we can achieve minimum event processing delays. This is a must to achieve timeliness of event processing at the Pinterest scale. 
We apply the separation of concerns principle: separate request specs and fetching execution into two layers. Feature request is expressed as the data source, request key, and feature key. Data plumbing is powered by an execution engine. Adding a new feature boils down to simply specify the request spec via materializer APIs, which requires only a few lines of code. 
Aggregation is the computation of stats from a session of user events. Having 50k QPS per signal, the system cannot scale with fully recomputing all events of a user, especial in the long-term user context, e.g., 90 days. The aggregator thereby adopts the incremental computing model with an intermediate state in a remote state store. 
The model has two benefits compared with re-computing every event at the request time: 
The figure below illustrates the computation paradigm. 
View is the client-facing layer that is responsible for lightweight transform of state. We separate view and aggregator to tailoring to two requirements: flexible user context and scalability. 
The Separation of Concerns principle knocks on our door again: state represents the heavy lifting of the computation. View transforms the states into a compact feature with customized session definitions suitable for the client, a.k.a., ML & serving system. Moreover, by sharing the state among multiple views, we can further save repeated computation and scale up the system. 
Development of the core infrastructure needs to be simple as well: The core logic of the framework we built relies heavily on data dependencies: external data sources for materialization, sequences of events and remote aggregator state, etc. The canonical programming model for this type of framework is based upon the design principle of dependency_inversion_principle. Dependency Injection is a concept of the passing of dependency (a service or data) into the object that would use it. We adopt Dagger2, a lightweight async DI framework, open-sourced by Google and Square to fulfill this need. 
On the other hand, the learning curve for Dagger2 is relatively high, so as a developer platform, we make our developer API dagger-agnostic. Specifically, 
Here we present how user events flow through the signal production pipeline continuously and are piped to Pinterest ML & serving systems. 
This starts with user engagement with the Pinterest website, such as clicking on a pin or creating search queries, which then bounces back to the backend and is tracked with users’ permission. The raw engagement event is lightweight to make tracking feasible for the infrastructure. The next step would be enriching them with rich content information (e.g., the Pin has various categories to denote its topic of interest). With enriched events, the pipeline can further condense the time sequence of events into a compact representation of a user’s instant, short-term and/or long-term propensity. This transformation is achieved via various aggregation and cutting-edge ML algorithms. 
Here is a diagram showing how all the component fits together. We take a two-component architecture: async (offline) event-driven processing and online processing. 
The first step in the signal processing journey is to read the raw events and hydrate them with external content-based features. This consists of three components: 
Aggregation happens online at the request time 
We introduced the core infrastructure in a month thanks to the adoption of the Dagger2 framework. Developing five signals took about two weeks by three engineers in order to turbo-charge some of our most critical ads retrieval and ranking initiatives. 
Next we’ll develop a new data processing architecture to move aggregation to event time, and unify user signals across Pinterest to this real-time infrastructure. 
Acknowledgments: Huge thanks to Siyang Xie for consulting on the design, Ning Zhang, Shu Zhang, Yiran Zhao, Tianbing Xu, Nishant Roy and the entire Ads ML Infra team, and George Yiu, Se Won Jang, Qingxian Lai, Connell Donaghy, Guodong Han, Jessica Chan, Saurabh Joshi, Sihan Wang who helped in improving and scaling the infrastructure. 
We would also like to give special thanks to Arun Prasad, Yitong Zhou and Bo Zhao from Content Personalization Team and John Zheng, Pihui Wei, Supeng Ge, Peifeng Yin, Crystal Lee, Xiaofang Chen from Ads Retrieval Team in our collaboration to develop and onboard an initial batch of user signals to the platform. 
We’re building the world’s first visual discovery engine. More than 320 million people around the world use Pinterest to dream about, plan and prepare for things they want to do in life. Come join us! 
Written by 
Written by",Pinterest Engineering,2019-12-05T17:52:31.985Z
"If You Aren’t Considering Cost, You Are Failing At Cyber Due Diligence | by Adam Bobrow | Cyber Risk Register | Medium","Everyone knows due diligence for an M&A deal must include a review of the acquired company’s cybersecurity systems. But checking systems and controls isn’t enough: If you aren’t estimating the cost of a potential information security meltdown, you are failing at cybersecurity due diligence. (Just ask Marriott.) 
You aren’t alone. Right now, M&A transactions are negotiated and executed every day without any principled estimates of how much a cybersecurity incident could cost. How can buyers make educated decisions and negotiate for protection without that information? 
The good news is that there is a viable, straightforward, and more useful way to develop an economic model that will give buyers the tools they need to estimate costs. 
To start, let’s look at cyber due diligence today. Buyers look at a potential acquisition’s existing controls and see how far the company is from meeting the cybersecurity standards it’s chosen to follow. It will also include an assessment of whether the buyer needs to supplement existing controls with more or different systems. Estimating the cost of implementing additional systems can inform negotiations over price, which is a good first step. 
Even more important, though, is estimating the cost of something going wrong. If Marriott had determined that it needed to spend a few million dollars on modernizing Starwood’s cybersecurity systems, it would have had a minimal effect on the the purchase price; the underlying liability may, in fact, be a billion dollar problem. 
Figuring out the value of the potential exposure requires a more sophisticated approach. No seller will give you the level of access you’d need to fully check their systems for a breach and no cybersecurity firm will guarantee they can find all potential intruders, who go to great lengths to remain undetected. Indeed, even if you could look for attackers on the system you would still miss breaches that have already happened, but haven’t yet produced a negative result. (For example, a disgruntled former employee takes home a thumb drive with valuable IP, but hasn’t yet sold it to a competitor.) 
Instead of looking for something that you probably cannot find, it is possible to do a sound analysis of the magnitude of a breach of the acquired company’s business, and then estimate what that would cost. One increasingly accepted method to to that is by implementing the Factor Analysis of Information Risk (FAIR) standard. The model built from that will empower you to truly consider cyber costs when negotiating and give you leverage, particularly on indemnification. 
Doing a FAIR analysis involves identifying the range of outcomes on a bad cybersecurity day, assessing how likely such a day is, and then generating a range of possible cost estimates. This actuarial approach provides the buyer with a means to understand its financial exposure in an acquisition, the same way that buyer looks at its financial exposure across a range of future forecasts for the economy, the job market, interest rates, and other factors relevant to the buying decision. With the relevant cybersecurity conclusions in hand, the buyer can adopt a range of mechanisms for managing that risk, including adopting a different cybersecurity approach, adjusting the price of the deal, or seeking cyber insurance coverage to indemnify against that risk. 
Some will argue that this practice won’t be adopted until a regulator forces buyers into it. In fact, the Securities and Exchange Commission (SEC) has already started down that path. 
Last February, the SEC released guidance that requires public companies to disclose both cyber incidents and risks. They went a step further to outline what surveillance systems are needed to ensure the disclosures of cyber breaches are informed. The rubric that the SEC followed closely resembled the FAIR standard’s model for measuring cybersecurity risk. If you don’t do this now, you’re already behind. And you’re not doing true due diligence. 
Need an expert to conduct a FAIR analysis of cybersecurity risk in M&A? Schedule a call with Foresight Resilience Strategies today. 
Written by 
Written by",Adam Bobrow,2019-01-13T21:23:19.215Z
Principal component analysis (PCA): Explained and implemented | by Raghavan | Medium,"It is very common in datascience tasks involving large number of features , that one is advised to PCA aka Principal component analysis . We is will start with a brief introduction to what and why of PCA . Then we will look into implementing PCA with explanation. 
The What , whys of PCA 
When there are lot of variables aka features n(> 10) , then we are advised to do PCA. PCA is a statistical technique which reduces the dimensions of the data and help us understand, plot the data with lesser dimension compared to original data. As the name says PCA helps us compute the Principal components in data. Principal components are basically vectors that are linearly uncorrelated and have a variance with in data. From the principal components top p is picked which have the most variance. 
In the above plot what PCA does is it draws a line across the data which has maximum variance , 2nd maximum variance , so on … But Wait , why is an axes with maximum variance important ? Lets consider our problem to be classification problem (Similar arguments can also be drawn for other problems too) . Our goal separate data by drawing a line (or a plane) between data. If we find out the dimension which has maximum variance, then it solves part of the problem, now all we have to use suitable algorithm to draw the line or plane which splits the data. 
Lets implement PCA 
Lets begin by generating random data with 3 dimensions with 40 samples . We will have two class with 20 samples per class. 
Now we will compute the scatter matrix or covariance of this matrix. Why scatter matrix ? scatter matrix records the relationship between variables , which is important to find a dimension of maximum variance . More on scatter martrix , covariance here . Either of scatter matrix or covariance can be used as covariance is just scaled version of scatter matrix. 
Now we know how each of variables are related to each other. We are just one step away from computing the principal components . Before that we will review a basic concept from Linear algebra : Eigenvalues and Eigenvectors. 
Eigenvalues and Eigenvectors 
Eigenvectors and Eigenvalues are a property of a matrix which satisfies the following equation . 
Where A denotes the matrix , x denotes the eigenvector and lambda denotes the eigenvalues. Now to understand the significance of eigenvector and eigenvalues , lets observe into the following gif . 
Here the original matrix is multiplied by a vector ( aka. undergoing a linear transformation), what we can observe is for lines colored blue and purple do not change direction , it only scales up , while the line colored red changes direction in addition it scaling up. 
A Eigenvector of a matrix (which can be seen as a linear transformation of matrix) is the condensed vector which summarizes one axes of the matrix. Another property of eigen vectors is that even if I scale the vector by some amount before I multiply it, I still get the same multiple of it as a result.We might also say that eigenvectors are axes along which linear transformation acts, stretching or compressing input vectors.They are the lines of change that represent the action of the larger matrix, the very “line” in linear transformation. Eigenvectors and Eigenvalues are defined for a square matrix. We prefer our eigenvectors to be always unit length one . 
If we have a eigenvector 
We compute the length of the vector by 
Then to make the eigenvector length one we do 
Since this is just the scalar operation , it is not going to affect the direction of the vector.A simple reason we do this is that we need all of our eigenvectors to be comparable ,so that we can choose the most valuable among them (the one having maximum variance). A matrix can have more than one eigenvector , and at max d eigenvectors if matrix is of dimention d X d. 
Back to PCA …. 
Lets continue on where we left of PCA , we have the scatter matrix which has the information about how one variable is related to the other variable. 
Now we use the numpy library to compute the Eigenvectors and eigenvalues 
By default numpy or any stats library gives out eigenvectors of unit length. Lets verify it 
Okay we could endup having upto 3 eigenvectors , since the size of our scatter matrix is 3 X 3 .Choosing k important eigenvectors from d vectors is same as droping d-k vectors . We now sort the eigenvectors by their eigenvalues and drop the least one. 
Now we choose k the largest eigenvectors : 
Finally we have the new axes across which we can project our samples , we just multiply the original vector with our chosen eigenvectors and plot. 
Given that we generated raw data , the plot may vary when we attempt to recreate. 
Comparing for the same data the PCA from sklearn 
Bingo !!! With this understanding by our side , we can define PCA as a process of finding the axes in our features space , viewed from which each samples in our data is separable in maximum way. 
Written by 
Written by",Raghavan,2018-08-19T05:52:28.203Z
My Path to Passing the AWS Machine Learning Certification | by Adam DeJans | Medium,"In June 2020 I passed the AWS Machine Learning - Specialty Certification Exam (MLS-C01) with a 93.2%. 
Earners of this certification have an in-depth understanding of AWS machine learning (ML) services. They demonstrated ability to build, train, tune, and deploy ML models using the AWS Cloud. Badge owners can derive insight from AWS ML services using either pretrained models or custom models built from open-source frameworks. 
One of the most difficult parts in preparing for this exam was trying to find exactly what to study for. In the past I’ve taken (and passed) Oracle’s Certified Java SE8 Programmer I exam and I found there to a plethora of information. There were many trusted resources including books and practice exams that prepare you well. However, when it comes to the AWS ML - Specialty Exam we don’t have this luxury seeing that the exam is relatively new. 
In this article, I will give an overview of my preparation strategy in hopes that it will help someone who is considering taking the exam as well. 
If you are new to AWS the first thing to do is to take the AWS Certified Cloud Practitioner (CCP) exam. The reason for this is two-fold. First, it provides the necessary foundation for the ML - Specialty exam. Second, it actually can be cheaper to do both the CCP and ML - Specialty exam instead of only the ML - Specialty exam. The reason it can be cheaper is because the CCP exam is $100 and the ML - Specialty exam is $300, but upon passing the CCP exam you will receive a 50% off voucher for another AWS certification exam which can be applied to the ML - Specialty exam, making the cost for both exams $250 (assuming you pass on the first attempt) instead of just $300 for only the ML - Specialty. 
For the 3 hour Machine Learning Specialty exam, you need high level knowledge of different AWS systems such as: Lambda, Batch, Data Pipelines, Step, IAM/Security, Amazon RDS, Athena, Glue, EMR, containers, etc. Aside from the high level knowledge of these systems you need to know how these systems work together so that you’re able to produce practical ML solutions within AWS. 
Don’t forget that the goal of this exam is aimed at verifying that you can apply ML in practice, including but not limited to: data analysis, data cleaning, model/hyper-parameter tuning, ETL, bias, variance, distributions, etc. From my experience I would say that about 40% of the exam is based on data analysis/ML concepts that could be answered with almost no knowledge of AWS (since I come from a mathematics background I found this to be the easy part). 
With that being said you will need to understand storage and database solutions such as S3, RDS and DynamoDB and have a good understanding of the various AWS managed AI services such as Comprehend, Rekognition and Transcribe, along with how to make these tools useful in application. 
The most important tool to learn how to use for this exam is SageMaker. Along with this you’ll unfortunately need to remember more than you’d like to about the SageMaker built-in algorithms (for example, the Amazon SageMaker implementation of XGBoost only supports CSV and libsvm formats for training and inference — not something I care to memorize). 
The exam itself is 3 hours long and consists of 65 scenario-based multiple-choice and multi-selection questions. It’s unlikely you’ll need the full 3 hours, I was able to complete the exam around 1 hour, leaving plenty of time to review difficult questions if needed. At the time of taking this AWS exam, AWS was offering all exams to be administered from home due to the Coronavirus pandemic. Upon submission you will be given a pass/fail message and within a few days you’ll be sent your official certification credentials. 
The AWS ML - Specialty Exam guide breaks the exam down into four categories: (1) Data Engineering, (2) Exploratory Data Analysis, (3) Modeling, and (4) Machine Learning Implementation and Operations. 
Domain 1: Data Engineering (20%) 
1.1 Create data repositories for machine learning. 1.2 Identify and implement a data-ingestion solution. 1.3 Identify and implement a data-transformation solution. 
Due to my lack of AWS experience with data engineering this section was challenging. If you read AWS documentation on Glue, Kinesis, and the like along with drafting some architectural solutions via diagrams you will start to realize how each service is used and this section will start to come natural. 
Domain 2: Exploratory Data Analysis (24%) 
2.1 Sanitize and prepare data for modeling. 2.2 Perform feature engineering. 2.3 Analyze and visualize data for machine learning. 
This section doesn’t have much to do with AWS, but rather data analysis in general. Anyone working within analytics should have no problem with this section, definitely the easiest section if you have any analytics background. 
Domain 3: Modeling (36%) 
3.1 Frame business problems as machine learning problems. 3.2 Select the appropriate model(s) for a given machine learning problem. 3.3 Train machine learning models. 3.4 Perform hyperparameter optimization. 3.5 Evaluate machine learning models 
Most of the exam seems to fall under this category. This is when it becomes important to know SageMaker, built-in algorithms, and AWS services; along with how apply them to practical business situations. 
Again, I come from a mathematics background so this part of the exam was also relatively easy. If you work in machine learning you shouldn’t find any of the ideas here to be new. 
Domain 4: Machine Learning Implementation and Operations (20%) 
4.1 Build machine learning solutions for performance, availability, scalability, resiliency, and fault tolerance. 4.2 Recommend and implement the appropriate machine learning services and features for a given problem. 4.3 Apply basic AWS security practices to machine learning solutions. 4.4 Deploy and operationalize machine learning solutions. 
This was by far the hardest section in the exam for me. It seemed that often times things needed to be memorized that are often looked up and found within documentation. 
These are the resources that I used to prepare myself from the Cloud Practitioner Exam to the Machine Learning - Specialty Exam. 
CCP: 
· digitalcloud.training: this was a great introduction to AWS and really helps with preparation for the the exam 
· WhizLabs CCP Practice exams: WhizLabs is always a classic providing high quality practice exams 
ML - Specialty 
· AWS Machine Learning Certification Exam|2020 Complete Guide: Udemy course that provids a recap of material on the AWS ML - Specialty exam. This course is not very practical and not necessary for someone with a background in data science. However, if you’re coming from a software development background this might be useful. 
· 2020 AWS SageMaker, AI and Machine Learning Specialty Exam: Udemy course that provided great hands on experience in a practical manner. Includes a practice exam that is representative of the actual exam. 
· WhizLabs ML - Specialty Practice exams: quality practice exams 
· AWS Certified Machine Learning Specialty Full Practice Exam: very thorough practice exam 
· AWS Certified Machine Learning Specialty: 3 PRACTICE EXAMS: 3 representative practice exams - pretty good deal. 
· Exam Readiness: AWS Certified Machine Learning — Specialty: AWS exam readiness course. Gives helpful tips and has representative quizzes. 
· The Activation Function Blog: very difficult practice quiz questions. 
· AWS Sample ML - Speciality Questions: representative questions with included solutions. 
· SageMaker Developer Guide: a good reference. 
· AWS Official Practice Exam: If you follow the advice of taking the CCP first you will receive a free voucher ($40 value) for this practice exam. The exam is is only 20 questions but represents the type of questions you’ll be asked. 
Hopefully this article will be useful to you and give some guidance. While the ML - Specialty certification is difficult it’s definitely do-able, even without the recommended years of experience. If you come from any type of data science background I recommend you to check out this certification or at least check out with AWS has to offer to help you scale your projects. 
Happy learning :) 
Written by 
Written by",Adam DeJans,2020-07-10T12:59:04.635Z
Natural Language Processing is Fun! | by Adam Geitgey | Medium,"This article is part of an on-going series on NLP: Part 1, Part 2, Part 3, Part 4, Part 5. You can also read a reader-translated version of this article in 普通话 or فارسی. 
Giant update: I’ve written a new book based on these articles! It not only expands and updates all my articles, but it has tons of brand new content and lots of hands-on coding projects. Check it out now! 
Computers are great at working with structured data like spreadsheets and database tables. But us humans usually communicate in words, not in tables. That’s unfortunate for computers. 
A lot of information in the world is unstructured — raw text in English or another human language. How can we get a computer to understand unstructured text and extract data from it? 
Natural Language Processing, or NLP, is the sub-field of AI that is focused on enabling computers to understand and process human languages. Let’s check out how NLP works and learn how to write programs that can extract information out of raw text using Python! 
Note: If you don’t care how NLP works and just want to cut and paste some code, skip way down to the section called “Coding the NLP Pipeline in Python”. 
As long as computers have been around, programmers have been trying to write programs that understand languages like English. The reason is pretty obvious — humans have been writing things down for thousands of years and it would be really helpful if a computer could read and understand all that data. 
Computers can’t yet truly understand English in the way that humans do — but they can already do a lot! In certain limited areas, what you can do with NLP already seems like magic. You might be able to save a lot of time by applying NLP techniques to your own projects. 
And even better, the latest advances in NLP are easily accessible through open source Python libraries like spaCy, textacy, and neuralcoref. What you can do with just a few lines of python is amazing. 
The process of reading and understanding English is very complex — and that’s not even considering that English doesn’t follow logical and consistent rules. For example, what does this news headline mean? 
“Environmental regulators grill business owner over illegal coal fires.” 
Are the regulators questioning a business owner about burning coal illegally? Or are the regulators literally cooking the business owner? As you can see, parsing English with a computer is going to be complicated. 
Doing anything complicated in machine learning usually means building a pipeline. The idea is to break up your problem into very small pieces and then use machine learning to solve each smaller piece separately. Then by chaining together several machine learning models that feed into each other, you can do very complicated things. 
And that’s exactly the strategy we are going to use for NLP. We’ll break down the process of understanding English into small chunks and see how each one works. 
Let’s look at a piece of text from Wikipedia: 
London is the capital and most populous city of England and the United Kingdom. Standing on the River Thames in the south east of the island of Great Britain, London has been a major settlement for two millennia. It was founded by the Romans, who named it Londinium. 
(Source: Wikipedia article “London”) 
This paragraph contains several useful facts. It would be great if a computer could read this text and understand that London is a city, London is located in England, London was settled by Romans and so on. But to get there, we have to first teach our computer the most basic concepts of written language and then move up from there. 
The first step in the pipeline is to break the text apart into separate sentences. That gives us this: 
We can assume that each sentence in English is a separate thought or idea. It will be a lot easier to write a program to understand a single sentence than to understand a whole paragraph. 
Coding a Sentence Segmentation model can be as simple as splitting apart sentences whenever you see a punctuation mark. But modern NLP pipelines often use more complex techniques that work even when a document isn’t formatted cleanly. 
Now that we’ve split our document into sentences, we can process them one at a time. Let’s start with the first sentence from our document: 
“London is the capital and most populous city of England and the United Kingdom.” 
The next step in our pipeline is to break this sentence into separate words or tokens. This is called tokenization. This is the result: 
“London”, “is”, “ the”, “capital”, “and”, “most”, “populous”, “city”, “of”, “England”, “and”, “the”, “United”, “Kingdom”, “.” 
Tokenization is easy to do in English. We’ll just split apart words whenever there’s a space between them. And we’ll also treat punctuation marks as separate tokens since punctuation also has meaning. 
Next, we’ll look at each token and try to guess its part of speech — whether it is a noun, a verb, an adjective and so on. Knowing the role of each word in the sentence will help us start to figure out what the sentence is talking about. 
We can do this by feeding each word (and some extra words around it for context) into a pre-trained part-of-speech classification model: 
The part-of-speech model was originally trained by feeding it millions of English sentences with each word’s part of speech already tagged and having it learn to replicate that behavior. 
Keep in mind that the model is completely based on statistics — it doesn’t actually understand what the words mean in the same way that humans do. It just knows how to guess a part of speech based on similar sentences and words it has seen before. 
After processing the whole sentence, we’ll have a result like this: 
With this information, we can already start to glean some very basic meaning. For example, we can see that the nouns in the sentence include “London” and “capital”, so the sentence is probably talking about London. 
In English (and most languages), words appear in different forms. Look at these two sentences: 
I had a pony. 
I had two ponies. 
Both sentences talk about the noun pony, but they are using different inflections. When working with text in a computer, it is helpful to know the base form of each word so that you know that both sentences are talking about the same concept. Otherwise the strings “pony” and “ponies” look like two totally different words to a computer. 
In NLP, we call finding this process lemmatization — figuring out the most basic form or lemma of each word in the sentence. 
The same thing applies to verbs. We can also lemmatize verbs by finding their root, unconjugated form. So “I had two ponies” becomes “I [have] two [pony].” 
Lemmatization is typically done by having a look-up table of the lemma forms of words based on their part of speech and possibly having some custom rules to handle words that you’ve never seen before. 
Here’s what our sentence looks like after lemmatization adds in the root form of our verb: 
The only change we made was turning “is” into “be”. 
Next, we want to consider the importance of a each word in the sentence. English has a lot of filler words that appear very frequently like “and”, “the”, and “a”. When doing statistics on text, these words introduce a lot of noise since they appear way more frequently than other words. Some NLP pipelines will flag them as stop words —that is, words that you might want to filter out before doing any statistical analysis. 
Here’s how our sentence looks with the stop words grayed out: 
Stop words are usually identified by just by checking a hardcoded list of known stop words. But there’s no standard list of stop words that is appropriate for all applications. The list of words to ignore can vary depending on your application. 
For example if you are building a rock band search engine, you want to make sure you don’t ignore the word “The”. Because not only does the word “The” appear in a lot of band names, there’s a famous 1980’s rock band called The The! 
The next step is to figure out how all the words in our sentence relate to each other. This is called dependency parsing. 
The goal is to build a tree that assigns a single parent word to each word in the sentence. The root of the tree will be the main verb in the sentence. Here’s what the beginning of the parse tree will look like for our sentence: 
But we can go one step further. In addition to identifying the parent word of each word, we can also predict the type of relationship that exists between those two words: 
This parse tree shows us that the subject of the sentence is the noun “London” and it has a “be” relationship with “capital”. We finally know something useful — London is a capital! And if we followed the complete parse tree for the sentence (beyond what is shown), we would even found out that London is the capital of the United Kingdom. 
Just like how we predicted parts of speech earlier using a machine learning model, dependency parsing also works by feeding words into a machine learning model and outputting a result. But parsing word dependencies is particularly complex task and would require an entire article to explain in any detail. If you are curious how it works, a great place to start reading is Matthew Honnibal’s excellent article “Parsing English in 500 Lines of Python”. 
But despite a note from the author in 2015 saying that this approach is now standard, it’s actually out of date and not even used by the author anymore. In 2016, Google released a new dependency parser called Parsey McParseface which outperformed previous benchmarks using a new deep learning approach which quickly spread throughout the industry. Then a year later, they released an even newer model called ParseySaurus which improved things further. In other words, parsing techniques are still an active area of research and constantly changing and improving. 
It’s also important to remember that many English sentences are ambiguous and just really hard to parse. In those cases, the model will make a guess based on what parsed version of the sentence seems most likely but it’s not perfect and sometimes the model will be embarrassingly wrong. But over time our NLP models will continue to get better at parsing text in a sensible way. 
Want to try out dependency parsing on your own sentence? There’s a great interactive demo from the spaCy team here. 
So far, we’ve treated every word in our sentence as a separate entity. But sometimes it makes more sense to group together the words that represent a single idea or thing. We can use the information from the dependency parse tree to automatically group together words that are all talking about the same thing. 
For example, instead of this: 
We can group the noun phrases to generate this: 
Whether or not we do this step depends on our end goal. But it’s often a quick and easy way to simplify the sentence if we don’t need extra detail about which words are adjectives and instead care more about extracting complete ideas. 
Now that we’ve done all that hard work, we can finally move beyond grade-school grammar and start actually extracting ideas. 
In our sentence, we have the following nouns: 
Some of these nouns present real things in the world. For example, “London”, “England” and “United Kingdom” represent physical places on a map. It would be nice to be able to detect that! With that information, we could automatically extract a list of real-world places mentioned in a document using NLP. 
The goal of Named Entity Recognition, or NER, is to detect and label these nouns with the real-world concepts that they represent. Here’s what our sentence looks like after running each token through our NER tagging model: 
But NER systems aren’t just doing a simple dictionary lookup. Instead, they are using the context of how a word appears in the sentence and a statistical model to guess which type of noun a word represents. A good NER system can tell the difference between “Brooklyn Decker” the person and the place “Brooklyn” using context clues. 
Here are just some of the kinds of objects that a typical NER system can tag: 
NER has tons of uses since it makes it so easy to grab structured data out of text. It’s one of the easiest ways to quickly get value out of an NLP pipeline. 
Want to try out Named Entity Recognition yourself? There’s another great interactive demo from spaCy here. 
At this point, we already have a useful representation of our sentence. We know the parts of speech for each word, how the words relate to each other and which words are talking about named entities. 
However, we still have one big problem. English is full of pronouns — words like he, she, and it. These are shortcuts that we use instead of writing out names over and over in each sentence. Humans can keep track of what these words represent based on context. But our NLP model doesn’t know what pronouns mean because it only examines one sentence at a time. 
Let’s look at the third sentence in our document: 
“It was founded by the Romans, who named it Londinium.” 
If we parse this with our NLP pipeline, we’ll know that “it” was founded by Romans. But it’s a lot more useful to know that “London” was founded by Romans. 
As a human reading this sentence, you can easily figure out that “it” means “London”. The goal of coreference resolution is to figure out this same mapping by tracking pronouns across sentences. We want to figure out all the words that are referring to the same entity. 
Here’s the result of running coreference resolution on our document for the word “London”: 
With coreference information combined with the parse tree and named entity information, we should be able to extract a lot of information out of this document! 
Coreference resolution is one of the most difficult steps in our pipeline to implement. It’s even more difficult than sentence parsing. Recent advances in deep learning have resulted in new approaches that are more accurate, but it isn’t perfect yet. If you want to learn more about how it works, start here. 
Want to play with co-reference resolution? Check out this great co-reference resolution demo from Hugging Face. 
Here’s an overview of our complete NLP pipeline: 
Whew, that’s a lot of steps! 
Note: Before we continue, it’s worth mentioning that these are the steps in a typical NLP pipeline, but you will skip steps or re-order steps depending on what you want to do and how your NLP library is implemented. For example, some libraries like spaCy do sentence segmentation much later in the pipeline using the results of the dependency parse. 
So how do we code this pipeline? Thanks to amazing python libraries like spaCy, it’s already done! The steps are all coded and ready for you to use. 
First, assuming you have Python 3 installed already, you can install spaCy like this: 
Then the code to run an NLP pipeline on a piece of text looks like this: 
If you run that, you’ll get a list of named entities and entity types detected in our document: 
You can look up what each of those entity codes means here. 
Notice that it makes a mistake on “Londinium” and thinks it is the name of a person instead of a place. This is probably because there was nothing in the training data set similar to that and it made a best guess. Named Entity Detection often requires a little bit of model fine tuning if you are parsing text that has unique or specialized terms like this. 
Let’s take the idea of detecting entities and twist it around to build a data scrubber. Let’s say you are trying to comply with the new GDPR privacy regulations and you’ve discovered that you have thousands of documents with personally identifiable information in them like people’s names. You’ve been given the task of removing any and all names from your documents. 
Going through thousands of documents and trying to redact all the names by hand could take years. But with NLP, it’s a breeze. Here’s a simple scrubber that removes all the names it detects: 
And if you run that, you’ll see that it works as expected: 
What you can do with spaCy right out of the box is pretty amazing. But you can also use the parsed output from spaCy as the input to more complex data extraction algorithms. There’s a python library called textacy that implements several common data extraction algorithms on top of spaCy. It’s a great starting point. 
One of the algorithms it implements is called Semi-structured Statement Extraction. We can use it to search the parse tree for simple statements where the subject is “London” and the verb is a form of “be”. That should help us find facts about London. 
Here’s how that looks in code: 
And here’s what it prints: 
Maybe that’s not too impressive. But if you run that same code on the entire London wikipedia article text instead of just three sentences, you’ll get this more impressive result: 
Now things are getting interesting! That’s a pretty impressive amount of information we’ve collected automatically. 
For extra credit, try installing the neuralcoref library and adding Coreference Resolution to your pipeline. That will get you a few more facts since it will catch sentences that talk about “it” instead of mentioning “London” directly. 
By looking through the spaCy docs and textacy docs, you’ll see lots of examples of the ways you can work with parsed text. What we’ve seen so far is just a tiny sample. 
Here’s another practical example: Imagine that you were building a website that let’s the user view information for every city in the world using the information we extracted in the last example. 
If you had a search feature on the website, it might be nice to autocomplete common search queries like Google does: 
But to do this, we need a list of possible completions to suggest to the user. We can use NLP to quickly generate this data. 
Here’s one way to extract frequently-mentioned noun chunks from a document: 
If you run that on the London Wikipedia article, you’ll get output like this: 
This is just a tiny taste of what you can do with NLP. In future posts, we’ll talk about other applications of NLP like Text Classification and how systems like Amazon Alexa parse questions. 
But until then, install spaCy and start playing around! Or if you aren’t a Python user and end up using a different NLP library, the ideas should all work roughly the same way. 
This article is part of an on-going series on NLP. You can continue on to Part 2. 
If you liked this article, consider signing up for my Machine Learning is Fun! newsletter: 
You can also follow me on Twitter at @ageitgey, email me directly or find me on linkedin. I’d love to hear from you if I can help you or your team with machine learning. 
Written by 
Written by",Adam Geitgey,2020-09-24T12:29:58.037Z
"5 steps to AWS Machine Learning Specialty Certification, Made Easy | by Daniel S. Flamarich | xplore.ai | Medium","How to prepare for the exam in 5 steps and what not to do. 
The AWS Machine Learning certification differs a bit from all the other certifications; you need to have a good grasp on all things Amazon Web Services, but also understand the basics of Machine Learning and Deep Learning. This may not be an easy task depending on your background. 
Many different profiles try this exam, from engineers to cloud experts, but also data scientists and analysts. Of course, the ideal in terms of having a good chance of passing the exam would be to be an experienced jack-of-all-trades that can feel confortable in all those topics. This is rarely the case, as people usually have strong knowledge of their field, but weaker knowledge when it comes to things they haven’t studied or worked with. We all know unicorns don’t exist. 
To help with this situation, this article will show you: 
Summarized, here are the 5 points that helped me pass. We’ll go through them below: 
1- Read the whitepapers, especially Sagemaker’s, but ideally all those from the main AWS services. 
2- Try as many AWS services as you can if you haven’t worked with them in the past. Play with them, feel comfortable. 
3- Forget AWS official content outline percentages; keep in mind it’s 50% ML/DL, 25% Sagemaker and 25% other AWS services. 
4- Don’t get stuck and/or think about how much time you have left; 180 minutes is enough, but use that time wisely. 
5- Don’t be fooled by the exam’s constant tricks that try to confuse you. Once you read a difficult question properly, you’ll notice it wasn’t that hard. 
Although you can find online resources and paid courses, the best way to understand AWS is to read the documentation of the services. Yes, it is a lot. But as you go through them, you’ll be able to connect the dots and acquire a general and much needed understanding. Amazon’s free training courses can help you too, but some of them are just simple tutorials that won’t give you a deep knowledge of the subject (they’re usually internal presentations and formations made public). The most important service is Sagemaker, and that’s where your effort should be. Pivot from there. 
If you don’t have it already, open an AWS account and start using its Machine Learning services. The Free Tier offers hours to compute, and lets you use more than 60 services for free for 12 months. There are great demos in there, and free of charge tools that will help you understand usage and case scenarios and to try pre-trained algorithms by yourself. This is priceless. 
Amazon provides a list of domains you’ll need to know, from Data Engineering and ML Implementations and Operations (each representing 20% of the exam), Exploratory Data Analysis (24%) and Modelling (36%). My view after passing the exam was that they didn’t make much sense. A more real partition would be this: 
Machine Learning and Deep Learning (50% of the exam) 
You should be comfortable with the whole ML cycle; from data collection and data preparation to exploratory data analysis and modelling. A good grasp on how to formulate problems and measure success is paramount. Which metrics to use play an important role in the exam. What types of data (structured, unstructured) exist and what makes good data in a machine learning project. 
Data preparation is a key step in data science, and you should expect a few questions on missing value handling, categorical encoding, imputation and other feature engineering steps. Pay attention to scaling (normalization and standardization) for numerical data, n-grams and bag-of-words for text data, as well as Term Frequency — Inverse Document Frequency (tf-idf), as you may have to solve simple corpus data problems. Also, keep in mind how and why recordIO protobuf format will work better in AWS jobs. 
Some questions may present you visualizations of data and ask you something about that data. This is clearly an attempt to test your skills in data analysis. You should be comfortable knowing the different types of graphs and their aim (comparison, composition, relationships and distributions). Be aware of how they are used and how they can help in exploratory data analysis. 
Model design is obviously present in the exam too. How to select a good model, what ML approach is better in a situation (regressions, classifications) and what metrics and strategies should be used. About the algorithms, you should be comfortable with all the classics (K-Means and its differences with K-Nearest Neighbours, Random Forests and Decision Trees) as well as with Convolutional Neural Networks. 
You’ll need to understand the concepts of training, test and validation data, identify potential biases introduced by insufficient splits, and additional measures that could be used to increase data value. What kind of generalization are we looking for in a ML process, how it will be consumed (real time, batch, API applications), and how to tell if the generalization is working (accuracy tests) is an all-present topic throughout the exam. You’ll need to be able to explain how Confusion Matrices, Recall, Precision and False Positive Rate work. 
About evaluating models, you’ll need to understand offline and online validation, and at least conceptually know about canary deployments. Underfitting and overfitting and how to overcome them, regression accuracy (RMSE), histograms and skewness in them, AUC metrics in classification, and all the trade-offs in evaluation that might call for different optimizations are all very important in the exam. Also, model tuning specifics and at least some grasp in Bayesian optimization will help a lot. 
Sagemaker (25% of the exam) 
While knowledge of Machine Learning is expected from you in the exam, the star of the game is Sagemaker, AWS’ central and pivotal service for all things AI. Sagemaker helps you through the whole ML cycle from start to finish, and you’ll probably have to explain how AWS services interact with it. Keep in mind that it will appear in questions quite often, and you need to know its intricacies when it comes to training models (how to create a training job API, how to set up an Elastic Container Repository, inferences). 
Another important thing to know are Sagemaker built-in algorithms. From Linear Learners to Factorization Machines, Image Analysis and Anomaly Detection, and differences amongst text analysis algorithms (managing and explaining when to use LDA, Neural Topic Modeling, Seq2Seq and Blazing Text can and probably will appear often). As a widely used algorithm both in regression and classification problems, XGBoost shows up too, as in comparisons with other algorithms. Keep in mind that knowing the main built-in algorithms hyperparameters and metrics is a must. 
Last but not least, knowing how to take Sagemaker models to production and how Jupyter notebooks work is needed. Sagemaker hosting services (how to create endpoint configurations) and inference pipelines to chain together algorithms, docker containers and elastic inference can pop up in the questions. You should also know how automatic scaling works. 
Other AWS services (25% of the exam) 
Here’s where the exam gets similar to other AWS certifications. If you already have a solutions architect or big data certification (I didn’t), all this will surely be familiar. The most important services here are AWS Glue and Athena (for ETL jobs), the Kinesis family (for streaming data), S3, RDS, DynamoDB and Redshift (as data stores) and the Hadooq cluster ecosystem Elastic Map Reduce (EMR). You should understand in depth how they all work, what are their strengths and why should be used in specific situations. The white papers for these services are quite good, but ideally you should have some experience working with them. If that’s your case, you are covered. 
Some questions may make you choose the correct order of services to use in a given business situation. You will need to know each service, its purpose and use-cases that will be presented to you in the questions. Keep in mind all monitoring and evaluation tools for deployments AWS has to offer. I’d recommend making an effort in understanding CloudWatch and CloudTrail in detail if you’ve never used them before. 
There’s a few AI developer services that you must know, but the good news is that they’re quite simple: Forecast, Lex, Personalize, Polly, Rekognition, Transcribe and Translate are all easy-to-use services available in the console that you just need to try yourself to understand. 
For deployments outside Sagemaker, AWS offers some things you should check out: Elastic Container Service, EC2 AMIs, Elastic Map Reduce and on-premises options (MXNet and TensorFlow frameworks). 
While you’ll have a bit less than three minutes per question (which is more than enough), I find the best strategy is to answer first all those questions you know, -or have little doubt about- and leave the harder ones for later. 
This means going through the questions once and mark the ones that make you doubt. Then, dedicate the bulk of your time to hard questions. If you do this right you may even have the last 20–30 minutes of the exam to make a last check and get an intuition of how you just did. 
You’ll notice some verbose questions in the exam, but after reading them, you’ll see they are just trying to confuse you. Read the answers carefully. Even before the questions themselves. They usually point to the service or solution better than the question. 
This is an important point: the exam will try to trick you, almost constantly. It’ll try to find where your knowledge is thinner with very similar answers that may superficially look the same. If a question seems complicated, leave it for later. The worst thing you can do is to get stuck in a question and spend 10 minutes there, because you’ll probably need them at the end of the test. 
There’s also a chance that you’ll need to do some calculations. Ask for paper and a pen, or whatever option is allowed, in your examination center. This may seem silly, but having the opportunity to write down doubts and scribble simple math could be a game changer in the exam, and who doesn’t want all the odds in her/his favour? 
Best of luck! 
Original article here. 
Written by 
Written by",Daniel S. Flamarich,2020-05-04T09:22:41.285Z
Unsupervised Machine Learning. In case you have not read the basis of… | by Harinath Selvaraj | coding&stuff | Medium,"In case you have not read the basis of machine learning, please read the below article, or else you are good to skip. 
Unsupervised learning a supervised learning algorithm draws inferences from data which does not have labels. 
Let’s look at the below example — 
There are students who do not require any external training and they can learn by themselves. This is the concept of unsupervised learning. 
An example of unsupervised learning is clustering. There are many clustering techniques such as 
K-means clustering 
Hierarchical clustering 
Fuzzy C Means clustering 
The concept of clustering is simple, there should be high-intra cluster similarity and low inter cluster similarity. 
K- Means Clustering with an Example 
Now we’ll go ahead and implement the K-means clustering algorithm on top of the iris dataset. 
let’s have a glance at the iris dataset - 
The iris dataset comprises of 5 columns. Before we build the k-means clustering algorithm on top of this dataset, we need to remove the 5th column because the K-Means Clustering can be applied only on top of the numerical values. 
Therefore, we’ll be selecting the first 4 columns from this data set and we’ll store it in iris_k object. 
Now, we see that the 5th column has been deleted and we just have the first four columns. 
We also need to convert this data frame into a matrix. We’ll use as.matrix function and convert this data frame into a matrix and store the result back into the iris_k object. 
It’s finally time to use the K-means function to divide our data set into clusters. This takes in two parameters — 1st is the data set and next is the number of clusters it is divided into. We are taking iris_k data set and dividing this entire data set into three clusters. 
You will see that 150 observations have been divided into three clusters. 
We’ll bind the clustering vector with the original data set to have a better analysis. Let’s have a glance at this clustered data object so you have binded the clustering vector object with the original data set. 
If we have a closer look at this data set, you will understand that the first 50 observations of this data set (or) all of the setosa species of the iris flower have been grouped into the third cluster and the rest of the hundred observations i.e.) virginica and versicolor have been grouped in cluster no.1 and cluster no.2. 
We have implemented the k-means algorithm on top of the iris data set 😃 
Reinforcement Learning 
Reinforcement learning is a type of machine learning algorithm, with the machine or agent in an environment, learns ideal behaviour in order to maximise its performance. 
An simple reward feedback is required for the agent to learn its behaviour. This is known as Reinforcement Signal. 
Let’s take pac-man for example : As long as pac-man keeps eating food, it earns points but when it crashes against a monster, it loses its life. This is how pac-man learns that it needs to eat more food and avoid monsters so as to improve its performance. 
Some real-world applications of reinforcement learning include Google self-driving car, the manufacturing robots used in the industry & autonomous flying helicopters used in the military. 
Thanks for reading 😃! 
Written by 
Written by",Harinath Selvaraj,2018-10-04T16:45:00.200Z
Should we be treating algorithms the same way we treat hazardous chemicals? | by Andrew Maynard | EDGE OF INNOVATION | Medium,"At first blush, algorithms and hazardous chemicals have precious little in common. One is created out of computer code, and exists in cyberspace; the other is made up of atoms and molecules, and forms a very real part of the world we physically inhabit. 
Yet despite these differences, both have an uncanny ability to impact our lives in ways that are neither desirable, nor always easy to predict. And because of this, there are surprising similarities in how the risks associated with the development and use of algorithms might be assessed. 
The responsible development and use of algorithms has, of course, been a focus of discussion for some time. At one extreme there’s the well-publicized yet rather speculative risk of superintelligent machines wiping out life on earth. On the other, there are the closer-to-home risks of algorithms deciding who lives and who dies in a self-driving car crash, or the dangers of them reflecting the racial and gender biases of their creators. 
These and other possible “algorithmic risks” are prompting a growing number of researchers to ask what might go wrong as we grapple with the fickle line between desired benefits and unacceptable harm. 
And yet the field of algorithmic risk still has a long way to go before research can reliably be used to make informed risk decisions. And this is where the parallels between algorithms and chemical risk assessment have the potential to accelerate progress. 
Reading a recent headline in in MIT Technology Review, I was struck afresh by how similar the challenges in algorithmic risk research are to many of those the chemicals community have struggled with for decades. 
Reporting on new research on self-driving cars and “predictive inequity”, the headline proclaimed “Self-driving cars may be more likely to hit you if you have dark skin.” This is a startling, unsettling claim. It’s also deeply worrying if you live somewhere where self-driving vehicles are commonplace on public roads, and you have dark skin. 
Social media coverage was even more decisive in how the paper was interpreted. Kate Crawford, co-founder of the AI Now Institute tweeted “Guess what? Study shows that self-driving cars are better at detecting pedestrians with lighter skin tones. Translation: Pedestrian deaths by self-driving cars are already here — but they’re not evenly distributed.” 
The paper itself is a little more guarded in its conclusions, as it couches indications of potential bias in qualifying caveats. Yet it does raise the possibility that the pedestrian-detection systems potentially used in self-driving cars may be better at identifying and avoiding people with lighter colored skin, compared to those with darker skin tones. 
This is what the authors refer to as “predictive inequity” or the possibility that technologies using such systems could be biased toward harming people based on the color of their skin. 
Given the seriousness of these findings, it’s not surprising that people are worried that self-driving cars may have baked-in biases. Yet as researchers in other areas of risk have discovered, knee-jerk reactions to seemingly-startling results rarely result in socially beneficial outcomes. 
So how should developers and others be conducting, weighing, and making sense of research into the potential risks of algorithms? 
Part of the answer lies in how risks in other domains are assessed and managed — and in particular, what ca be learned from how we assess and manage the potential risks from commercial chemicals. 
Commercial chemicals — those chemicals that are intentionally manufactured and used to make the things our lives so-often rely on — are often a double-edged sword when it comes to risks and benefits. Yet despite the aspirations of some to lead a “chemical-free” life, manufactured substances are the backbone of modern society. 
Commercially produced chemicals are used to produce the clothes we wear, the houses we live in (and what we put in them), the cars we drive, the roads we drive on, the electronic devices we can’t live without, the food we eat, the services we rely on, and much more besides. 
Without commercial chemicals, it’s not too much of a stretch to say that modern society as we know it would collapse. 
And yet, this dependency comes at a price. If not used responsibly, commercial chemicals and their products can disrupt the climate, destroy ecosystems, clutter oceans, cause disease and death, and an awful lot more. 
Because of this tension between utility and impact, businesses and regulators have had to learn the hard way how to understand and manage the risks of these substances, so that users can take advantage of their very-real benefits. 
Of course, it should be pointed out that, despite substantial progress over the past century, they still have a lot to learn. 
For instance, scientists and regulators still struggle to understand and manage the potential effects of chronic exposure to some chemicals — endocrine disruptors for instance. And there remain many ways that chemicals interact with our bodies and the environment that researchers don’t yet fully understand — including the effects of exposures within critical periods of development, such as in the first trimester of pregnancy. 
Yet despite these challenges, scientists have developed an impressive array of methods and tools for assessing and managing risks from chemical exposures. And many of these have an unexpected relevance to the potential risks of algorithms. 
At the hear of risk assessment is evidence, and how it’s used; irrespective of whether the risk is from chemical exposure, algorithms, or some other source. And one of the biggest dangers in making sense of risk is ignoring, misinterpreting, or misusing evidence. 
This danger becomes especially relevant where limited or poor quality evidence is combined with assumptions about possible risks. This is where cognitive biases and mental shortcuts all too easily easily fill in the data gaps and create a compelling but often-deceptive perception of reality. 
When risk studies come along that seem to support intuitive fears or deeply held beliefs it’s all too easy to take them at face value. And it’s tempting to use them to drive decisions and regulations that feel like they’re the right thing to do. 
Yet this reliance on assumptions and beliefs inevitably leads to decisions that are based less on evidence, and more on how we perceive the world to be. 
These are natural tendencies, and we’re all subject to them — especially when the health or life of someone we care for is at stake. Yet they can all-too-easily lead to “cherry picking” studies and data that seem to confirm prior assumptions, irrespective of their validity. 
Because of this, communities working on health risks have developed intricate ways to avoid human bias from interfering with how research is conducted and interpreted. 
Despite this though, it’s all-too-common for well-meaning groups (including researchers) to fall into the trap of unquestioningly accepting research that supports their prior beliefs, while rejecting studies that don’t. 
This is seen for instance in communities that refuse to have their children vaccinated because they “cherry pick” misleading and even discredited research to support their position. It’s also seen in the ways that questionable studies are spun to elicit fear around the use of genetic modification in food. 
It’s cases like these that lead risk professionals being deeply skeptical about using single studies, and relying on prior assumptions to drive decisions that potentially affect people’s lives. And one way this skepticism is demonstrated is through the many protocols and initiatives in place around the world to avoid bias creeping into risk assessments. 
Many of these protocols come across as onerous and pedantic to the uninitiated. And to be fair, there are plenty of researchers who feel the same way. Yet they save lives. And they do so by helping to avoid well-meaning but potentially disastrous decisions being made on the flimsiest of evidence. 
“businesses and regulators have had to learn the hard way how to understand and manage the risks of commercial chemicals so that users can take advantage of their very-real benefits” 
One of the better-known initiatives that bring checks and balances to the process of health-related risk assessment is the Cochrane network. 
The Cochrane initiative is, in the words of the network, committed to “improved health where decisions about health and health care are informed by high-quality, relevant and up-to-date synthesized research evidence.” 
Here, the Cochrane Handbook for Systematic Reviews of Interventions is something of a gold standard when it comes to evaluating research into how substances potentially affect people’s health. It’s long, it’s meticulous, it’s demanding. And it works. 
Yet it doesn’t address research into the health and safety implications of algorithms and how they’re used. 
This is of course no surprise — the handbook is, after all, designed to address more conventional health risks. Yet if algorithms such as the ones in the predictive inequity study above have the potential to lead to harm if used inappropriately, there’s an argument to be made for them being subject to similar levels of rigor. 
This is especially true if there’s a risk of over-enthusiastic interpretation of research on algorithmic risks leading to decisions that appear naïve, misguided, and ultimately harmful, with hindsight 
Given this, it’s worth asking whether the methods and frameworks behind chemicals risk assessment be applied to algorithms. In other words, is there a need for an equally rigorous set of checks and balances for algorithmic risk research? 
Of course, an algorithm is not a chemical. It doesn’t have a physical form, nor a chemical identity that determines how it interacts with biological systems. Yet like many chemicals, algorithms have the potential to endanger lives if not developed and used responsibly. 
And once you get beyond the different mechanisms behind how they act, the analogy between algorithms and chemicals becomes intriguingly compelling. 
This is where there are a number of concepts embedded within chemical risk assessment that are potentially useful. 
Five of these stand out as being especially helpful. These include: 
These concepts are deeply embedded in how researchers, manufacturers and regulators assess risks from commercial chemicals. And they are directly applicable to research on algorithm-based risks. 
If something is hazardous — that is, if it has the potential to cause harm — why wouldn’t risk assessors recommend that it isn’t used? Presumably it’s better to be safe than sorry when it comes to working with potentially dangerous things, whether they are chemicals or algorithms? 
Unfortunately risk professionals learned centuries ago that conflating hazard with risk ultimately doesn’t help anyone, as everything has the potential to cause harm buried somewhere within it. And as a result, one of the pillars of modern risk assessment is navigating the difference between hazard and risk. 
Of course, many chemicals in our lives are relatively benign. But some have a high potential to cause harm — they’re highly hazardous. The gasoline in your car for instance, or the bleach under your sink. 
Yet unless this potential is somehow unleashed, the risks associated with these substances remain low. 
It’s a little like the difference between observing an enraged grizzly bear from a distance (high hazard but low risk), and coming face to face with the same bear on the ground (definitely high risk). 
In the case of bleach, the risk is negligible until someone does something stupid, like drink it. And the gasoline in your car only becomes a risk if it is spilled, the car malfunctions, or there’s a crash. 
This same distinction between the potential to cause harm and actual risk can be applied to algorithms. 
For instance, research on a hypothetical use of an algorithm can indicate that, if circumstances are right, it could cause serious harm. If a particular algorithm could conceivably be used to control aircraft or medical devices for example, and it’s shown to have serious flaws, it clearly represents a hazard. 
Yet this hazard will only become a risk if the algorithm is deployed without appropriate modifications and safeguards. 
This distinction between hazard and risk is highly relevant to algorithmic risk. But to make sense, it needs to be accompanied by an understanding of what leads to hazard being translated into risk in the first place. 
In the chemical world, it’s exposure that makes the difference between hazard and risk. No exposure means no risk, even if a chemical is potentially deadly. 
But what’s the equivalent to “exposure” when it comes to algorithms? Is there even an equivalent? 
I think there is. Users of systems and devices are effectively exposed to algorithms if they are affected by the decisions those algorithms lead to. And it’s not just users. Anyone who is potentially impacted by the deployment of an algorithm can be thought of as being exposed to it in some way. 
For example, and again going back to the predictive inequity paper, a poorly-designed obstacle seek-and-avoid system that’s used in self-driving cars “exposes” pedestrians to a potential hazard. In this case, the exposure doesn’t seem to be uniform, but is instead potentially weighted toward people with darker skins. 
This is where the concept of “algorithmic exposure” becomes valuable as a way of assessing whether a potential to cause harm under very specific conditions — in the lab or under computer simulations for instance —is likely to be converted into an actual risk in the real world. In some cased it will, but not always. 
For instance, in the predictive inequity paper, the object-identification algorithms being studied were representative of real-world applications. But they were not identical to the full-blown systems currently being used in self-driving cars. 
These on-the-road systems complement optical imaging with LIDAR detection and proprietary machine learning. And they’re designed to detect whole objects based on their size, shape, and differentiation from what’s around them. 
Because of this, it’s unlikely that the systems used in on-the-road self-driving cars would exhibit precisely the same behaviors as the researchers saw. At best, it’s unclear whether the algorithmic hazards observed would lead to algorithmic risk, because there is no clear algorithmic exposure route. 
This in no way negates the research, which established an important potential for poorly conceived systems to cause harm. But it does underline the need to identify viable exposure pathways in extrapolating research findings to real-life scenarios. 
But where such pathways do exist, how do we make sense of how much harm might occur, and — importantly — what sort of harm might result? 
This is another question that is deeply embedded within chemical risk assessment. Evidence of risk is is important. But it’s just one step in the process of evaluating the possible consequences of exposure. 
The potential consequences of being exposed to harmful chemicals can vary from the mundane — low-level, reversible effects for instance, that are more an irritation than a serious threat — to the severe, including disease, disability, and even death. 
This is where consequences matter, even if the probability of harm occurring is high. 
Most people for instance wouldn’t consider intestinal discomfort to be on the same level as bowel cancer, even if the chances of each occurring after an exposure was similar. Likewise, there’s a world of difference between indigestion and heart failure. 
Consequences matter in chemical risk assessment. And there’s no reason why they shouldn’t matter in the same way when making sense of algorithmic risks. 
The question is, how do we begin to make sense of, and weigh the relative importance of, different types of risks that may result from how algorithms are designed and used? 
“If we’re serious about ensuring the safe and beneficial development and use of algorithms, we need to treat them with the same rigor and respect that we do the chemicals that our lives depend on” 
Just as with chemical exposures, algorithms are likely to be responsible for many types of possible harm. This is especially true when algorithms are used to support decisions and behaviors that are directly tied to health. 
But this is also true where algorithms lead to people being threatened in other ways. For instance, how do we begin to parse the consequences of algorithms that result in people’s livelihood being threatened? Or their liberty, dignity, and self-respect? 
Of course, the specific ways in which harm is caused will be quite different between chemicals and algorithms. And yet, the underlying idea that the type of harm is as important as the probability of harm occurring, is common to both. 
In chemical risk assessment though, this is not the end of the process. Once there is an understanding of what harm might be caused and how, there’s a final, crucial, step before the risk can be assessed. 
This involves modeling the relationship between exposure and response (or, more accurately, dose and response), so that the consequences of different exposures can be predicted. And it’s this step that’s at the heart of informing what action is needed to address a particular risk. 
Dose response models allow chemical risk assessors to estimate how much “stuff” is likely to lead to what sort of impact, within a large population. In many ways it’s a crude science as the data these models are based on are usually messy — but it’s getting better. 
Yet even with the messiness, dose-response modes are essential for assessing chemical risks and managing them. 
And this raises the question of whether similar predictive models are possible to construct for algorithms. 
Using the predictive inequity paper as an example, it’s possible to think of “exposure” as the likelihood of pedestrians encountering self-driving cars using object-detection algorithms that are biased against people with darker skin tones. And in the same vein, it’s possible to equate “harm” to such encounters leading to injury or death. 
From here, it should be possible in principle to predict the chances of harm occurring for a given level of encounters with cars running the suspect algorithm. 
This would be the algorithmic equivalent of a chemical dose-response relationship — an “algorithmic exposure-response relationship” if you like. And its power lies in the ability to estimate the probability of pedestrians with darker skins preferentially being harmed by self-driving cars. 
Such an estimate — even if it’s possible — isn’t sufficient to determine whether a given level of risk is acceptable or not. It merely provides a number that developers, regulators, advocacy groups and others can use in making informed decisions as they grapple with what an “acceptable” level of risk versus an “unacceptable” level is. 
Of course, the ideal is that there’s no such thing as acceptable risk. Yet when it comes to chemical risks, we’ve had to come to terms as a society with the reality there’s no such thing as zero risk. And this means that, if we want to enjoy the benefits of the “stuff” that commercial chemicals enable, we also have to be clear-eyed about how much risk we as a society are willing to put up with. 
In effect, there’s always a risk of not developing or using a product, that needs to be balanced against the risks that using it might entail. And this is where being able to predict risk is especially helpful. 
In the case of skin-tone bias, there’s a very strong case to be made that there is no such thing as “acceptable” risk. And without doubt, developers and manufacturers should be striving to achieve this. At the same time, human drivers have been shown to exhibit a similar bias. And so even here, there’s a tradeoff between the relative risks of deploying good but not perfect technologies, and accepting the current state of affairs. 
This example highlights just how challenging risk decisions can become. And it underlines the importance of using solid evidence in making them. 
But even if you can estimate probabilities of specific types of harm occurring, how do you know that the evidence you are using, and the procedures you’re relying on, are trustworthy? 
There’s an old computer adage that says “garbage in, garbage out.” It’s an adage that deeply applicable to chemical risk assessment, as garbage evidence leads to garbage risk assessments. And by inference — and coming full circle — it’s equally relevant to understanding and acting on the potential risks of algorithms. 
This brings us back to the approaches that researchers and others use to make sense of evidence. 
In chemical risk assessment, there is often a deep respect for individual studies that push the boundaries of what is known, and a parallel deep skepticism around using these studies in isolation for making decisions. 
It’s a skepticism that’s based on the understanding that a single study can be useful, but inevitably is just one small piece of a much larger puzzle. And to treat such a piece as if it’s the whole picture is, at best, misleading. 
This skepticism is also based on the reality that most studies have their limitations. And here, risk researchers often rely on a number of rules of thumb to make sense of new research. 
These rules of thumb include (but are far from limited to) being on the lookout for implicit bias in studies, treating research that is statistically weak with caution, not trusting conclusions based on a single study, and relying heavily on the emerging weight of evidence from multiple studies. 
Risk researchers also tend to put a high premium on checks and balances in research, and are suspicious (but not off-the-cuff dismissive) of studies where the researchers may have a vested interest in the outcomes. 
In the world of chemicals risk assessment, these approaches to how research is conducted and the use of evidence lead to a level of rigor which, while far from perfect, does help to avoid serious errors. 
And there’s no reason why similar approaches couldn’t be extended to research on algorithm-based risks. 
In fact, I would go so far as to say that they must be, lest we end up building our understanding of algorithmic risks on an evidentiary stack of cards that collapses at the first hurdle. 
Of course, at the end of the day, algorithms and chemicals aren’t the same. And chemical risk assessment methods are not nearly as robust as we’d like them to be. 
And yet, there’s still a lot to be learned from chemicals about how to evaluate the potential risks of algorithms as we try to develop and use them safely. 
As a starting point, we need to establish robust ways of thinking critically about algorithmic risk. And we need to develop rigorous checks and balances that support evidence-based decisions on algorithm safety. 
We also need to develop better ways of ensuring algorithms don’t lead to risks that are all too easily overlooked — such as preferentially affecting the lives and livelihoods of people of color, or discriminating on the basis of gender or religion. 
Multiple efforts are already underway here. The AI Now Institute for instance is doing sterling work on the safe and responsible development and use of algorithms and artificial intelligence more generally. And organizations like Deloitte are digging into novel approaches to managing algorithmic risk. 
There’s also a vibrant and growing community of researchers that are grappling with the potential downsides of algorithms that look good on the surface, but have the potential to lead to discrimination and harm. 
But these are only scratching the surface of what needs to be done if algorithms — and machine learning and AI more broadly — are to be developed and used as safely and as beneficially as possible. 
This is where there’s more that can be learned from how we assess and manage chemical risks than we might imagine. 
If we’re serious about ensuring the safe and beneficial development and use of algorithms, we need to learn how treat them with the same rigor and respect that we do the chemicals that our lives depend on. 
Unless, that is, we’re content to repeat the mistakes of the past as we reinvent the future. 
Written by 
Written by",Andrew Maynard,2019-03-05T19:32:28.709Z
Image Similarity using Deep Ranking | by Akarsh Zingade | Medium,"The ability to find a similar set of images for a given image has multiple use-cases from visual search to duplicate product detection to domain specific image clustering. Especially in Google’s Reverse Image Search where the search engine retrieves similar images to your query image. 
Image similarity is the measure of how similar two images are. In other words, it quantifies the degree of similarity between intensity patterns in two images. 
Initially, image similarity models considered category-level image similarity. For example, two images are considered similar as long as they belong to the same category. This category-level image similarity is not sufficient for the search-by-example image search application. Search by query image requires the distinction of differences between images within the same category- to recognise the fine grained differences in the similar images. 
One way of buiding image similarity model is to extract features such as Gabor filters, SIFT [1], Local Binary Patterns, HOG [2] and then use these feature to compute the similarity between the features. The other way is to use deep learning model to aid in identifying similar images for a query image. Today, we will talk about a model called Deep Ranking [3], which learns fine-grained image similarity by characterising the fine-grained image similarity relationship with a set of triplets. 
A triplet contains a query image, a positive image, and a negative image, where the positive image is more similar to the query image than the negative image. The below image illustrates this: 
In the above image, each column is a triplet. The upper, middle and lower rows correspond to query image, positive image, and negative image- where the positive image is more similar to the query image that the negative image. 
The image similarity relationship is characterized by relative similarity ordering in the triplets. Deep ranking models can employ this fine-grained image similarity information, which is not considered in category-level image similarity models or classification models, to achieve better performance. 
Let’s say you are looking at three cars parked in a road- a black car, a white car and a dark-gray car. For image classification models, these three cars are all just cars. It doesn’t concern itself with the colour and other aspect of the car. For a similar image ranking model, it would look at the colours and other aspects of the cars as well. If a query image is a “black car”, the similar image ranking model would rank the “dark gray car” higher than the “white car”. Due to this, image classification models may not fit directly to task of distinguishing fine-grained image similarity. The authors of the Deep Ranking paper propose to learn fine-grained image similarity with a deep ranking model, which characterizes the fine-grained image similarity relationship with a set of triplets. 
The metric used to measure the similarity between images is probably the most important thing in building image similarity models. While there are different metrics one can use to define the similarity, the most popular ones are L1-norm (also known as Manhattan distance) and L2 norm( also known as Euclidean distance). This paper gives a really good explanation and a comparison between Manhattan and Euclidean distance in the context of image similarity. Their results suggest that, in the domain of natural images of the kind we have used, the Manhattan distance may better capture the human notions of image similarity. In the case of Deep Ranking, the authors have used Squared Euclidean distance as the similarity metric. The smaller the distance the more similar the images are. 
Note: Unfortunately, Medium editor doesn’t support super-scripting and sub-scripting for letters. ‘Pi+’ means ‘P’ subscript ‘i’ with superscript ‘+’ and ‘Pi-’ means ‘P’ subscript ‘i’ with superscript ‘-’. 
Neural Networks are universal function approximators. The whole Deep Ranking architecture can be thought of as a function that would map the image to a point in the Euclidean space. The goal is to learn an embedding function that assigns smaller distance to more similar images. This can be expressed as : 
Here, ‘f’ is the embedding function to would map the image to a vector. Pi is the query image, Pi+ is the positive image, Pi- is the negative image and ‘r’ is the similarity distance between two images. 
The hinge loss for the triplet is defined as: 
Where ‘l’ is the hinge loss for the triplet, ‘g’ is a gap parameter that regularizes the gap between the distance of the two image pairs: ( Pi , Pi+ ) and ( Pi , Pi- ), and ‘D’ is the euclidean distance between the two euclidean points. 
Here, you are optimising the model in such a way that the distance between query image and positive image is not only lesser than the distance between query image and negative, but is lesser by an amount of ‘g’. 
The most crucial component is to learn an image embedding function ‘f’. Traditional methods typically employ hand-crafted visual features, and learn linear or nonlinear transformations to obtain the image embedding function. Here, a deep learning technique is employed to learn image similarity models directly from images. The Deep Ranking network looks like this: 
The network consists of 3 parts- The triplet sampling, the ConvNet and the Image Similarity ranking. 
This network takes image triplets as input. One image triplet contains a query image ‘pi’ , a positive image ‘pi+’ and a negative image ‘pi-’ , which are fed independently into three identical deep neural networks ‘f(.)’ with shared architecture and parameters. A triplet characterizes the relative similarity relationship for the three images. The deep neural network ‘f(.)’ computes the embedding of an image ‘pi’ : f(pi) ∈ Rd , where ‘d’ is the dimension of the feature embedding, and ‘R’ represents the Real number space. 
The ranking layer on the top evaluates the hinge loss of a triplet. During learning, it evaluates the model’s violation of the ranking order, and back-propagates the gradients to the lower layers so that the lower layers can adjust their parameters to minimize the ranking loss. 
The ConvNet in the image above is a novel multi-scale deep neural network architecture that employs different levels of in-variance at different scales. 
The ConvNet encodes strong invariance and captures the image semantics. The other two parts of the network take down-sampled images and use shallow network architecture. Those two parts have less invariance and capture the visual appearance. Finally, we normalize the embeddings from the three parts, and combine them with a linear embedding layer. 
The ConvNet can be any Deep Neural Network. The ConvNet contains stacked convolutional layers, max-pooling layer, local normalization layers and fully-connected layers. 
A convolutional layer takes an image or the feature maps of another layer as input, convolves it with a set of ‘k’ learnable kernels, and puts through the activation function to generate ‘k’ feature maps. The convolutional layer can be considered as a set of local feature detectors. 
A max pooling layer performs max pooling over a local neighborhood around a pixel. The max pooling layer makes the feature maps robust to small translations. 
A local normalization layer normalizes the feature map around a local neighborhood to have unit norm and zero mean. It leads to feature maps that are robust to the differences in illumination and contrast. 
The stacked convolutional layers, max-pooling layer and local normalization layers act as translational and contrast robust local feature detectors. A fully connected layer computes a non-linear transformation from the feature maps of these local feature detectors. 
The authors used AlexNet[4] for the ConvNet. The dimension of the embedding is 4096 in this case. To avoid overfitting, dropout[5] with probability of 0.6 is applied to all the fully connected layers 
Although ConvNet achieves very good performance for image classification, the strong in-variance encoded in its architecture can be harmful for fine-grained image similarity tasks. The experiments conducted by the authors showed that the multi-scale network architecture outperforms single scale ConvNet in fine-grained image similarity task. 
An effective strategy to select the most important triplets for rank learning is crucial. Uniformly sampling the triplets is sub-optimal, because we are more interested in the top-ranked results returned by the ranking model. The authors employ an online importance sampling scheme to sample triplets. 
Note: ‘ri,j’ is ‘r’ subscript ‘i,j’, ‘ci’ is ‘c’ subscript ‘i’, ‘Tp’ is ‘T’ subscript ‘p’ and ‘zi’ is ‘z’ subscript ‘i’. 
Suppose we have a set of images ‘P’, and their pairwise relevance scores ri,j = r(pi , pj ). Each image ‘pi’ belongs to a category, denoted by ‘ci’ .The total relevance score of an image ‘ri’ is defined as: 
The total relevance score of an image ‘pi’ reflects how relevant the image is in terms of its relevance to the other images in the same category. 
Then, a positive image ‘pi+’ from the images sharing the same categories as ‘pi’ is sampled. Since the top-ranked images are more relevant and important, more positive images ‘pi+’ with high relevance scores ‘ri,i+’ are sampled. The probability of choosing an image ‘pi+’ as positive image is: 
where ‘Tp’ is a threshold parameter, and the normalization constant ‘Zi’ equals the sum of the probability of the positive image, sharing the the same categories with ‘pi’, being selected. 
There two types of negative image samples. The first type is out-of-class negative samples, which are the negative samples that are in a different category from query image ‘pi’. They are drawn uniformly from all the images with different categories with ‘pi’. The second type is in-class negative samples, which are the negative samples that are in the same category as ‘pi’ but is less relevant to ‘pi’ than ‘pi+’. In order to ensure robust ordering between ‘pi+’ and ‘pi-’ in a triplet ti = (pi , pi+ , pi- ), it is also required that the margin between the relevance score ‘ri,i+’ and ‘ri,i-’ should be larger than ‘Tr’ , that is: 
Note: ‘kj’ is ‘k’ subscript ‘j’, ‘uj’ is ‘u’ subscript ‘j’, ‘pj/’ is ‘p’ subscript ‘j’ with superscript ‘/’. ‘/’ is just a dash. 
Learning deep ranking models requires large amount of data, which cannot be loaded into main memory. The sampling algorithms that require random access to all the examples in the dataset cannot be used. The authors propose an online triplet sampling algorithm: 
Whether in-class or out-of-class negative samples are sampled is controlled by a out-of-class sample ratio parameter. An illustration of this sampling method is shown below: 
The authors used two sets of training data to train the model. The first training data was ImageNet[6] ILSVRC-2012 dataset, which contains roughly 1000 images in each of 1000 categories. In total, there are about 1.2 million training images, and 50,000 validation images. This dataset is utilized to learn image semantic information. They used it to pre-train the “ConvNet” part of the model using soft-max cost function as the top layer. 
The second training data is the relevance training data, responsible for learning fine-grained visual similarity. It was collected from 100,000 search queries (using Google image search), with the top 140 image results from each query. There were about 14 million images. The relevance score for the images was computed using a golden feature. The golden feature is a weighted linear combination of twenty seven features. For the details of the golden features, please refer Section 6.1 of the original paper. 
The deep ranking model is compared with hand-crafted features. For each hand-crafted feature, its performance using its best experimental setting is shown below. The evaluated hand crafted visual features include Wavelet[7], Color (LAB histogram), SIFT-like features, SIFT-like Fisher vectors[8], HOG, and SPMK Taxton features with max pooling [9]. Along with these, two image similarity models- L1HashKCPA[10] and OASIS[11]- are learned on top of the concatenation of all these visual features. The performance comparison is shown below. 
We can see that any individual feature without learning does not perform very well. The L1HashKCPA feature achieves reasonably good performance with relatively low dimension, but its performance is inferior to Deep Ranking model. The OASIS algorithm can learn better features because it exploits the image similarity ranking information in the relevance training data. By directly learning a ranking model on images, the deep ranking method can use more information from image than two-step- feature extraction and model learning approach. Thus, it performs better both in terms of similarity precision and score-at-top-30. 
The Deep Ranking model performs better in terms of similarity precision than the golden features, which are used to generate relevance training data. This is because the Deep Ranking model employs the category-level information in ImageNet data and relevance training data to better characterize the image semantics. The score-at-top-30 metric of Deep Ranking is only slightly lower than the golden features. 
The Deep Ranking model is compared with the following architectures: 
The performances are shown below: 
In all the experiments, the Euclidean distance of the embedding vectors of the penultimate layer before the final softmax or ranking layer is exploited as similarity measure. 
The ranking model greatly increases the performance. The performance of single-scale ranking model is much better than ConvNet. The two networks have the same architecture except the single-scale ranking model is fine-tuned with the relevance training data using ranking layer, while ConvNet is trained solely for classification task using logistic regression layer. 
The single-scale ranking performs very well in terms of similarity precision, but its score-at-top-30 is not very high. The Deep Ranking model, which employs multi-scale network architecture, has better similarity precision and score-at-top-30. Finally, although training an OASIS model or linear embedding on the top increases performance, their performance is inferior to Deep Ranking model, which uses back-propagation to fine-tune the whole network. 
One very important experiments the authors ran were to see how the fraction of the out-of-class negative samples in online triplet sampling algorithm affected the performance. The score-at-top-30 metric of Deep Ranking model decreases as more out-of-class negative samples. However, having a small fraction of out-of-class samples (like 20%) increases the similarity precision metric a lot. You can see how the performance decreases as the fraction of out-of-class samples are increased in the below graph: 
They also compared the performance of the weighted sampling and uniform sampling with 0% out-of-class negative samples. In weighted sampling, the sampling probability of the images is proportional to its total relevance score ‘rj’ and pairwise relevance score ‘ri,j’, while uniform sampling draws the images uniformly from all the images (but the ranking order and margin constraints should be satisfied). Although the two sampling methods perform similarly in overall precision, the weighted sampling algorithm does better in score-at-top-30. 
A comparison of the ranking examples of ConvNet, OASIS feature (L1HashKPCA features with OASIS learning) and Deep Ranking is shown below: 
The ConvNet captures the semantic meaning of the images very well, but it fails to take into account some global visual appearance, such as color and contrast. On the other hand, Oasis features can characterize the visual appearance well, but fall short on the semantics. The proposed deep ranking method incorporates both the visual appearance and image semantics. 
The dataset is available here. Unfortunately, only 5,033 triplets are available as opposed to the 14,000 triplets used in the paper. The reason they aren’t available is because the public URLs for those images are no longer valid. 
I have implemented this model using Keras library. The tricky part with the implementation is the 3 parallel networks for query, positive and negative image. This is the one I am talking about. There are 3 major parts in the implementation: 
Unfortunately, the relevance score for the images are not available. So, we are going use a slightly modified version of the sampling method described in the paper. We will use the following method: 
The implementation is modular enough to change this algorithm. What we will do is, instead of sampling the triplets on the go, we will output the triplet into a text file where each line will contain the triplet in the order of query image, positive image and the negative image. Since the main program looks at this text file for the triplets, you can easily use other sampling techniques to generate this text file. 
Training 3 parallel multi-scale network would be very memory expensive. If we can somehow train only 1 multi-scale network that would be equivalent to training 3 parallel multi-scale network would reduce the memory cost by a factor of 3. What we will do is, instead of having 3 parallel multi-scale network which takes query image, positive image and negative image at the same time, we will pass the query image, positive image and negative image sequentially to 1 multi-scale network. We know that the loss function sees batch size number of images to compute loss, so we use the loss function to identify with embedding belong to query image, positive image and negative image and calculate the hinge loss. The tensor passed onto the loss layer will contain the image embedding in each row. Each row corresponds to each input image in the batch. Since, we passed the query image, positive image and negative image sequentially, the first row will correspond to query image, the second to positive image and the third to negative images, and then same repeats until the end of the batch. This way, in the loss function, we get the embedding of all images and we just have to compute the hinge loss. 
To implement the multi-scale network where the outputs of the ConvNet and the 2 small networks we will have to use the Merge layer in Keras. We will use the concatenate layer to concatenate the outputs of the ConvNet and the 2 small network. The code snippet for merging: 
To implement the loss function, we will have to write a custom loss function that would compute the euclidean distance between the query image and the positive image, and the euclidean distance between the query image and the negative image. And then would calculate the hinge loss. The snippet of the loss function: 
NOTE: The batch size should always be a multiple of 3. Since the triplet contains 3 images and the triplet images are passed sequentially, the batch size has to be multiple of 3. 
To pass the path of the triplet text file to the Image Generator, the Image Generator class has been modified to take the path of the triplet text file as an input. So, to the Image Generator’s flow_from_directory, you will pass the path of the triplet text file to ‘triplet_file’ variable. 
Here’s the snippet: 
Note: The shuffle parameter should be set to False. 
That’s it! We are done with the implementation! You can find the whole code here. 
Credits 
All the credits go to the authors of the Deep Ranking[12] (Learning Fine-grained Image Similarity with Deep Ranking) paper. I have merely explained their work and implemented it. This post wouldn’t have been possible without them :) 
[1] Object Recognition from Local Scale-Invariant Features- http://www.cs.ubc.ca/~lowe/papers/iccv99.pdf 
[2] Histograms of Oriented Gradients for Human Detection- https://courses.engr.illinois.edu/ece420/fa2017/hog_for_human_detection.pdf 
[3] Learning Fine-grained Image Similarity with Deep Ranking- https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/42945.pdf 
[4] ImageNet Classification with Deep Convolutional Neural Networks- https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf 
[5] Dropout: A Simple Way to Prevent Neural Networks from Overfitting- https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf 
[6] ImageNet: A Large-Scale Hierarchical Image Database- http://www.image-net.org/papers/imagenet_cvpr09.pdf 
[7] Fast Multiresolution Image Querying- https://grail.cs.washington.edu/projects/query/mrquery.pdf 
[8] Large-Scale Image Retrieval with Compressed Fisher Vectors- http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.401.9140&rep=rep1&type=pdf 
[9] Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories- http://ieeexplore.ieee.org/document/1641019/ 
[10] Improved Consistent Sampling, Weighted Minhash and L1 Sketching- https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/36928.pdf 
[11] Large scale online learning of image similarity through ranking- http://jmlr.csail.mit.edu/papers/volume11/chechik10a/chechik10a.pdf 
[12] Learning Fine-grained Image Similarity with Deep Ranking- https://users.eecs.northwestern.edu/~jwa368/pdfs/deep_ranking.pdf 
Written by 
Written by",Akarsh Zingade,2018-05-28T09:15:44.886Z
Understanding Principal Components Analysis(PCA) | by Chathurangi Shyalika | Data Driven Investor | Medium,"Principal Components Analysis is an unsupervised learning class of statistical techniques used to explain data in high dimension using smaller number of variables called the principal components. 
PCA finds directions of maximal variance of data. 
It finds directions that are mutually orthogonal. Mutually orthogonal means it’s a global algorithm. Global means, that all the directions, all the new features that they find have a big global constraint, namely that they must be mutually orthogonal. 
PCA transforms a high-dimensional dataset into a smaller-dimensional subspace prior to running a machine learning algorithm on the data. 
Principle Components are the linear combinations of the original variables in the dataset. 
The first principal component is a linear combination of the original variables. 
In the new coordinate system, the first axis corresponds to the first principal component. It is the component that explains the greatest amount of the variance in the data. 
The second principal component is selected such that it lies perpendicular to the first principal component. 
Here is how the dataset looks like projected onto the first two principal components. 
PCA is widely used for dimensionality reduction. 
PCA transforms the dataset into a new, lower-dimensional subspace, means into a new coordinate system. 
Dimensionality reduction results in multiple benefits like data visualization, computational time while executing any algorithm for analysis. 
Following steps involved. 
An eigenvector is a vector whose direction remains unchanged when a linear transformation is applied to it. 
The principle components are the eigenvectors of the covariance matrix of the original dataset. They correspond to the direction (in the original n-dimensional space) with the greatest variance in the data. 
Each eigenvector has a corresponding eigenvalue. It is a scalar. . The corresponding eigenvalue is a number that indicates how much variance there is in the data along that eigenvector (or principal component). 
For more information on Eigenvectors and Eigenvalues refer the following link. 
5. Next, sort the eigenvectors by decreasing eigenvalues and choose k eigenvectors with the largest eigenvalues to form a d×k dimensional matrix 
6. Use this d×k eigenvector matrix to transform the samples onto the new subspace. 
For example, suppose we have 3-dimensional dataset X and we want to reduce it to 2-dimension. So, for this case d = 3 and k =2. First, we start by calculating the mean vector for each of the 3 dimensions followed by co-variance matrix. In the next step eigenvector and their corresponding eigenvalues are computed using the covariance matrix. Now we sort the eigenvectors by decreasing eigenvalues and choose 2 eigenvectors with the highest eigenvalues to form a matrix M of dimension 3×2. Now transform the sample space into new subspace multiplying the transpose of matrix M with X. 
Great visualization of this process is available in the following reference. 
It tends to be highly affected by outliers in the data. 
To overcome this issue many robust versions of PCA has been developed, including RandomizedPCA, sparsePCA etc. 
[1] https://analyse-it.com/docs/user-guide/multivariate/principal-components 
[2] https://shantijha.wordpress.com/2016/06/18/principal-component-analysis/ 
[3]http://www.visiondummy.com/2014/03/eigenvalues-eigenvectors/ 
[4] http://www.sosmath.com/matrix/eigen1/eigen1.html 
[5] http://www.sosmath.com/matrix/eigen2/eigen2.html 
[6] http://www.lauradhamilton.com/introduction-to-principal-component-analysis-pca 
Hope you got a basic understanding of Principal Components Analysis through this blog post. If you have any issues or comments on this blog post please leave a comment below. 
Cheers! 
In each issue we share the best stories from the Data-Driven Investor's expert community. Take a look 
Written by 
Written by",Chathurangi Shyalika,2019-05-24T22:59:12.293Z
NLP — BERT & Transformer. Google published an article… | by Jonathan Hui | Medium,"Google published an article “Understanding searches better than ever before” and positioned BERT as one of its most important updates to the searching algorithms in recent years. BERT is a language representation model with impressive accuracy for many NLP tasks. If you understand better what people ask, you give better answers. Google says 15% of the Google queries are never seen before. The real issue is not on what people ask. Instead, it is how many ways a question may be asked. Previously, Google search was keyword-based. But this is far from understanding what people ask or clarifying the ambiguity in human language. That is why Google has utilized BERT in its search engine. In the example below, BERT understands the intentions of “Can you get medicine for someone pharmacy” better and returns more relevant results. 
In the last article, we cover the word embedding and let’s move on to BERT now. Strictly speaking, BERT is a training strategy, not a new architecture design. To build a whole system with this concept, we need to study another proposal from Google Brain first. That is the Transformer. The concept is complex and will take some time to explain. BERT just need the encoder part of the Transformer. But for completeness, we will cover the decoder also but feel free to skip it according to your interests. 
Let’s see one application of the Transformer. OpenAI GPT-2 is a transformer-based language model with 1.5 billion parameters. As I type the paragraph below, the grayed part is automatically generated with the GPT-2 model. 
And the following is the original paragraph. 
To map an input sequence to an output sequence, we can apply sequence-to-sequence learning using an encoder-decoder model. For example, seq2seq translates a sentence in one language to another language. It can also apply to text summarization, image captioning, conversational modeling and many other NLP tasks. 
We assume you have some basic background on this already. So we will move on. But if you need further information, Google the phrase “sequence to sequence” or “Seq2Seq” later. 
For many years, we use RNN, LSTM or GRU in these models to parse the input sequence to accumulate information and to generate the output sequence. But this approach suffers a few setbacks. 
To avoid making the wrong choice, 
we can design a model including both forward and backward RNN (i.e. a bidirectional RNN) and then add both results together. 
We can also stack pyramidal bidirectional RNN layers to explore context better. 
But at one point, we may argue that in order to understand the context of the word “view” below, we should check over all the words in a paragraph concurrently. i.e. to know what the “view” may refer to, we should apply fully-connected (FC) layers directly to all the words in the paragraph. 
However, this problem involves high-dimensional vectors and makes it like finding a needle in a haystack. But how do humans solve this problem? The answer may land on “attention”. 
The picture below contains about 1M pixels. But most of our attention will focus on the blue-dress girl below. 
When creating a context for our predictions, we should not put equal weight on all the information we get. We need to focus! We should create a context of what interests us. But this focus can shift in time. For example, if we are looking for the ferry, our attention may focus on the ticket booth and the waiting lines behind the second lamp post instead. So how can we conceptualize this into equations and deep networks? 
In RNN, we make predictions based on the input xt and the historical output h(t-1) for timestep t. 
For an attention-based system, we look into the whole input x on each step but x will be modified with attention. 
We can visualize that the attention process masks out information that is currently not important. 
For example, for each input feature xᵢ, we train an FC layer with tanh output to score how important feature i is (or the pixel) under the previous output h. For instance, our last output h maybe the word “ferry” and the score will be computed as: 
Then, we normalize the score using a softmax function. 
The attention Z will be the weighted output of the input features. In our example, the attention may fall around the ticket sign. Note, there are many realizations of the attention concept. The equation here is just one of them. The key point is we introduce an extra step to mask out information that we care less at the current step. In the next few sections, we will develop the concept further before we introduce the details and the math. 
Query, Keys, Values (Q, K, V) 
We can expand the concept of attention with queries, keys, and values. Let’s go through our example again. This time the query is “running”. In this example, a key classifies the object in a bounded rectangle and value is the raw pixels in the bounded box. In the example below, one of the boundary boxes contains the key “girl”. From some perspective, a key is simply an encoded value for “value”. But in some cases, a value itself can be used as a key. 
To create attention, we determine the relevance between the query and the keys. Then we mask out the associated values that are not relevant to the query. For example, for the query “ferry”, the attention covers the waiting lines and the ticket sign below. 
Now, let’s see how we apply attention to NLP and start our Transformer discussion. But Transformer is pretty complex. It is a new encoder-decoder concept with attention. We will take some time to discuss it. 
Many DL problems involve a major step of representing the input with a dense representation. This process forces the model to learn what is important in solving a problem. The extracted features are called the latent features, hidden variables or a vector representation. Word embedding creates a vector representation of a word that we can manipulate with linear algebra. One major problem is words can have different meanings in different contexts. In the example below, word embedding uses the same vector in representing “bank”. But it has different meanings in the sentence. 
To create a dense representation of this sentence, we can apply RNN to parse a sequence of words in the form of embedding vectors. We gradually accumulate information in each timestep and produce a vector representation at the end of the pipeline. But one may argue that when the sentence is getting longer, early information may be forgotten or override. This may get worse if our input is a long paragraph. 
Maybe, we should convert a sentence to a sequence of vectors instead, one vector per word. In addition, the context of a word will be considered during the encoding process through attention. For example, the word “bank” will be treated and encoded differently according to the context. 
Let’s integrate this concept with attention using query, key, and value. We decompose sentences into single words. Each word acts as a value and we use the word itself as the key to its value. 
Each word form a single query. So the sentence above has 21 queries. How do we generate attention for a query, say Q₁₆ for the word “bank”? We compute the relevancy of the query word “bank” with each key in the sentence. The attention is simply a weighted output of the values according to the relevancy. Conceptually, we “grey out” non-relevant values to form the attention. 
By going through Q₁ to Q₂₁, we collect all 21 attentions. This 21-vectors represent the sentence above. 
Let’s get into more details. But in our demonstration, we use the sentence below instead which contains 13 words. 
New England Patriots win 14th straight regular-season game at home in Gillette stadium. 
In the encoding step, Transformer uses learned word embedding to convert these 13 words, in one-hot-vector form, into 13 512-D word embedding vectors. Then they are passed into an attention-based encoder to pick the context information for each word. 
For each word-embedding vector, there will be one output vector. These 13 word-embedding vectors will fit into position-wise fully connected layers (details later) to generate a sequence of 13 encoded vectors in representing the sentence. Each of these output vector hᵢ will be encoded in a 512-D vector. Conceptually, the output hᵢ encodes the word xᵢ with its context taking into consideration. 
Let’s zoom into this attention-based encoder more. The encoder actually stacks up 6 encoders on the left below. The output of an encoder is fed to the encoder above. Each encoder takes 13 512-D vectors and output 13 512-D vectors. For the first decoder (encoder₁), the input is the 13 512-D word embedding vectors. 
Scaled Dot-Product Attention 
The first part of each encoder performs the attention. Each word in the sentence serves as a single query. In our example, we have 13 words and therefore 13 queries. But we don’t compute attention one-at-a-time for each query. 
Instead, all 13 attentions can be computed concurrently with Q, K, and V pack all the queries, keys and values into matrices. The result will be packed in a 13 × 512 matrix also. The matrix product QKᵀ measures the similarity between the query and the key. When the dimension of the vector increase, the dot products QKᵀ grow large and push the softmax function into regions with diminishing gradient problem. To correct that, Transformer divides the dot product with a scale factor related to the root of the dimension. 
Multi-Head Attention 
In the last section, we generate one attention per query. But we can pay attention to multiple areas. 
Multi-Head Attention generates h attention per query. Conceptually, we pack h scaled dot-product attention together. 
The diagram below shows two attention, one in green and the other in yellow. 
For encoders and decoders in the Transformer, we use 8 attentions each. So why do we need 8 but not 1 attention? In each attention, we will transform Q, K, V linearly with a different trainable matrix respectively. 
Each transformation gives us a different projection for Q, K, and V. So 8 attentions allow us to view relevancy from 8 different “perspectives”. This eventually pushes the overall accuracy higher, at least empirically. 
The transformation also reduces their output dimension so even 8 attentions are used, the computational complexity remains about the same. 
In multi-head attention, we concatenate the output vectors followed by a linear transformation. 
Here is the encoder with multi-head attention. 
Skip connection & Layer normalization 
Transformer applies skip connection (residual blocks in ResNet) to the output of the multi-head attention followed by a layer normalization. Both techniques make training easier and more stable. In batch normalization, we normalize an output dimension based on the corresponding statistics collected from the training batches. In layer normalization, we use values in the same layer output to perform the normalization. This is more suitable for time sequence data. We will not elaborate them further and it is not critical to understanding the concept also. It is a common technique to make training more stable or easier. 
Position-wise Feed-Forward Networks 
Then we apply fully-connected layers (FC) with ReLU activation. But this operation is applied to each position separately and identically. It is a position-wise feed-forward because the ith output depends on the ith attention of the attention layer only. 
Similar to the attention, Transformer also uses skip connection and layer normalization. 
Positional Encoding 
Politicians are above the law. 
This sounds awfully wrong. But it demonstrates the ordering and the position of words matters. CNN or RNN discovers hierarchical local information nicely. But for the attention layer in Transformer, we don’t impose any explicit rules on the ordering of features. This makes training harder. 
Positional Encoding encodes the absolute or relative positional information into the word embedding. It adds a position embedding with the same dimension to the word embedding. The final embedding of a word is the sum of the word embedding and the position embedding that models the word position (a.k.a. f(position)). This provides additional information to the model to learn better. 
This position embedding can be fixed or learned. For a fixed position embedding, we can use the equation below. The periodic function allows us to embed relative word position information into the word embedding. 
The position embedding can also be learned like other DL parameters. But we use two sets of position parameters aᵢⱼ, one for the values and one for the keys. aᵢⱼ models the position embedding between position i and j. We add the values in a to the attention function on the right below. This allows the attention output to depend on the query relevancy as well as the words’ position also. Therefore, we can learn aᵢⱼ just like W (details). 
Alternatively, we can just model the relative distance between two words. So instead of modeling a(i, j), we learn w(j-i) where i and j are the corresponding word positions. But for words beyond a distance of k, we clip w and use the value of w(k) or w(-k) instead. Therefore, we only need to learn 2×k + 1 set of parameters. We don’t need to get into more details for now. If you want more information, please refer to the original research paper. 
This is the encoder. Next, we will discuss the decoder. Nevertheless, this section is optional because BERT uses the encoder only. It will be nice to know the decoder. But it is relatively long and harder to understand. So skip the next six sections if you want. 
The vector representation h of a sentence, created by the encoder, will fit into the decoder for training or inference. The following is a simplified view of the decoder during training. 
As recalled, attention can be composed of a query, keys, and values. For the decoder, the vector representation h will be used as keys and values for the attention-based decoder. The ground truth label and the predictions will be used as the query for training and inference respectively. 
In training, we shift the true label translation (“Los Patriots de …”) by one timestep to the right and fit it to the attention as the query. But we will defer the discussion on the attention-based decoder. We will come back to it later. 
Embedding and Softmax in Training (Optional) 
The output of the attention decoder is fit to a linear layer followed by a softmax in determining the output words. In the encoding-decoding process, we use word embeddings to transform words into vectors. The linear layer in the decoder is the inverse of the word embeddings process. In practice, these word embeddings and the linear layer can share the same weight (or its inverse). In fact, accuracy may improve. 
Inference (Optional) 
In inference, we predict one output label at a time. For the next time step, we can use all the previous predictions as the input query to the attention decoder. 
Encoder-decoder attention (Optional) 
Let’s get into the details of the encoder-decoder attention. That is the core part of the decoder. Recall previously, we apply linear transformations to the input word embeddings to create Q, K, and V respectively. In the encoder, the values for key, value and the query are originated from the same input word sequence. 
For the Transformer decoder, the key and value are originated from the encoder output h. But the query is from the predicted output sequence so far. The decoder has two attention stages (in blue below). 
The first stage prepares the vector representation for the query needed for the second stage. We will use the attention mechanism to create a vector representation for the predicted outputs so far and act as the input for the query of the attention in the second stage. Then, we combine K and V, prepared from the vector representation h from the encoder, to create the attention we need for the current time step. 
Let’s consider what happened when translating “New England Patriots win …” to the ground truth “Los Patriots de Nueva Inglaterra …”). At step 3 of the decoding process, our prediction output so far is “Los Patriots”. We feed “Los Patriots” into the first attention stage to generate a vector representation. After the Layer Normalization, we apply a linear transformation on it. This is the Q we need for the second attention stage. 
So to generate the attention, we use vector representation h for the sentence to generate K and V, and the previous attention output for Q. Intuitively, we ask for the output prediction “Los Patriots” so far, what is the attention in h that we should use to make the next prediction “de”. 
Once the attention is computed, we pass it through the Position-wise Feed-Forward Network. The attention decoder stacks up these 6 decoders with last output passes through a linear layer followed by a softmax in predicting the next word “de”. 
Here is the diagram for the whole Transformer. 
Training (optional) 
During training, we do know the ground truth. The attention model is not a time sequence model. Therefore, we can compute output predictions concurrently. For the input to the decoder, we just shift the ground truth word sequence to the right for one time step. 
But, for the prediction at position i, we make sure the attention can only see the ground truth output from position 1 to i-1 only. Therefore, we add a mask in the attention to mask out information from position i and beyond when creating attention for position i. 
Soft Label (Optional) 
To avoid overfitting, the training also uses dropout and label smoothing. Label smoothing targets the probability prediction for the ground truth to be lower than 1.0 (say 0.9) and for non-ground truth to be higher than 0 (say 0.1). This avoids getting over-confidence with specific data. In short, being overconfidence about a data point may be a sign of overfitting and hurt us in generalizing the solution. We will not elaborate on all the details further here. 
So far we have focused our discussion on sequence-to-sequence learning, like language translation. While this type of problem covers a wide range of NLP tasks, there are other types of NLP Tasks. For example, in question and answer (QA), we want to spot the answer in a paragraph regarding a question being asked. 
There is another type of NLP task called Natural Language Inference (NLI). Each problem contains a pair of sentences: a premise and a hypothesis. An NLI model predicts whether a hypothesis is true (entailment), false (contradiction), or undetermined (neutral) given a premise. 
In these problems, the models take two separate sentences (or paragraphs) as input. Below is two other examples. The first one determines the sentiment of a sentence. The second one answer a question with a given context. 
Next, we are going to model a vector representation expandable to handle these tasks also. That will be the final discussion on BERT. 
With word embedding, we create a dense representation of words. But in the section of Transformer, we discover word embedding cannot explore the context of the neighboring words well. Also, how can we create a dense representation for other NLP inputs, including those in QA and NLI? In addition, we want a representation model that is multi-purposed. NLP training is intense! Can we pre-trained a model and repurpose it for other resources without building a new model again? 
Let’s have a quick summary of BERT. In BERT, a model is first pre-trained with data that requires no human labeling. Once it is done, the pre-trained model outputs a dense representation of the input. To solve other NLP tasks, like QA, we modify the model by simply adding a shallow DL layer connecting to the output of the original model. Then, we retrain the model with data and labels specific to the task. 
In short, there is a pre-training phase in which we create a dense representation of the input (the left diagram below). The second phase retunes the model with task-specific data, like MNLI or SQuAD, to solve the target NLP problem. 
Model 
BERT uses the Transformer encoder we discussed to create the vector representation. In contrast to other approaches, it discovers the context concurrent rather than directionally. 
Input/Output Representations 
But first, let’s define how input is assembled and what output is expected for the pre-trained model. First, the model needs to take one or two word-sequences to handle different spectrums of NLP tasks. 
All input will start with a special token [CLS] (a special classification token). If the input composes of two sequences, a [SEP] token will put between Sequence A and Sequence B. 
If the input has T tokens, including the added tokens, the output will have T outputs also. Different parts of the output will be used to make predictions for different NLP tasks. The first output is C (or sometimes written as the output [CLS] token). It is the only output used to derive a prediction for any NLP classification task. For non-classification tasks with only one sequence, we use the remaining outputs (without C). For QA, the outputs corresponding to the paragraph sequence will be used to derive the start and the end span of the answer. 
So, how do we compose the input embedding? In BERT, the input embedding composes of word piece embedding, segment embeddings, and position embedding of the same dimension. We add them together to form the final input embedding. 
Instead of using every single word as tokens, BERT breaks a word into word pieces to reduce the vocabulary size (30,000 token vocabularies). For example, the word “helping” may decompose into “help” and “ing”. Then it applies an embedding matrix (V × H) to convert the one-hot vector Rⱽ to Rᴴ. 
The segment embeddings model which sequence that tokens belong to. Does it belong to the first sentence or the second sentence. So it has a vocabulary size of two (segment A or B). Intuitively, it adds a constant offset to the embedding with value based on whether it belongs to sequence A or B. Mathematically, we apply an embedding matrix (2 × H) to convert R² to Rᴴ. The last one is the position embedding in H-Dimension. It serves the same purpose in the Transformer in identifying the absolute or relative position of words. 
Pretraining 
BERT pre-trains the model using 2 NLP tasks. The first one is the Masked LM (Masked Language Model). As shown below, we use the Transformer decoder to generate a vector representation of the input. Then BERT applies a shallow deep decoder to reconstruct the word sequence(s) back. 
Here is an example of the Masked LM and BERT is trained to predict the missing words correctly. 
Masked LM 
In the Masked LM, BERT masks out 15% of the WordPiece. 80% of the masked WordPiece will be replaced with a [MASK] token, 10% with a random token and 10% will keep the original word. The loss is defined as how well BERT predicts the missing word, not the reconstruction error of the whole sequence. 
We do not replace 100% of the WordPiece with the [MASK] token. This teaches the model to predict missing words, not the final objective of creating vector representations for the sequences with context taken into consideration. BERT replaces 10% with random tokens and 10% with the original words. This encourages the model to learn what may be correct or what be wrong for the missing words. 
Next Sentence Prediction (NSP) 
The second pre-trained task is NSP. The key purpose is to create a representation in the output C that will encode the relations between Sequence A and B. To prepare the training input, in 50% of the time, BERT uses two consecutive sentences as sequence A and B respectively. BERT expects the model to predict “IsNext”, i.e. sequence B should follow sequence A. For the remaining 50% of the time, BERT selects two-word sequences randomly and expect the prediction to be “Not Next”. 
In this training, we take the output C and then classify it with a shallow classifier. 
As noted, for both pre-training task, we create the training from a corpse without any human labeling. 
These two training tasks help BERT to train the vector representation of one or two word-sequences. Other than the context, it likely discovers other linguistics information including semantics and coreference. 
Fine-tuning BERT 
Once the model is pre-trained, we can add a shallow classifier for any NLP task or a decoder, similar to what we discussed in the pre-training step. 
Then, we fit the task-related data and the corresponding labels to refine all the model parameters end-to-end. That is how the model is trained and refined. So BERT is more on the training strategy rather than the model architecture. Its encoder is simply the Transformer encoder. 
Model 
But the model configuration in BERT is different from the Transformer paper. Here are a sample configuration used for the Transformer encoder in BERT. 
For example, the base model stacks up 12 decoders, instead of 6. Each output vector has a 768 dimension and the attention uses 12 heads. 
Source Code 
For those interested in the source code for BERT, here is the source code from Google. For Transformer, here is the source code. 
NLP training is resource intense. Some BERT models are trained with 64 GB TPU using multiple nodes. Here is an article on how to scale the training with Nvidia GPUs. 
Attention Is All You Need 
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding 
Written by 
Written by",Jonathan Hui,2020-03-24T15:59:40.690Z
"NLP: Word Embedding Techniques Demystified | by Rabeh Ayari, PhD | Towards Data Science","Word Embedding is a technique of word representation that allows words with similar meaning to be understood by machine learning algorithms. Technically speaking, it is a mapping of words into vectors of real numbers using the neural network, probabilistic model, or dimension reduction on word co-occurrence matrix. 
Word embedding can be learned using a variety of language models. For example, a ‘dog’ will be represented by the vector [0.75, 0.22, 0.66, 0.97]. If all the words in a dictionary are encoded in such a way, it becomes possible to compare the vectors of the words with each other, for example by measuring the cosine distance or the euclidean distance between the vectors. A good representation of words will then make it possible to find that the word ‘pet’ is closer to the word ‘dog’ than it is to the word ‘soccer’ or ‘engine’. Therefore, these representations allow us to hope that, in the vector space where the embedding is made, we will have the equation king-man+woman = queen or even the equation London-England+Italy = Rome. 
Word embeddings are also very useful in mitigating the curse of dimensionality, a very recurring problem in artificial intelligence. Without word embedding, the unique identifiers representing the words generate scattered data, isolated points in a vast sparse representation. With word embedding, on the other hand, the space becomes much more limited in terms of dimensionality with a widely richer amount of semantic information. With such numerical features, it is easier for a computer to perform different mathematical operations like matrix factorization, dot product, etc. which are mandatory to use shallow and deep learning techniques. There are many techniques available at our disposal to achieve this transformation. In this article, we will be covering: Bag-Of-Words, TF-IDF, Word2Vec, Doc2vec and Doc2vecC. 
Bag-Of-Words (a.k.a. BOW) is a popular basic approach to generate document representation. A text is represented as a bag containing plenty of words. The grammar and word order are neglected while the frequency is kept the same. A feature generated by bag-of-words is a vector where n is the number of words in the input documents vocabulary. 
For instance, there are two documents: 
The vocabulary of these two documents is: 
[“I”, “love”, “Italian”, “food”, “and”, “Tunisian”, “here”,] 
This vocabulary will produce feature vectors of length 8 (i.e. Vocabulary Cardinality). Given such vocabulary, the bag-of-word feature representation of these two documents are: 
Using this technique we create the bag-of-words representation of each document in our dataset. If our dataset contains a big number of unique words some of that are not used very frequently, which is usually the case. So, among those, we chose N most frequent words and create a feature vector of dimension Nx1. These feature vectors are then used for any machine learning task. 
The Term Frequency-Inverse Document Frequency (a.k.a. TF-IDF) is another way to represent a document based on its words. With TF-IDF, words are given weights by TF-IDF importance instead of only frequency. TF-IDF provides a statistical measure used to evaluate the importance of the words with respect to the document in a collection or corpus. There are many commonly used words for each dataset that appear many times in the document but do not provide any important information. The weight increases in proportion to the number of occurrences of the word in the document. It also varies according to the frequency of the word in the corpus. Variants of the original formula are often used in search engines to assess the relevance of a document based on the user’s search criteria. 
By definition, TF-IDF embedding is composed by two terms: the first computes the normalized Term Frequency (TF), a.k.a. the occurrence a word appears in a document, divided by the total number of words in that document; the second term is the Inverse Document Frequency (IDF) which computes the logarithm of the number of the documents in the corpus divided by the number of documents where the specific term appears. 
As shown in the figure below, scikit-learn library includes in the submodule sklearn.feature_extraction.text gathering utilities to build feature vectors from text documents in which we will find CountVectorizer. CountVectorizer easily convert a collection of raw documents to a matrix of TF-IDF numerical features. 
One of the most efficient techniques to represent a word is Word2Vec. Word2vec is a computationally efficient predictive model for learning word embeddings from raw text. It plots the words in a multi-dimensional vector space, where similar words tend to be close to each other. The surrounding words of a word provide the context to that word. 
Let’s start with a high-level insight about where we’re going. Word2Vec uses a well-known trick in deep learning. In this trick, we train a simple neural network with a single hidden layer to perform a fake task, without a real need of the results of the task we trained it on. Instead, the main objective consists of learning a representation of the input data which are actually the ‘word vectors’ gathered from the learned weights of the hidden layer. Such an approach may remind us of the way we train auto-encoders to perform dimensionality reduction. 
Word2Vec can rely on either one of two model architectures in order to produce a distributed representation of input words: Continuous Bag-of-Words (CBoW) or Continuous Skip-Gram as shown in the figure below. Vector representation extracts semantic relationships based on the co-occurrence of words in the dataset. 
The CBoW and skip-gram models are trained using a binary classification to discriminate between the real target word and the other words in the same context. The accuracy at which the model predicts the words depends on how many times the model sees these words within the same context throughout the dataset. The hidden representation is changed by more words and context co-occurrences during the training process, which allows the model to have more future successful predictions, leading to a better representation of word and context in the vector space. Skip gram is much slower than CBOW, but performs more accurately with infrequent words. 
Let’s go deeper into details to understand the difference between CBOW architecture and Continuous skip-gram. 
In both models, when we say ‘surrounding’, there is actually a ‘window size’ parameter to the algorithm. The size of the context window limits how many words before and after a given word would be included as context words of the given word. For example, window size of 3 will include the 3 words to the left and the 3 words to the right for each observed word in the sentence as context. Increasing the window size increases the training time as more word-context pairs need to be trained. Also it may capture context words that are not relevant to the current word. Decreasing the context words can capture relations between words and stop words which is not preferred. 
Doc2Vec is another widely used technique that creates an embedding of a document irrespective to its length. While Word2Vec computes a feature vector for every word in the corpus, Doc2Vec computes a feature vector for every document in the corpus. Doc2vec model is based on Word2Vec, with only adding another vector (paragraph ID) to the input. The Doc2Vec model, by analogy with Word2Vec, can rely on one of two model architectures which are: Distributed Memory version of Paragraph Vector (PV-DM) and Distributed Bag of Words version of Paragraph Vector (PV-DBOW). 
In the figure below, we show the model architecture of PV-DM: 
The above diagram is based on the CBOW model, but instead of using just nearby words to predict the word, we also added another feature vector, which is document-unique. So when training the word vectors W, the document vector D is trained as well, and at the end of training, it holds a numeric representation of the document. 
The inputs consist of word vectors and document Id vectors. The word vector is a one-hot vector with a dimension 1xV. The document Id vector has a dimension of 1xC, where C is the number of total documents. The dimension of the weight matrix W of the hidden layer is NxV. The dimension of the weight matrix D of the hidden layer is CxN. 
Doc2vecC tackles the problem of Doc2vec by including a global context through capturing the semantic meanings of the document. The architecture is very similar to Word2vec. In Doc2VecC, the average of the word embeddings in a document is used to represent the global context. But, unlike Word2vec, here at every iteration, words are randomly sampled from the document (document corruption). Vectors from those words are then averaged to obtain a document vector. This document vector is then used to predict the current word using the local context words as well as the global context. The global context is generated through an unbiased drop-out corruption. This corrupted document is represented as: 
Then, the probability of observing a word w_t given the local context c_t and global context x_i is given as: 
Once we have learned U using neural networks, each document can be simply represented as an average of the word embedding in that document. Hence, the document vector i is given as : 
The output of the model shares both global and local semantics of the dataset. Averaging helps us obtain vectors for unseen documents as well, hence tackling the problem presented by Doc2vec. Since only a fraction of the words are used for training, the training time is much lower in comparison to Doc2vec. In any document, words derive context from their neighbouring words. Hence to provide more context, we append our data set with publicly available data sets that are subsets of textual data for businesses, reviews, and user data for personal, educational, and academic purposes. 
Rabeh Ayari is a Principal Data Scientist working on applied AI problems for credit risk modelling & fraud analytics and conducting original research in machine learning. My areas of expertise include data analysis using deep neural networks, machine learning, data visualization, feature engineering, linear/non-linear regression, classification, discrete optimization, operations research, evolutionary algorithm and linear programming. Feel free to drop me a message here! 
Hands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look 
Written by 
Written by","Rabeh Ayari, PhD",2020-03-08T07:54:45.181Z
"Understanding Page Rank. In simple terms, PageRank is an… | by Sarthak Anand | Medium","PageRank is an intuitive way of ranking web pages, which formed the basis for Google’s web indexing algorithm during its early phase. In this article, you’ll learn about the intuition behind page rank and implementing page rank in python. The article is divided into the following sections: 
The intuition behind the Page-Rank is based on the idea that the popularity of a webpage is determined not only by the number of incoming links but also by the kind of incomings links. Citations from highly ranked pages contribute more than lower ranked web pages. The page rank of a web page A cited by web page B shown in Fig. 1 is given the equation: 
Page rank over the complete web graph is calculated until it converges to final values. 
To have a better understanding of how page rank works, consider a graph (shown by Fig. 2) of web pages having links shown by the arrow. Note that, if there are web pages with no out link then they do not contribute to the page ranking (they are usually referred to as dangling pages). 
Initially, page rank of all the web pages is taken as 1. The weight of the edge is the probability of going from a web page X to Y ( the web page A has 2 out links, therefore, the probability to visit each web page is 1/2 ). After expressing the web graph in terms of probabilities the web graph looks something like the Fig.3. The page rank of each web page is determined by applying the PageRank equation. This process is repeated until the algorithm converges i.e. the values of page rank do not change beyond a small value ( know as epsilon usually fixed as 1e-4 ). The damping factor (d) introduced is to add some randomness over the web graph i.e. d is a probability that a user will move to the linked web page and 1-d is the probability of choosing a random web page, it is usually taken as 0.85. 
For iteration 1: 
PR(C)=(1-d) + d*( P(A,C)*PR(A) + P(B,C)*PR(B) + P(D,C)*PR(D)) 
PR(C)=(0.15)+ 0.85*( 0.5*1 + 1*1 + 1*1) = 2.275 
The PR(C) can also be calculated by matrix dot product. 
Similarly, extending the equation for all the web pages we end up with the equation: 
Where matrix C represents the probability transition ( C[i][j] = probability of the user transitioning from page i to page j). The C matrix of our example can be expressed as the matrix represented above. Also, the initial page ranks are as assigned 1 for all the web pages. The PRs of web pages are calculated until the PRs converge to a certain value. 
Page Rank implementation in python: 
Final ranking of our example are C > A > B > D ! 
Notice: page rank of A is high even when it has only one incoming link. 
Written by 
Written by",Sarthak Anand,2019-03-10T20:57:30.946Z
What Is Games ‘User Experience’ (UX) and How Does It Help? | by Player Research | Medium,"Nearly every major game development studio is pivoting toward a more player-centric development process and culture. New internal teams and staff embodying the ‘voice of the player’ are commonplace, including Community Management, Analytics, UX Design and Games User Research. 
For decades, ‘UX’ (or ‘user experience’) staff have been helping gamedev teams improve their development processes and craft better gameplay experiences for their players by leveraging psychology and human sciences. UX has proven to be an instrumental voice in crafting seminal video games — The Last of Us, Portal, Destiny, Monument Valley, Clash Royale, to name but a few — yet much of this games UX knowledge is siloed inside larger studios and publishers, and inaccessible to the wider gamedev community. 
UX’s diversity of phrases, processes and perspectives can be confusing — What does it do? Who is it for? So, for developers unfamiliar with UX or wanting an easy overview: What is Games UX? What does it do for you and your team? How can I know if my game needs UX attention? What is the difference between UX and UI? 
About the AuthorSebastian Long is a Games UX Researcher at multi-award-winning UX research and analytics consultancy Player Research. Seb has led UX research projects for ~200 games, including best-loved franchises like FIFA, Little Big Planet, Harry Potter, LEGO, Sonic, Talking Tom and many more titles, from indie through to AAA. 
User Experience (UX) is a particular discipline of design, centred around the psychology of the end-user — or in gamedev’s case, the player — and their behaviours, thinking processes and capabilities. UX is part of a large toolkit used to ensure the experience that you’ve designed is truly reflected in the mind of your player. It applies a true knowledge of players’ behaviours and thinking processes, and couples it with data-collection, an iterative design process, and many types of testing with real players. 
UX is where the science of the player meets the art of game design. 
Everyone on your gamedev team is a designer, not just those with ‘design’ in their job title. Seemingly small choices made by every one of the team will impact how the game experience manifests in players. A programmer choosing a UI grid layout over a list, or a legal department demanding that players absolutely must scroll to the bottom of the EULA to proceed, or a voiceover artist choosing which words to emphasise in a spoken tutorial prompt. Each small choice could have minimal or monstrous implications on the players’ holistic experience. But we can be detached from the true impact of our choices; there are just so many to make. 
Resultantly, every gamedev discipline could benefit from being shown the actuality of their choices on how players truly think and behave. With this knowledge teams can collectively change their minds, iterate their designs, justify changes, and gain confidence that their final choice is the right one. 
This is the remit of UX: focusing on the true impact of design choices on players — and helping teams collaborate to make those choices. Hiring staff to “do UX” and become a player-centric voice in the studio is therefore an investment in directing the team’s attention, reality-checking design choices, informing the team’s judgement, and facilitating communication. 
UX provides constant guidance, going back-and-forth between aiding design and then testing it on real players. As a whole, this process reduces key risks inherent to making things for people to use, such as ‘complexity-creep’, games being difficult-to-understand, or having to re-do work over and over if players “don’t get it”. 
UX helps make better games. 
It is extremely difficult to remain objective about things we’ve crafted ourselves, or things we’re extremely familiar with. We can easily become oblivious to causes of disparities between the design of our game and real player’s’ actual experience of our game. This can damage our games; the fun that we know exists might not be resolved in our players. 
The problem of not knowing if players are experiencing the game as intended can result in unnecessary confusion during the already challenging process of development: 
Will players understand the game rules? Will they ‘get it’? Do we need more features? Are the ones we have enough? Is the friction in the game where we intend it to be, and balanced? Is the game experience correct-enough for us to release yet? Which parts of the game needs our focus during iteration? 
Leaving these questions unanswered or — worse — guessing incorrectly, leads to games being undesirably different from their intended experience — players don’t ‘find the fun’ — leading to lost development time, lost revenue and lost fans. 
In crafting a game you’re having to constantly guess how players might think, perceive, learn and react, in order to inform the design of the game and how it communicates itself to players: “I think a player would see that ‘enemy nearby’ feedback and head in the opposite direction”. There are many innocent reasons for these guesses to be wrong, and for the players’ thoughts and actions to undesirably differ to your intent. 
These disparities are fundamentally human in nature, not technical; they’re about predicting thoughts, behaviours and perceptions of other people. Being incorrect isn’t a symptom of naivety or inexperience, but is inherent to designing artefacts for other people to use. 
Below are some of the most impactful player-centric challenges teams face during development; perhaps you’ll recognise your own gamedev experience in some of them: 
There are barriers to seeing disparities as creators: 
…and barriers to recognising these disparities in others… 
…and barriers can exist in company culture or processes … 
These challenges are a hurdles for game developers all over the world, large and small. Each challenge can result in games having ‘cognitive friction’ in unintended places, such as UI, controls and communicating game rules. They can result in games being mechanically unsuited to the audience they were intended for. These factors are hugely influential on fun, on game reviews, and ultimately on commercial success. 
But despite the significance of these challenges, the responsibility and ‘player science’ know-how needed to address them falls outside the remit of all traditional game development roles. Teams often think about these problems, but often aren’t empowered to take necessary action to overcome them. 
Furthermore, the looming presence of these difficult-to-answer questions can have an impact on day-to-day team morale and company culture. They contribute to anxiety about one’s individual creative outputs. They cause conflict and power struggles as the team’s interpretations differ and clash; without knowing what is right, conflict can devolve into who is right. 
To mitigate these risks we need a someone in the team that: 
These are the responsibilities of ‘user experience’ professionals — or ‘UX’ for short. 
UX, as a wider discipline outside of gamedev, has been built upon decades of actionable knowledge on designing for busy, everyday people. User Experience professionals embody ‘user-centricity’ across all sorts of design domains, from jetfighter cockpit design, to admin software, to the design of medical instruments, to apps, phones and physical products. Designers of all kinds of everyday things have the same human-centric challenges listed above in common with gamedev, yet gamedev is among the slowest adopters of UX thinking and processes in response to them. But we are catching up; in the last 5 years UX groups have been built inside nearly every major game publisher and developer globally, hiring individuals who’re passionate about helping gamedev teams realise their creative vision using player science. 
Responsibilities are typically broken into 4 key job roles: 
Together, these four professional roles empower studios to follow an evidence-driven and structured development process, leveraging ‘player sciences’. 
UX is both a body of knowledge and a series of formal processes. Each team uses these differently, depending on the core competencies of the studio, the game genre, the development stage, the challenges that define the project, and so on. We’ll go through each development phase in turn to outline the UX tasks, and how they overcome the risks above. No matter what genre, scale or business model — even on live games — the same UX processes apply. 
UX feedback changes over the course of development. Each item above is a formalised UX approach, with specific objectives, tasks, sample sizes and deliverables. Each is designed to overcome common design missteps made during that phase. 
At a game project’s conception, ideas are unstructured, wild, and exciting. To help inspire and focus creative teams, UX conducts ideation and concept research: “Who is our player? What do we want to make?” and identify the best-in-class experiences: “What little details do competitors integrate to ensure great experiences?”. 
Game design teams can learn from their audiences’ play habits and traits, so UX teams conduct ‘exploratory research’ through interviews to discover players’ expectations of concept art and prototypes; examine competitor titles for experiential weaknesses and opportunities; interview genre-fans to understand their fascinations and frustrations, and so on. These data are typically provided to teams in the form of presentations, written reports, and as part of collaborative workshops. 
In this way, teams can be helped to discover their game in themselves and in their players. 
As a game design pillars crystallize around specific ideas and prototypes, teams need increasingly focused feedback and contributions to the specifics of the project: iteratively informing design choices and then assessing them using playtests and research methods. A focus on up-front iteration and informing prototypes avoids having to double-back later in development. 
UX Designers help teams make informed choices about UI — List or grid? — interaction design — Button or gesture? — feedback design, prompting, icon design, mental models, colour language, terminology… 
Without UX, each of these choices is made by developers imagining how players feel or interact with that thing: “I think a player would use this power-up and then they’d understand what it does”. But developers aren’t like players, and they’re often ‘too close’ to a project to make valid assumptions. Consequently, this imaginary player will too closely reflect the developer’s own knowledge, experience and perspective on the world, leading to flawed decision-making. 
UX tries to make choices right-the-first-time by raising questions and contextualising their impact as the choice is being made, not in hindsight. A User Experience Designer could assert that “Players might have difficulty seeing that feedback because it will have insufficient contrast against the light-coloured backgrounds” based on their knowledge of human visual perception. So too they might flag creeping visual complexity based on an understanding of human attention; and so on. A UX Designer’s output might be documentation like wireframes or mockups, but also full-fidelity art, reports, or simply in collaborating with existing UI Design and Game Design peers. 
Games are complex, and some flaws will slip through into code. The team will likely be too close to the project to see them, so instead we’ll need to leverage real players’ perspective. 
Week by week, Games User Researchers take a recent build, reflecting the most up-to-date design choices, and test it using specifically-designed experiments with real players. They’ll take research questions: “Can we prove this tutorial is effective at teaching? Do players reliably recognise the enemy weakspot?” and devise means of testing players for answers, followed by presenting their insights back to the team. Using different players for every single playtest means never being caught out by assumed knowledge in players, and constantly revisiting the learnability of your game mechanics. UX Research is traditionally delivered as a written report or presentation, or using internal issue-tracking tools like JIRA. 
With a Data Scientist involved, player behaviour can be visualised and better understood, even allowing metrics to be tracked over time for a ‘bigger picture’ of development progress. For live games or titles in early access, Community Managers with research training can also leverage access to their fanbases for insight into their experiences and frustrations. 
The frequency and focus of playtests depends greatly on the challenges of the particular project, but every-few-weeks is not uncommon. Playtests start early in development, with small studies using prototypes and on-paper designs to check usability, accessibility and learnability. Later they’ll move to larger and longer playtests covering the player’s emotions, subjective impressions, and difficulty balancing; each aims to bring the game closer to excellence. 
Once smaller pieces of the project come together, teams move onto the bigger questions: is our game fun? 
UX’s foundations in the sciences — Cognitive and Experimental Psychology — do provide some means to explore player’s motivation, emotion and satisfaction, however, such ethereal data is harder to obtain and analyse than the black-and-white, observable outcomes of good usability and learnability. There is greater need for experimental control, carefully written questions, more playtesters, standardised methods, and taking measures over time. 
UX teams exploring fun rely on iterative full-game playthroughs with tens or hundreds of real players, each providing carefully-gathered subjective feedback and analytics data, toward a more concrete understanding of their holistic experience. Conducted correctly, a game’s experience can be benchmarked, checked and compared against itself over time, forming the ‘bigger picture’ that teams need to make big decisions. 
Many studios without a UX voice wait until this point to begin getting player feedback, in doing so they have bypassed the majority of opportunities to save time and money avoiding the redesigns they’ll discover are necessary at this stage. Without having eked out the usability and learnability flaws in the preceding months and years, teams can struggle to handle a sudden barrage of fundamental flaws. 
These full-game playthroughs need careful analysis. Because players lack the introspection and language to explain their emotions and explain their actions, it is common for usability and understandability frustrations conflate their feedback on fun. Because players cannot appreciate the experiential intent of the game, their verbatim reactions can be misleading. All the UX disciplines — Design, Research, Data Science — lean heavily on academic backgrounds to ensure data is soundly collected, analysed and presented to mitigate bias. 
Techniques for observing and measuring emotion, advanced data visualisation, biometrics and eyetracking are areas of active research — some of many developments in player science that this article doesn’t have room to cover. 
Together, UX Designers, Games User Researchers and Data Scientists work with teams round-and-round the production loop. Each has a particular voice, harmonising to refine implementation of the design intent; they’ll prevent and diagnose flaws, using quantitative and qualitative approaches, bringing together fine detail and big data. Each of their perspectives is required; omitting any one may lead to an uninformed conclusion. 
Note the lack of emphasis on player’s opinion. UX doesn’t bend a game to the whims of a focus group, nor against the will of the creative team. Success isn’t necessarily marked by players saying “I like this” or “I’d buy this”, but pragmatically by metrics set by the team themselves: “Are players able to demonstrate understanding of the game, and behaving as we intend them to?” “Does the difficulty, measured by completion time and deaths, align with our intent?”. 
Players’ actions often speak louder than words. Any Developer that has attended a proper playtest will share stories of playtesters frustrating for minutes over some small flaw, only to suggest “yeah, it was fine”. Every task and process is designed to value contextualised behaviour and structured interview data over players raw opinion. 
Without actively steering a studio toward this kind of knowledge-seeking and feedback-gathering culture, teams aren’t empowered to seek it themselves. Without a leadership voice calling for these check-ins, they’re eschewed in favour of just getting on with it. This is understandable: the prospect of re-doing hard work, or teams learning that their best work isn’t being experienced-as-intended, isn’t pleasant. 
But blindly moving forward is false progress. By not addressing the player-centric challenges, they’re tainting the work being produced. At best this means re-doing hard work, at worst teams iterate themselves into bankruptcy, or release to lukewarm reception. Better to invest in getting it right early; better to redo days of work than months or years of work. 
Teams feeling they’re “not ready yet” or “in too much flux” are suffering the very uncertainty that UX processes are designed to overcome. 
Finding examples to highlight how the development challenges listed above lead to failure isn’t difficult. Games with ‘poor UX’ are every game you’ve never heard of, every game that never passed into the zeitgeist. Failed games and failed development studios don’t blame ‘poor UX’ for bad reviews, layoffs or bankruptcy; gamedev has its own well-developed lexicon for poor experiences: inaccessible, cluttered, clunky, too easy, unbalanced, shallow, clone, money-grubbing, greedy, deceptive, unoriginal… All are interpretations of unrealised intent — of poor UX. I’m sure you’ve played your own fair share of ‘bad games’, perhaps even sensing that they’re a diamond in the rough. “If only they’d changed X”. It can seem so obvious in hindsight. 
Successful products have releases with now-infamous usability or learnability issues in one form or another too: the widely-bemoaned Pip-Boy and settlement UI in Fallout 4, and the UI overhauls undergone by Gran Turismo 5 were both the result of failure to address ease-of-use issues. Luckily neither were core to gameplay. Pokemon Go suffered severe learnability issues at launch, leading to a raft of ‘how to play’ articles, but ultimately delivered experientially, through novelty and brand. No Man’s Sky failed to capture the experience players anticipated, despite generally good usability and learnability. Sometimes marketing dollars or branding can overcome UX frustrations, but not always, as Mario Run’s “disappointing” commercial performance attests. 
The challenges listed the start of this article don’t always manifest as a single UX issue clanger, but find any poor game review on Metacritic or app stores and there will be criticisms that fall under UX’s remit: difficulty-in-use, low understandability or inconsiderate design. These topics do matter to players, and so they should matter to you. 
The short answer is: everyone. Every individual contributor to a project leaves some mark on the experience of the player, be that aurally, visually, mechanically or otherwise. All Developers are creators, and all are responsible for the impact of their work on the players’ experience. 
Some parts of UX can be adopted without hiring UX staff, such as running playtests, surveys and collecting analytics data. But there is no substitute for expertise in research, psychology and interaction design, bringing those decades of human/machine study. Player data is hugely powerful in altering the course of a project, so ensuring you’re investing in capable staff avoids inexpertly-gathered data, biased analysis, unscientific research. Each can do more harm than good. 
Understanding your studio-specific challenges and competencies is a great place to begin. Some UX might already be part of your processes: playtesting, analytics, maybe some existing-player surveys or formalised competitor analysis. How could they have been employed earlier, and more predictively? Is the staffer getting the academic and moral support they need? Gamedev post-mortem articles very often cite regret for not beginning with player-centric tasks earlier and more actively. 
Perhaps rally the team and discuss your experiential risks in the project; does the team recognise any of the challenges listed above impacting their work? How does the team currently combat each of the challenges listed at the start of this article? 
The principle that “good design starts from day one” always hold true. Don’t fall into the trap of ‘checking it tomorrow’, because it never feels like the right time. Investing in UX ‘pays out’ in time saved or reallocated later in the project. Delays only serve to devalue UX’s potential contribution. One cannot reclaim development time already spent. 
Try parsing your studio’s previous journalistic reviews and storefront comments; could their criticisms be rooted in not understanding the game, or the impact of tutorials? Do comments reveal players having unintended ease-of-use frustrations, or issues with feedback, balance or navigation? Do analytics reveal metrics below our projections for retention or other player behaviour? If so, you’ve evidence for the return-on-investment for investment in player science on your next project. 
Time is the biggest challenge, pacing earlier stages of development to greatly speed up the later stages. Setting aside monthly budget and time for playtesting and research is a challenge for teams who’ve never experienced player-centric development. Know that these roles specifically exist and continues to thrive in all creative domains because their processes pay out in quality and time over the lifetime of production. 
Player-centric processes are more than hiring new people and setting aside budget; the company culture may need to change also. Hiring UX staff isn’t enough if the studio is unwilling to steer the company toward this empathetic design process, and empower those staff to instigate change. 
There is much to learn from studios who’ve already embedded player science professionals. EA’s Veronica Zammitto shared years of experience at GDC in 2017; Ubisoft, Riot and Epic have all shared their experiences in transitioning to a player-centric culture and process. Celia Hodent’s just-published book considers the applicability of UX and neuroscience to game development. There is a wealth of information about specific UX methods, processes and case studies available, particularly via the IGDA GURSIG community, and the UX Summit. Let’s learn and share together, toward better experiences for our players. 
Games User Experience is a discipline of science and design for overcoming the difficulties in making games that deliver on their experiential intent. It uses formalised processes and job roles to discover flaws in a game’s design and its means of communicating itself to the player. UX leverages a body of academic knowledge on designing for humans than spans decades of study, across many domains. 
Without UX approaches, games fall victim to difficulties that are inherent to creativity, including a lack of objectivity, and challenges in teaching and accommodating players that are dissimilar to ourselves. These factors ultimately affect the perceived quality of the game, critical reception and enjoyment. 
When game teams embrace the 4 key roles (Design, Research, Data Science and Leadership), a more empathetic and confident studio culture can form around the core development loop (design, implement, measure, assess), which leads to better games, happier staff and a more productive studio. 
Applying these practices to game development delivers successful games, faster, cheaper and closer to the design team’s creative vision. The combined power and efficacy of these player sciences will ensure their continued path to toward becoming a core game development discipline. 
Illustrated by the author. With thanks to Lanie Dixon, Aaron Walker, Jozef Kulik, Masse Moughal, Alistair Greo, Morgan Kennedy, Kitty Crawford and other proofreaders for your valuable comments and guidance. 
Written by 
Written by",Player Research,2017-11-28T11:12:22.322Z
BERT — A Practitioner’s Perspective | by Nirupam Purushothama | The Startup | Medium,"BERT stands for “Bidirectional Encoder Representations from Transformers”. It is currently the leading language model. According to published results it (or its variants) has hit quite a few language tests out of the park. If interested your can read the paper here: Bidirectional Encoder Representations from Transformers. The appendix section (of the paper) discusses the different types of language tasks that they tested BERT on. 
There are many articles online that explain what BERT does. I found most of them cumbersome and loaded with details that make it even harder to understand. Let me try to explain it in simple terms (i.e. not as a researcher who is planning to improve BERT but as a person who is interested in using BERT) 
Let us first understand BERT as a black-box. 
The above picture is taken from the paper. Let us first understand what the inputs and outputs for BERT are. You can see that there are multiple ways in which you can submit inputs to BERT. 
Each output tag (C or T) listed in the output is a vector in a H-dimensional space. Where H is 768 (as per the paper) and most implementations of BERT give you an embedding in 768 dimensions. 
Cool! So, far so good. If you are just interested in using BERT then you are good to go. You can directly install a few open source libraries and start playing with BERT. Examples are listed below. 
You can check the examples quoted here to see how straight-forward it is to use BERT. Probably not more than 10 lines of code. I will not quote the code here, you can check the links for code. 
The major motivation behind BERT seems to be to build a model that is pre-trained with an existing corpus and then the same model can be fine-tuned to be used for different tasks. For example in the above figure we see that the same BERT model is being used for various tasks. So, what the research team did was to build a BERT model and trained it using English-Wikipedia (2500M words) and Books corpus (800M words) on two tasks. The learning tasks are also simple: 
So, in the previous step you have a BERT that is pre-trained with some corpus and on some learning tasks. Now you have a model that outputs a set of Tags with each tag/output ‘T’ in a H-dimensional space (768 dimensions as per the paper). Cool! Now what you need to do is to fine-tune the entire model for your use-case. You can attach this BERT output layer to another layer of your choice (i.e. a multi-label classifier etc. ). The paper lists a set of 11 tasks that they fine tuned the pre-trained BERT for and then the results they obtained for those tasks. When they fine-tune they make sure that weights are tuned across the entire model (and not just to the layers attached on-top of BERT). 
BERT basically leverages Transformers. This paper “Attention is all you need” discusses the concept of transformers. Transformers is nothing but an encoder-decoder architecture. For full details, refer to this article: http://jalammar.github.io/illustrated-transformer/ (Pictures are taken from this article). Even this article is good: https://towardsdatascience.com/transformers-141e32e69591. Now Transformers are the bleeding edge in NLP and they are replacing/replaced LSTM based RNN models. 
From the above example you can see that transformer is leveraged for translation of sentences (French to English in the above example) or even for sentence prediction. When we peer within the encoder and decoder, the layers at a superficial level are as follows: 
And transformers in turn leverages a concept of Attention. The idea behind Attention is that the context of a word in a paragraph is captured, in some sense, by all the other words in that paragraph/document. You can refer to the links I mentioned above for more details. 
Now attention can be achieved using multiple methods. Google’s attention paper I mentioned above does not use RNN/CNN on top of a attention layer. But I believe there are other approaches like the one mentioned in this video (not me btw :) ) where attention is used in conjunction with RNNs: 
With reference to the Google paper (which is a well cited one), when you zoom into the encoding and decoding layers, transformer architecture looks like this. 
The fun part is that instead of attention this one talks about multi-head attention. What is the difference? 
Single attention apparently brings focus to a single area of the picture / corpus at a time. Whereas multi-head attention ensures that multiple areas of the corpus / image are focused upon at the same time. 
More details can be found here: NLP — Bert & Transformer 
And Google paper proposes their multi-head attention mechanism like this: 
Now, if you are still interested in digging into the details relating to what those V, K and Q vectors are and what linear operations are applied on them etc. then the best place to get those details is the paper. Even this article [NLP — Bert & Transformer] has a very good description of these details. If I talk about them in my article then it will become super long and boring. For this article it is sufficient enough to understand that there is something called as attention and it is used in a transformer. 
Transformer is used in BERT. So, that is a major use. As I said already they are replacing the LSTM based RNNs, so probably whenever you think about RNN for your project/problem you should also give Transformer a thought and see if you can train a transformer instead of a RNN for your use-case. It is an encoder decoder architecture and you should be able to use Transformer in such use-cases. Some typical problems that are tackled with a transformer are as follows: 
Now coming to BERT, we already said that it builds on top of transformers. BERT basically builds on the knowledge gained from other models built a few years ago, E.g. ELMo and OpenAI GPT. The major difference is that BERT does a fully-connected model where ELMO and OpenAI GPT use only feed-forward connections or separate blocks where you do forward and backward connections. This method of doing a fully-connected transformer network seems to ensure that attention is properly distributed across the entire network thereby ensuring that the model learns the language. 
The paper talks about the reasons behind the apparent success of BERT over other models. But no one really knows the precise reasons behind the success of these models (or for that matter any deep-learning model). This is an active area of research and you may want to consider it for your thesis :) . Also, it means that DL modeling is more art and less science. But the BERT paper deserves credit because the authors seem to be well aware of many other architectures, their inner workflows and a hunch about what may or may-not work. They also addressed three key things which makes BERT out-right appealing and interesting. 
There are many libraries available for BERT. Some notable ones that I have come across are: 
In conclusion, if you are just consuming pre-trained BERT then it is pretty straight-forward. Huggingface also has some fine-tuned models that others have shared with the community. If you wish to fine-tune BERT for your own use-cases and if you have some tagged data then you can use huggingface transformers and pyTorch to fine-tune a pre-trained BERT for your use-case. So, why are you waiting. Just install and explore BERT. 
Written by 
Written by",Nirupam Purushothama,2020-07-31T09:30:01.835Z
What is Zero Trust and how can it transform your enterprise security? | by Fyde | Medium,"The security status quo, built for old problems, is no match for the challenges of a global, untethered workforce. Why? 
The shift to data-centric enterprises and digital transformation is driving a corresponding increase in the number and sophistication of cyber-attacks. To protect valuable data and meet the demands of modern business, organizations must adopt new security strategies. The security status quo, built for old problems, is no match for the challenges of a global, untethered workforce. Why? 
To answer this question, we need to examine the trust concept. Establishing trusted access for networks, apps, devices, and users has become a significant security problem due to rapid technological changes and the evolution of work. Traditionally, corporations trusted their network boundaries to be secure. Myriad solutions guaranteed a locked-down perimeter. However, the old perimeter security concept does not apply anymore: if attackers gain access to a corporate endpoint or workload once, they can move laterally through internal systems, often unnoticed. 
“The assumption that systems and traffic within a data center can be trusted is flawed. Modern networks and usage patterns no longer echo those that made perimeter defense make sense many years ago. As a result, moving freely within a “secure” infrastructure is frequently trivial once a single host or link there has been compromised.” (Barth, D., Gilman, E. (2004). Zero Trust Networks). 
The new reality means that a remotely-working, global workforce requires new ways of work, productivity, and security. Established solutions such as next-gen firewalls, VPNs, web gateways, and network access control are insufficient to meet those needs. To ensure secure enterprise operations, organizations need visibility into access and processes. However, legacy security solutions do not provide sufficient visibility into networks, apps, and data outside the organizational security perimeter. Therefore, those assets cannot be trusted. 
To achieve trust, you need visibility into the apps, data-flows and user identities that access corporate resources, including details such as location, time, network, and device. If an organization cannot confirm these elements, it cannot accurately establish trust for any sources accessing its data. 
Furthermore, as the cloud becomes the standard for innovation and development speed, organizations that operate purely on-premises and within their perimeter will be left behind. To drive cloud transformation and bolder business competitiveness, organizations need to apply new frameworks to secure data, network, apps, endpoints, and users outside their perimeter. Enter the Zero Trust model. Zero Trust establishes trust in an unknown network by securing network communication and access so that the physical security of the transport layer can be reasonably disregarded. 
Think of Zero Trust as a smarter sister of the traditional security architecture that helps prevent leaks of confidential data and lowers the risk of successful cyber attacks on your business. The Zero Trust network is built upon five fundamental assertions: 
Zero Trust is also about gaining the essential visibility required to establish trust for employees and partners to work effectively and enhance data security. User trust is critical, but it is insufficient. Even a trusted user should not access company data in an untrusted environment. Context is an essential element to establish trust in a Zero Trust world. 
A Forrester study found that two-thirds of organizations using Zero Trust-powered technologies were more confident when adopting mobile working models, and 44% were more confident when securing DevOps environments. 67% of all enterprise resources are exposed to access-related risk, and a Zero Trust security approach is the best strategy to manage access to enterprise resources. Zero Trust is not only for the big players, it is also vital for small businesses. According to the 2018 Verizon Data Breach Investigations Report, 61% of all data breaches affected small businesses. 
To better understand how to lead your organization “the Zero Trust way”, answer the following questions: 
If most of your answers are “No,” explore the main roadblocks to Zero Trust security for your organization and create a roadmap that can get you there. 
Sinan Eren, the CEO, and co-founder of Fyde states that on your journey to Zero Trust security, your organization will initially benefit from discovering all the endpoints, applications, and workloads in your network and infrastructure. This initial discovery will lead to a robust inventory of users, devices, and services/apps. Once this organization-wide inventory is established, it will pay immediate dividends on your vulnerability remediation and data protection investments. As the next step in your Zero Trust journey, you will then be able to establish better visibility into who is accessing which service or application, using what device, when and from where. It will help drive down compliance costs and will provide a robust system of record to show to your auditors. This level of visibility over time can lead to a global policy for access control, which will shape your strategy for data protection and privacy. 
Fyde’s Zero Trust approach to network security supports the borderless, global business. It provides remote, conditional, and contextual access to resources and reduces over-privileged access and associated third-party risks. With Fyde, employees and partners can access corporate apps such as Jira, Confluence, Bitbucket, Gitlab, Kibana, MS RDP, SSH, and cloud workloads. Fyde is compatible with all apps, from legacy to SAML/HTTPS, and supports access to multiple infrastructure sites without switching access profiles. To ensure secure access, Fyde solves intermittent connectivity challenges and empowers a mobile-driven and dynamic enterprise. 
To learn more about how Fyde can enable this transformation, check out Fyde Enterprise and reach out to us on enterprise@fyde.com. 
Originally published at https://www.fyde.com. 
Written by 
Written by",Fyde,2019-09-10T21:12:14.199Z
Active Learning: the Theory. Active learning is still a niche… | by Olga Petrova | Scaleway | Medium,"This article was first published on April 14, 2020 on Scaleway’s official blog and is reposted here for your convenience. 
For a PyTorch implementation of active learning for an image classification problem, check out part 2: 
Active learning is still a relatively niche approach in the machine learning world, but that is bound to change. After all, active learning provides solutions to not one, but two challenging problems that have been poisoning the lives of many a data scientist. I am, of course, talking about the data: namely, its (a) quantity and (b) quality. 
Let us start with the former. It is no secret that training modern machine learning (ML) models requires large quantities of training data. This is especially true for deep learning (ML carried out via deep artificial neural nets), where it is not uncommon for training sets to number in the hundreds of thousands and beyond. To make matters worse, many practical applications come in the form of supervised ML tasks: i.e., not only do we require all these training samples, but we also need a way to label them. Labeling data is time-consuming and expensive. It is bad enough if it involves human experts manually annotating text or images, but what if labeling entails invasive medical tests to confirm a patient’s diagnosis? Or drilling down into the rock to test for oil? There are plenty of scenarios where unlabeled data may be easy to obtain, yet the labeling budget may impose severe limitations. This is precisely where semi-supervised learning shines: leveraging unlabeled data to achieve a supervised ML task with fewer labels that would be needed otherwise. It is a well-established fact that the performance of semi-supervised models often strongly depends on the selection of the training samples that have been labeled. Roughly speaking, the more “representative” or “informative” the labeled samples are, the better. If we have the choice of what samples to label, however, how can we determine which ones would be of most use for our task? Sometimes this can be seen from a manual inspection (although the approach of combing through unlabeled data manually does not scale particularly well), and other times it cannot. Would it not be nice if the model itself could tell us which datapoints it would prefer to know the labels for? Well, it can — in fact, that is precisely what active learning is all about. 
The issue of data quality does not only manifest itself in the semi-supervised setting. Even if all of your data is already labeled, more is not always better when it comes to training an ML model. On the contrary, outliers and other types of spammy data may lead your model astray. If these represent a significant portion of your training set, a model that is trained in an active learning regime, where unhelpful data is ranked down, may even outperform a fully supervised model, that had access to the entire dataset from the start! 
Active learning is part of the so-called Human-in-the-Loop (HITL) machine learning paradigm. The idea behind HITL is to combine human and artificial intelligence to solve various data-driven tasks. Depending on how you look at it, all of machine learning is at least somewhat HITL, but some areas more than others. In active learning, human participation is as explicit as it is iterative: 
The model in question can be any supervised ML algorithm, active learning puts no restrictions on you in that regard. For the sake of example, let us assume that we are dealing with image classification. Implementation-wise, you can think of active learning as something that is wrapped around your classifier. In fact, the classifier itself does not require any changes compared to its plain old passive learning version. Passive learning is basically the kind of machine learning that we are all used to, where labeled examples are sampled at random, rather than in accordance with the feedback received from the classifier. Standard supervised machine learning can be viewed as a special case of passive learning — one where you just happened to randomly sample all of your available training data! 
The most nontrivial part of active learning lies is step 3 above: how does the model decide which samples are the most beneficial to label at the current stage? Turns out, there are multiple ways of doing this. 
Let us first state the obvious: one has to start somewhere. Simply initializing your classifier to a random state and throwing unlabeled data at it will not get you very far. Better alternatives include: 
There are other, more sophisticated things that one can do, of course. You can, for example, try clustering your data first, and sample points from each cluster. This approach is not always possible, but is particularly helpful when we do not know how many classes are there in the underlying data distribution. 
However we choose to start our training, there are several choices to be made along the way. One decision to make is: 
In a streaming scenario, the model is presented with training samples, one at a time. The model will then either ignore the sample, or query its label. In our classifier example, one could imagine there being a hyperparameter that is a probability threshold for the most likely class. If the sample’s probability does not reach this threshold, the oracle gets a query, otherwise the sample goes to the ignore pile. The idea here is that if we are quite sure about the sample’s label, there is a good chance that we are right and confirming our suspicions is not the best use of our labeling budget. 
Another way to do active learning is via pooling. In this case the model evaluates the class probabilities for all of the unlabeled data, and selects some part of it to be labeled at the next iteration. The selection is based on a query strategy, and this is where things start to get really interesting: 
With active learning being such a young and rapidly developing field, there is no shortage of query strategies floating around. The most common one, however, is uncertainty sampling, where the model wants to get the labels for the samples whose class assignment it is most uncertain about. This can take a few different forms: 
In addition to uncertainty sampling, there are other possible approaches, such as: 
With a query strategy at hand, you are ready to start the iterative process of active learning. (For the actual code example, stay tuned for part 2!) But before we call it a day, let us look at some of the common issues that arise in real use cases and the ways to deal with them: 
The major potential issue has already been mentioned when we discussed the uncertainty sampling approaches above. The least confidence method in particular (although that is not to say that it is the only one) has the unfortunate property of gravitating towards the outliers. In practice, one solution to this is to iterate between a few different query strategies, e.g. from least confidence to random, then to margin sampling, etc. Another reason to employ multiple query strategies (including the random one!) is making sure that we explore more of the relevant feature space instead of focusing on a limited area. 
Speaking of feature space exploration, in the event that we do not know what classes are there in our problem, we face a difficult challenge of class discovery. This is not specific to active learning, but having a limited labeling budget (the reason we are probably using AL on the first place) certainly does not help. So how do you train a classifier when you do not know how many classes are there? Apart from unsupervised clustering (perhaps combined with encoding samples into latent space with reduced dimensionality via a self-supervised autoencoder model), you would likely have to rely on what you get via the random selection. 
Active learning may not be a one-size-fits-all, but without a doubt, it is a heavily under-utilized technique that can, and will, bring a lot of value to your commercial machine learning projects. For an example of active learning in action, follow the link below for the second part of this blog post: 
Written by 
Written by",Olga Petrova,2020-05-05T16:42:52.943Z
Performing Real-time Anomaly Detection using AWS | by Gautam Krishna | Slalom Data & Analytics | Medium,"A serverless approach to detect anomalies in real-time data — by Gautam Krishna, Reuben Hilliard, and Preeti Modgil 
Anomaly detection in real-time streaming data from a variety of sources has applications in several industries. Devices that generate such streaming data are varied and can include vehicle sensors, manufacturing equipment sensors, GPS devices, medical equipment, building sensors, etc. Streaming data for anomaly detection have been generated in some applications for many years, such as pressure and temperature sensors on manufacturing equipment. However, such anomaly detection has been limited to real-time hard-coded rules-based detection rather than true machine learning of what might be an anomaly. 
Applications for detecting anomalies in real-time streaming data exist in many industries like finance, IT, medical, manufacturing, social media, e-commerce, etc. Anomaly detection can provide useful, actionable real-time information as well as data for longer-term use. However, implementations of anomaly detection workflows are still limited and there is potential for this to become an industry differentiator for any company looking to implement. In this blog, we will leverage Amazon Web Services to build a real-time anomaly detection application. 
The AWS services that we will be leveraging in this post are: 
For the purpose of this demo, we leveraged the use-case of collecting On-Board Diagnostics II (OBD II) sensor data from a vehicle. The OBD II device of automated vehicles collects data from various sensors and sends it to a cloud-based server in real-time for detailed analysis. However, this area won’t be in-scope for this blog. We will be simulating OBD data based on distributions fitted on the OBD data available publicly from Kaggle. We randomly introduce outlier datapoints by scaling the OBD data points by a factor of 2 (arbitrarily chosen). The following are the major steps in our AWS based anomaly detection framework. 
When steps 4 through 6 above speak to the real-time handling of OBD data, the steps 7 through 9 below speak about persisting the data into an object store for future deeper dive analysis. 
As we discussed in steps 5 and 6 above, we developed a couple of ways to track anomaly data points — one using a real-time dashboard via ElasticSearch + Kibana and second, an email notification via Amazon Simple Notification Service (SNS). 
A snapshot of the real-time dashboard is shown below. We had set a threshold of 2.0 — meaning, anytime we get an anomaly score 2 or more, it will show up in the dashboard. 
Also, we configured a lambda function (as mentioned in step 5 above) that triggers a notification via Amazon SNS. The users who subscribe to the SNS Topic will get a notification that looks like below: 
The Random Cut Forest algorithm leveraged in Kinesis Analytics is based on the Robust Random Cut Forest, first implemented by a team of researchers at Amazon. The algorithm is focused on speed and specifically designed and optimized for streaming data. It is also quite robust, with no assumptions on the distribution of the underlying data and no preprocessing required. 
The algorithm samples several random data points then cuts them to the same number of points and creates trees. It then looks at all the trees together to determine whether any single data point is an anomaly. The fewer times needed to subdivide the data before isolating the target data point, the more likely it is that this data point is an anomaly for that sample. 
At this point, it is worth noting that Amazon SageMaker also supports anomaly detection using Random Cut Forest — but it is primarily designed for batch predictions. The SageMaker implementation requires the users to train the model using a static dataset and the model outputs anomaly scores for each data-point in the input dataset. A sample implementation of Random Cut Forest on Amazon SageMaker can be found in this blog. 
The architecture for setting up streaming data, performing real-time analytics on it, serving up the results as dashboards or alerts, and then saving off the relevant data for downstream uses can all be achieved within the AWS framework. Each of these components can be customized based on the industry application and use case. For example, in the case of medical devices, real-time monitoring and anomaly alerting can provide vital and valuable information to front-line medical workers to prioritize taking action. This can be especially helpful in resource-stressed environments like the COVID epidemic. Yet other use cases for anomaly detection and real-time dashboards can add up to providing longer-term cost savings, for example, with building sensors and associated energy consumption patterns. In many cases, the data sourced for real-time analytics for operational purposes (e.g. manufacturing equipment sensor anomalies) can then be aggregated and stored for further downstream applications like executive dashboards (e.g. equipment failures and resulting production outages) as well as future machine learning applications (e.g. predicting what type of failure and length of outage might result from a sensor anomaly). 
Indeed, the applications and ROI for implementing real-time anomaly detection systems can be varied. However, depending on the industry and specific use-case, it should be possible to determine a reasonable ROI prior to embarking on such an initiative. For further information on how this might benefit your organization, please contact us here. 
Written by 
Written by",Gautam Krishna,2020-06-10T18:46:31.970Z
A Transformer Chatbot Tutorial with TensorFlow 2.0 | by TensorFlow | TensorFlow | Medium,"A guest article by Bryan M. Li, FOR.ai 
The use of artificial neural networks to create chatbots is increasingly popular nowadays, however, teaching a computer to have natural conversations is very difficult and often requires large and complicated language models. 
With all the changes and improvements made in TensorFlow 2.0 we can build complicated models with ease. In this post, we will demonstrate how to build a Transformer chatbot. All of the code used in this post is available in this colab notebook, which will run end to end (including installing TensorFlow 2.0). 
This article assumes some knowledge of text generation, attention and transformer. In this tutorial we are going to focus on: 
Sample conversations of a Transformer chatbot trained on Movie-Dialogs Corpus. 
Transformer 
Transformer, proposed in the paper Attention is All You Need, is a neural network architecture solely based on self-attention mechanism and is very parallelizable. 
A Transformer model handles variable-sized input using stacks of self-attention layers instead of RNNs or CNNs. This general architecture has a number of advantages: 
The disadvantage of this architecture: 
If you are interested in knowing more about Transformer, check out The Annotated Transformer and Illustrated Transformer. 
Dataset 
We are using the Cornell Movie-Dialogs Corpus as our dataset, which contains more than 220k conversational exchanges between more than 10k pairs of movie characters. 
“+++$+++” is being used as a field separator in all the files within the corpus dataset. 
movie_conversations.txt has the following format: ID of the first character, ID of the second character, ID of the movie that this conversation occurred, and a list of line IDs. The character and movie information can be found in movie_characters_metadata.txt and movie_titles_metadata.txt respectively. 
Samples of conversations pairs from movie_conversations.txt 
movie_lines.txt has the following format: ID of the conversation line, ID of the character who uttered this phase, ID of the movie, name of the character and the text of the line. 
Samples of conversation text from movie_lines.txt 
We are going to build the input pipeline with the following steps: 
Notice that Transformer is an autoregressive model, it makes predictions one part at a time and uses its output so far to decide what to do next. During training this example uses teacher-forcing. Teacher forcing is passing the true output to the next time step regardless of what the model predicts at the current time step. 
The full preprocessing code can be found at the Prepare Dataset section of the colab notebook. 
Sample preprocessed conversation pair 
Attention 
Like many sequence-to-sequence models, Transformer also consist of encoder and decoder. However, instead of recurrent or convolution layers, Transformer uses multi-head attention layers, which consist of multiple scaled dot-product attention. 
Scaled dot product attention 
The scaled dot-product attention function takes three inputs: Q (query), K (key), V (value). The equation used to calculate the attention weights is: 
As the softmax normalization being applied on the key, its values decide the amount of importance given to the query. The output represents the multiplication of the attention weights and value. This ensures that the words we want to focus on are kept as is and the irrelevant words are flushed out. 
Multi-head Attention Layer 
The Sequential models allow us to build models very quickly by simply stacking layers on top of each other; however, for more complicated and non-sequential models, the Functional API and Model subclassing are needed. The tf.keras API allows us to mix and match different API styles. My favourite feature of Model subclassing is the capability for debugging. I can set a breakpoint in the call() method and observe the values for each layer’s inputs and outputs like a numpy array, and this makes debugging a lot simpler. 
Here, we are using Model subclassing to implement our MultiHeadAttention layer. 
Multi-head attention consists of four parts: 
Each multi-head attention block takes a dictionary as input, which consist of query, key and value. Notice that when using Model subclassing with Functional API, the input(s) has to be kept as a single argument, hence we have to wrap query, key and value as a dictionary. 
The input are then put through dense layers and split up into multiple heads. scaled_dot_product_attention() defined above is applied to each head (broadcasted for efficiency). An appropriate mask must be used in the attention step. The attention output for each head is then concatenated and put through a final dense layer. 
Instead of one single attention head, query, key, and value are split into multiple heads because it allows the model to jointly attend to information at different positions from different representational spaces. After the split each head has a reduced dimensionality, so the total computation cost is the same as a single head attention with full dimensionality. 
Transformer 
Transformer uses stacked multi-head attention and dense layers for both the encoder and decoder. The encoder maps an input sequence of symbol representations to a sequence of continuous representations. Then the decoder takes the continuous representation and generates an output sequence of symbols one element at a time. 
Positional Encoding 
Since Transformer doesn’t contain any recurrence or convolution, positional encoding is added to give the model some information about the relative position of the words in the sentence. 
The positional encoding vector is added to the embedding vector. Embeddings represent a token in a d-dimensional space where tokens with similar meaning will be closer to each other. But the embeddings do not encode the relative position of words in a sentence. So after adding the positional encoding, words will be closer to each other based on the similarity of their meaning and their position in the sentence, in the d-dimensional space. To learn more about Positional Encoding, check out this tutorial. 
We implemented the Positional Encoding with Model subclassing where we apply the encoding matrix to the input in call(). 
Transformer with Functional API 
With the Functional API, we can stack our layers similar to Sequential model but without the constraint of it being a sequential model, and without declaring all the variables and layers we needed in advance like Model subclassing. One advantage of the Functional API is that it validate the model as we build it, such as checking the input and output shape for each layer, and raise meaningful error message when there is a mismatch. 
We are implementing our encoding layers, encoder, decoding layers, decoder and the Transformer itself using the Functional API. 
Checkout how to implement the same models with Model subclassing from this tutorial. 
Encoding Layer 
Each encoder layer consists of sublayers: 
We can use tf.keras.utils.plot_model() to visualize our model. (Checkout all the model plots on the colab notebook) 
Encoder 
The Encoder consists of: 
The input is put through an embedding which is summed with the positional encoding. The output of this summation is the input to the encoder layers. The output of the encoder is the input to the decoder. 
Decoder Layer 
Each decoder layer consists of sublayers: 
As query receives the output from decoder’s first attention block, and key receives the encoder output, the attention weights represent the importance given to the decoder’s input based on the encoder’s output. In other words, the decoder predicts the next word by looking at the encoder output and self-attending to its own output. 
Decoder 
The Decoder consists of: 
The target is put through an embedding which is summed with the positional encoding. The output of this summation is the input to the decoder layers. The output of the decoder is the input to the final linear layer. 
Transformer 
Transformer consists of the encoder, decoder and a final linear layer. The output of the decoder is the input to the linear layer and its output is returned. 
enc_padding_mask and dec_padding_mask are used to mask out all the padding tokens. look_ahead_mask is used to mask out future tokens in a sequence. As the length of the masks changes with different input sequence length, we are creating these masks with Lambda layers. 
Train the model 
We can initialize our Transformer as follows: 
After defining our loss function, optimizer and metrics, we can simply train our model with model.fit(). Notice that we have to mask our loss function such that the padding tokens get ignored, also we are writing our custom learning rate. 
Evaluation 
To evaluate, we have to run inference one time-step at a time, and pass in the output from the previous time-step as input. 
Notice that we don’t normally apply dropout during inference, but we didn’t specify a training argument for our model. This is because training and mask are already built-in for us, if we want to run model for evaluation, we can simply call model(inputs, training=False) to run the model in inference mode. 
To test our model, we can call predict(sentence). 
Summary 
Here we are, we have implemented a Transformer in TensorFlow 2.0 in around 500 lines of code. 
In this tutorial, we focus on the two different approaches to implement complex models with Functional API and Model subclassing, and how to incorporate them. 
If you want to know more about the two different approaches and their pros and cons, check out when to use the functional API section on TensorFlow’s guide. 
Try using a different dataset or hyper-parameters to train the Transformer! Thanks for reading. 
Written by 
Written by",TensorFlow,2019-05-23T22:59:29.839Z
SEO is Not Hard — A step-by-step SEO Tutorial for beginners that will get you ranked every single time | by Austen Allred | Startup Grind | Medium,"Note: This one of one of the chapters of Secret Sauce: A step-by-step growth hacking guide. Secret Sauce breaks down every channel just like this one, so if you think this is valuable check it out. It’s for sale now. 
SEO is simply not as hard as people pretend like it is; you can get 95% of the effort with 5% of the work, and you absolutely do not need to hire a professional SEO to do it, nor will it be hard to start ranking for well-picked key terms. 
Of all the channels we’ll be discussing, SEO is the one that there is the most misinformation about. Some of it is subtle, but some of it is widely spread and believed by so-called SEO consultants who actually don’t know what they’re doing. 
SEO is very simple, and unless you’re a very large company it’s probably not worth hiring somebody else to do. 
In order to understand what we need to do for SEO let’s look back at how Google started, how it’s evolving today, and develop a groundwork from which we can understand how to get ranked on Google. 
The idea for PageRank — Google’s early ranking algorithm — stemmed from Einstein. Larry Page and Sergei Brin were students at Stanford, and they noticed how often scientific studies referred to famous papers, such as the theory of relativity. These references acted almost like a vote — the more your work was referenced the more important it must be. If they downloaded every scientific paper and looked at the references, they could theoretically decide which papers were the most important, and rank them. 
They realized that because of links, the Internet could be analyzed and ranked in a similar way, except instead of using references they could use links. So they set about attempting to “download” (or crawl) the entire Internet, figuring out which sites were linked to the most. The sites with the most links were, theoretically, the best sites. And if you did a search for “university,” they could look at the pages that talked about “university” and rank them. 
Google works largely the same way today, although with much more sophistication and nuance. For example, not all links carry the same weight. A link from an authoritative site (as seen by how many links a site has pointing at it) is much more valuable than a link from a non-authoritative site. A link from the New York Times is probably worth about 10,000 links from sites that don’t have much authority. 
At the end of the day the purpose of Google is to find the “best” (or most popular) web page for the words you type into the search bar. 
All this means is we need to make it clear to google what our page is about, and then make it clear that we’re popular. If we do that we win. In order to do that, we’ll follow a very simple process that works every single time with less effort than you probably think is required. 
Google is a very smart company. The sophistication of the algorithms they write is incredible; bear in mind that there are currently cars driving themselves around Silicon Valley powered by Google’s algorithms. 
If you get too far into the SEO rabbit hole you’ll start stumbling upon spammy ways to attempt to speed up this process. Automated software like RankerX, GSA SER, and Scrapebox, instructions to create spam or spin content, linkwheels, PBNs, hacking domains, etc. 
Some of that stuff works very short term, but Google is smart and it is getting smarter. It gets harder to beat Google every day, and Google gets faster at shutting down spammy sites every day. Most don’t even last a week before everything you’ve done disappears and your work evaporates. That’s not the way you should do things. 
Instead of Internet-based churn and burn we’ll be focusing on building equity in the Internet. So if you see some highly-paid SEO consultant telling you to use software and spun content to generate links, or when you see some blackhatter beating the system, just know that it’s not worth it. We’re going to build authority and get traffic fast, but we’re going to do it in a way that doesn’t disappear or cripple your site in the future. 
The first step in getting our site ready to rank is making it clear to Google what our site is about. 
For now we’re going to focus our home page (our landing page) on ranking for one keyword that isn’t our brand or company name. Once we do that and get that ranking we can branch out into other keywords and start to dominate the search landscape, but for now we’ll stay laser focused. 
The first thing we need to do is to figure out what that keyword is. Depending on how popular our site is and how long it’s been around, the level of traffic and difficulty we’ll get from this effort may vary. 
There’s a concept we need to be familiar with known as the “long tail.” 
If we were to graph “popularity” of most things with “popularity” being the Y axis and the rank order being the X axis, we’d get something like a power law graph: 
There are some big hits that get the majority of attention, and after a few hits the graph falls sharply. The long-tail theory says that as we become more diverse as a society the yellow end of the above graph will stretch forever and get taller. 
Think of Amazon. They probably have a few best-selling products, but the majority of their retail revenue comes from a wide variety of things that aren’t bought anywhere nearly as often as their best-selling products. Similarly, if we were to rank the popularity of the songs played in the last 10 years, there would be a few hits that would garner the majority of plays, and an enormous number of songs that have only a few plays. Those less popular products and songs are what we call the long tail. 
In SEO this matters because, at least in the beginning, we’re going to go after long tail keywords — very exact, intention-driven keywords with lower competition that we know can win, then gradually we’ll work our way to the left. 
Our site isn’t going to outrank ultra-competitive keywords in the beginning, but by being more specific we can start winning very targeted traffic with much less effort. 
The keywords we’re looking for we will refer to as “long-tail keywords.” 
In order to find our perfect long-tail keywords, we’re going to use a combination of four tools, all of which are free. 
The process looks like this: 
Don’t be intimidated — it’s actually very simple. For this example we’ll pretend like we were finding a keyword for this book (and we’ll probably have to build out a site so you see if we’re ranked there in a few months). 
In this step we’re simply going to identify a few keywords that seem like they might work. Don’t concentrate too much on culling the list at this point, as most bad keywords will be automatically eliminated as a part of the process. 
So since this is a book about growth hacking, I’m going to list out a few keywords that would be a good fit: 
Growth hacking 
Growth marketing 
Internet marketing 
Growth hacking guide 
Growth hacking book 
Book about growth hacking 
What is growth hacking 
Growth hacking instructions 
That’s a good enough list to start. If you start running out of ideas go ahead and check out keywordshitter.com. If you plug in one keyword it will start spitting out thousands of variations in just a few minutes. Try to get a solid list of 5–10 to start with. 
Now we’ll plug each keyword into UberSuggest. When I plug the first one — “growth hacking” — in, I get 246 results. 
Clicking “view as text” will let us copy and paste all of our keywords into a text editor and create an enormous list. 
Go through that process with each keyword you came up with. 
Now we’ll assume you have 500+ keywords. If you don’t, try to start with something more generic and broad as a keyword, and you’ll have that many quickly. Ideally you’ll have over 1500. 
Now that we have a pretty good list of keywords. Our next step is to figure out if they have enough search volume to be worth our while. 
You’ll likely notice that some are so far down the long tail they wouldn’t do much for us. For example, my growth hacking list came up with “5 internet marketing techniques.” We probably won’t go after that one, but instead of guessing we can let Google do the work for us. This will be our weeding out step. 
The Google Keyword Planner is a tool meant for advertisers, but it does give us some rough idea of traffic levels. 
Google doesn’t make any promise of accuracy, so these numbers are likely only directionally correct, but they’re enough to get us on the right track. 
You’ll have to have an AdWords account to be able to use the tool, but you can create one for free if you haven’t use AdWords in the past. 
Once you’ve logged in, select “Get search volume data and trends.” 
Paste in your enormous list of keywords, and click “Get search volume.” Once you’ve done so, you’ll see a lot of graphs and data. 
Unfortunately the Keyword Planner interface is a little bit of a nightmare to work within, so instead we’re going to export our data to excel with the “download” button and play with it there. 
Now what we’re going to do is decide what traffic we want to go after. 
This varies a bit based on how much authority your site has. So let’s try to determine how easy it will be for you to rank. 
Go to SEMrush.com and enter your URL, looking at the total backlinks in the third column: 
As a general rule (this may vary based on how old your site is, who the links are from, etc.), based on the number of links you have, this is the maximum level of “difficulty” you should go after. 
Number of Backlinks 
Maximum Difficulty 
< 30:<40 
<100:40–50 
<1000:50–70 
1000+:70+ 
Go ahead and sort the data by difficulty, and eliminate all of the stuff that is too high for your site (don’t worry, we’ll get those keywords later). For now you can simply delete those rows. 
One important thing to note is that Google gives us this volume as “exact match” volume. This means that if there is a slight variation of a keyword we will see it if the words are synonyms, but not if they are used in a phrase, so the traffic will be underestimated from what you would expect overall. 
Now with that disclaimer sort the traffic volume highest to lowest, and from this data pick out five keywords that seem like a good fit. 
Here are mine: 
growth hacking strategies 
growth hacking techniques 
growth hacking 101 
growth hacking instagram 
growth hacking twitter 
Mine all look the same, but that may not necessarily be the case. 
Unfortunately the “keyword difficulty” that Google gives us is based on paid search traffic, not on natural search traffic. 
First, let’s use Google Trends to view the keyword volume and trajectory simultaneously. You can enter all of the keywords at the same time and see them graphed against each other. For my keywords it looks like this: 
The ones I’m most excited about are purple and red, which are “Growth hacking techniques” and “Growth hacking Twitter.” 
Now we’ll take a deeper look at what the competition is like for those two keywords. 
In order to analyze how difficult it will be to rank for a certain keyword, we’re going to have to look at the keywords manually, one by one. That’s why we started by finding some long-tail keywords and narrowing the list. 
This process gets a lot easier if you download the SEOQuake Chrome extension. Once you’ve done that, do a Google search and you’ll notice a few changes. 
With SEOQuake turned on the relevant SEO data of each site is displayed below each search result. 
We’re going to alter what is displayed, so in the left-hand sidebar click “parameters” and set them to the following: 
Now when you search, you’ll see something like this 
SEOQuake adds a ranking number, and the following at the bottom: 
The Google Index: This is how many pages from this base URL Google has indexed 
Page Links: The number of pages linking to the exact domain that is ranking according to SEMrush’s index (usually very low compared to reality, but since we’ll be using this number to compare it wil be somewhat apples to apples) 
URL Links: The number of pages pointing to any page on the base URL 
Age: The first time the page was indexed by the Internet Archive 
Traffic: A very rough monthly traffic number for the base URL 
Looking at these we can try to determine approximately what it would take to overtake the sites in these positions. 
You’ll notice that the weight of the indicators change. Not all links are from as good of sources, direct page links matter much more than URL links, etc., but if you google around and play with it for a while you’ll get a pretty good idea of what it takes. 
If you have a brand new site it will take a month or two to start generating the number of links to get to page one. If you have an older site with more links it may just be a matter of getting your on-page SEO in place. Generally it will be a mixture of both. 
Keep in mind that we’re going to optimize our page for this exact keyword, so we have a bit of an advantage. That said, if you start to see pages from sites like Wikipedia, you will know it’s an uphill battle. 
Here are a couple of examples so you can see how you should think through these things, starting with “Growth hacking techniques.” 
Entrepreneur.com is definitely a big name, and “growth hacking techniques” is in the title explicitly. This will be difficult to beat, but there are no links in the SEMRush index that point direct to the page. 
(By the way, I wonder how hard it would be to write an article for entrepreneur.com — I could probably do that and build a few links to that easily, even linking to my site in the article). 
Yongfook.com, have never heard of that site. 206 total links, not much traffic, this one I could pass up. It does have quite a bit of age and “Growth hacking tactics” in the title explicitly, so that would make it tough, but this one is doable to pass up after a while. 
Alright, so quicksprout is relatively popular, a lot of links, good age, lots of traffic, a few links direct to the page but not a ton. 
But the word “tactics” doesn’t even appear here. This page isn’t optimized for this keyword, so I could probably knock it out by being optimized specifically for “growth hacking tactics.” 
Let’s jump down a ways to see how hard it would be to get on the front page. 
17 total pages indexed? Created in 2014? No links in the index, even to the root URL? This one’s mine. I should be able to front-page easily. 
So this looks like a good keyword. Now we just have to get the on-page SEO in place and start building a few links. 
Now that we have our keyword selected, we need to make sure Google knows what our site is about. This is as simple as making sure the right keywords are in the right places. Most of this has to do with html tags, which make up the structure of a webpage. If you don’t know html or understand how it works, just pass this list to a developer and they should be able to help you. 
Here is a simple checklist you can follow to see if your content is optimized. 
☐ Your keyword is in the <title> tag, ideally at the front (or close to the front) of the tag 
☐ Your keyword is close to the beginning of the <title> tag (ideally the first words) 
☐ The title tag contains less than the viewable limit of 65 characters (optional but recommended) 
☐ Your keyword is in the first <h1> tag (and your page has an <h1> tag) 
☐ If your page contains additional header tags (<h2>, <h3>, etc) your keyword or synonyms are in most of them 
☐ Any images on the page have an <alt> tag that contain your chosen keyword 
☐ Your keyword is in the meta description (and there is a meta description) 
☐ There is at least 300 words of text on the page 
☐ Your keyword appears in the URL (if not the homepage) 
☐ Your keyword appears in the first paragraph of the copy 
☐ Your keyword (or synonyms — Google recognizes them now) is used other times throughout the page 
☐ Your keyword density is between .5% and 2.5% 
☐ The page contains dofollow links to other pages (this just means you’re not using nofollow links to every other page) 
☐ The page is original content not taken from another page and dissimilar from other pages on your site 
If you have all of that in place you should be pretty well set from an on-page perspective. You’ll likely be the best-optimized page for your chosen keyword unless you’re in a very competitive space. 
All we have left now is off-page optimization. 
Off-Page SEO is just a fancy way to say links. (Sometimes we call them backlinks, but it’s really the same thing.) 
Google looks at each link on the web as a weighted vote. If you link to something, in Google’s eyes you’re saying, “This is worth checking out.” The more legit you are the more weight your vote carries. 
SEOs have a weird way to describe this voting process; they call it “link juice.” If an authoritative site, we’ll say Wikipedia for example, links to you, they’re passing you “link juice.” 
But link juice doesn’t only work site to site — if your homepage is very authoritative and it links off to other pages on your site, it passes link juice as well. For this reason our link structure becomes very important. 
There are a number of tools that let you check how many links are pointing to a site and what the authority of those pages are. Unfortunately none of them are perfect — the only way to know what links are pointing to your site is to have crawled those pages. 
Google crawls most popular pages several times per day, but they don’t want you manipulating them, so they update their index pretty slowly. 
That said, you can check at least a sample of Google’s index in the Google Search Console (formerly known as Webmaster Tools). Once you navigate to your site, In the left-hand side select “Search Traffic” then “Links to your site.” There’s a debate raging over whether or not this actually shows you all of the links Google knows about (I’m 99% convinced it’s only a sample), but it’s at least a representative sample. 
To see all of your links, click on “More” under “Who links to you the most” then “Download this table.” This, again, seems to only download a sample of what Google knows about. You can also select “Download latest links” which provides more recent links than the other option. 
Unfortunately this doesn’t let us see much a to the value of the links, nor does it show us links that have dropped or where those links are from. 
To use those there are a wide variety of tools: If you have a budget I’d go with ahrefs.com as they have the biggest index, followed by Moz’s Open Site Explorer (most of the data you can get with a free account, if not then it’s slightly cheaper than ahrefs), and finally SEMrush, which is free for most purposes we need. MajesticSEO uses a combination of “trust flow” and “citation flow” which also works fairly well to give you an idea as to the overall health and number of links pointing to your site. 
All of these use different internal metrics to determine the “authority” of a link, but using them to compare apples to apples can be beneficial. 
HTML links look something like this: 
<a href=”http://www.somesite.com” title=”keyword”>Anchor text</a> 
Where http://www.somesite.com is the place the link directs you to, the title is largely a remnant of time gone by, and the linked text — think the words that are blue and you click on — is called the “anchor text.” 
In addition to the amount of link juice a page has, the relevance of the anchor text matters. 
Generally speaking you want to use your keyword as the anchor text for your internal linking whenever possible. External linking (from other sites) shouldn’t be very heavily optimized for anchor text. If 90% of your links all have the same anchor text Google can throw a red flag, assuming that you’re doing something fishy. 
If you’re ever creating links (like we’ll show you in the future) I only ever use something generic like the site name, “here” or the full URL. 
Generally speaking you don’t want orphan pages (those that aren’t linked to by other pages), nor do you want an overly-messy link structure. 
Some say the ideal link structure for a site is something like this: 
That’s close, but it gets a couple things wrong. First, you’ll never have a structure that organized, and second, in an ideal world every page would link to every other page on its same level. This can easily be done with a footer that feels like a sitemap or “recommended” pages. That allows you to specify anchor text, and pass link juice freely from page to page. 
Unfortunately it’s impossible to draw such a web without it becoming a mess, so you’ll just have to imagine what that actually looks like. 
We have just one more thing to go over before we start getting those first links pointing to our site. 
Most of SEO is managing stuff that can go wrong. There is a lot of that, but we’ll go over what will cover 99% of needs, and you can Google if there’s something really crazy. 
Robots.txt 
Almost every site has a page at url.com/robots.txt — even google has one. 
This is just a plain text file that lets you tell search engine crawlers what to crawl and not to crawl. Most are pretty good about listening, except the Bingbot, which pretty much does whatever it wants no matter what you tell it. (I’m mostly kidding.) 
If you don’t want Google to crawl a page (maybe it’s a login page you don’t want indexed, a landing page, etc.) you can just “disallow” it in your robots.txt by saying disallow: /somepage. 
If you add a trailing / to it (e.g. disallow: /somepage/) it will also disallow all child pages. 
Technically you can specify different rules for different bots (or user agents), but it’s easiest to start your file with “User-agent: *” if you don’t have a need for separate crawling rules. 
Disavow 
Google will penalize spammy sites, and unfortunately this causes some bad behavior from bad actors. Say, for example, you wanted to take out a competitor. You could send a bunch of obviously spammy links to their site and get them penalized. This is called “negative SEO,” and is something that happens often in highly contested keywords. Google generally tries to pretend like it doesn’t happen. 
In the case that this does happen, however, you can “Disavow” links in the Search Console, which is pretty much saying, “Hey Google, don’t count this one.” I hope you’ll never have to use it, but if you hire (or have hired) a bad SEO or are being attacked by a competitor, that is how you combat it. 
Nofollow 
A link can have a property called “nofollow” such as this: 
<a href=”http://www.somesite.com” title=”keyword” rel=”nofollow”>Anchor text</a>. 
If you want to link to somebody but you don’t want it to count as a vote (you don’t want to pass link-juice), or you support user-generated content and want to deter spammers, you can use a nofollow link. Google says it discounts the value of those links. I’m not convinced they discount them heavily, but other SEOs are so they seem to deter spammers if nothing else. 
Redirects 
If you’re going to change a URL, but you don’t want its link juice to disappear, you can use a 301 redirect. A 301 will pass a majority of the link juice. 
Canonical URLs 
If you have two pages that are virtually the same, you can add something like <link rel=”canonical href=”https://www.someurl.com/somepage”> to say “hey, treat this page as if it were that page instead, but I don’t want to 301 it.” 
And with that, we’re ready to build our first links. 
Link building is where SEO really starts to matter, and where a lot of people end up in a world of hurt. 
The best way to build links is to not build links. I’ve worked for companies in the past that don’t have to ask for them, they just flow in from press, customer blogs, their awesome blog posts, etc. If this is an option (and we’ll go over a couple of ways to make it more likely) you’re in a great place. 
If not, at least in the beginning, we’re going to manually create just a few. 
We’re going to create them in legitimate ways and not hire somebody in India to do so. That is a recipe for disaster, and I can’t even count the number of times I’ve seen that take down a site. 
The easiest way to build high quality links are what SEOs call “web 2.0s.” That’s just a way to say “social sites” or sites that let you post stuff. Now tweeting a link into the abyss won’t do you anything, but profiles, status pages, etc. do carry some weight. And if they come from a popular domain that counts as a link. 
Some of the easiest are: 
Twitter (in your bio) 
Github (the readme of a repo) 
YouTube (the description of a video — it has to actually get views) 
Wordpress (yes, you’ll have to actually create a blog) 
Blogger (same here) 
Tumblr 
Upvote-based sites (HackerNews, GrowthHackers, Inbound.org, etc.) 
If nothing else you can start there and get a half dozen to a dozen links. There are always big lists of “web 2.0s” you can find online, but keep in mind if you’re going to build something out on a blogging platform you’re going to have to really build something out. That’s a lot of content and time, but you have to do it the right way. 
We generally keep a bigger list of Web 2.0s here. Some may be out of date, but you should probably only build a half dozen to a dozen Web 2.0s anyway. 
Another way to get link juice is by purchasing an expired domain. This is more difficult to do, but there are a lot of options such as expireddomains.net. (Google “expired domains” and you’ll find dozens of sites monitoring them.) 
You’ll want to purchase a domain that has expired and restore it as closely as you can to its original form using an archive. These sites likely have some link juice to pass on and you can pass it to yourself. 
Another way to find places you can build links is by using a link intersection tool. These find sites that link to “competitor a” and “competitor b” but not to you. Theoretically, if they link to both of your competitors, they should be willing to link to you. Moz, Ahrefs, LunaMetrics and others have link intersection tools that work quite well. 
Now that we have a few basic links flowing, we’re going to work on some strategies that will send continual links and press, eventually getting to a point where we don’t have to build any more links. 
Awesome — you have a site that converts well, your SEO is in place, ready for you to drive traffic. Now what? 
As you’re probably learned at this point, a site that converts very well but has no traffic flowing to it still converts zero traffic. 
We’re going to fix that. 
This section takes a lot of time and effort, and in the beginning you’ll likely wonder if you’re doing anything at all. Remember that class in college that is so difficult it’s the point where most people give up, effectively weeding out the people who aren’t ready to major in a specific subject? 
Well this is the weeder-out chapter of growth hacking. 
The reason so many people stumble on this step is the same reason people stumble on so many steps that take a little effort under time — losing weight, investing in a 401(k), etc. In the beginning you’re going to have a little seedling of traffic, and you’ll be looking up to those who have giant oak trees, thinking, “I must be doing something wrong.” You’re not doing anything wrong. The traffic starts as a trickle before it becomes a flood. 
But don’t worry if you’re a startup. Our goal is to get enough traffic that continuing to do this effort will be sustainable (meaning we won’t die before we start to see the rewards), but at the same time we’re building equity in the Internet. 
The type of traffic we want to build is the type that will compound and will never go away. We want to create traffic today that will still give us a little trickle in five years. Combining hundreds (or thousands) of little trickles, our site that converts, and a great product we will create a giant river. 
Future chapters will go into depth on the networks we need to drive traffic from, so in this chapter we’re going to focus on traffic that’s network-agnostic. Traffic that we can’t get by tapping any specific network. 
Just to give you some idea of scale, I’ve seen this process drive over 500,000 visits per day, though the build up to that level took almost a full year. What could you do with 500,000 visits per day? 
To start we’re going to use the keywords we found in the SEO chapter, and inject ourselves (and our company) into the conversation wherever it’s taking place. 
To do this we’re going to use software called BuzzBundle. 
This software lets us do a few things: 
This step takes thought, effort, and a real human who understands what they’re typing. I don’t often say this, but you cannot effectively automate this step without it becoming spammy. If you’re trying to replicate the automated SEO spam you’ve seen on various blogs and sites this will probably work, but you’ll get banned, your clickthrough will be a fraction of what it could be, and you’ll be banned 
We’re not going to fire up some awful software to drop spun mentions of garbage onto various comment sections online hoping that brings us SEO traffic. Our comments must do two things: 
If you do these two things a few changes will take place: First, you’ll notice that people click on your links because you’re a thoughtful person who likes to contribute. Second, people will respect your company because you’re a thoughtful person who likes to contribute. 
And with that disclaimer, we’ll move on to the nitty gritty of how this is done. 
Now that all of that is out of the way, let’s fire up BuzzBundle and get to work. 
The first thing you’ll want to do in BuzzBundle is go to Accounts -> Add new accounts. This is the starting point for everything we’ll do, as we need accounts to comment. 
One thing you’ll notice about BuzzBundle is that it lets you use multiple accounts. I find it beneficial to think from multiple perspectives and therefore multiple points of view, but I don’t want to go too far overboard and be spammy. 
I’d recommend doing something simple — create 2–3 personas, each of whom you identify with (or are you), and enter them into your BuzzBundle accounts. 
Personally I don’t even change my name, I just use a different one (eg. Austen J. Allred vs. Austen Allred) or use a few photos, just so it isn’t literally the same name and same photo blanketing the Internet. 
Disqus is a comment system used all over the place, and it carries some caveates. Disqus will ban you if you use the same link in every post, so there are two workarounds: 
Both of these work, but the second one is much easier in my view. 
Using links with our UTM parameters here will be very beneficial. We’ll be able to track traffic back to each individual blog or site, and if necessary double down on the ones that are driving traffic. 
If you ever start to run into problems with getting your link posted, it may be useful to use a few link shorteners or some 301 redirects. 
To keep it simple you can use a link shortener that 301s such as bit.ly, or if you want to spend a little more time you can set up your own site and 301 the traffic from a certain page to your money site. 
Let’s get started with the BuzzBundle. 
First, it’s going to ask you for a keyword. We already have a keyword from the SEO section, but we may want to do something even a bit more generic. For this one I’m going to go with “growth hacking.” 
Simply hit “go” and let BuzzBundle get started. 
It will load different content types into different columns, but generally we are going to be scrolling through until we find something that looks compelling and like we can actually contribute to. 
The first thing I clicked on was this: 
It’s a review of another book about growth hacking. All I had to do was comment, tag the author, ask him if he were willing to review our book, and offer to send him one for free. (If you’re reading this now it’s going to be pretty awkward). 
My assumption is this person will find the conversation to be completely authentic, because it is. The fact that there’s now a link on his video that people who are searching for something else will find is just an added bonus. 
As an aside, I much prefer to hold “shift” and click on a link to open it in my normal browser if I’m just going to be commenting as myself. 
The next one I found was a roundup of great growth hacking blog posts from the week. 
I left the following comment: 
Note how I followed him on Twitter so that it’s obviously personal and not an automated spam comment. I even went a little bit overboard and tweeted at him just for kicks. 
That is how you get people on your team. 
As you get further along and have an idea of how to get a good response, I’d recommend starting to sort by reach, ramping up the number of keywords you’re searching for, and possibly even gasp upgrading to the paid version of BuzzBundle. 
Written by 
Written by",Austen Allred,2020-05-12T15:55:01.909Z
Real-time anomaly detection with exponentially-distributed data | by Steven Raybon | Towards Data Science,"Consider the following: You run a website that allows people to book an appointment with their doctor. Over the past month you have observed that someone books an appointment about every 5 minutes with some slight variation. However, while monitoring the appointment volume one day you see that there have not been any booked appointments for 10 minutes. And then 15 minutes. And then 20 minutes. At what point should you become concerned and follow up to make sure there’s not a problem possibly prohibiting people from booking an appointment? 
This is a common statistical question that is derived from considering “interarrival times” or the time in between events. If the events in question are independent of one another and the rate at which they occur is stable over time, then the interarrival time is exponentially distributed. Formally, we have the following: 
where f(y) is the density function of y, a random variable of interarrival times. The cumulative distribution function F(y) represents the probability of observing a value less than y. 
This is a very nice, simple distribution that will allow us to make some quick inferences about observed data in real time. Consider the above example again. We stated that the average interarrival time between booked appointments was 5 minutes. So each minute, on average, we observe 0.2 appointments — this is the arrival rate, or the lambda in the model specified above. 
Our goal is to come up with a threshold such that after a sufficient amount of time passes without a booked appointment, we would consider the inactivity anomalous. The threshold is somewhat arbitrary — like most significance thresholds in statistical inference. But let’s come up with a threshold (c) such that the probability of passing that threshold is less than 1%. Mathematically, this looks like 
This tells us that the probability of observing an interarrival time beyond 23.02 minutes is less than 1%. If we consider only this one event, we would think that observing this interarrival time is rare and we would classify it as an outlier and follow up. We can look at how the threshold changes when we change our tolerance level (also called a “Survival Probability”) from 1% to other values: 
Intuitively, the probability of observing higher interarrival times decreases. If we were set our tolerance level at 0.1% instead we would have a threshold of 34 minutes. This “survival function” has a lot of applications in other areas like insurance (think life insurance and mortality rates). We can see that starting around 20 minutes, the probabilities become very small and asymptotically approach zero. Values out in this tail should cause us to pause and investigate. 
Now let’s pause and reflect on what this result gives us. What this derivation represents is a probability of observing a single random event . With enough data or events, these outlier events will happen eventually¹ . This is just the nature of the exponential distribution — all positive values have some probability of occurring. In the real world, this probably isn’t true due to natural limitations but it’s approximated. Derivation of this threshold does not give us a magic number that perfectly identifies an anomaly. It is meant to raise some eyebrows and spur investigation into what is likely an anomaly. Or put differently, it is meant to flag events that are unlikely to be the result from pure randomness. 
Moreover, it is very important to verify the assumptions made earlier about the independence of events and the stability of the arrival rate. If either of these fail to be true, the model breaks down. 
To go from a mathematical exercise to an applied, production-ready algorithm takes several additional steps. The more substantive component with this entire process is not the statistics, but the integration into a production environment. I’m not going to dive into the gory details of setting up a production pipeline — many of these details are context-specific anyway (Azure vs AWS vs Google Cloud will all be different). However, I provide some details I’ve found particularly important to this type of exercise. 
There are practical hurdles that show up when dealing with real data. As stated above, even if you are confident that the data are independent, the arrival rate can change very easily over time. In the booked appointment case, the underlying patient behavior can change or new providers could be added to your network and more patients would then have access to online scheduling (likely increasing the arrival rate). Additionally, the arrival rate can differ by time of day or even day of week. In this case, it makes sense to think about conditioning the mean based on these types of variables. The trade-off is the reduction in sample size which can prohibit obtaining an accurate estimate of the mean, as well. 
Next, depending on how data is extracted, true real-time detection may be impossible. If data is extracted directly from some real-time streaming service then it’s possible to get to a more real-time feel. However, if there’s an ETL pipeline between your algorithm and the initial data source, there could be a substantial delay. 
If there is a delay it doesn’t have to be the end of the world. Depending on the context, it may not matter at all. For example, if the ETL pipeline runs every 5 minutes and the captured data have a 2 minute mean arrival time, then anomalies calculated at 1% would appear around the 9:12 minute mark — which means the algorithm would pick it up at the 10 minute mark, adding 48 seconds to the detection process. That shouldn’t be a deal breaker in almost any case. You just need to decide what works best in your context. 
Lastly, if the ETL gets backed up or fails then this could possibly trigger the algorithm to classify events as outliers once the ETL starts running again. Obviously, these aren’t true outliers. 
If these hurdles can be avoided (or sufficiently dealt with) then the algorithm is straight forward. The core code can be summarized with just a couple lines of code. There is only one comparison that takes place which is (i) the current time elapsed since the last event and (ii) the threshold that was derived above. If the elapsed time is greater than the threshold then flag the event; if not then do nothing. Simple! 
We have walked through a method of deriving a simple outlier detection technique for exponentially-distributed data. With some basic assumptions a nice model and formula can be applied to interarrival times to flag those times that are “too long” as defined beforehand. This method is not a fool-proof method of identifying actual anomalies, but a method of identifying events that are unlikely to happen by chance. 
While the heart of this discussion is a mathematical exercise, the implementation and the associated hurdles are the more important and substantive component. Some of these hurdles include (1) choosing a consistently accurate arrival rate to include in the model, (2) ETL pipelines introducing delays to real-time data access and (3) ETL failures potentially introducing false positives in the results. Once these have been adequately addressed the actual algorithm is a simple procedure. 
¹ Consider a collection of 1,000 events. Each with probability 1% of being classified an outlier. The probability of not seeing an outlier in the entire set is .99¹⁰⁰⁰ = 0.004% which is very small. In other words, while it might be unlikely to observe an outlier for any given event, but it is very likely at least one will pop up in a large enough sample. 
Hands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look 
Written by 
Written by",Steven Raybon,2019-07-23T13:36:16.657Z
"Topic Modeling Tutorial with Latent Dirichlet Allocation (LDA) | by Michel Kana, Ph.D | Towards Data Science","Curious about Contents? Feeling lost in countless posts, tweets, comments, articles, pages, documents? Seeking out what people write about? You are not alone. 
In this tutorial, I provide a practical guide with proven hands-on Python code for discovering the abstract topics that occur in a collection of texts or documents. 
We are going to apply Latent Dirichlet Allocation (LDA) to a set of tweets and split them into topics. Let’s get started! 
We will be working with tweets from the @realDonaldTrump Twitter account. The dataset consists of all tweets from the account from 5/4/2009. 
The dataset is from http://www.trumptwitterarchive.com/archive. An extract of the raw archive is given below. 
Pre-processing the text is important because language is ambiguous at all levels: lexical, phrasal, semantic. We make the tweets all lowercase and remove stopwords. They are commonly used words, which do not add significant meaning to the tweet. Below are examples of stopwords from the natural language toolkit (NLTK). 
Given the nature of tweets, we want to be careful with punctuation marks and terms like RT (used for re-tweets) and VIA (used to mention the original author), which are not in the default English stop-word list. We also want to remove URLs at the end of the tweets. 
We also tokenize our tweets. Tokenizers work similar to regular expressions and are used to divide tweets into lists of words. Below we provide the full code for pre-processing tweets. 
When we are interested in categorizing text, classifying it based on topic, we often do not want to look at the sequential pattern of words. Rather we would represent the text as a bag of words, as if it were an unordered set of words, while ignoring their original position in the text, keeping only their frequency. You can learn more about natural language processing in our article below. 
The most common words and their count are as follows. 
Latent Dirichlet Allocation (LDA) is a probabilistic transformation from bag-of-words counts into a topic space of lower dimensionality. Tweets are seen as a distribution of topics. Topics, in turn, are represented by a distribution of all words in the vocabulary. But we do not know the number of topics that are present in the corpus and the tweets that belong to each topic. With LDA we want to treat the assignment of the tweets to topics as a random variable itself which is estimated from the data at hand. 
Finding the right number of topics for LDA is an art. Topic Coherence technique is usually preferred to Perplexity techniques. With coherence, we quantify the coherence of a topic by measuring the degree of semantic similarity between its high scoring words. This leads to topics which are more human interpretative. The technique selects the top frequently occurring words in each topic. It then computes and aggregates all pairwise scores (UMass) for each of the words to calculate the coherence score for a particular topic. 
Below, we visualize the average coherence score per topic for a range of models trained with a different number of topics. The number of topics for which the average score plateaus, is the sweet spot we are looking for. So our best guess for the number of topics is around 6. We should consider that, typically a corpus with very short documents (tweets in our case) tends to be more difficult to apply to coherent models than a corpus with longer documents. 
Now, we can run LDA on the texts using the optimal value of found via the analysis above. LDA is a generative approach, where for each topic, the model simulates probabilities of word occurrences as well as probabilities of topics within the document. The top 10 words for each of the topics are displayed below. 
Although the plausibility of the topics is questionable, we can see the following meaningful match. Topic 0 deals with meetings between the leaders of China, North Korea, and the United States. Topic 1 seems to relate to the FBI investigation into Russian interference in the last presidential election. Topic 4 seems to be about border security and building a border wall. 
We demonstrated how statistical modeling helps finding what people are tweeting about. The code provided in this article can be generalized to many other tasks, aiming at discovering the abstract topics that occur in a collection of documents. 
D. Blei, A. Ng, and M. Jordan. Latent Dirichlet allocation. Journal of Machine Learning Research, 3:993–1022, January 2003. D. Blei and J. Lafferty. Topic Models. In A. Srivastava and M. Sahami, editors, Text Mining: Theory and Applications. Taylor and Francis, 2009. B. Chen, X. Chen, andW. Xing. “Twitter Archeology” of Learning Analytics and Knowledge Conferences. In Proceedings of the Fifth International Conference on Learning Analytics And Knowledge (pp. 340–349). New York, NY, 2015. 
Thank you for reading. I look forward to your feedback or questions. 
Hands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look 
Written by 
Written by","Michel Kana, Ph.D",2020-02-02T18:28:06.573Z
"How to Improve Data Labeling Efficiency with Auto-Labeling, Uncertainty Estimates, and Active Learning | by Hyun Kim | Towards AI — Multidisciplinary Science Journal | Sep, 2020 | Medium","In this post, we will be diving into the machine learning theory and techniques that were developed to evaluate our auto-labeling AI at Superb AI. More specifically, how our data platform estimates the uncertainty of auto-labeled annotations and applies it to active learning. 
Before jumping right in, it would be useful to have some mental buckets into which the most popular approaches can be categorized. In our experience, most works in deep learning uncertainty estimation fall under two buckets. The first belongs to the category of Monte-Carlo sampling, having multiple model inferences on each raw data and using the discrepancy between those to estimate the uncertainty. The second method models the probability distribution of model outputs by having a neural network learn the parameters of the distribution. The main intention here is to give breadth to the kind of techniques we explored and hope to offer some clarity on how and why we arrived at our unique position on the subject. We also hope to effectively demonstrate the scalability of our particular uncertainty estimation method. 
Before we dive into the various approaches to evaluate the performance of auto labeling, there is a note of caution to be exercised. Auto-label AI, although extremely powerful, cannot always be 100% accurate. As such, we need to measure and evaluate how much we can trust the output when utilizing auto labeling. And once we can do it, the most efficient way to use auto-labeling is then to have a human user prioritize which auto-labeled annotation to review and edit based on this measure. 
Measuring the “confidence” of model output is one popular method to do this. However, one well-known downside to this method is that confidence levels can be erroneously high even when the prediction turns out to be wrong if the model is overfitted to the given training data. Therefore, confidence levels cannot be used to measure how much we can “trust” auto-labeled annotations. 
In contrast, estimating the “uncertainty” of model output is a more grounded approach in the sense that this method statistically measures how much we can trust a model output. Using this, we can obtain an uncertainty measure that is proportional to the probability of model prediction error regardless of model confidence scores and model overfitting. This is why we believe that an effective auto-label technique needs to be coupled with a robust method to estimate prediction uncertainty. 
One possible approach to uncertainty estimation proposed by the research community is obtaining multiple model outputs for each input data (i.e. images) and calculating the uncertainty using these outputs. This method can be viewed as a Monte-Carlo sampling-based method. 
Let’s take a look at an example 3-class classification output below. 
y1 = [0.9, 0.1, 0] 
y2 = [0.01, 0.99, 0] 
y3 = [0, 0, 1] 
y4 = [0, 0, 1] 
Here, each of the y1 ~ y4 is the model output from four different models on the same input data (i.e. the first model gave the highest probability to class #1, etc.). The most naive approach would be using four different models to obtain these four outputs, but using Bayesian deep learning or dropout layers can give randomness to a single model and allow us to obtain multiple outputs from a single model. 
A specific type of a Monte-Carlo based method is called Bayesian Active Learning by Disagreement (BALD) [1, 2]. BALD defines uncertainty as following: 
uncertainty(x) = entropy(Avg[y1, … , yn]) - Avg[entropy(y1), … , entropy(yn)] 
Like our example output above, BALD assumes we can obtain multiple outputs (y1 ~ yn) for a single input data (x). For example, x can be an image and each y can be a multi-class softmax output vector. 
According to the BALD formulation, a model prediction has low uncertainty only when multiple model outputs assign a high probability to the same class. For example, if one model output predicted a “person” class with a high probability and another model output predicted a “car” class with a high probability, then the combined uncertainty would be very high. Similarly, if both models put approximately the same probability to both a “car” class and “person” class, the uncertainty would be high. Seems reasonable. 
However, one downside to using a Monte-Carlo sampling approach like BALD for estimating auto-label uncertainty is that one needs to create multiple model outputs for each input data leading to more inference computation and longer inference time. 
In the next section, we’ll take a look at an alternative approach that remedies this problem. 
Let’s assume that the model prediction follows a particular probability distribution function (PDF). Instead of having a model optimize over prediction accuracy, we can have the model directly learn the prediction PDF. 
If we do so, the neural network output will not be a single prediction on input data but rather the parameters that define or determine the shape of the probability distribution. Once we obtain this distribution, we can then easily obtain the final output y (i.e. the softmax output) by sampling from the probability distribution described by these parameters. Of course, we could Monte-Carlo sample from this distribution and calculate the model uncertainty as we did in the section above but that would beat the purpose of uncertainty distribution modeling. 
By using a Distribution Modeling approach, we can directly calculate the output variance using well-known standard formulas for the mean and variance (or other similar randomness measures) of the distribution. 
Because we only need one model prediction per data to calculate the auto-label uncertainty, this method is computationally much more efficient than Monte-Carlo during inference time. Distribution modeling, however, has one caveat. When a model directly learns the probability distribution, the class score y is probabilistically defined and thus the model has to optimize over the “expected loss”. Oftentimes this makes an exact computation impossible or the closed-form equation may be extremely complicated, meaning it can be inefficient during training time. 
Luckily, this is not an issue in a practical sense because for neural networks the computation time for calculating the loss and its gradient, whether it’s a simple loss or a complex loss formulation, is insignificant compared to the total backward calculation time. 
There is an existing work on applying this uncertainty distribution modeling approach to multi-class image classification tasks, called “Evidential Deep Learning” [3]. We’ll take a closer look into this work in the next section. 
Evidential Deep Learning (EDL) is a type of uncertainty estimation method that uses the uncertainty distribution modeling approach explained above. Specifically, EDL assumes that the model prediction probability distribution follows a Dirichlet distribution. 
Dirichlet distribution is a sensible choice for this purpose because of several reasons: 
1.) Dirichlet distribution is used to randomly sample a non-negative vector that sums to 1, and is thus suitable for modeling a softmax output. 
2.) The formula for calculating the expected loss and its gradient for a Dirichlet distribution is exact and simple. 
(For more details on calculating the expected loss of a Dirichlet distribution, take a look at Equations 3, 4 and 5 of the paper [3]. Each equation corresponds to a multinomial distribution, cross-entropy loss and sum-of-squares loss, respectively.) 
3.) Most importantly, Dirichlet distribution not only has a formula for calculating the variance measure, but there is also a formula for calculating the theoretical uncertainty measure that falls between 0 and 1. 
(This is because we can map the Dempster-Shafer theory of evidence to a Dirichlet distribution. If you are curious, you can read more about this here [4].) 
Let’s go one step further into the last point, #3. We can calculate the uncertainty of a Dirichlet distribution with the following formula: 
uncertainty(x) = Sum[1] / Sum[1 + z] 
where, 
The neural network output z is equal to the Dirichlet distribution parameter, and semantically, the size of each element of the vector z corresponds to the level of certainty for each class. 
Let’s visualize the Dirichlet distribution for two cases of z. 
As Sum[z] gets larger, the distribution gets narrower, centered around the mean value. 
As Sum[z] gets smaller, the distribution gets wider and flattens out, and becomes a uniform distribution when z = 0. 
For example, when a given input x is close to the decision boundary (a “hard example”), there is a higher chance that: 
In order to optimize over the expected loss, the model during training learns to reduce the z value so that it can flatten out the prediction probability distribution and reduce the expected loss. 
On the flip side, when the input x is far away from the decision boundary (an “easy example”), the model learns to increase z to narrow down the distribution so that it’s close to a one-hot vector pointing to the predicted class. 
The two types of uncertainty estimation we’ve looked at above, Monte-Carlo methods such as BALD and Uncertainty Distribution Modeling methods such as EDL, are both designed for an image classification problem. And we saw that both methods each have drawbacks: Monte-Carlo method requires multiple outputs from each input and is inefficient during inference time; Uncertainty Distribution Modeling requires that the prediction probability distribution follows a certain known type of distribution. 
We have combined these two methods and invented a patented hybrid approach that has been applied to our Auto-label engine. Here are the two best practices for using our Auto-labeling feature and its uncertainty estimation for Active Learning. 
One of the most effective ways to use uncertainty estimation is for labeling training data. 
A user can first run Superb AI’s Auto-label to obtain automatically labeled annotations as well as the estimated uncertainty. The uncertainty measure is shown in two ways: 
1) Image-level Difficulty — our auto-label engine scores each image as easy, medium or hard. Based on this difficulty measure, a human reviewer can focus on harder images and more effectively sample data for review. 
2) Annotation-level Uncertainty — the auto-label engine also scores the uncertainty of each annotation (bounding box, polygon), and requests your review on those that fall below a threshold. In the example below, most of the vehicles were automatically labeled, and you can see that our AI requested user review on the annotations that are smaller or further back in the scene (marked in yellow). 
Another way to use uncertainty estimation is for efficiently improving production-level model performance. 
Most samples in a training dataset, in fact, are “easy examples” and don’t add much to improving the model’s performance. It’s the rare data points that have high uncertainty, or the “hard examples”, that are valuable. Therefore, if one can find “hard examples” from the haystack the model performance can be improved much more quickly. 
To illustrate this point, we’ve applied our uncertainty estimation technique on the COCO 2017 Validation Set labels. Here are the results. 
First, we’ve plotted a histogram of image-level uncertainty. As you can see, most of the images have a very low uncertainty below 0.1 and there is a long tail distribution of uncertainty levels up until ~4.0. The green, yellow and red colors indicate how these would appear on our Suite platform as Easy, Medium and Hard. 
Here’s an illustration of the differences in image-level uncertainty. As the uncertainty measure increases, you can see that the image becomes more complex — more objects, smaller objects and more occlusions between them. 
Next, we’ve plotted a histogram of annotation (bounding box) uncertainty. Again, most bounding box annotations have very low uncertainty (below 0.05), and interestingly for this case, the histogram is more or less uniform for uncertainty measures between 0.1 and 0.3. 
Again, to illustrate the difference in annotation uncertainty, there are four “person class” bounding boxes at varying uncertainty levels above. As the uncertainty increases, you can see that the person is occluded, blurry, and smaller (further back in the scene). 
In summary, even one of the most widely used datasets like COCO is heavily centered around easy examples. This will probably be the case for your dataset(s) as well. However, as described in this article, being able to incorporate uncertainty estimation frameworks to quickly identify hard examples near the decision boundary will undoubtedly assist in overall prioritization and active learning. 
In the next part of our Tech Series, we will talk about “Improving Auto-Label Accuracy With Class-Agnostic Refinement” followed by “Adapting Auto-Label AI To New Tasks With Few Data (Remedying The Cold Start Problem)”. To get notified as soon as it is released on Towards AI, please subscribe here. 
SUPERB AI’s Suite is a powerful training data platform that empowers ML teams to create training data pipelines that are interoperable and scalable. Whether you are building perception systems for autonomous driving, cancer prognostics, or threat detection, Superb AI provides a faster approach to building AI starting with training data management. If you want to try the platform out, sign up for free today! 
[1] Houlsby, Neil et al. “Bayesian Active Learning for Classification and Preference Learning.” ArXiv abs/1112.5745 (2011). 
[2] Gal, Yarin et al. “Deep Bayesian Active Learning with Image Data.” ICML (2017). 
[3] Sensoy, Murat et al. “Evidential Deep Learning to Quantify Classification Uncertainty.” NeurIPS (2018). 
[4] Jøsang, Audun et al. “Interpreting Belief Functions as Dirichlet Distributions.” Symbolic and Quantitative Approaches to Reasoning with Uncertainty (2007). 
Towards AI publishes the best of tech, science, and engineering. Subscribe with us to receive our newsletter right on your inbox. For sponsorship opportunities, please email us at pub@towardsai.net Take a look 
Written by 
Written by",Hyun Kim,2020-09-02T00:01:00.948Z
Anomaly Detection and Typical Challenges with Time Series Data | by Amelia Williams | Medium,"Anomaly detection, popularly known as outlier detection is a data mining process that aims to discover unexpected events or rare items in data and to determine details about their occurrences. Anomaly detection in time series data brings its own challenges due to seasonality, trends and the need to more complex multivariate analysis to yield better results. 
In this multi-part blog series, we will discuss key aspects of anomaly detection, typical challenges that we encounter in doing anomaly detection for time series data, and finally discuss approaches for doing multivariate anomaly detection. 
Automatic anomaly detection is a critical aspect in today’s IT world where the sheer volume of data makes it impossible to tag outliers manually. Anomaly Detection can be used in IT Operations to trigger prompt troubleshooting, help avoid loss in revenue, and maintain the reputation and branding of the company. To serve this purpose, large companies have built their own anomaly detection services to monitor their business, product and service health. Anomalies once detected send out alerts to the IT Operators to make timely decisions related to incidents. It is extremely important to detect an Anomaly to run the ITOps smoothly, predict future interruptions and therefore prevent any incident that might lead to data damage and unnecessary resource spent. Also, the anomalies need to be detected at the right time to yield best results and avoid any false alerts that mislead the operations. 
With the advent of Artificial Intelligence and AIOps platforms, the need for manual monitoring of anomalies and alerts has drastically reduced. Earlier human intervention was required at every level, to monitor and identify the anomalies within the ITOps. With AI changing the face of technology, a machine is trained to understand the pattern over a specific time period to detect and predict any existing or future anomalies on its own, without any human intervention. 
read more https://bit.ly/34uInxp 
Originally Published by CloudFabrix Software Inc 
Written by 
Written by",Amelia Williams,2019-12-18T09:36:49.953Z
"The Computational Complexity of Graph Neural Networks explained | by Franziska Lippoldt | Sep, 2020 | Medium","Unlike conventional convolutional neural networks, the cost of graph convolutions is “unstable” — as the choice of graph representation and edges corresponds to the graph convolutions complexity — an explanation why. 
Dense vs Sparse Graph Representation: expensive vs cheap? 
Graph data for a GNN input can be represented in two ways: 
A) sparse: As a list of nodes and a list of edge indices 
B) dense: As a list of nodes and an adjacency matrix 
For any graph G with N vertices of length F and M edges, the sparse version will operate on the nodes of size N*F and a list of edge indices of size 2*M.The dense representation in contrast will require an adjacency matrix of size N*N. 
While in general the sparse representation is much less expensive in usage inside a graph neural network for forward and backward pass, edge modification operations require searching for the right edge-node pairs and possibly adjustment of the overall size of the list, which leads to variations on the RAM usage of the network. In other words, sparse representations minimize memory usage on graps with fixed edges. 
While being expensive to use, the dense representation has the following advantages: Edge weights are naturally included in the adjacency matrix, edge modification can be done in a smooth manner and integrated into the network, finding edges and changing edge values does not change the size of the matrix. Those properties are crucial for Graph Neural Networks that rely on in network edge modifications. 
Taking GNN applications into perspective, the decision between sparse and dense representations can be formulated into two questions: 
While the first question is straight forward to answer after inspection of the graphs, the second question depends on the structure of the neural network as well as the complexity of the graph. For now, it appears that large graphs (not talking about Toy examples here), benefit from pooling and normalisation/stabilization layers between layers of graph convolutions. 
Edges and complexity of convolution 
While some graphs arise naturally from data by well defined relations between nodes, for example an interaction graph of transactions between accounts in the last month, some more complex problem statements, especially for dense representations, do not naturally have a clear edge assignment for each vertex. 
If the edge assignment is not given through the data, different edge variations will lead to different behaviour and memory costs of the graph neural network. 
For this, it makes sense to start with an array as a graph representation, this could be an image for example. For an array of size MxN, the smallest amount of edges that connects every node with every other node through several hops is through connecting the current node with previous and the next node. In this case there is M*N nodes and 2*M*N edges. A simple one hop convolution performs 2*M*N operations. However, this operation would be “slow” — in order for the node information in the middle of the array to reach the first node of the array, this would require around 0.5*M*N convolutions. 
A typical approach chosen for graph convolutions on images is to take the 8 direct neighbours for edge connection into accounts, in this case there is 8*M*N edges, hence each simple graph convolution has the cost of 8*M*N. For information from the center node of the array to reach the first node, being able to walk diagonally, this takes max(M,N) convolutions. 
For so called self attention based approaches, every node would be connected to every other. While this requires (M*N)*(M*N) edges, during one convolutional operation the information of any node to any other node can be transmitted. 
From all of those three examples above, it becomes clear that the number of edges determines the complexity of the convolution. This is unlike conventional convolutional layers, where filter sizes often come in 3x3 format and are determined by the network design, not the image input. 
Additional Info: Dense vs Sparse Convolutions 
The choice of dense or sparse representation not only affects the memory usage, but also the calculation method. Dense and sparse graph tensors require graph convolutions that operate on dense or sparse inputs (or alternatively as seen in some implementations convert between sparse and dense inside the network layer). Sparse graph tensors would operate on sparse convolutions that use sparse operations. From a very naive point of view it would be logical to assume that dense computations would be more expensive but faster than sparse, because sparse graphs would require processing of operations in the shape of a list. However, libraries for sparse tensor operations are available for both PyTorch and Tensorflow that simplify and speed up sparse operations. 
Additional Info: Towards other edge criteria 
From grid to k-nearest neighbours: If the graph’s nodes are not arranged in a grid as in the example images, a generalization of this approach is to find the k-nearest neighbors for each node. This approach can be further generalized by using feature point input instead of positions as node coordinates, with the “distance” function in the k-nn set up serving as a similarity measurement between features. 
From Euclidean distance to other evaluation methods: While some graph network applications benefit from physical coordinates or pixel positions (such as traffic prediction or image analysis), other graphs and edge relations might arise from similarity criteria based on connections or knowledge, such as social networks or transaction graphs. 
Related resources: 
This article expresses the author’s own opinion and is not necessarily in accordance with the opinion of the company the author is working for. 
Written by 
Written by",Franziska Lippoldt,2020-09-10T01:56:00.904Z
Audio to text conversion using AWS Transcribe and Sentiment Analysis using Comprehend API | by Rana singh | Analytics Vidhya | Medium,"It is an Automatic Speech Recognition (SAR) service by Amazon.it is capable of recognizing speech from existing audio or videofile, or from a stream of audio or video content and also from an audioinput coming directly from your computer’s microphone. 
Amazon Transcribe uses advanced machine learning technologies to recognize speech in audio files and transcribe them into the text You can use Amazon Transcribe to convert audio to text and to create applications that incorporate the content of audio files, For example, you can transcribe the audio track from a video recording to create closed captioning for the video. 
It is a fully managed application service in the machine learning stack, you don’t have to provision any of the servers or manage any infrastructure, you can simply supply the source file through an S3 bucket and will get the transcribed output via the same or different bucket or could be in a bucket that is being “owned by amazon”. 
https://docs.aws.amazon.com/transcribe/?id=docs_gateway 
It is supported in 11 regions for the ones who do not know about what an AWS region is, it is basically a Geographical boundary defined by AWS and it contains multiple Availability Zones(know as Data Centres). To give fault tolerance and load balancing capabilities to AWS services in that region or across multiple regions simultaneously. that being said not all of the Services launched by AWS made available in all of the regions. 
Supported formats: • FLAC, MP3, MP4, or WAVSupported duration and size:• Less than 4 hours in length or less than 2 Gb of audio dataYou must specify the language and format of the input file.For best results:• Use a lossless format, such as FLAC or WAV, with PCM 16 bit encoding.• Use a sample rate of 8000 Hz for telephone audio. 
You can specify that Amazon Transcribe identify between 2 to 10 speakers in the audio clip. 
A custom vocabulary is a list of specific words that you want Amazon Transcribe to recognize in your audio input. These are generally domain specifi c words and phrases, words that Amazon Transcribe isn’t recognizing, or proper nouns.You can have up to 100 vocabularies in your account. The size limit for a custom vocabulary is 50 Kb. You can have it defined in either a list format or a table format. 
An IAM Role is basically a set of permissions that can be assumed by someone(or an entity) to gain access to the allowed services as per their responsibility and allowed scope, roles are a way of providing temporary credentials that aws generates to ensure maximum security for our workloads, role contains temporary access key id and secret key and one additional component which is security token, these temporary keys generated by roles are used to provide desired access to the entity who assumes a role, and these keys are generally valid for 12 hours and security token component make sure to generate new keys 5 minutes before of the expiry of the 12 hour duration so we don’t have to worry about rotating these keys by our self and it just happens automatically. 
https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html 
https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios.html 
https://docs.aws.amazon.com/transcribe/latest/dg/API_Operations.html 
https://github.com/ranasingh-gkp/Amazon_Transcribe-/blob/master/How-to-Use-AWS-SDK-Software-Development-Kit-for-Python-Boto-and-Running-a-Transcription-Job.pdf 
2. Linking the name of each audio file to the speaker 
3. set key access with AWS platform 
4. Set S3 credential and check bucket 
5. Creating a new S3 bucket to upload the audio files 
6. Uploading the files to the created bucket 
7. Define the file URLs on the bucket using S3 convention for file paths 
8. Create Vocabulary list for transcribing 
9. Function to start Amazon Transcribe job 
10. Create sagemaker role 
11. Iterate over the audio files URLs on S3 and call the start_transcription function defined above. 
12. Download JSON file after transcribing from the S3 bucket 
13. Delete Transcribe job which is taking the name from the bucket 
14. Verify Amazon Transcribe jobs that are under the status COMPLETE 
Result: 
The outcome is JSON file of hindi audio that comprise of hindi Transcript of audio, Diarization, timestempt of each words with confidence score. 
On the last part of our analysis we are going to use Amazon Comprehend for sentiment analysis of the speeches. As mentioned before, AWS offers a pre-trained model that you can use to return the percentage of 4 different sentiments: positive, negative, mixed or neutral. 
To perform the sentiment analysis we simply need to provide the text as a string and the language. One limitation imposed by Amazon Comprehend is the size of the text. 
Sentiment: Sentiment allows you to understand whether what the user is saying is positive or negative. Or even neutral, sometimes that’s important as well. You want to know if there’s not sentiment, that might be a signal. 
Entities: This feature goes through the unstructured text and extracts entities and actually categorizes them for you. So things like people, or things like organizations will be given a category. 
Language detection: So for a company that has a multilingual application, with a multilingual customer base. You can actually determine what language the text is in. So you know if you have to translate the text itself, or take some other kind of business action on the text. 
Key phrase: think of this as noun phrases. So where entities are extracted, is maybe proper nouns. The key phrase will catch everything else from the unstructured text, so you actually can go deeper into the meaning. What were they saying about the person? What were they saying about the organization for example? 
Topic modeling: Topic modeling works over a large corpus of documents. And helps you do things like organize them into the topics contained within those documents. So it’s really nice for organization and information management. 
For example Social Analytics: 
Detail step by step followed in sentiment analysis 
2. Reading JSON file from the directory 
3. set key access with AWS platform 
4. Set JSON input and output directory 
5. Get file path from the input directory 
6. Set comprehend function for sentiment value in 5000 byte chunk. it can be analyzed to 5000 bytes (which translates as a string containing 5000 characters). As we are dealing with texts transcripts that are larger than this limit, we created the start_comprehend_job function that split the input text into smaller chunks and calls the sentiment analysis using boto3 for each independent part. 
7. Set transcribe function to use amazon comprehend for sentiment value in dataframe 
8. Define the main function 
9. Run the main function 
Result: 
Sentiment analysis for each selected speech. It has four numerical outcome with sentiment lebel i.e positive, negative, neutral and mixed. 
Google images 
https://docs.aws.amazon.com/pt_br/comprehend/latest/dg/guidelines-and-limits.html 
Written by 
Written by",Rana singh,2020-02-22T07:15:00.048Z
Finding shortest paths with Graph Neural Networks | by David Mack | Octavian | Medium,"In this article we show how a Graph Network with attention read and write can perform shortest path calculations. This network performs this task with 100% accuracy after minimal training. 
Here at Octavian we believe that graphs are a powerful medium for representing diverse knowledge (for example BenevolentAI uses them to represent pharmaceutical research and knowledge). 
Neural networks are a way to create functions that no human could write. They do this by harnessing the power of large datasets. On problems for which we have capable neural models, we can use example inputs and outputs to train the network to learn a function that transforms those inputs into those outputs, and hopefully generalizes to other unseen inputs. 
We need to be able to build neural networks that can learn functions on graphs. Those neural networks need the right inductive biases so that they can reliably learn useful graph functions. With that foundation, we can build powerful neural graph systems. 
Here we present a “Graph network with attention read and write”, a simple network that can effectively compute shortest path. It is an example of how to combine different neural network components to make a system that readily learns a classical graph algorithm. 
We present this network both as a novel system in of itself, but more importantly as the basis for further investigation into effective neural graph computation. 
The code for this system is available in our repository. 
Given a question “What is the length of the shortest path between station A and B?” and a graph (as a set of nodes and edges), we want to learn a function that will return an integer answer. 
Machine learning on graphs is a young but growing field. For a survey of the different approaches and their history see Graph Neural Networks: A Review of Methods and Applications or our introduction. We’ve also included a list of surveys at the end of our introduction article. 
The classic algorithms for calculating shortest paths are A*, Dijkstra’s and Bellman Ford. These are robust and widely implemented. Dijkstra’s is most similar to our use case, of finding the shortest path between two specific nodes with no available path cost heuristic. 
The earliest work on neural based solutions to shortest path was motived by communications and packet routing, where approximate methods faster than the classical algorithms were desired. These operate quite different from today’s neural networks, they used iterative back-propagation to solve shortest path on a specific graph. Examples of work in this area include Neural networks for routing communication traffic (1988), A Neural Network for Shortest Path Computation (2000) and Neural Network for Optimization of Routing in Communication Networks (2006). 
In this work we seek to build a model that will work on many unseen graphs, in stark contrast to those methods, which solve for a single graph. Furthermore, we seek to offer a foundation for learning more complex graph functions from pairs of input questions and expected outputs. 
A recent, groundbreaking approach to the problem was DeepMind’s Differentiable neural computers (PDF), which computed shortest paths across the London Tube map. It did this by taking the graph as a sequence of connection tuples and learning a step-by-step algorithm using a read-write memory. It was provided with a learning curriculum, gradually increasing the graph and question size. 
By contrast, our solution performs much better (100% vs 55.3%), on bigger paths (length 9 vs 4), does not require curriculum learning, does not require the training of an LSTM controller, has fewer parameters and is a simpler network with fewer components. 
Whilst we haven’t found any other published solutions to this exact problem, there are many instances of similar techniques being used for different problems. A couple of relevant examples: 
To our knowledge, ours is the first example of combining attention read and write with a graph network. 
Given the question “How many stations are between station 1 and station 15” and a rail network, we’d like the correct answer, e.g. “6”. 
More concretely, we’ll train the network with Graph-Question-Answer tuples. Each tuple contains a unique randomly generated graph, an English language question and the expected answer. 
For example: 
These are split into non-overlapping training, validation and test sets. 
This data setup produces a network that will work on new, never previously seen graphs. That is, it’ll learn a graph algorithm. 
We’ll use the CLEVR-Graph dataset to generate the graphs, questions and answers 
When building a machine learning solution, and not achieving high accuracy, it can be hard to know whether the model has a deficiency or if the data has inherent noise and ambiguities. 
To remove this uncertainty we’ve employed a synthetic dataset. This is data that we generated, based on our own set of rules. Thanks to the explicit structure of the data, we can be confident a good model can score 100% accuracy. This really helps when comparing different architectures. 
CLEVR-graph contains a set of questions and answers about procedurally generated transport network graphs. Here’s what one of its transport networks looks like (it’s modeled on the London Tube) and some example questions and answers: 
Every question in CLEVR-graph comes with an answer and a unique, procedurally generated graph. 
CLEVR-graph can generate many different types of questions. For this article we’ll generate just those that pertain to shortest paths. There is one template for this (“How many stations are between A and B?”) and it is combined with a randomly chosen pair of stations from each randomly generated graph to give a graph-question-answer triple. 
The graph-question-answer triples are generated as a YAML file, which we then compile into TFRecords. 
As there is only one question template, the training data lacks the variety you’d get in a more natural (human) source. This makes the dataset easier to solve. We leave language diversity as a future extensional challenge (and would love to see readers’ solutions!). 
We’ll build a neural network in TensorFlow to solve our problem. TensorFlow is a popular library for building neural networks and comes with many useful components. The code for this system is available in our repository. 
The system we’ll build takes a question, performs multiple iterations of processing, then finally produces an output: 
The structure we’ll use is a recurring neural network (RNN) — in an RNN the same cell is executed multiple times sequentially, passing its internal state forward to the next execution. 
The RNN cell takes the question and graph as inputs, as well as any outputs from earlier executions of the cell. These are transformed, and an output vector and updated node state are generated by the cell. 
Inside the RNN cell are two major components: a graph network and an output cell. Their detail is key to understanding how this network works. We’ll cover those in detail in the next sections. 
The RNN cell passes forward a hidden state, the “node state”. This is a table of node states, one vector per node in the graph. The network uses this to keep track of on-going calculations about each node. 
The RNN cell is executed a fixed number of times (experimentally determined, generally longer than the longest path between two nodes) and then the final cell’s output is used as the overall output of the system. 
That completes a brief survey of the overall structure. The next sections will outline the input to the network, and how the RNN cell works. 
The first part of building the system is to create an input data pipeline. This provides three things: 
All of these are pre-processed into TFRecords so they can be efficiently loaded and passed to the model. The code for this process is in build.py in the accompanying GitHub repository. You can also download the pre-compiled TFRecords. 
The question text 
There are three steps to transform the English question into information the model can use: 
The graph 
The graph is represented by three data-structures in the TFRecord examples: 
Each of these are multi-dimensional tensors. 
Names and properties are represented using the same text-encoding scheme (e.g. as integer tokens passed into an embedding) used for the question text 
The expected answer 
The expected answer (for this dataset, always an integer from zero to nine) is represented as a single textual token (i.e. as an integer), using the same encoding scheme as for the question text and node/edge properties. 
The expected answer is used during training mode for loss calculation and back-propagation, during validation and testing it’s used to measure model accuracy and to pinpoint failing data examples for debugging. 
The heart of the network is an RNN. It consists of an RNN cell, which is repeatedly executed, passing its results forwards. 
In our experiments, we used 10 RNN iterations (in general, the number of iterations needs to be greater than or equal to the longest path being tested for). 
This RNN cell does four things each iteration: 
With just these four steps, the network is capable of readily learning how to calculate shortest paths. 
The graph network is the key to this model’s capabilities. It enables it to compute functions of the graph’s structure. 
In the graph network each node n has a state vector S(n,t) at time t. We used a state vector of width 4. Each iteration, the node states are propagated to the node’s neighbors adj(n): 
The initial states S(n,0) are zero valued vectors. 
Two more pieces are required for this simple state propagation to be capable of shortest path calculations: a node state write and a node state read. 
Node state write 
The node state write is a mechanism for the model to add a signal vector to the states of particular node(s) in the graph: 
The mechanism begins by extracting words from the question, to form the write query q_write. This query will be used to select node states to add the write signal p to. 
The write query is generated using attention by index, which calculates which indices in the question words Q should be attended to (as a function of the RNN iteration id r, a one-hot vector), then extracts them as a weighted sum: 
The write signal is calculated by taking the RNN iteration id and applying a dense layer with sigmoid activation. 
Next the write signal and the write query are fed into an attention by content layer to determine how the write signal will be added to the node states. Attention by content is simply the standard dot-product attention mechanism, where each item is compared with the the query by dot product, to produce a set of scores. The scores are then fed through softmax to become a distribution that sums to one: 
In this instance, scores are calculated as the dot product of each node state’s associated node id with the write query. Finally, the write signal is added to the node states in proportion to the scores: 
Node state read 
Next, the state is read from the graph, in a similar fashion to how the signal was written. A read query is calculated from the input question words, again using attention by index: 
Then the read query is used to extract a value from the node states using attention by content. Like before, the read query is compared to each node’s id to create a score distribution: 
The final read out value is then computed using the weighted sum of the node states: 
The final important piece of the RNN is the output cell. It’s essential to the network’s success (removing the previous output look-back decreases accuracy to 95%). 
Here’s an overview of the output cell: 
The output cell has two parts: 
The output cell can combine outputs from earlier iterations with the current graph network output. This allows the cell to repeatedly combine previous outputs, offering a form of simple recursion. This also helps the network to easily look back to the output of an earlier iteration, regardless of total number of RNN iterations. 
The network’s hyper-parameters were experimentally determined. The learning rate was identified using the Learning Rate Finder protocol, and other parameters such as node state size, number of graph read/write heads and number of RNN iterations were determined through grid search. 
The network achieves 100% test accuracy after 9k training cycles (2 minutes on a MacBook Pro CPU). This fast convergence shows that the network has a strong inductive bias towards solving this problem. 
I’ve included a prediction-mode attention visualization to let you see what the network is doing. It shows where the read, write and output attention heads are focussed: 
The attention is used in mostly obvious ways: 
With any solution to a problem, it’s worth comparing it to other approaches. Here we compare this model to Differentiable Neural Computers and the standard classical approach. 
Compared to the classic approach, Dijkstra’s, this approach (and indeed most neural approaches) is less efficient: 
However, our approach has the one major benefit that it has the potential to learn different functions depending on the training examples. 
Compared to Differentiable Neural Computers our approach performs much better: 
As part of this work, we explored using a Gated Recurrent Unit (GRU) at each node as the node-state update function. This worked, however the extra training effort due to the increased parameters brought no benefit, so ultimately the GRU was disabled. We leave as future work using an extension of the presented architecture to learn different graph functions. 
An important tool in understanding the role of different parts of a neural network is ablation analysis, i.e. removing a piece and seeing how the network performs. A brief analysis was performed for this network (the network itself is the result of removing many pieces from a more complex network). In each case, the comparison is to the benchmarked 100% test accuracy. 
Reducing RNN iterations decreases accuracy as the network can no longer discriminate between paths longer than the iteration count (e.g. 3 iterations achieved 40% test accuracy, 7 iterations achieved 69% test accuracy. Note that for practical reasons, the classes are not fully balanced). 
Removing output cell look-back to previous RNN outputs reduced test accuracy to 95%. 
Thanks to Andrew Jefferson for the encouragement to write this article and for his input and reviews. 
Octavian’s mission is to develop systems with human-level reasoning capabilities. We believe that graph data and deep learning are key ingredients to making this possible. If you interested in learning more about our research or contributing, get in touch. 
Written by 
Written by",David Mack,2019-01-10T23:29:12.725Z
Understanding Recommendation Engines in AI | by Humans For AI | humansforai | Medium,"Written by Deepa Naik 
If you decide to conduct a study on consumer behavior in shopping and take a survey of “people who ‘do not’ enjoy shopping”, there will only a meagre percentage of them in the category ; however you take a headcount of “people who do not like to shop alone” and yes, your poll just changes drastically. Anyone who wants to shop, never ever wants to do it alone. This behavior of having “company” for shopping may on the outside just seem to be a characteristic of man as a social animal, but there is more to it than just that. 
Growing up we have always looked for the company for shopping. Just take shopping for clothes, for example, we have always asked for advice — be it your siblings as kids or your besties at college or colleagues at work. Shopping trips traditionally were hours or even day long trips — researching the latest fashion, driving the bargains across various shops and the try-out sessions. However as time progressed, shopping trips started becoming a short affair and the besties and friends were replaced by the more “professional” personal shopper — who in turn could give you good recommendations for “the look” and “the image — the corporate meeting look, the cocktail party et al. Times were changing… are changing but one thing was sure, you still wanted the recommendations. 
Currently, shopping trips have become even shorter and it just takes a few minutes and a few clicks on the internet. The recommendation and advice are coming as messages in emails or advertisements — exclusively tailored and personalized for you. The handbag that I shopped for the other day was less than a five-minute shopping trip — online. Generally, it takes me hours to choose a bag. This time I received some rather tempting recommendations of bags in my email and all I had to do was click and pay and wait for the delivery to happen. The catch here was that I had bought my earlier bags online and they knew exactly what I liked and didn’t. 
Understanding Recommendations Engine 
Recommendations Engines — one of the concepts in Artificial Intelligence is fast gaining momentum. It is a perfect marketer tool especially for e-commerce / online businesses and is very useful to increase turn around (sales, profits etc.) 
Recommendation Engines (also called as Recommender Systems) started off becoming popular in the retail industry, mainly in online retail/e-commerce for personalized product recommendations. One most common usage is for Amazon’s section on “Customer who bought this item also bought …”. Recommendation Engine is seen as an intelligent and sophisticated salesman who know the customer taste, style and thus can make more intelligent decisions about what recommendations would benefit the customer most thus increasing the possibility of a conversion. Though it started off in e-commerce, it is now gaining popularity in other sectors, especially in Media. Some of the examples are YouTube “Recommended Videos” or Netflix “Other Movies You May Enjoy”. Other industries are beginning to use recommendation engines, such as the transportation industry. Waze uses it for intelligent navigation systems; IBM uses it for traffic control systems. Lately, GE started a Kaggle competition to find the best routes to save energy for the airline industry. 
In their paper titled “Recommendation Systems: Principles, methods, and evaluation”, F.O. Isinkaye et al define Recommendation Engines / Recommender Systems as follows: 
“Recommendation Engines / Recommender systems are information filtering systems that deal with the problem of information overload by filtering vital information fragment out of a large amount of dynamically generated information according to user’s preferences, interest, or observed behavior about the item. Recommendation Engines / Recommender system has the ability to predict whether a particular user would prefer an item or not based on the user’s profile” 
Some examples of recommendation engine usage are seen in the following 
As we move into an era of data explosion, it is becoming more and more relevant to find ways to scan through the huge amount of data. Recommendation Engines become a great tool for filtering and ensure that the consumer gets to see the data that is relevant for his taste, his style and preferences and ensures he spends minimum time searching for the right data. 
E-commerce / online stores carry a large product listing. If you want to buy an item on Amazon, you will find the listing in thousands, not just a few hundreds. Out of this vast sea of products we want to ensure that we present the most appropriate and the most relevant recommendation to the customer. 
For a recommendation system to be good another important characteristic is it should be able to continuously learn and adapt itself flexibly to new user behavior. It also needs to be providing data real time. For example, a large number of special offers, changes in the assortments and price changes that happen make good recommendations obsolete shortly after having been made. A good recommendation engine must, therefore, be able to act in a very dynamic environment. 
Recommendation systems are based on algorithms that “learn” from past data. The data used maybe about the products preferred liked or bought by the customer in the past or it could be products preferred, liked or bought by “similar” customers. Based on this criterion the following types of recommendation engines are built. 
This is based on customer’s behaviors, activities or preferences and predicting what customers will like based on their similarity to others 
This is based on items liked by the customer and keywords used to describe the items. It also takes into consideration the preferences chosen by the customer 
These are becoming popular where the combination of both the methods listed above is used. There is a trade-off that needs to be made in what to filter. 
Developing models for product recommendation algorithms is a growing research area. This deals with a field of Artificial Intelligence called machine learning and related techniques. 
Recommendation Engine is your companion and advisor to help you make the right choices by providing you tailored options and creating a personalized experience for you. 
Currently, it is seen in online retail and media industries. It is catching up in transportation. Other industries where it is gaining fast acceptance is financial services (example: granular view of a customer can help augment existing fraud detection techniques), healthcare ( example: personalized health care by analyzing vast amounts of information regarding an individual such as patient history, electronic medical records, lifestyle information, etc. ). 
It is beyond a doubt that recommendation engines are getting popular and critical in the new age of things. It is going to be in the best interest to learn to use recommendation engines for businesses to be more competitive and consumers to be more efficient. 
In a nutshell, recommendation engines are a contemporary form of artificial intelligence at play. 
Reference: 
http://dataconomy.com/2015/03/an-introduction-to-recommendation-engines/ 
https://www.toptal.com/algorithms/predicting-likes-inside-a-simple-recommendation-engine 
http://www.sciencedirect.com/science/article/pii/S1110866515000341 
https://mapr.com/blog/recommendation-engines-driving-customer-interactions-next-best-action/ 
About the Author: 
Deepa is a founding member of Humans For AI, a non-profit focused on building a more diverse workforce for the future leveraging AI technologies. Learn more about us and join us as we embark on this journey to make a difference! 
Written by 
Written by",Humans For AI,2017-06-03T16:35:55.634Z
Biologically-Inspired AI: Genetic Algorithms | by James Le | Cracking The Data Science Interview | Medium,"There have been significant advances in recent years in the areas of neuroscience, cognitive science, and physiology related to how humans process information. This semester, I’m taking a graduate course called Bio-Inspired Intelligent Systems. It provides broad exposure to the current research in several disciplines that relate to computer science, including computational neuroscience, cognitive science, biology, and evolutionary-inspired computational methods. In an effort to open-source this knowledge to the wider data science community, I will recap the materials I will learn from the class in Medium. Having some knowledge of these models would allow you to develop algorithms that are inspired by nature to solve complex problems. 
Previously, I’ve written a post about optimization and local search algorithms. In this post, we’ll get a broad introduction to genetic algorithms. 
A genetic algorithm is a search technique used in computing to find true or approximate solutions to optimization and search problems. They are categorized as global search heuristics. They are also a particular class of evolutionary algorithms that use techniques inspired by evolutionary biology such as inheritance, mutation, selection, and crossover. Finally, they can be implemented as a computer simulation in which a population of abstract representations (called chromosomes or the genotype or the genome) of candidate solutions (called individuals, creatures, or phenotypes) to an optimization problem evolves toward better solutions. 
Traditionally, solutions are represented in binary as strings of 0s and 1s, but other encodings are also possible. The evolution usually starts from a population of randomly generated individuals and happens in generations. In each generation, the fitness of every individual in the population is evaluated, multiple individuals are selected from the current population (based on their fitness), and modified (recombined and possibly mutated) to form a new population. The new population is then used in the next iteration of the algorithm. Commonly, the algorithm terminates when either a maximum number of generations has been produced, or a satisfactory fitness level has been reached for the population. If the algorithm has terminated due to a maximum number of generations, a satisfactory solution may or may not have been reached. 
You can find a couple of key terms related to genetic algorithms below, along with their description: 
A typical genetic algorithm requires 2 things to be defined: 
A standard representation of the solution is as an array of bits. Arrays of other types and structures can be used in essentially the same way. The main property that makes these genetic representations convenient is that their parts are easily aligned due to their fixed size, that facilitates simple crossover operation. Variable length representations may also be used, but crossover implementation is more complex in this case. 
On the other hand, the fitness function is defined over the genetic representation and measures the quality of the represented solution. The fitness function is always problem dependent. 
For instance, in the knapsack problem, we want to maximize the total value of objects that we can put in a knapsack of some fixed capacity. A representation of a solution, in this case, might be an array of bits, where each bit represents a different object, and the value of the bit (0 or 1) represents whether or not the object is in the knapsack. Not every such representation is valid, as the size of objects may exceed the capacity of the knapsack. The fitness of the solution is the sum of values of all objects in the knapsack if the representation is valid, or 0 otherwise. In some problems, it is hard or even impossible to define the fitness expression; in these cases, interactive genetic algorithms are used. 
A visual display of the fitness function in genetic algorithms is shown below. 
The most common type of genetic algorithm works like this: 
Let’s walk through a general framework for genetic algorithm based on the basic steps above. 
Initially many individual solutions are randomly generated to form an initial population. The population size depends on the nature of the problem but typically contains several hundreds or thousands of possible solutions. Traditionally, the population is generated randomly, covering the entire range of possible solutions (the search space). Occasionally, the solutions may be “seeded” in areas where optimal solutions are likely to be found. 
During each successive generation, a proportion of the existing population is selected to breed a new generation. Individual solutions are selected through a fitness-based process, where fitter solutions are typically more likely to be selected. 
Certain selection methods rate the fitness of each solution and preferentially select the best solutions. Other methods rate only a random sample of the population, as this process may be very time-consuming. Most functions are stochastic and designed so that a small proportion of less fit solutions are selected. This helps keep the diversity of the population large, preventing premature convergence on poor solutions. Popular and well-studied selection methods include roulette wheel selection and tournament selection. 
The next step is to generate a second generation population of solutions from those selected through genetic operators: crossover and/or mutation. 
For each new solution to be produced, a pair of “parent” solutions is selected for breeding from the pool selected previously. By producing a “child” solution using the above methods of crossover and mutation, a new solution is created which typically shares many of the characteristics of its “parents.” New parents are selected for each child, and the process continues until a new population of solutions of appropriate size is generated. 
These processes ultimately result in the next generation population of chromosomes that is different from the initial population. Generally, the average fitness will have increased by this procedure for the population, since only the best organisms from the first generation are selected for breeding, along with a small proportion of less fit solutions, for reasons already mentioned above. 
3.1 — Crossover 
The most common type is single point crossover. In single point crossover, you choose a locus at which you swap the remaining alleles from one parent to the other. This is complex and is best understood visually. 
As you can see above, the children take one section of the chromosome from each parent. The point at which the chromosome is broken depends on the randomly selected crossover point. This particular method is called single point crossover because only one crossover point exists. Sometimes only child 1 or child 2 is created, but often times both offspring are created and put into the new population. 
Crossover does not always occur, however. Sometimes, based on a set of probability, no crossover occurs and the parents are copied directly to the new population. The probability of crossover occurring is usually 60% to 70%. 
3.2 — Mutation 
After selection and crossover, you now have a new population full of individuals. Some are directly copied, and others are produced by crossover. In order to ensure that the individuals are not all exactly the same, you allow for a small chance of mutation. 
You loop through all the alleles of all the individuals, and if that allele is selected for mutation, you can either change it by a small amount or replace it with a new value. The probability of mutation is usually between 1 and 2 tenths of a percent. Mutation is fairly simple. You just change the selected alleles based on what you feel is necessary and move on. Mutation is, however, vital to ensuring genetic diversity within the population. 
This generational process is repeated until a termination condition has been reached. Common terminating conditions are: 
The very basic pseudocode of genetic algorithms follows: 
Lastly, let’s discuss the benefit of using genetic algorithms in comparison to that of Differential Evolution. 
Most symbolic AI systems are very static. Most of them can usually only solve one given specific problem since their architecture was designed for whatever that specific problem was in the first place. Thus, if the given problem were somehow to be changed, these systems could have a hard time adapting to them, since the algorithm that would originally arrive at the solution may be either incorrect or less efficient. 
Genetic algorithms were created to combat these problems; they are basically algorithms based on natural biological evolution. The architecture of systems that implement genetic algorithms is more able to adapt to a wide range of problems. 
A GA functions by generating a large set of possible solutions to a given problem. It then evaluates each of those solutions and decides on a “fitness level” for each solution set. These solutions then breed new solutions. The parent solutions that were more “fit” are more likely to reproduce, while those that were less “fit” are more unlikely to do so. In essence, solutions are evolved over time. This way you evolve your search space scope to a point where you can find the solution. Genetic algorithms can be incredibly efficient if programmed correctly. 
If you’re interested in this material, follow the Cracking Data Science Interview publication to receive my subsequent articles on how to crack the data science interview process. 
— — 
If you would like to follow my work on Computer Science and Intelligent Systems, you can check out my Medium and GitHub, as well as other projects at https://jameskle.com/. You can also tweet at me on Twitter, email me directly, or find me on LinkedIn. Or join my mailing list to receive my latest thoughts right at your inbox! 
Written by 
Written by",James Le,2020-05-10T23:12:10.429Z
Real-Time Anomaly Detection for Cognitive Intelligence | by Xenonstack | XenonStack AI | Medium,"Classical Analytics — Around ten years ago, the tools for analytics or the available resources were excel, SQL databases, and similar relatively simple ones when compared to the advanced ones that are available nowadays. The analytics also used to target things like reporting, customer classification, sales trend whether they are going up or down, etc. In this article, we will discuss Real-Time Anomaly Detection. 
As time passed by the amount of data has got a revolutionary explosion with various factors like social media data, transaction records, sensor information, etc. in the past five years. With the increase of data, how data is stored has also changed. It used to be SQL databases the most and analytics used to happen for the same during the ideal time. 
The analytics also used to be serialized. Later, NoSQL databases started to replace the traditional SQL databases since the data size has become huge and the analysis also changed from serial analytics to parallel processing and distributed systems for quick results. 
Cognitive Analytics — In the contemporary world, the analytics mainly target at predictions with impeccably high accuracy trying to be as close to human understanding as possible, basically trying to mimic machines with the Cognitive intelligence that humans have. This analysis should be accurate, fast and with constant Learning. The output is expected in real time and also predict future events. 
Cognitive Computing helps in accelerating human intelligence by human Learning, thinking and adaptivity. By moving towards the machine capacity, it not only helps in augmenting the human potential; instead, it will increase the creativity of the individual and create new waves of innovations. The key areas of capability are - 
Sensory Perception 
Machines are enabled in such a way that they can stimulate the senses of humans such as smell, touch, taste, and hearing. Therefore, they are developed in terms of machine simulation such as visual and auditory perception. 
The deduction, Reasoning, and Learning 
In this case, the machines are simulated with human thinking for decision making. Therefore various technologies such as machine learning, Deep Learning, and neural networks are deployed as a system to intelligence to extract meaningful and useful information and apply the judgment. 
Data processing 
In this larger dataset is accessed to facilitate the decision-making process and provides practical suggestions. Therefore, hyperscale-computing, knowledge representation, and natural language processing togetherly provide the required processing power to enable the system for engaging in real-time. 
The purpose of cognitive computing is to create the frame for computing such that complex problems are solved easily without human intervention. Features are listed below - 
Adaptive 
It is one of the first steps in developing the machine learning-based cognitive system. The solution imitates to adapt the human ability with the Learning from the surroundings. It is dynamic for data gathering, understanding goals, and requirements. 
Interactive 
The cognitive solution should dynamically interact bidirectionally in nature with each element in the system such as processes, devices, users and cloud services. The system can understand human input and provides the results using natural language processing and deep learning models. 
Iterative and Stateful 
The system should be able to learn from previous iterations and ready to return the information which is specifically crucial at that time. The system must follow data quality and visualization methodologies so that it provides enough information and the data sources can operate the reliable and updated data. 
Contextual 
The system should be able to understand, identify and even extract the contextual elements from the data such as meaning, syntax, time, location, task, goal and many more. The system removes the multiple sources of information like structured, sensor inputs, unstructured and semi-structured data. 
Cognitive applications use deep Learning and neural network algorithms to control technological applications such as data mining, pattern recognition, and natural language processing. 
The system gathers a variety of information and processes it with the previous report it already knows. After the completion of data analysis, it integrates with the adaptive page displays to visualize the content for specific audiences at specific situations. 
Cognitive Computing extends the level of analytics at next level using new technologies. It is used when the vast corpus of textual data is available as a free text document. Therefore, cognitive Computing is used to analyze these documents to support a wide range of activities. Cognitive system supports the training of model using data containing a large set of training examples like a train the model using an extensive collection of questions with their corresponding answers. 
While being busy trying to analyze data sometimes, it ends up with unexpected occurrences otherwise called as ANOMALIES. An “Anomaly” means abnormal or unexpected behavior or deviation from a regular trend. 
What are the Anomalies that are encountered in daily life? 
Earlier, anomalies seldom occur. If the anomaly is not detected and rightful actions are not taken, soon the consequences may prove to be costly in situations like Network intrusion, change in log patterns, data leak, fraud transactions, Insider trading and many more. Just imagine the loss that could incur if any of the lists mentioned above occurs. 
The anomaly detection in docker runs at the same level as the docker daemon and keeps track of the events. When an event informs that the new container has started, the anomaly detection also begins simultaneously. The anomaly detection algorithm queries the daemon to find the process IDs that are running inside a container. Then the syscalls are recorded using the process ‘ID’s, root privileges and place and are sent to Redis queue using message service. The behavior is analyzed by the anomaly detection process to send notifications to the administrators. 
Hadoop is one of the widely used big data platforms among various industries and business. With the increase in usage, the data availability has also increased so is the necessity for detecting anomalies in Hadoop clusters. Traditionally, the rule-based models are used for alerting the occurrence of anomalies using domain knowledge and experience about the target system. The problem occurs when hidden patterns are not able to explore where the anomalies lie in business problems where the scope of domain knowledge is less. For this purpose pattern recognition techniques DBSCAN and PCA are used to find out the anomalies without any prior experience. 
Internet of things in simple terms is the connection between everyday usage devices with internet. So, once the device is connected with internet access to the data of that device is obtained. Essential tools that require anomaly detection techniques are the ones used in industries and business organizations which has sensors. Data has been received continuously with every passing second through these sensors. Notably, in the maintenance of the systems, the sensors have to be monitored to predict the anomaly. These predictions have high economic value because in IoT as multiple things are interlinked. 
While working using immense data, the following things are required? 
What are the features that are responsible for the anomaly? 
What transformations should be made on these features to detect patterns? 
What patterns signify anomaly? 
To explain with a simple example, ‘let’s consider a sensor gives temperature values of special equipment in an industry. Change in sensor values is used to know if the equipment is stable or about to fail. Tracking is done on the statistical measures mean and standard deviation of the temperatures over some time. If there are changes, that is a shift in a mean or considerable fluctuations in standard deviation values, there is something wrong with the equipment, and immediate action is required. This notification is sent as an alert. With the advances in technology, multiple machine learning and statistical techniques are used to identify and predict anomalies accurately. The significant advantage is once the complete system is automated one need not always keep track of equipment to know if everything is okay or not. 
Deep Learning basically can be thought of as an extension to Artificial Neural Networks(ANN). An artificial neural network consists of input nodes which are passed to a series of hidden layers, and an activation function is used which signifies the signal strength that is supposed to be sent to connected nodes. The output is the result of a classification or regression problem. The model uses the result and the error to learn and update the model by changing the parameters involved. Deep Learning is a complicated case which includes mathematical concepts like matrix algebra, probability, etc. and intense hardware resources. The model training with Neural networks and deep Learning happens using an objective function like stochastic gradient process. The predict the value, calculate the error and update the model to reduce the error. The implementation of neural network and deep Learning can be done using Keras, TensorFlow which are open source and has libraries in python. 
Recurrent neural networks store information in the hidden layers which are updated when new data is feed to the network. These can be used in time series. The applications of RNN are in handwriting recognition, speech recognition, log analysis, anomaly detection, etc. So, ‘RNN’s are used to find out anomalies in any of the mentioned use cases. 
Well, if everything goes on fine and let us assume, a reasonably good anomaly detection system is developed, should this be it? NO. It should also be able to determine what are the factors responsible for the anomaly. The challenge is what if the data is enormous? Across so many fields/variables, which fields are responsible for an anomaly? To find this is an essential task because it helps in taking immediate action to reduce the loss if the root cause is found out. This analysis to find out the reasons for anomalies is called correlation analysis. 
Once the correlation is done, the next step is to predict the potential anomalies that may occur in the future. So, how to predict future anomalies? One of the methods is to use Bayesian classification and Markov models. If a component with various features is given and if it is failing any function, ends up failing the element is an anomaly. Based on the trends of the features Markov model helps in identifying the future values up to some k timestamps, and Bayesian method is used to predict the probability of anomaly symptoms. 
The feature “A” is expected to fail after the timestamp “T” which results in failing up of component “C.” If feature A is repaired within timestamp T, it will end up making the component “C” running as usual. 
If it is unable to predict the reason initially and if the component fails, the loss is - 
Instead, one should be careful enough in predicting the failure of any feature, by taking several actions before the occurrence. Hence, instead of monitoring every component in the system and resolving the issues, optimization is necessary followed by the automation of this process. Improving the steps from finding out anomalies through to preventing anomalies. 
Supervised Anomaly detection — In this approach, historical data is used which says data points and class defining if each position is abnormal or not. It is similar to the classification problem. So, the class variable as the anomaly column is taken and applied with the models such as “Random Forest,” “XGB,” “SVM” or regression algorithms to train the data. This model is used to the new data point to know if it is an anomaly or not. One should be careful about the ratio of an anomaly to no-anomaly in the dataset. It ‘shouldn’t be too high — for example, more than 1:10 since it becomes a class imbalance problem. 
Unsupervised Anomaly detection — Clustering techniques are applied in this as it is not known before if a data point is anomaly or not. So, clustering algorithms are used to detect anomalies. 
K-Means clustering — This algorithm is required to be given with the number of clusters to be formed as an initial input, based on that value the algorithm provides the same amount of clusters as an output. At present, by this process, it will consider all the data points and forms clusters. Restrict the distance of the boundary from the centroid from each cluster. The restriction can be 95 percentile or 99 percentile based on the requirement. The points outside this range after the clustering process is done are the anomalies. 
DBSCAN — This is also a clustering algorithm which is different from k-means. In DBSCAN minimum points and maximum distance as parameters are chosen. The algorithm initially starts with a random point and select the locations in the Ɛ neighborhood and links them. Each of those will continue the same process. With this, the patterns are, and all the possible clusters are formed. The leftover points are considered as anomalies. 
In the case of Supervised Algorithm, the data should be trained already, and the model can detect only that kind of anomaly which is learned by it before. Therefore, this algorithm is not feasible for detecting all types of anomalies. So, Unsupervised Algorithms can be used instead of supervised algorithms. This can identify any anomalies from data, i.e. the anomalies that are never seen before. 
Log Data is a time series of data. So, data point being an anomaly depends on time. A particular value might be an anomaly at one-time stamp but not in another and vice-versa. A lot of information generated by the server like memory usage, CPU usage, read-write operations, power consumption, etc. 
Let’s consider power consumption usage to detect if there is any abnormal behavior. 
The basic idea is that firstly get the data to look at the extreme values. Normal distribution may be considered and look for absolute values and conclude them as anomalies. So, based on a mean and standard deviation of the benefits of the memory usage, get the points which are beyond the 90% or 95% or 99% range based on our requirement and conclude them as anomalies. In that case, all the values above 210 as usage values are considered as anomalies. Let us look at the plot for the same data. But is this explanatory enough for time series data? 
Moving averages are used assuming that there ‘wouldn’t occur sudden changes with time. If there is one, then it is an anomaly. To explain the moving average in simple terms, usage value at the timestamp should be close to the average usage of past few timestamps. The number of timestamps to take into consideration depends on the data. This technique is called a simple moving average. In this technique, the latest points are as important as the old ones. 
Another technique is the exponential moving average. In this technique, the latest points are given more importance in the analysis that the old ones. 
St+1 = α*yt+(1−α)*St , 0 < α ≤ 1 , t > 0. 
Neither techniques are superior. The method of being used depends upon the problem and the data. The point to consider here is exponential moving average is sensitive to the latest points but not the simple moving average. 
Sometimes one get misled by the data by ignoring the specific scenarios which seems anomaly but not. For instance, on an off day, when there are no employees the data is expected to be low, during such scenarios, two problems are analyzed .- 
To deal with such incidents, impute the values of the off day with the expected benefits assuming the day was not off and continue the analysis. This way precision and recall could be adjusted. 
In specific use cases anomaly detection has to work in real-time. As soon as the anomaly is detected several measures can be taken to mitigate the loss. The techniques used in real-time anomaly detection have to evolve with time. Static methods based on an existing training data which was formed taking an actual sample may not serve the purpose of fundamental discoveries. ‘That’s because data changes are fast with immense volume and accordingly the models have to learn from data for rightful predictions. The actions that are carried out for solving the problem of anomaly can be delayed but the detection of an anomaly in real time cannot be missed. This is because the data containing the anomaly can consist of information that can further lead to loss or gain in business. 
For building the real-time anomaly detection platform following are the requirements 
Capability to correlate metrics across nodes, VMs, containers, and applications, capacity planning and proactive Monitoring, ability to generate alerts, notifications, and reports are must-haves in a monitoring solution of such caliber, and any solution with all the above capabilities will surely see widespread adoption 
Architecture’s structure can be based on five critical pillars. 
They are Collection of data, Aggregation of data, Visualization, Alerts/Notifications, and Exportation. And Analytics Many of the challenges mentioned at the beginning of the article are remedied by this approach and should be present in the ideal next-generation monitoring tool. 
The challenges faced by visualization are taken care of a well thought out UI which consumes the API exposed by the solution and provides a single integrated dashboard and a different metrics store for the answer. It should also offer a unique set of APIs for monitoring host, virtual machines, containers, and applications. The five pillars also improve the usability in an ideal solution, which should come packaged with scripts that make it easy to set up on a single workstation and also have a rich set of APIs for on-boarding, catalog, labels, availability, metrics, dashboards, and exporter. 
For tackling challenges brought on by next-gen technologies such as containers, support must be extended to agents for host, containers, and applications. Next generation of elastic infrastructure is continuously moving; hence it is imperative to have a solution which can continually discover new deployment units, which requires a service discovery mechanism. Capability to statically as well as dynamically onboard nodes, containers and applications and an out of the box support for regular expressions which can be utilized by the end user to get information from a range of servers, containers and applications lend a futuristic outlook to the solution.To know more about Anomaly Detection we recommend taking the following steps - 
Originally published at https://www.xenonstack.com on May 8, 2019. 
Written by 
Written by",Xenonstack,2020-05-20T12:01:06.851Z
Types of Distance Metrics in Machine Learning. | by Priscila Tamang Ghising | Medium,"As we all know that Machine Learning, Deep Learning algorithms are some of the buzzwords that we can not avoid in the industry. In fact, its impact is growing day by day. Since AI has the potential to vastly change the way that humans interact, not only with the digital world. but also with each other. Machine Learning being a sub-area of artificial intelligence helps to generate artificial knowledge and helps to develop adequate solutions on the basis of its algorithms. 
Machine Learning undoubtedly being one of the greatest tool, helps people to work more creatively and efficiently. It has proved to be a greatest tool since it offers the possibility to identify the error early which saves time and money. We all are aware about the different types of machine learning like -Supervised, -Unsupervised, -Partially supervised, -Active learning. We are also aware about different types of methods used like -Statistical, -Mathematical methods and many more. 
So, today in this article i am going to discuss about different types of distances used in machine learning. We all know that distance plays an important role in our daily lifestyle and so many things depend on the distance. Similarly, distance also plays an important role in machine learning. Distance is the key part of the machine learning algorithms. They are used in many machine learning algorithms like supervised, k-nearest neighbors, K-means clustering and many more. 
“Distance metric uses distance function which provides a relationship metric between each elements in the dataset.” 
Lets understand what exactly it means. As well know that we use Machine Learning algorithms for classifying or recognizing images and for retrieving information through an Image’s content. For example — Face recognition, Censored Images online, Retail Catalog, Recommendation Systems etc. So, what exactly does distance metric do is that it helps algorithms recognize similarities between the contents and provide the relationship between each elements. Choosing a good distance metric will improve how well a classification or clustering algorithms performed. 
Therefore, A Distance Metric employs distance functions that tell us the distance between the elements in the dataset. 
We all are aware about the famous Pythagorean theorem! Where we calculate the distance between two data points using this theorem and the formula is like: 
So machine learning algorithm uses this formula as a distance function. Therefore a distance function provides distance between the elements of a set. If the distance is zero then elements are equivalent, if the distance is small, the elements are likely similar and if the distance is large, the degree of similarity will be low. 
There are several distance metrics used and it is very important to chose then appropriate one to avoid errors and misinterpretations. So the different types of distances are: 
Euclidean Distance represents the shortest distance between two points. Let’s say if we want to calculate the distance between two cities then we generally think about the number of kilometres we have to drive which is considered as a straight line. 
Figure to understand it in more detail and the formula is as follows: 
For n-points, the general formula is as follows: 
Which is also Minkowski Distance formula by setting p’s value to 2. 
Let’s say ((1,2,3) (4,5,6)) are two sample points and we have to calculate the Euclidean distance between these points. 
Point_1= (1,2,3) 
Point_2= (4,5,6) 
Point_1, Point_2 
euclidean.distance = distance.euclidean(point_1, point_2) 
print(‘Euclidean Distance b/w’, point_1, ‘and’, point_2, ‘is: ‘, euclidean_distance) 
Euclidean distance between these points is: 5.19615. 
Manhattan Distance is the sum of absolute differences between points across all the dimensions. Let’s say if we want to calculate the two data points in a grid like path then we use minkowski distance formula to find manhattan distance by setting p’s value as 1. It is also known as city block distance. 
Figure to understand it in more detail and the formula is as follows: 
Distance is calculated using an absolute sum of differences: 
where xi and yi are the variables of vector x and y like x=(x1,x2,x3) and y=(y1,y2,y3). So, the distance is : (x1-y1)+(x2-y2)+(x3-y3)…..+(xn-yn). 
manhattan_distance = distance.cityblock(point_1, point_2) 
print(‘Manhattan Distance b/w’, point_1, ‘and’, point_2, ‘is: ‘, manhattan_distance) 
Manhattan distance between these points is: 9 
It is basically the generalized form of Euclidean and Manhattan distance. Before getting into Minkowski distance. Lets try to understand the three important words that is related to Minkowski distance. They are “Normed vector space”, “Norm”, “Vector space”. 
Minkowski distance is a metric in normed vector space. So, what exactly is normalized vector space, norm and vector space? 
A normed vector space is nothing but a vector space of real and complex numbers on which norm is defined. 
A vector space is nothing but a set of vectors that is added together and multiplied by numbers. 
A norm is basically a real valued function defined on the vector space which means it assigns only positive length to each and every vector. It has following properties: 
Figure to understand it in more detail and the formula is as follows: 
The general formula is: 
Since, its a generalized formula we can change the value of p and calculate the distances in 3 different ways like: 
minkowski_distance = distance.minkowski(point_1, point_2, p=3) 
print(‘Minkowski Distance b/w’, point_1, ‘and’, point_2, ‘is: ‘, minkowski_distance) 
Minkowski distance between these points is: 4.326748. 
Hamming Distance measures the similarity between two strings of the same length. Where the distance between two strings of the same length is the number of positions at which the corresponding characters are different. It is also referred as binary strings or bitstrings as it calculates the distance between two binary vectors. 
Lets try to understand in more detail through example. Let us assume two strings. 
“HANDY” and “CILDY” 
Since, the length of these strings are equal, we can calculate hamming distance. Now, we will try to match the strings by going through character by character. The first character of both the strings are (H and C) and is different. Similarly, the second character of both the strings are (A and I) and is again different and it goes on till we reach the fourth and fifth character of both the strings where the character of both the strings are (D and D, Y and Y) and are same. Here, 3 characters are different and the last two characters are same. Therefore, the hamming distance here will be 3. 
The general formula is: 
string_1=’handy’ 
string_2=’cildy’ 
hamming_distance = distance.hamming(list(string_1), list(string_2)*len(string_1) 
print(‘Hamming Distance b/w’, string_1, ‘and’, string_2, ‘is: ‘, hamming_distance) 
Hamming distance between these two strings is: 3. 
Cosine similarity is a metric used to measure how similar the documents are irrespective of their size. It is defined to equal the cosine of the angle between them. The smaller the angle, higher the cosine similarity. The cosine similarity helps overcome the ‘count-the-common-words’ or Euclidean distance approach. 
Cosine similarity formula can be derived from the equation of dot products. 
As we know “Cos0=1”, “Cos90=0” and “Cos180=-1”. Now let’s understand what these values refer to and how is it related to finding out the similarities. 
A norm is nothing but the total length of all the vectors in a space. Norms are used to express the distances. Higher the norm, bigger the vector. Norm may come in many forms and many names, like Euclidean distance, Mean-squared Error, etc. 
Now, there are different ways to calculate the magnitude of the vectors. In this article we will focus on L1,L2 and Lp norms. 
The Lp norm of N-dimensional vector is defined as: 
It is nothing but the sum of the magnitudes of the vectors in a space. Commonly known as Manhattan distance. It is calculated as the sum of the absolute values of the vector or as the sum of absolute difference of the components of the vectors. 
Say, we have a vector X=[1,2]: 
Then it is calculated as: 
||X||1 = |a1| + |a2| where a1=1 and a2=2 
||X||1 = |1| + |2|= 3 
Therefore, ||X||1 = 3 
It is nothing but the shortest distance between two points. Commonly known as Euclidean distance. It is calculated as the square root of the sum of the squared vector values. The result is always the positive distance value. 
Say, we have a vector X=[3,4]: 
Then it is calculated as: 
||X||2 = sqrt(a1² + a2² ) where a1=3 and a2=4 
||X||2 = sqrt(3² + 4² ) = sqrt(9+16) = sqrt(25) = 5 
Therefore, ||X||2 = 5 
In this article, we came across few popular distance/similarity metrics and how these are used to solve complicated machine learning problems. 
Written by 
Written by",Priscila Tamang Ghising,2020-06-06T19:05:08.352Z
"Real-time Anomaly Detection in VPC Flow Logs, Part 1: Introduction | by Igor Kantor | Medium","Two things are all the rage these days — cloud and machine learning. 
Personally, I have been using Amazon Web Services for so long that it feels like a second home. When it comes to Machine Learning (ML), however, I feel like a Stranger in a Strange Land. 
Therefore, I decided to write a new series — one that uses what I know (AWS) as a conceptual bridge to what I am trying to learn (ML). 
And because my ML skills are still weak, this series will be more exploratory in nature. For example, rather than tell you exactly how to do something based on my experience, I will use these posts to showcase my attempts, stumbles and all. 
That said, let’s get started! 
I think a good way to learn something new is to try to solve a problem that you know is solvable in principle but you don’t know exactly how to solve it. 
So, when a friend forwarded me a video from re:Invent 2017, titled “Real-Time Anomaly Detection Using Amazon Kinesis” I got super excited. Finally, here is a chance to learn something new, using something I already know! 
Sadly, the video is very light on details. While it gives a good overview of the Kinesis product offerings, the presenter says nothing on how to actually build any of that stuff. That is where we come in! 
NOTE: if you need a primer on what VPC Flow Logs are, Amazon documentation does a good job explaining it: 
VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC. 
Accordingly, we are going to sit down and build a “Real-Time Anomaly Detection” pipeline using Amazon Kinesis! Again, the documentation does a really good job explaining the concepts (and limitations!) of Kinesis. For now, it is enough to say that, 
Amazon Kinesis makes it easy to collect, process, and analyze real-time, streaming data so you can get timely insights and react quickly to new information. 
Perfect! 
Now, let’s see where this fits into a bigger picture by looking at a proposed architecture in Part 2. 
Written by 
Written by",Igor Kantor,2018-02-12T18:29:25.403Z
Machine Learning — Latent Dirichlet Allocation LDA | by Jonathan Hui | Medium,"Public opinion dominates election results. Unfortunately, with the proliferation of social media, public opinion can be manipulated by technology. For example, a policy writer of a candidate can collect billions of tweets (and posts) to analyze voter sentiments. Instead of pre-categorized the topics, the policy writer wants the tweets to self-identify what are the important topics that voters care. 
LDA is usually one of the last topics taught in an ML class. Maybe, it is too close to the holiday break, the topic is usually covered in a breeze. In reality, it is important but involves so many concepts that it is hard to cover in a short time. In this article, we will dig deeper into those theories and the math involved. 
None of the tweets will explicitly say what topics they are. In fact, a single tweet may be related to multiple topics and a single word may imply different topics. Our objective is grouping words in the tweets into K topics (say 4) and rates the likelihood of each word for each topic. But remember that a word may belong to multiple topics. From the ML perspective, we group words based on the criteria that they should frequently appear in the same tweet. After the grouping, we review words that belong to the same group. From there, we may realize what the topic is. 
This trained model can be repurposed as a generative model that generates a document by itself. Say, we have a joint probability model for the topic proportions of a document. For example, p(T₁=0.7, T₂=0.3, T₃=0, T₄=0) indicates the chance that a document has 70% of words related to topic T₁ and 30% of words related to topic T₂. 
And, for each topic, we have a list of words with its occurrence likelihood. However, we will not include un-important words, like those stop words “a”, “the”, “and” etc…, in the list 
To generate a new document, we sample from p to get T₁’, T₂’, T₃’, T₄’. These values add up to one and represent the ratio of the topics that the new document should cover. For each word in the document, we sample a topic Tc using the probability value from T₁’, T₂’, T₃’, T₄’. Then, we choose a word according to the occurrence likelihood in the chosen topic Tc. 
The generated document is unlikely grammatically sound. But throw in the language model and more complex models in selecting words based on previous selections, we may not be too far from replacing a human policy writer from a machine. Indeed, the state of the art technology in the generative model produces very realistic articles. 
Let’s get into a few more examples. Here is a visualization on the possible topics for the Star Wars Wikipedia page. To my surprise, Star Wars is about politics as much as entertainment. 
Here is another analysis which it finds the frequent topics that a Wikipedia page relates to. 
The second example below models the topics on the NY Times articles. The following are the 10 topics that it discovered with the top 10 words associated. If we examine them closely, it will not be hard to discover what these topics are. 
Topic modeling is mainly unsupervised learning that categorizes, organizes, tags and generates information. So given a corpus, we want to find the distribution of words for each topic. But we are dealing with a chicken and egg problem. We don’t know the topics a document may cover. But we expect them to cover a small set of topics and each topic should have the smallest possible set of words. But these objectives can be contradictive. When parsing all the documents, we may find a cluster of words in a set of documents to identify a potential topic. But how do we identify this set of documents? There are too many moving parts. Changing one will change the other. We need an algorithm that can solve this dilemma. Let’s frame this topic modeling technically. 
Each topic is simply a distribution over words. Each document contains a mixture of topics and words are drawn from these topics. 
Let’s imagine how a generative model produces an article discussed before. But first, let’s talk about the Dirichlet distribution. We don’t need to dig too deep in the math but it will be nice to know what it does. Dirichlet distribution is defined as: 
where Τ is the gamma function. For a joint probability with m variables, the output of Dirichlet is m-dimensional and takes m parameters to model it. For example, the model p(x₁, x₂, x₃, x₄) will have model parameters α₁, α₂, α₃, and α₄. In LDA, we model both word distribution for each topic and topic proportion for each document using Dirichlet distributions. For example, by learning αᵢ, we learn the topics proportion that a document may cover. The higher the value for αᵢ relative to others, the more likely the topic i will be picked. 
In a Dirichlet distribution, the sampled value from p is a probability distribution (e.g. x₁=0.7, x₂=0.2, x₃=0.1, and x₄=0) which always sums up to one. That is why we call Dirichlet distribution a distribution of distributions. There is a special case in the Dirichlet distribution called symmetric Dirichlet distribution. All αᵢ will be equal and therefore, we use a single scalar α in representing the model. As the value of α decreases, the sparsity increases, i.e. most values in a sampled data will be zero or close to zero (e.g. x₁=0.9, x₂=0, x₃=0.1, and x₄=0). 
This sparsity can be parameterized in any Dirichlet distribution, not just the symmetric Dirichlet distribution. It allows us to control the sparsity of topics proportions or word distributions. For example, it allows us to build a model that a document should cover a small number of topics only (high sparsity). 
Below are the steps in generating a document. In step 1, we have a V-dimensional Dirichlet distribution for the word distribution for each topic (βk). V is the size of the vocabulary and we have K topics. In step 2(a), we draw a topic proportions for each document. In step 2(b), we sample a topic assignment for each word and we draw a word from the word distribution for the corresponding topic. 
Like other ML modelings, we want to infer the joint probability given our observations, 
We infer the hidden variables or latent factors θ, z, β by observing the corpse of documents, i.e. finding p(θ, z, β | w). 
But before submerging ourselves into equations and models, let’s have a big picture of what we are doing. For each document, we keep track of the topic proportions θ and the most likely topic assignment z for each document word. In step 3 to 8, we fix the word distribution β for each topic. We find what is the most likely topic proportion θ and z for each document word. 
Then in step 9, we turn the table around and aggregate the results from all documents to update the word distribution β for each topic. Since θ, β and z are interdependent, we cannot optimize them all at once. Therefore we optimize one variable at a time while holding the other variables fixed. That is the alternating optimizing step of 5, 6, and 9 above. Even this may lead to a local optimal or a suboptimal solution, the generated solutions are often high in quality. 
However in LDA, θ, β and z will be modeled with probability models instead of a point estimate which only hold the most likely estimate only. These models keep track of all likelihood and its certainty. So we keep richer information in each iteration. We can visualize LDA as 64-bit double-precision floating-point operations while a point estimate truncates all the values into integers for every iteration. Technically, the probability model makes training easier and more stable. But this makes the math very hard. 
Let’s use a Graphical model to demonstrate the variable dependency. For example, we link Z (the topic selection) and β (the word distribution of a topic) to W (topic selection of the word) to show W depends on Z and β. 
(The equations are originated from different sources. Even they may come from the same author, slightly different notations or indexing are used. Be aware.) 
The joint probability modeled by the Graphical model is: 
Topic parameter η and proportions parameter α are treated as some prior knowledge on the word distribution and the topic proportion distribution respectively. There are different approaches to model or choose η and α. But for our discussion, we stick with a common approach in treating them as hyperparameters that can be chosen heuristically. The topic parameter η discussed here is a scalar parameter for the Dirichlet distribution which influences the sparsity of the Dirichlet distribution. For the proportions parameter α, we will use a vector. From some perspective, these parameters allow a human expert to influence the Dirichlet distribution (using expert knowledge or empirical experiment) rather than learning it purely from the data. These parameters act as prior to the posterior calculation. 
Using this joint probability, we infer the distribution of the hidden variables (β, θ, z) given the evidence w. 
However, inferring a posterior is hard in general. Integrating over variables β, θ is intractable. They depend on each other and it is NP-hard. 
So we are going to approximate the posterior p(β, θ, z|w) with distribution q(β, θ, z) using variance inference. The key concept of variance inference is approximate p with q using some known families of distribution that is easy to model and to analyze. 
Then, we train the model parameters to minimize the KL-divergence between q and p. 
Mean-field variational inference 
However, it remains extremely hard to model q for a high dimensional distribution. To reduce the complexity further, we make bold independence assumptions similar to the Graphical model and break down the joint probability into independent subcomponents. 
Here is a more detail description for each sub-components. 
Here, instead of modeling a joint probability for multiple variables, we model each variable independently, i.e. each variable vᵢ will be model as qᵢ(vᵢ|ρᵢ) with a specific distribution family that found appropriate, e.g. using a multinomial distribution for the topic assignment. 
This is called the Mean-field variational inference which breaks up the joint distribution into distributions of individual variables that are tractable and easy to analyze. 
In real life, many independence assumptions may be false. But we should view it as imperfect rather than wrong. Indeed, empirical results often demonstrate good quality results. 
As mentioned before, to optimize inter-dependent variables, we separate them into groups with variables independent of each other in each group. In LDA, we have the parameters modeling the topic assignment and the topic proportions separated. 
The most difficult steps are step 5,6 and 9. So, let’s see how we model p with q by minimizing their KL-divergence. As shown in a previous article, it is not easy to optimize KL-divergence directly. So let us introduce the Evidence lower bound (ELBO) below: 
Let’s evaluate the relationship between the KL-divergence and ELBO. 
Since KL-divergence is always positive, log Z is always greater or equal ELBO. ELBO is the lower bound of the log evidence (log Z) for any q. However, when q equals p, log Z and ELBO have the same value, i.e. by maximizing ELBO, we are minimizing KL-divergence. 
When minimizing ELBO, we don’t need Z. No normalization is needed. In contrast, KL’s calculation needs the calculated entity to be a probability distribution. Therefore, we need to compute the normalization factor Z if it is not equal to one. Calculating Z is hard. This is why we calculate ELBO instead of KL-divergence. 
The corresponding ELBO that we want to maximize in LDA is: 
where the expectation value is calculated w.r.t. q. 
Graphical model 
Let’s simplify the Graphical model so we can explain the LDA in steps. 
where xᵢ is an observation depending on β and zᵢ. We want to model the posterior underlined in red by factorizing p into sub-components. We will model these conditional probabilities as 
R.H.S. is the exponential family of distribution. 
It is a generalization of many distributions including Gaussian, binomial, multinomial, Poisson, Gamma, Dirichlet and beta distributions (details). This makes the equations a little bit abstract but establishes a general framework for us to move forward. Also, it provides a general solution for some problems that thought to be tough to solve. Let’s demonstrate how to express a Dirichlet distribution as an exponential family of distribution. 
So instead of modeling the Dirichlet distribution with α, it is generalized to the exponential family modeled with η. For each type of distribution, we will have a specific definition for T(θᵢ) and h(x). In a Dirichlet distribution, T(θᵢ) = log θᵢ and h(x)=1. So if the Dirichlet distribution has α = (2, 2, 2), the new model parameter for the exponential family of distribution will be η = (1, 1, 1). 
Here are the details of expressing Dirichlet distribution and the multinomial distribution into the exponential family. It looks scary but should be manageable. But again, we don’t need to over-worry this detail too much. 
ELBO & Mean-field variational inference 
Next, we apply mean-field variation inference to approximate p with q. To minimize the KL-divergence, we maximize the ELBO L in finding a variation distribution q(β, z) that approximate the posterior p(β, z|x). 
q will be approximate with the mean-field variation inference. 
The corresponding model will be 
with β and zᵢ modeled as 
Our objective is to approximate p with q by optimizing the ELBO below (R.H.S.) w.r.t. λ and 𝜙ᵢ. 
We take a derivative on L w.r.t. λ and set it to 0, the optimal λ* will be 
And the optimal 𝜙ᵢ* will be 
(Don’t worry about the proof and the equations. Some details will be covered in the next section.) 
Usually, the expectation E[f(x, y)] is defined as: 
But in our context, we calculate the expectation over all variables except the one we want to optimize (say 𝜙, λ in LDA or x below). 
Again, we optimize them in alternating steps. 
Now, let’s extend the graphical model to resemble the LDA problem that we discuss before. 
The topic proportions θ will be modeled by a Dirichlet distribution with parameter γ. The topic assignment Z, say [trade], will be modeled by Multinomial distribution with 𝜙. 
For each document, the optimal 𝜙 and γ in each iteration will be. 
Once the documents are processed, we update the parameters for the topics. 
where the summation indicates the expectation that the word w is assigned to topic k. In a Dirichlet distribution, as λ_kw increases, the chance of selecting the word w in topic k also increases. 
Here are steps in performing one iteration of the Mean-field variational inference. In step 2(b), we evaluate the expectation 𝔼[log θ] and 𝔼[log β] into digamma function (details later). 
where the digamma function is 
In variational inference, the optimal parameter is often expressed in some expectation form. 
We usually decompose η into simpler components and compute the corresponding expectation. One of the common expectation we want to compute is E[log θ]. Let’s see how it is computed if θ is Dirichlet distributed. 
First, we transform the Dirichlet distribution p(θ|α) into the exponential family of distribution. 
Next, we will take advantage of a well-known property for the exponential family of distribution. Its first derivative of A(η) equals the expectation of sufficient statistics. For T(θᵢ) =log θᵢ, A’(η) = 𝔼(log (θ|α)). So the expected value we want to calculate equals the derivative of A(η). With A(η) equals 
The expected log, 𝔼(log (θ|α)), will be: 
We will use this in the LDA proof. 
Now, let’s prove the solution for the LDA that we skip before and fill up some of the details in this section. The proof here is originated from the LDA paper with a slightly different notation. So let’s align with the notation first. Here is the factorization of the joint distribution. 
And the Graphical model is: 
The ELBO L is: 
With the concept in Mean-field variational inference, q is factorized as: 
We expand the first term in ELBO according to the Graphical model. For the second term, we apply the assumption that the joint probability of q can be broken up into individual variables. Here is the expanded result. 
Next, we optimize L w.r.t. γ and 𝜙. Expanding all five terms above in the R.H.S. takes times. So we will only demonstrate the expansion for the first term of L (log p(θᵢ|α)) in our discussion. 
Recall θ is Dirichlet distributed with parameter α. 
In the last section, if θ is Dirichlet distributed, we also establish 
So the first term E[log p(θ|α)] equals 
The ELBO can be further expanded below with each term in a separate line. 
where Ψ is 
Next, let’s optimize L relative to the variational parameters 𝜙 and γ. 
Optimize 𝜙 modeled as a multinomial distribution 
𝜙ni is the probability that the nth word in a doc is in the topic i. 𝜙 has a multinomial distribution. That is 𝜙ni sums to one over all the topics. Remove terms that are un-related with 𝜙 from L. We optimize the reduced L w.r.t. 𝜙. But to enforce the constraint, we add a Lagrange multiplier below. 
By setting its derivative to 0, we get the optimal value for 𝜙ni. 
Optimize γ modeled as a Dirichlet distribution 
Now, it is time to optimize the topic proportions for L w.r.t γ. γ models the topic proportions with a Dirichlet distribution. 
As indicated by the summation, it is natural to see γᵢ increases as more words in the document belonging to the topic i. As γᵢ is larger than other topics, the corresponding topic will have a higher chance to be picked according to the Dirichlet distribution which has an expected value of θᵢ calculated as: 
Here is the final algorithm after putting everything together. 
Dirichlet distribution is the prior conjugate of the multinomial distribution. In Bayes’ Theorem, if the prior is Dirichlet distributed and the likelihood is multinomial distributed, the posterior will be Dirichlet distributed and can be computed easily (detail). 
That is some of the math behind LDA that makes life easier. 
As mentioned in the beginning, our model will not provide a grammatically correct article or generate words with coherent meaning. One major problem is our graphical model is too simple. Words are sampled independently. 
In this section, we will briefly cover other possibilities in the Graphical model to expand its application. 
Correlated topic models 
By drawing components from a Gaussian distribution, the value xᵢ in a sample data will be correlated by the covariance matrix Σ. For example, topics in sequential words can be correlated now. 
Dynamic topic models 
Topics of interests may change over time. 
Even we can collect billions of tweets, we may divide it chronically to identify the shift of public interests. For example, βkt will be the word distribution for topic k at time t. We can then model the dependency between consecutive time periods. 
This article is long but I hope you understand LDA much deeper in theory and in math now. 
Latent Dirichlet Allocation paper 
Topic models 
Probabilistic topic models 
Written by 
Written by",Jonathan Hui,2019-07-31T21:15:10.982Z
Real-Time Anomaly Detection — A Deep Learning Approach | by Abacus.AI | Abacus.AI Blog (Formerly RealityEngines.AI) | Medium,"Pattern recognition is a crucial aspect of modern data analytics. These patterns can be studied to better understand the underlying structure of data and monitor behavior over time. However, there are often rare items or observations that seem to differ significantly from these patterns. These items are called anomalies (or outliers), and anomaly detection is the practice of identifying these rare items in order to understand what caused them. While some anomalies can be written off as random noise or insignificant glitches, a lot of important cases are related to bank fraud, cybersecurity issues, medical problems, malfunctioning equipment, and more. 
Let’s start with an example of two-dimensional data. In this case, the easiest way to detect the anomaly is by visualizing the set. Comparing the data on one dimension at a time won’t produce any results, but by looking at the problem with both parameters taken into account simultaneously, the outlier is clearly seen. This is a neat way to explain what anomaly detection is concerned with, but data in real-life scenarios can depend on tens or hundreds of parameters. When visualization is no longer an option, deep learning turns out to be a game-changer. 
Many years of experience in the field of machine learning have shown that deep neural networks tend to significantly outperform traditional machine learning methods when an abundance of data is available. 
There are many available deep learning techniques, each with their strengths and weaknesses. In the case of Deep Anomaly Detection (DAD), the algorithm of choice is usually defined by 3 key factors: the type of data being used,; the learning model; and the type of anomaly being detected. 
Data can be broadly broken down into two categories: sequential (audio, text, etc.) and non-sequential (images, sensor data, etc.). The table below illustrates which models perform better in which case, where CNN stands for Convolutional Neural Network, RNN — Recurrent Neural Network, LSTM — Long Short Term Memory Network, and AE — Autoencoder. As studies have shown, deep learning models can learn complex feature relations on high-dimensional input data — the more layers, the better. 
Methods for DAD algorithms can also be categorized by the kind of training model being used. Depending on the availability of labels, either semi-supervised or unsupervised learning is deployed. 
DAD techniques also differ based on the training objectives employed: 
Broadly speaking, anomalies can be classified by three types: point, contextual, and group anomalies, with deep learning techniques demonstrating success in all three cases. 
Once the DAD model has finished its learning, its output for data can be either a label (“normal”, “anomaly”) or a ranking score, showing exactly “how anomalous” a certain data point is. 
Perhaps the main drivers of interest behind DAD techniques are real-time applications for Big Data. There are many scenarios when data has to be analyzed on the fly since doing it offline would either produce no results whatsoever or even cause certain losses. These scenarios usually deal with vast amounts of quickly changing data in a complex environment. Due to the scalability of neural networks, deep learning techniques are a perfect fit for this task. 
According to Cisco, 2.3 Zettabytes of IP traffic will go through the Internet in 2020, a 62% increase compared to 2015. In addition to that, most of the traffic (71%) will be going through less secure non-PC devices such as tablets, smart TVs, consoles, and various IoT devices. This is a growing concern for cybersecurity since all of this traffic needs to be monitored in real-time to prevent potential hacks. Intrusion detection is a primary application of anomaly detection since malicious activity tends to look irregular in comparison to everyday operations. 
Fraud can happen in many areas, including telecoms, healthcare, banking, and insurance. Traditional machine learning algorithms have been used in fraud detection, but once again difficulties arise when the detection needs to happen immediately. A prime example is insider trading. Data in stock markets changes over the span of milliseconds and anomaly detection has already been successfully used to detect insider trading fraud. In this case, real-time monitoring is necessary to prevent people from making illegal profits. 
Safety is the most important concern of the autonomous vehicle industry. Data from cameras and internal sensors needs to be continuously monitored in order to prevent potential car accidents, or in less severe cases — prevent unnecessary traffic jams. 
Medical monitoring services require constant attention so that a response to sudden changes in a patient’s vital signals can happen in a timely manner. Additionally, anomaly detection can be applied to medical images in order to help diagnose diseases. 
Any systems where a malfunction could lead to heavy financial losses or even health hazards can benefit from timely anomaly detection. Areas include monitors for electricity infrastructure, signals from fire alarms, railway signaling and control, air traffic control, and more. 
RealityEngines provides you with state-of-the-art Fraud and Security solutions such as: 
Setup is simple and takes only a few hours — no Machine Learning expertise required from your end. Be sure to check out our website for more information. 
Written by 
Written by",Abacus.AI,2020-04-27T18:22:23.897Z
"Time series anomaly detection with “anomalize” library | by Mahbubul Alam | Sep, 2020 | Towards Data Science","Time series data have a wide range of application cases — from tracking key performance indicators (KPI) and understanding business trends to advanced modeling and forecasting. Anomaly detection is one such useful application. Business applications such as credit card fraud detection require powerful techniques that can take time series data as inputs and identify anomalies in real-time. 
Anomaly detection is a well-researched domain with many tools and techniques available. Quite a few R packages are out there for anomaly detection such as tsoutlier and AnomalyDetection. However, I recently became convinced that anomalize is the most intuitive and easy-to-use library — for both novice and advanced data scientists. 
So the purpose of today’s article is to demonstrate the implementation of anomalize anomaly detection library in three easy steps. 
Let’s dive right in. 
Like in any other machine learning algorithm, preparing data is probably the most important step you can take towards anomaly detection. On the positive side though, you’ll likely use only one column at a time. So unlike hundreds of features in other machine learning techniques, you can focus on only one column that is being used for modeling. 
Make sure that you go through the usual ritual of data cleaning and preparation such as taking care of missing values etc. One essential step is to make sure that the dataset is in a tibble or tbl_time object in the end. 
Let’s first install the libraries we are going to need: 
For this demo we are in good luck, no data processing required. We are going to fetch stock price data using tidyquant library. 
First, let’s implement anomalize with the data that we just fetched and then talk about what’s going on. 
Few things are going on here, the library takes in input data and applies three separate functions to it. 
First,time_decompose() function decomposes “close” column of the time series data into “observe”, “season”, “trend” and “remainder” components. 
Second,anomalize() function performs anomaly detection on the “remainder” column and gives outputs in 3 columns: “remainder_l1”, “remainder_l2” and “anomaly”. The last column here is what we are after, it’s “yes” if the observation is an anomaly and “no” for a normal data point. 
The final function time_recompose() puts everything back into order by recomposing “trend” and “season” columns created earlier. 
For all intents and purposes, our anomaly detection is complete in the previous step. But we still need to visualize the data and the anomalies. Let’s do that and visually check out the outliers. 
The figure is pretty intuitive. Each dot is an observed data point in the dataset and red circles are anomalies as identified by the model. The shaded areas are the upper and lower limits of the remainders. 
If you have come along thus far, you have successfully implemented a sophisticated anomaly detection technique in three simple steps. That was easy because we used default parameters and didn’t change anything. As we saw in the figure above, this out of the box model performed pretty well in detecting outliers. However, you might come across complex time series data that will require better model performance by tuning parameters in step 2. You can read the model documentation and the quick starter guide to get a sense of the parameters, what they do and how & when to change them. 
If you liked this article you can follow me on Twitter or LinkedIn. 
Hands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look 
Written by 
Written by",Mahbubul Alam,2020-09-17T03:51:25.375Z
Twitter’s Anomaly Detection Algorithm Simplified | by Adarsh Bulusu | Medium,"Have you ever wondered how does a company know when they’re receiving too much data? How do draw the line between a lot and too much? 
Well, that is what Anomaly Detection tries to solve. Anomaly Detection is using a statistical approach to find outliers (or “spikes” in time series data) that deviate from what we would expect. 
Anomaly detection is an extremely important aspect of today’s finance & economy sector as companies want to analyze how their data is being consumed and where they may have a problem or a potential avenue for growth. 
In 2015, Twitter came up with a neat robust approach to finding anomalies in continuous data (time-series data). They made their work open-source and list some possible uses as identifying bots or scammers or in general tracking user engagement. [1] 
In their work, they attempt to capture two types of anomalies. 
The algorithm used builds off the ESD Test for anomalies. The ESD Test is a simple outlier test, similar to the Grubb’s Test, that is applied to an approximately normal distribution. 
The generalized ESD Test does not work primarily due to its being parametric test (assumes normal distribution) and Twitter’s dataset contains multiple modes (seasonal data). 
Also since Twitter does not say “hey let’s put 17 anomalies into out dataset today” — the Grubb’s test and ESD don’t work — as an upper limit on the number of anomalies must be given (but we do not know the number of anomalies in our data). 
They leveraged time-series decomposition by breaking up the data into 3 primary sections: residual, seasonality, and trend. After taking the raw data and breaking it down they analyzed the residual graph. In the previous multi-modal dataset there was a large standard deviation but the residual curve shows a tall, skinny curve meaning a small standard deviation. 
So now by looking at the residual we have an approximately normal distribution. Next, the algorithm applies an ESD test on the residual dataset. But not just any ESD test. 
The Twitter team altered the ESD test to take in the robust parameters of the median (median of the absolute deviations) and the sigma of the MAD. 
In summary, the algorithm first uses STL decomposition to break up the raw time series. Then it detects global and local anomalies by applying an SH-ESD test (Seasonal Hybrid ESD Test) which has the input parameters of Mean Absolute Deviation Median and its corresponding standard deviation. Now, the algorithm can account for local and global anomalies. Problem solved. 
My explanation is a bit generalized and if you want to see the algorithm’s performance or exact mechanics refer to the official paper. 
This system does have many drawbacks as well, however, it’s cool to see a company tackle such a large problem using such a simple and efficient algorithm. 
Open-Source Github Package in R: https://github.com/twitter/AnomalyDetection 
Written by 
Written by",Adarsh Bulusu,2020-08-03T00:18:09.502Z
Active Learning: Your Model’s New Personal Trainer | by ODSC - Open Data Science | Medium,"First, some facts. Fact: active learning is not just another name for reinforcement learning; active learning is not a model; and no, active learning is not deep learning. 
What active learning is and why it may be an important component of your next machine learning project was the subject of Jennifer Prendki’s presentation at ODSC London 2018. Prendki is VP of Machine Learning at Figure Eight, previously Crowdflower, a human-in-the-loop machine learning company that’s on a mission “to empower data scientists to train, test, and tune machine learning for a human world”. In other words, they employ an army of mechanical turkers that label data by hand, a costly endeavor from which they — and others with vast stores of unlabeled data — want to get the most bang for their buck, which they do through active learning. 
Before getting into the specifics, imagine how prohibitively expensive it would be to hand-classify every Netflix video into genres. Nobody wants to binge that much, and because it would be inefficient for Netflix to pay a human army for classification at that scale, they need to build a classifier that does it automatically. But if bucco bucks are spent on a classifier eventually responsible for automatically classifying every video, they’d want to ensure it’s worth its salt. In other words, the classifier will classify videos well in the real world even if a given movie is a particularly difficult movie to place into a genre. Identifying the movies most likely to trip up the model — and learning from them before it trips — is active learning. 
Active learning is the process by which your model chooses the training data it will learn the most from, with the idea being that your model will predict better on your test set with less data if it’s encouraged to pick the samples it wants to learn from. 
In general, it works like this: 
Train and retrain your model in a series of loops. Before you begin looping: 
In loop 1: 
In loop 2: 
In theory, you continue looping in this way until your model eclipses some threshold for some performance metric you supply an apriori. In practice, however, knowing when to stop is a little trickier, because there isn’t a documented theoretical framework to inform the number of loops, suggesting that the optimal strategy is application-dependent. 
Here’s that in a reductive piece of clipart. 
Image Credit: Settles, 2009 
This is how it works, in general. But there are at least three different frameworks to actively learn in, and another three strategies on how to select samples for labeling, also called querying the oracle. They basically differ in how many samples are queried at a time and how those samples are selected. 
One can deploy active learning in a few frameworks. You could: 
The bread and butter of active learning is how you’ll select samples within each framework. Options are the following strategies: 
As Prendki points out, the world is the modeler’s oyster, and the strategy you decide to exact can be whatever you want. Say you want to actively learn based on confidence, so to avoid polluting your model with the gunk at the bottom of your distributional barrel, you split your least confident rows into tertiles (or any p-tile). Then grab 90% of each loop’s rows from the bottom third, but the remaining 10% from within 20% of the median band of the top third. That’s confusing in English, but you could do it, because the point is: you can experiment with different query-selection strategies. 
So What Should I Try? 
Oh come on. You know it depends. Pooling with confidence-based uncertainty sampling is probably the most well-researched approach to active learning in the literature, but as Prendicki points out, that’s not because it’s intrinsically the superior approach. What you decide on, according to Prendki, depends on your application and budget. 
When you pool, you know exactly how many samples you’re selecting for labeling by the oracles who demand payment for their oracling, giving you precise finesse over your budget. However, it’s computationally more expensive because you’re retraining on the entire dataset in each loop. 
When you stream samples over to your oracle, you reduce computational cost, but budgeting in this environment becomes nebulous as the number of rows you end up sampling is unknown until you finish training. It also doesn’t look at all the data. Passive active sampling, as it were. Plus it’s difficult to set the threshold, unless you have some credible a priori justification, which I find I am typically short of. 
When Can I Use Active Learning? 
Where Can I Learn More? 
While writing this post, I was remiss that I couldn’t find an example of active learning in the wild, written in R, so stay tuned for a lightweight tutorial on how to build active learning into a classification model to improve accuracy. 
References 
Original story here. 
— — — — — — — — — — — — — — — — — — 
Read more data science articles on OpenDataScience.com, including tutorials and guides from beginner to advanced levels! Subscribe to our weekly newsletter here and receive the latest news every Thursday. 
Written by 
Written by",ODSC - Open Data Science,2018-12-12T19:01:01.771Z
"Topic Modeling (NLP) LSA, pLSA, LDA with python | Medium","Topic Modeling: Art of Storytelling in NLP 
Topic Modeling is an unsupervised approach to discover latent (hidden) semantic structure of text data (often called as documents). 
Why Topic Modeling? 
Each document is built with hierarchy, from words to sentences to paragraphs to documents. Thus, extracting topics from documents helps us analyze our data and hence brings more value to our business. Isn’t it great to have some algorithm that does all the work for you. Yes!! Topic modeling is an automated algorithm that requires no labeling/annotations. Given a bunch of documents it gives you an intuition about the topics(story) your document deals with. 
Applications of Topic Modeling: 
Other than this topic modeling can be a good starting point to understand your data. In the later part of this post, we will discuss more on understanding documets by visualizing it’s topics and words distribution. 
Before we start, here is a basic assumption: 
Given some basic inputs, Let us first start to explore about various topic modeling techniques and at the end we’ll look into an implementation of Latent Dirichlet Allocation (LDA), the most popular technique in topic modeling. If you’re already aware about LSA, pLSA and looking for detailed explanation on LDA or it’s implementation, please feel free to skip next two sections and start with LDA. 
Latent Semantic Analysis: 
LSA creates vector based representation of text by capturing the co-occurences of words and document. 
Here, M — number of documents with Vocabulary(V) is approximated with two matrices (Topic Assignment Matrix and Word-Topic Matrix). 
Steps: 
Then we pick top-k topics, (i.e) X = Uₖ * Sₖ * Vₖ. 
However LSA being the first Topic model and efficient to compute, it lacks interpretability. 
Probabilistic Latent Semantic Analysis: 
pLSA is an improvement to LSA and it’s a generative model which aims to find latent topics from documents by replacing SVD in LSA with a probabilistic model. 
Steps: 
In simple context, we sample a document first then based on the document we sample a topic and based on the topic we sample a word, which means d and w are conditionally independent given a hidden topic ‘z’. 
Limitations: 
Latent Dirichlet Allocation: 
LDA is a Bayesian version of pLSA. 
Dirichlet Distribution is a multivariate generalization of beta distribution. Basically, Dirichlet is a “distribution over distribution”. 
LDA uses Dirichlet priors for the document-topic and topic-word distribution. 
Assumption: 
Steps: 
This is how it assumes each word is generated in document. 
Our goal here is to estimate parameters φ, θ to maximize p(w; α, β). The main advantage of LDA over pLSA is that it generalizes well for unseen documents. 
Implementation of LDA in python: 
I will be using 20Newsgroup data set for this implementation. I have reviewed and used this dataset for my previous works, hence I knew about the main topics before hand and could verify whether LDA correctly identifies them. 
This dataset is available in sklearn and can be downloaded as follows: 
Below are the categories of news data: 
Basically, they can be grouped into below topics: 
Let’s start with our implementation on LDA, 
Pre-processing Data: 
LDA requires some basic pre-processing of text data and the below pre-processing steps are common for most of the NLP tasks (feature extraction for Machine learning models): 
Next step is to convert pre-processed tokens into a dictionary with word index and it’s count in the corpus. We can use gensim package to create this dictionary then to create bag-of-words. 
Running LDA 
Now it’s time for us to run LDA and it’s quite simple as we can use gensim package. We need to specify the number of topics to be allocated. We can set Dirichlet parameters alpha and beta as “auto”, gensim will take care of the tuning. Let’s start with 5 topics, later we’ll see how to evaluate LDA model and tune it’s hyper-parameters. 
Visualizing the topics: 
First let’s print topics learnt by the model . 
I have manually grouped(added in comments) them to those 5 categories mentioned earlier and we can see LDA doing a pretty good job here. 
Let’s Visualize LDA with pyLDAVis tool, 
It’s an interactive visualization tool with which you can visualize the distance between each topic (left part of the image) and by selecting a particular topic you can see the distribution of words in the horizontal bar graph(right part of the image). 
Evaluating LDA: 
There are two methods that best describes the performance LDA model. 
Perplexity is the measure of uncertainty, meaning lower the perplexity better the model. We can calculate the perplexity score as follows: 
Eventhough perplexity is used in most of the language modeling tasks, optimizing a model based on perplexity will not yield human interpretable results. Hence coherence can be used for this task to make it interpretable. 
Coherence is the measure of semantic similarity between top words in our topic. Higher the coherence better the model performance. It can be measures as follows, 
Tuning LDA model: 
Given the ways to measure perplexity and coherence score, we can use grid search based optimization techniques to find best parameters for: 
I hope you have enjoyed this post. For more learning please find the complete code in my github. I encourage you to pull it and try it. 
References: 
Written by 
Written by",MageshDominator,2019-12-26T14:51:34.916Z
🌻The Best and Most Current of Modern Natural Language Processing | by Victor Sanh | HuggingFace | Medium,"Over the last two years, the Natural Language Processing community has witnessed an acceleration in progress on a wide range of different tasks and applications. 🚀 This progress was enabled by a shift of paradigm in the way we classically build an NLP system: for a long time, we used pre-trained word embeddings such as word2vec or GloVe to initialize the first layer of a neural network, followed by a task-specific architecture that is trained in a supervised way using a single dataset. 
Recently, several works demonstrated that we can learn hierarchical contextualized representations on web-scale datasets 📖 leveraging unsupervised (or self-supervised) signals such as language modeling and transfer this pre-training to downstream tasks (Transfer Learning). Excitingly, this shift led to significant advances on a wide range of downstream applications ranging from Question Answering, to Natural Language Inference through Syntactic Parsing… 
“Which papers can I read to catch up with the latest trends in modern NLP?” 
A few weeks ago, a friend of mine decided to dive in into NLP. He already has a background in Machine Learning and Deep Learning so he genuinely asked me: “Which papers can I read to catch up with the latest trends in modern NLP?”. 👩‍🎓👨‍🎓 
That’s a really good question, especially when you factor in that NLP conferences (and ML conferences in general) receive an exponentially growing number of submissions: +80% NAACL 2019 VS 2018, +90% ACL 2019 VS 2018, … 
I compiled this list of papers and resources 📚 for him, and I thought it would be great to share it with the community since I believe it can be useful for a lot of people. 
Disclaimer: this list is not intended to be exhaustive, nor to cover every single topic in NLP (for instance, there is nothing on Semantic Parsing, Adversarial Learning, Reinforcement Learning applied to NLP,…). It is rather a pick of the most recent impactful works in the past few years/months (as of May 2019), mostly influenced by what I read. 
Generally speaking, a good way to start is to read introductive or summary blog posts with a high-level view that gives you enough context ✋ before actually spending time reading a paper (for instance this post or this one). 
These references cover the foundational ideas in Transfer Learning for NLP: 
As a good rule of thumb, you should read papers that you find interesting and spark joy in you! 🤷‍♂️🌟 
There are plenty of amazing resources available you can use that are not necessarily papers. Here are a few: 
Books: 
Course materials: 
Blogs/podcasts: 
Others: 
That’s it for the pointers! Reading a few of these resources should already give you a good sense of the latest trends in contemporary NLP and hopefully, help you build your own NLP system! 🎮 
One last thing that I did not talk about much in this post, but that I find extremely important (and sometimes neglected) is that reading is good, implementing is better! 👩‍💻 You’ll often learn so much more by supplementing your reading with diving into the (sometimes) attached code or trying to implement some of it yourself. Practical resources include the amazing blog posts and courses from fast.ai or our 🤗 open-source repositories. 
What about you? What are the works that had the most impact on you? Tell us in the comments! ⌨️ 
As always, if you liked this post, give us a few 👏 to let us know and share the news around you! 
Many thanks to Lysandre Debut, Clément Delangue, Thibault Févry, Peter Martigny, Anthony Moi and Thomas Wolf for their comments and feedback. 
Written by 
Written by",Victor Sanh,2020-08-31T15:00:46.038Z
Hands-on Machine Learning Model Interpretation | by Dipanjan (DJ) Sarkar | Towards Data Science,"Interpreting Machine Learning models is no longer a luxury but a necessity given the rapid adoption of AI in the industry. This article in a continuation in my series of articles aimed at ‘Explainable Artificial Intelligence (XAI)’. The idea here is to cut through the hype and enable you with the tools and techniques needed to start interpreting any black box machine learning model. Following are the previous articles in the series in case you want to give them a quick skim (but are not mandatory for this article). 
In this article we will give you hands-on guides which showcase various ways to explain potential black-box machine learning models in a model-agnostic way. We will be working on a real-world dataset on Census income, also known as the Adult dataset available in the UCI ML Repository where we will be predicting if the potential income of people is more than $50K/yr or not. 
The purpose of this article is manifold. The first main objective is to familiarize ourselves with the major state-of-the-art model interpretation frameworks out there (a lot of them being extensions of LIME — the original framework and approach proposed for model interpretation which we have covered in detail in Part 2 of this series). 
We cover usage of the following model interpretation frameworks in our tutorial. 
The major model interpretation techniques we will be covering in this tutorial include the following. 
Without further ado let’s get started! 
We will be using a lot of frameworks and tools in this article given it is a hands-on guide to model interpretation. We recommend you to load up the following dependencies to get the maximum out of this guide! 
Rememeber to call the shap.initjs() function since a lot of the plots from shap require JavaScript. 
You can actually get the census income dataset (popularly known as the adult dataset) from the UCI ML repository. Fortunately shap provides us an already cleaned up version of this dataset which we will be using here since the intent of this article is model interpretation. 
Let’s take a look at the major features or attributes of our dataset. 
We will explain these features shortly. 
Let’s view the distribution of people with <= $50K (False) and > $50K (True) income which are our class labels which we want to predict. 
Definitely some class imbalance which is expected given that we should have less people having a higher income. 
Let’s now take a look at our dataset attributes and understand their meaning and significance. 
We have a total of 12 features and our objective is to predict if the income of a person will be more than $50K (True) or less than $50K (False). Hence we will be building and interpreting a classification model. 
Here we convert the categorical columns with string values to numeric representations. Typically the XGBoost model can handle categorical data natively being a tree-based model so we don’t one-hot encode the features here. 
Time to build our train and test datasets before we build our classification model. 
For any machine learning model, we always need train and test datasets. We will be building the model on the train dataset and test the performance on the test dataset. We maintain two datasets (one with the encoded categorical values and one with the original values) so we can train with the encoded dataset but use the original dataset as needed later on for model interpretation. 
We also maintain our base dataset with the actual (not encoded) values also in a separate dataframe (useful for model interpretation later). 
We will now train and build a basic boosting classification model on our training data using the popular XGBoost framework, an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. 
Here we do the usual, use the trained model to make predictions on the test dataset. 
Time to put the model to the test! Let’s evaluate how our model has performed with its predictions on the test data. We use my nifty model_evaluation_utils module for this which leverages scikit-learn internally to give us standard classification model evaluation metrics. 
By default it is difficult to gauge on specific model interpretation methods for machine learning models out of the box. Parametric models like logistic regression are easier to interpret given that the total number of parameters of the model are fixed regardless of the volume of data and one can make some interpretation of the model’s prediction decisions leveraging the parameter coefficients. 
Non-parametric models are harder to interpret given that the total number of parameters remain unbounded and increase with the increase in the data volume. Some non-parametric models like tree-based models do have some out of the box model interpretation methods like feature importance which helps us in understanding which features might be influential in the model making its prediction decisions. 
Here we try out the global feature importance calcuations that come with XGBoost. The model enables us to view feature importances based on the following. 
Note that they all contradict each other, which motivates the use of model interpretation frameworks like SHAP which uses something known as SHAP values, which claim to come with consistency guarantees (meaning they will typically order the features correctly). 
ELI5 is a Python package which helps to debug machine learning classifiers and explain their predictions in an easy to understand an intuitive way. It is perhaps the easiest of the three machine learning frameworks to get started with since it involves minimal reading of documentation! However it doesn’t support true model-agnostic interpretations and support for models are mostly limited to tree-based and other parametric\linear models. Let’s look at some intuitive ways of model interpretation with ELI5 on our classification model. 
We recommend installing this framework using pip install eli5 since the conda version appears to be a bit out-dated. Also feel free to check out the documentation as needed. 
Typically for tree-based models ELI5 does nothing special but uses the out-of-the-box feature importance computation methods which we discussed in the previous section. By default, ‘gain’ is used, that is the average gain of the feature when it is used in trees. 
One of the best way to explain model prediction decisions to either a technical or a more business-oriented individual, is to examine individual data-point predictions. Typically, ELI5 does this by showing weights for each feature depicting how influential it might have been in contributing to the final prediction decision across all trees. The idea for weight calculation is described here; ELI5 provides an independent implementation of this algorithm for XGBoost and most scikit-learn tree ensembles which is definitely on the path towards model-agnostic interpretation but not purely model-agnostic like LIME. 
Typically, the prediction can be defined as the sum of the feature contributions + the “bias” (i.e. the mean given by the topmost region that covers the entire training set) 
Here we can see the most influential features being the Age, Hours per week, Marital Status, Occupation & Relationship 
Here we can see the most influential features being the Education, Relationship, Occupation, Hours per week & Marital Status 
It is definitely interesting to see how similar features play an influential role in explaining model prediction decisions for both classes! 
Skater is a unified framework to enable Model Interpretation for all forms of models to help one build an Interpretable machine learning system often needed for real world use-cases using a model-agnostic approach. It is an open source python library designed to demystify the learned structures of a black box model both globally(inference on the basis of a complete data set) and locally(inference about an individual prediction). 
Skater originally started off as a fork of LIME but then broke out as an independent framework of it’s own with a wide variety of feature and capabilities for model-agnostic interpretation for any black-box models. The project was started as a research idea to find ways to enable better interpretability(preferably human interpretability) to predictive “black boxes” both for researchers and practitioners. 
You can typically install Skater using a simple pip install skater. For detailed information on the dependencies and installation instruction check out installing skater. 
We recommend you to check out the detailed documentation of Skater. 
Skater has a suite of model interpretation techniques some of which are mentioned below. 
Since the project is under active development, the best way to understand usage would be to follow the examples mentioned in the Gallery of Interactive Notebook. But we will be showcasing its major capabilities using the model trained on our census dataset. 
A predictive model is a mapping from an input space to an output space. Interpretation algorithms are divided into those that offer statistics and metrics on regions of the domain, such as the marginal distribution of a feature, or the joint distribution of the entire training set. In an ideal world there would exist some representation that would allow a human to interpret a decision function in any number of dimensions. Given that we generally can only intuit visualizations of a few dimensions at time, global interpretation algorithms either aggregate or subset the feature space. 
Currently, model-agnostic global interpretation algorithms supported by skater include partial dependence and feature importance with a very new release of tree-surrogates also. We will be covering feature importance and partial dependence plots here. 
The general workflow within the skater package is to create an interpretation, create a model, and run interpretation algorithms. Typically, an Interpretation consumes a dataset, and optionally some metadata like feature names and row ids. Internally, the Interpretation will generate a DataManager to handle data requests and sampling. 
We will use the following workflow: 
Feature importance is generic term for the degree to which a predictive model relies on a particular feature. The skater feature importance implementation is based on an information theoretic criteria, measuring the entropy in the change of predictions, given a perturbation of a given feature. The intuition is that the more a model’s decision criteria depend on a feature, the more we’ll see predictions change as a function of perturbing a feature. The default method used is prediction-variance which is the mean absolute value of changes in predictions, given perturbations in the data. 
Partial Dependence describes the marginal impact of a feature on model prediction, holding other features in the model constant. The derivative of partial dependence describes the impact of a feature (analogous to a feature coefficient in a regression model). This has been adapted from T. Hastie, R. Tibshirani and J. Friedman, Elements of Statistical Learning Ed. 2, Springer. 2009. 
The partial dependence plot (PDP or PD plot) shows the marginal effect of a feature on the predicted outcome of a previously fit model. PDPs can show if the relationship between the target and a feature is linear, monotonic or more complex. Skater can show 1-D as well as 2-D PDPs 
Let’s take a look at how the Age feature affects model predictions. 
Looks like the middle-aged people have a slightly higher chance of making more money as compared to younger or older people. 
Let’s take a look at how the Education-Num feature affects model predictions. 
Looks like higher the education level, the better the chance of making more money. Not surprising! 
Let’s take a look at how the Capital Gain feature affects model predictions. 
Unsurprisingly higher the capital gain, the more chance of making money, there is a steep rise in around $5K — $8K. 
Remember that relationship is coded as a categorical variable with numeric representations. Let’s first look at how it is represented. 
Let’s now take a look at how the Relationship feature affects model predictions. 
Interesting definitely that married folks (husband-wife) have a higher chance of making more money than others! 
We run a deeper model interpretation here over all the data samples, trying to see interactions between Age and Education-Numand also their effect on the probability of the model predicting if the person will make more money, with the help of a two-way partial dependence plot. 
Interesting to see higher the education level and the middle-aged folks (30–50) having the highest chance of making more money! 
We run a deeper model interpretation here over all the data samples, trying to see interactions between Education-Num and Capital Gain and also their effect on the probability of the model predicting if the person will make more money, with the help of a two-way partial dependence plot. 
Basically having a better education and more capital gain leads to you making more money! 
Local Interpretation could be possibly be achieved in two ways. Firstly, one could possibly approximate the behavior of a complex predictive model in the vicinity of a single input using a simple interpretable auxiliary or surrogate model (e.g. Linear Regressor). Secondly, one could use the base estimator to understand the behavior of a single prediction using intuitive approximate functions based on inputs and outputs. 
LIME is a novel algorithm designed by Riberio Marco, Singh Sameer, Guestrin Carlos to access the behavior of the any base estimator(model) using interpretable surrogate models (e.g. linear classifier/regressor). Such form of comprehensive evaluation helps in generating explanations which are locally faithful but may not align with the global behavior. Basically, LIME explanations are based on local surrogate models. These, surrogate models are interpretable models (like a linear model or decision tree) that are learned on the predictions of the original black box model. But instead of trying to fit a global surrogate model, LIME focuses on fitting local surrogate models to explain why single predictions were made. 
The idea is very intuitive. To start with, just try and unlearn what you have done so far! Forget about the training data, forget about how your model works! Think that your model is a black box model with some magic happening inside, where you can input data points and get the models predicted outcomes. You can probe this magic black box as often as you want with inputs and get output predictions. 
Now, you main objective is to understand why the machine learning model which you are treating as a magic black box, gave the outcome it produced. LIME tries to do this for you! It tests out what happens to you black box model’s predictions when you feed variations or perturbations of your dataset into the black box model. Typically, LIME generates a new dataset consisting of perturbed samples and the associated black box model’s predictions. On this dataset LIME then trains an interpretable model weighted by the proximity of the sampled instances to the instance of interest. Following is a standard high-level workflow for this. 
We recommend you to read the LIME chapter in Christoph Molnar’s excellent book on Model Interpretation which talks about this in detail. 
Skater can leverage LIME to explain model predictions. Typically, its LimeTabularExplainer class helps in explaining predictions on tabular (i.e. matrix) data. For numerical features, it perturbs them by sampling from a Normal(0,1) and doing the inverse operation of mean-centering and scaling, according to the means and stds in the training data. For categorical features, it perturbs by sampling according to the training distribution, and making a binary feature that is 1 when the value is the same as the instance being explained. The explain_instance() function generates explanations for a prediction. First, we generate neighborhood data by randomly perturbing features from the instance. We then learn locally weighted linear (surrogate) models on this neighborhood data to explain each of the classes in an interpretable way. 
Since XGBoost has some issues with feature name ordering when building models with dataframes, we will build our same model with numpy arrays to make LIME work without additional hassles of feature re-ordering. Remember the model being built is the same ensemble model which we treat as our black box machine learning model. 
Skater gives a nice reasoning below showing which features were the most influential in the model taking the correct decision of predicting the person’s income as below $50K. 
Skater gives a nice reasoning below showing which features were the most influential in the model taking the correct decision of predicting the person’s income as above $50K. 
We have see various ways to interpret machine learning models with features, dependence plots and even LIME. But can we build an approximation or a surrogate model which is more interpretable from a really complex black box model like our XGBoost model having hundreds of decision trees? 
Here in, we introduce the novel idea of using TreeSurrogates as means for explaining a model's learned decision policies (for inductive learning tasks), which is inspired by the work of Mark W. Craven described as the TREPAN algorithm. 
We recommend checking out the following excellent papers on the TREPAN algorithm to build surrogate trees. 
Briefly, Trepan constructs a decision tree in a best-first manner. It maintains a queue of leaves which are expanded into subtrees as they are removed from the queue. With each node in the queue, Trepan stores, 
The stored subset of training examples consists simply of those examples that reach the node. The query instances are used, along with the training examples, to select the splitting test if the node is an internal node or to determine the class label if it is a leaf. The constraint set describes the conditions that instances must satisfy in order to reach the node; this information is used when drawing a set of query instances for a newly created node. The process of expanding a node in Trepan is much like it is in conventional decision tree algorithms: a splitting test is selected for the node, and a child is created for each outcome of the test. Each child is either made a leaf of the tree or put into the queue for future expansion. 
For Skater’s implementation, for building explainable surrogate models, the base estimator(Oracle) could be any form of a supervised learning predictive model — our black box model. The explanations are approximated using Decision Trees(both for Classification/Regression) by learning decision boundaries similar to that learned by the Oracle (predictions from the base model are used for learning the Decision Tree representation). The implementation also generates a fidelity score to quantify tree based surrogate model’s approximation to the Oracle. Ideally, the score should be 0 for truthful explanation both globally and locally. Let’s check this out in action! 
NOTE: The implementation is currently experimental and might change in future. 
We can use the Interpretation object we instantiated earlier to invoke a call to the TreeSurrogate capability. 
We can now fit this surrogate model on our dataset to learn the decision boundaries of our base estimator. 
We do this since the feature names in the surrogate tree are not displayed (but are present in the model) 
We can now visualize our surrogate tree model using the following code. 
Here are some interesting rules you can observe from the above tree. 
Feel free to derive more interesting rules from this and also your own models! Let’s look at how our surrogate model performs on the test dataset now. 
Let’s check the performance of our surrogate model now on the test data. 
Just as expected, the model performance drops a fair bit but still we get an overall F1-score of 83% as compared to our boosted model’s score of 87% which is quite good! 
SHAP (SHapley Additive exPlanations) is a unified approach to explain the output of any machine learning model. SHAP connects game theory with local explanations, uniting several previous methods and representing the only possible consistent and locally accurate additive feature attribution method based on what they claim! (do check out the SHAP NIPS paper for details). We have also covered this in detail in Part 2 of this series. 
SHAP can be installed from PyPI 
or conda-forge 
The really awesome aspect about this framework is while SHAP values can explain the output of any machine learning model, for really complex ensemble models it can be slow. But they have developed a high-speed exact algorithm for tree ensemble methods (Tree SHAP arXiv paper). Fast C++ implementations are supported for XGBoost, LightGBM, CatBoost, and scikit-learn tree models! 
SHAP (SHapley Additive exPlanations) assigns each feature an importance value for a particular prediction. Its novel components include: the identification of a new class of additive feature importance measures, and theoretical results showing there is a unique solution in this class with a set of desirable properties. Typically, SHAP values try to explain the output of a model (function) as a sum of the effects of each feature being introduced into a conditional expectation. Importantly, for non-linear functions the order in which features are introduced matters. The SHAP values result from averaging over all possible orderings. Proofs from game theory show this is the only possible consistent approach. 
An intuitive way to understand the Shapley value is the following: The feature values enter a room in random order. All feature values in the room participate in the game (= contribute to the prediction). The Shapley value ϕᵢⱼ is the average marginal contribution of feature value xᵢⱼ by joining whatever features already entered the room before, i.e. 
The following figure from the KDD 18 paper, Consistent Individualized Feature Attribution for Tree Ensembles summarizes this in a nice way! 
Let’s now dive into SHAP and leverage it for interpreting our model! 
Here we use the Tree SHAP implementation integrated into XGBoost to explain the test dataset! Remember that there are a variety of explainer methods based on the type of models you are building. We estimate the SHAP values for a set of samples (test data) 
This returns a matrix of SHAP values (# samples, # features). Each row sums to the difference between the model output for that sample and the expected value of the model output (which is stored as expected_value attribute of the explainer). Typically this difference helps us in explaining why the model is inclined on predicting a specific class outcome. 
SHAP gives a nice reasoning below showing which features were the most influential in the model taking the correct decision of predicting the person’s income as below $50K. The below explanation shows features each contributing to push the model output from the base value (the average model output over the training dataset we passed) to the actual model output. Features pushing the prediction higher are shown in red, those pushing the prediction lower are in blue. 
Similarly, SHAP gives a nice reasoning below showing which features were the most influential in the model taking the correct decision of predicting the person’s income as greater than $50K. 
One of the key advantages of SHAP is it can build beautiful interactive plots which can visualize and explain multiple predictions at once. Here we visualize model prediction decisions for the first 1000 test data samples. 
The above visualization can be interacted with in multiple ways. The default visualization shows some interesting model prediction pattern decisions. 
Definitely interesting how we can find out patterns which lead to the model making specific decisions and being able to provide explanations for them. 
This basically takes the average of the SHAP value magnitudes across the dataset and plots it as a simple bar chart. 
Besides a typical feature importance bar chart, SHAP also enables us to use a density scatter plot of SHAP values for each feature to identify how much impact each feature has on the model output for individuals in the validation dataset. Features are sorted by the sum of the SHAP value magnitudes across all samples. Note that when the scatter points don’t fit on a line they pile up to show density, and the color of each point represents the feature value of that individual. 
It is interesting to note that the age and marital status feature has more total model impact than the capital gain feature, but for those samples where capital gain matters it has more impact than age or marital status. In other words, capital gain affects a few predictions by a large amount, while age or marital status affects all predictions by a smaller amount. 
SHAP dependence plots show the effect of a single (or two) feature across the whole dataset. They plot a feature’s value vs. the SHAP value of that feature across many samples. SHAP dependence plots are similar to partial dependence plots, but account for the interaction effects present in the features, and are only defined in regions of the input space supported by data. The vertical dispersion of SHAP values at a single feature value is driven by interaction effects, and another feature can be chosen for coloring to highlight possible interactions. 
You will also notice its similarity with Skater’s Partial Dependence Plots! 
Let’s take a look at how the Age feature affects model predictions. 
Just like we observed before. the middle-aged people have a slightly higher shap value, pushing the model’s prediction decisions to say that these individuals make more money as compared to younger or older people 
Let’s take a look at how the Education-Num feature affects model predictions. 
Higher education levels have higher shap values, pushing the model’s prediction decisions to say that these individuals make more money as compared to people with lower education levels. 
Let’s take a look at how the Relationship feature affects model predictions. 
Just like we observed during the model prediction explanations, married people (husband or wife) have a slightly higher shap value, pushing the model’s prediction decisions to say that these individuals make more money as compared to other folks! 
Let’s take a look at how the Capital Gain feature affects model predictions. 
The vertical dispersion of SHAP values at a single feature value is driven by interaction effects, and another feature is chosen for coloring to highlight possible interactions. Here we are trying to see interactions between Age and Capital Gainand also their effect on the SHAP values which lead to the model predicting if the person will make more money or not, with the help of a two-way partial dependence plot. 
Interesting to see higher the higher capital gain and the middle-aged folks (30–50) having the highest chance of making more money! 
Here we are trying to see interactions between Education-Num and Relationship and also their effect on the SHAP values which lead to the model predicting if the person will make more money or not, with the help of a two-way partial dependence plot. 
This is interesting because both the features are similar in some context, we can see typically married people with relationship status of either husband or wife having the highest chance of making more money! 
Here we are trying to see interactions between Marital Status and Relationship and also their effect on the SHAP values which lead to the model predicting if the person will make more money or not, with the help of a two-way partial dependence plot. 
Interesting to see higher the higher education level and the husband or wife (married) folks having the highest chance of making more money! 
Here we are trying to see interactions between Age and Hours per week and also their effect on the SHAP values which lead to the model predicting if the person will make more money or not, with the help of a two-way partial dependence plot. 
Nothing extra-ordinary here, middle-aged people working the most make the most money! 
If you are reading this, I would like to really commend your efforts on going through this huge and comprehensive tutorial on machine learning model interpretation. This article should help you leverage the state-of-the-art tools and techniques which should help you in your journey on the road towards Explanable AI (XAI). Based on the concepts and techniques we learnt in Part 2, in this article, we actually implemented them all on a complex machine learning ensemble model trained on a real-world dataset. I encourage you to try out some of these frameworks with your own models and datasets and explore the world of model interpretation! 
In Part 4 of this series, we will be looking at a comprehensive guide to building interpreting models on unstructured data like text and maybe even deep learning models! 
Stay tuned for some interesting content! 
Note: There are a lot of rapid developments in this area including a lot of new tools and frameworks being released over time. In case you want me to cover any other popular frameworks, feel free to reach out to me. I’m definitely interested and will be starting by taking a look into H2O’s model interpretation capabilities some time in the future. 
The code used in this article is available on my GitHub and also as an interactive Jupyter Notebook. 
Check out ‘Part 1 — The Importance of Human Interpretable Machine Learning’ which covers the what and why of human interpretable machine learning and the need and importance of model interpretation along with its scope and criteria in case you haven’t! 
Also Part 2 — Model Interpretation Strategies’ which covers the how of human interpretable machine learning where we look at essential concepts pertaining to major strategies for model interpretation. 
Have feedback for me? Or interested in working with me on research, data science, artificial intelligence or even publishing an article on TDS? You can reach out to me on LinkedIn. 
Hands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look 
Written by 
Written by",Dipanjan (DJ) Sarkar,2019-04-06T18:51:33.227Z
AWS Machine Learning Speciality Certification in a month | by Dipika Baad | Towards Data Science,"Last week I passed my AWS Machine Learning Speciality exam. I thought this will be something worth learning and useful in my day to day job. I booked the date one month ahead and planned accordingly to achieve it. I can’t recommend enough if you are in this domain and your work is related to AWS. In this post, I will share my experience of exam preparation and exam experience in itself. 
I had worked in Data Science Field before and during my Masters studies in Data Science. In my current job, I wanted to focus on Data Engineering related work where I worked on creating data pipelines in AWS for the data platform. I had worked in Data Science projects, projects involving machine learning deployment in production and projects using AWS services before taking the exam. I personally wanted to learn more about machine learning in production with AWS. Thought this will help me in delivering end-to-end machine learning products in AWS. Nowadays, most of the companies are looking for these skills, knowing only data science might not be sufficient. 
There are different levels of certifications available on AWS website. I did not start with Associate level certificates as I had relevant experience to showcase that already but one can take those to know about most of the services provided by AWS. I was looking for machine learning specific and this Speciality certificate hit that spot very well. That is why I aimed directly for Machine Learning Speciality exam. That’s when I decided to book the date. I knew that I can postpone the test 24 hours before the exam if I feel like I am not prepared. But giving myself less time pushed me to stay on the plan. 
First thing I did to check if I can pull this off or not was to check the sample questions from their Machine Learning Speciality Certification Page. I did not know the answers right away but I knew which parts I can handle after some studying and which parts I already knew. Machine Learning related stuff like NLP, optimization and models I was comfortable with. AWS specific solutions and built-in algorithms I thought needed some work on my side. This just gave me an idea if it was possible to pass the exam or not with some effort in one month. 
On the same page, AWS has given the exam guide which covers the main domains that will be covered in exam. This also gives a better idea of how much is already known to you and what needs more preparation. For me I had touched those domains in one or the other way during some of my projects so it was a good indicator that I can understand those topics and use cases. 
Following are the main resources I used for preparation: 
I can totally recommend the first two resources for preparation as I felt the practice test gave a better idea of how the exam questions are designed and it also covers most of the type and content of the exam domains. First course covers the necessary material for the exam and all the domains mentioned. It is nicely structured and to the point material for the exam. They haven’t covered the basics of ML and the details of ML models because it is assumed that if you are appearing for this exam you should have experience of using data science in practice. 
For example, as an experienced machine learning practitioner you should know how to optimize the model, what all parameters needs to be tuned and how to clean the data in order to improve the model. It includes deep learning built-in algorithms when and where to use it. Based on use cases whether you should choose custom models or transfer learning on top of built-in models itself etc. But the course won’t go into details. That makes sense as per the exam prerequisites. 
For me the course and exam tests were really helpful in tackling the actual problems in the exam. AWS itself gives lot of digital courses and classroom courses as well. I found some courses mentioned in points 3 & 4 above which were useful. Course 3 went into bit of depth of solutions provided and various use cases which were not in the Udemy course. I liked that it had few labs which was good to get some hands-on become comfortable with the tools used. Course 4 was something I watched out of interest, if you are interested and you have time you can go through that as well. 
AWS SageMaker Developer guide is very well documented resource for SageMaker and I found it useful to go through it apart from these courses. It is hard to cover everything in a course, so if time permits it is useful to go through the documentation. Especially I went into details of built-in algorithms. 
In the first one and half weeks I went through the Udemy course and some readings related to those topics. I gave the first practice test exam and I had pretty good idea about what type of questions are there and what I need to pay attention to in the course. 
After in the next 2 weeks I went through the course while making notes for points which I had discovered I need to remember explicitly. Along with that I went through some AWS courses mentioned above and as well as their documentation. 
In the last 5 days I gave the test exam multiple times. I also went through AWS sample questions. That’s when I knew I can totally appear for this exam and no need to postpone. 
Actual exam to be honest I felt was difficult than what I expected or prepared for. I was really satisfied with the difficulty level as they considered that you have knowledge about practical implementation and machine learning knowledge in general as well. I was able to complete the exam in one and half hours without going back to questions for reviewing. Many prefer to mark few questions for review and you can go back to those questions at the end where you were unsure. 
Some of my previous knowledge from projects and my mathematical background in machine learning was useful in tackling those problems. It was not part of something I prepared in this previous one month. One should know based on underlying data which models to choose and what is the reasoning behind that. Similarly you should be able choose the right built-in models or services for a given use case and resource restrictions etc. I felt like this test exam I practiced before the exam prepared me for thinking in that direction but be prepared to find some questions related to ML topics that might not be covered in the courses mentioned above. 
I hope this material and planning experience will help you if you are aiming for it :) 
As always — Happy experimenting and exploring! 
Hands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look 
Written by 
Written by",Dipika Baad,2020-08-12T09:40:31.954Z
Collaborative Filtering and Embeddings — Part 1 | by Shikhar Gupta | Towards Data Science,"Recommendation systems are all around us. From Netflix to Amazon to even Medium, everyone is trying to understand our taste so that they can drive us into continuous engagement. You’ll be amazed by the amount of work that goes behind this. Let’s try to understand the mechanics of one such way of developing recommendation systems. 
In this series of articles I’ll explain collaborative filtering, a very common technique for developing automated recommendation systems (although the use case is not limited to recommendations). However, what makes this discussion interesting is how we can understand a more general concept of embeddings while implementing this technique. If you haven’t heard about this term no need to worry, you’ll have a good idea by the end of this post. 
Collaborative filtering and embeddings — Part 2 
Most of the ideas presented in this post are derived from the Deep learning MOOC -v2 conducted by Jeremy Howard as part of the Data Institute. This post is just my attempt to share some amazing stuff that I’m learning during this course. 
Now, without further ado let’s begin… 
In the context of recommendation systems, collaborative filtering is a method of making predictions about the interests of user by analysing the taste of users which are similar to the said user. The idea of filtering patterns by collaborating multiple viewpoints is why it is called collaborative filtering. 
The underlying assumption of the collaborative filtering approach is that if a person A has the same opinion as a person B on an issue, A is more likely to have B’s opinion on a different issue than that of a randomly chosen person 
- Wikipedia [2] 
Collaborative filtering is a general concept and there are several algorithms to implement it. Checkout this post which compares performance of different algorithms. 
One such algorithm is probabilistic matrix factorisation. Don’t get scared by the name. We’ll understand this using a dummy example in excel. 
As shown in figure 2, we have data on the ratings given by different users for different movies on a scale of 0–5. Blank cell means the user hasn’t rated the movie. Our goal is to model this system as closely as possible. 
In Figure 3, you can see the complete framework. Let’s try to understand what is happening here for a specific combination of user-movie (Jon-Forrest Gump). 
In essence the model is just this equation: 
Model prediction of rating (3.25 in yellow cell) = Dot product of the 2 green vectors (embedding vectors)+ 0.14 (Movie bias) + 0.87 (User bias) 
Similarly we’ll predict rating for each user-movie combination except the ones where user-movie rating is not present in the actual data. Based on this we’ll calculate the loss (RMSE (root mean squared error) in our case: red cell at the bottom) by comparing the predicted ratings to actual ratings. 
It’s evident that our predictions are based on these 2 embedding matrices and 2 bias matrices. But what about the numbers in them, they seem random and they are actually randomly initialised. So what are the correct set of numbers that will make our predictions close to actual ratings? 
Yeah you guessed it right. We’ll learn these numbers by minimising the loss using some optimisation algorithm. As a matter of fact this can be done in excel!! 
Go to Data →Solver and you’ll see the following window. In Set Objective select the loss cell and in Changing Variable Cells select both embedding and bias matrix range. Click on solve and the solver will start learning the right values in the embedding and bias matrix by minimising the loss. 
We can see in figure 5 that solver has learnt the values in embedding and bias matrix. Our loss has reduced from 26.04 to 4.58. Our predictions from the model also seem to be close to the actual ratings. So now we have a model that seems to work fine. But we still don’t know why it is effective. Basically there are 3 key ideas behind this algorithm. 
Key idea #1: Finding the right representation of each user and each movie as embeddings 
It’s time to address the elephant in the room. Let’s look at a specific example. Here, Jon and Forrest Gump are being represented by vectors (bunch of 5 numbers that solver has learnt). These vectors are called embeddings. Essentially in a 5 dimensional vector space, Jon and Forrest Gump are represented by these 2 embedding vectors. 
Embeddings are multi-dimensional vector representation of a particular entity 
This way of representing entities as high dimensional vectors is the key. Such representation can capture complex relations between different entities. 
There is no specific rule around the dimensionality of embedding vectors and it’s more about experimentation. The dimensionality should be enough to capture the complexity of the entity. A 1-D or 2-D vector might not capture the complexity of Jon. But it should not be unnecessarily high too. Our Jon and Forrest Gump are not that complex. 
You can think of these numbers as capturing different characteristics of the entity it is representing. For example, the first number in Jon (1.34) may represent how much he likes fantasy fiction. And first number in Forrest Gump (-1.72) tells us the extent to which we can consider Forrest Gump as fantasy (which is very less as the number is negative). 
Note: Theoretically, this is a good way to think about numbers in the embedding vector but we can never really say what each number is actually capturing. We can just make an educated guess. 
Key idea #2: If a movie and a user are close in a vector space then it’s more likely that the user will rate the movie high 
There are different ways to capture closeness: dot product, euclidean distance, cosine similarity. In our example we have used dot product. 
Key idea #3: There is an inherent nature of the entity which is independent of its interaction with other entities 
In simple words some users are more critical compared to others. Similarly some movies can be considered universally good and will be rated high by most of the users. This information is captured by bias. 
Now we have a trained model which has learnt the correct embeddings and bias for each user and movie. Consider a case where we have to recommend a movie among a set of movies to a user (assuming all the movies in the set are not seen by that user). 
Using the calculated embeddings and biases we can predict the rating that the user will give to each movie in the set. The suggested movie will be the one with the highest predicted rating. 
Predicted rating = Embedding vector dot product (user, movie) + user bias + movie bias 
Link to the excel file: https://github.com/shik3519/collaborative-filtering/blob/master/collab_filter.xlsx . Inspired by this file →[3] 
The method discussed in this post is definitely not the best way to implement collaborative filtering. There are better performing algorithms using neural networks but the core idea of embeddings is common. 
Checkout this post which talks about the neural net implementation along with many others. 
In the next article, I’ll talk about how we can implement collaborative filtering using a library called fastai developed by Jeremy Howard et al. This library is built on top of pytorch and is focused on easier implementation of machine learning and deep learning models. 
Also, we’ll get to know how we can interpret and visualise embeddings using t-SNE, Plotly and Bokeh (Python interactive visualisation library that targets modern web browsers for presentation). Below is a teaser of what’s in store. 
Collaborative filtering and embeddings — Part 2 
[1] https://commons.wikimedia.org/wiki/File%3ACollaborative_filtering.gif 
[2] https://en.wikipedia.org/wiki/Collaborative_filtering 
[3]https://github.com/fastai/fastai/blob/master/courses/dl1/excel/collab_filter.xlsx 
[4]http://www.wired.co.uk/article/how-do-netflixs-algorithms-work-machine-learning-helps-to-predict-what-viewers-will-like 
Hands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look 
Written by 
Written by",Shikhar Gupta,2019-04-01T02:01:58.794Z
Introduction to Search Engine Optimization (SEO) | by Mridul Kesharwani | Medium,"You’ve finished your web design, uploaded your files, and set up your blog, but you’re still not getting as many visitors as you hoped for. What gives? Chances are you haven’t started working on one of the most important ways to market your site, Search Engine Optimization (SEO). 
Search Engine Optimization refers to the collection of techniques and practices that allow a site to get more traffic from search engines (Google, Yahoo, Microsoft). SEO can be divided into two main areas:- 
01. Off-Page SEO (work that takes place separate from the website). 
02. On-Page SEO (website changes to make your website rank better). 
This tutorial will cover both areas in detail! Remember, a website is not fully optimized for search engines unless it employs both on and off-page SEO. 
SEO is not purchasing the number 1 sponsored link through Google Adwords and proclaiming that you have a 1 ranking on Google. Purchasing paid placements on search engines is a type of Search Engine Marketing (SEM), and is not covered in this tutorial. 
SEO is not ranking 1 for your company’s name. If you’re reading this tutorial, you probably already know that ranking for popular terms is darn near impossible, but specific terms, such as a company name, is a freebie. The search engines usually are smart enough to award you that rank by default (unless you are being penalized). 
If a website is currently ranked 10 on Google for the search phrase, “How to Make Egg Rolls,” but wants to rise to 1, this website’s needs to consider SEO. Because search engines have become more and more popular on the web, nearly anyone trying to get seen on the web can benefit from a little SEO loving. 
Keyword Research:- Before you can start optimizing your site for the search engines, you must first know which terms you want to target. A good start would be to choose 3 or 4 keywords you would like your website to rank well for. With these keywords in your mind, you can then set a goal to rank in the top 10 results on Google for each of them (we refer to Google because if you can rank well there, you’ll rank well on the other search engines). These keywords can be either broad or specific, but you’ll want to study our list of pros and cons of each before choosing. 
Broad Keywords:- A broad keyword is one that many people search for, because they may only have a vague idea of what they’re looking for. Broad keywords tend to be very short and aren’t very specific (e.g. “shoes” or “sports”). These keywords are difficult to rank #1 for because so many other websites might have an article or two that mention shoes. However, if you can rank well for a broad keyword, you will be receiving a great deal of traffic. 
Hard to rank for, but worth it in the long run. We recommend that beginners only choose a broad keyword if their industries are not very competitive. 
Specific Keywords:- A specific keyword is something that contains many adjectives or words that make the search very targeted. The people doing these types of searches know exactly what they want (e.g. “used black high heel shoes”). These keywords are much less competitive and are easier to rank for on search engines. The downside is that they receive a great deal less volume of searches per month. In terms of traffic, you will need to have several 1 rankings for specific keywords to equal one 1 ranking broad keyword. 
Easier to rank for and it’s highly targeted traffic. The only downside is that the number of visitors you will receive is relatively low. 
Unique or Branded Keywords:- These are the words that are specific to only your company. They are one of the easiest ways to get traffic. However, some companies will release a new product, with a unique name, and then forget to optimize for that keyword on their website. Their SEO savvy competitors can then pick up the slack and take over the top rankings for these terms. If you have a popular brand or product, make sure that you have optimized for these freebie keywords. 
Keyword Research Tools:- Keyword research tools are 2 parts voodoo magic and 1 part hard statistic. This is partly due to Google not releasing actual numbers and partly due to overeager SEO Tool developers trying to sell their products. Because there is such a sizable uncertainty in all keyword research tools, it is best to use as many different sources as you can,. Even with multiple sources, you should only take the information you gather as a recommendation, rather than a fact. 
Yahoo has been releasing their keyword search information for years, and many tools are based on this specific data. We’ve collected a wide variety of helpful tools that will give you a general idea of which keywords you should target when making and optimizing your websites. 
Picking a Short List:- To put the optimizing tactics that we teach to good use, we recommend that you try to target no more than 2 or 3 keyword phrases per page. A common mistake by many SEO beginners is to stuff 500 different keywords on one page and wait for the #1 rankings to roll in. That might have worked 10 years ago, but the algorithms that search engines use these days are much more sophisticated and are not tricked by this. That’s why it’s best to start small and be concise with the keywords that you choose. New sites, in particular, will find it nearly impossible to rank well for many keyword phrases upon first starting out. 
PageRank is a ranking system that previously was the foundation of the infamous search engine, Google. When search engines were first developed, they ranked all websites equally and would return results based only on the content and meta tags the pages contained. At the time, however, the PageRank system would revolutionize search engine rankings by including one key factor: a site’s authority. 
To determine how important, or authoritative, a site was Google chose several big sites, such as cnn.com, dmoz.org, and espn.com. These sites were clear authorities, and Google figured that if these websites choose to link to another site (let’s say site B) then site B would receive a piece of that site’s authority. If site B were to link to another site (how about C), then site C would also receive a piece of authority, though much smaller. 
Using this system of passing authority, Google would then count up how much authority a site had and give it a PageRank from 0 to 10. The PageRank system has become more complicated since then, but this is how it all started. 
If you would like to see what Page Rank your site has or other sites have, install Google’s Toolbar. Google has made a small green bar that starts at 0 PageRank (a blank bar) all the way up to 10 (a full green bar, which is 100% authoritative). It should be noted that the PageRank shown in the toolbar is an estimate released by Google, and it is only updated every 3 months or so. 
When PageRank first came out, only Google was using the technology, but as other search engines have seen how much it improved Google’s accuracy, nearly every search engine has added the PageRank system in to be at least part of their algorithm. In the past, while many of the search engines were still working on adding PageRank to their search algorithm, some couldn’t wait to make their own and instead signed deals with Google to have them power their results (Yahoo did this for quite some time). 
Apart from search engines, SEO (Search Engine Optimization specialists), link buyers, webmasters, marketers, and anyone interested in a site’s value will often look to the Google PageRank when trying to quickly determine the importance of a site. 
When Google was in its childhood, PageRank was the single most important factor for ranking well. However, as soon as the SEO community caught on to this, there was a lot of people who found ways to artificially boost their clients’ PageRank. Those sites became more authoritative than Google thought they should be. Since then, Google and other search engines have constantly refined how important PageRank is, and its importance has definitely declined through the years. 
One tactic Google uses is to update Google Toolbar PageRank values four times a year instead of every week, making it difficult for SEOs to know a site’s real PageRank. Another tactic is to prevent a site that has been known to sell links from passing any of its PageRank (authority) on to sites that it links to. However, Google can’t use that tactic too much because then they run the risk of preventing good sites from being ranked as they should be. 
Now we’ve come to the part where you actually have to do work! It’s tough, but getting a high PageRank for your site should definitely be part of your long-term SEO strategy. The only way to get PageRank is to get a link from a site that already has PageRank. This means that getting a ton of links from PageRank 0 sites will not help your score. However, a single link from a site with a PageRank 6 can immediately boost your site to a PageRank 5 if the site is trusted by Google and is not linking to a massive amount of other sites. The process of increasing your PageRank is directly tied to link acquisition. Link acquisition is getting links from other sites, be it via natural or through link purchasing. We cover both of these topics in greater detail, and you should read each lesson to learn more about the benefits and drawbacks of each. 
Connect to Me Social Media Platform or What’s Up:- +91–9425159112 
Facebook, Google+, Instagram, LinkedIn, Pinterest, Twitter and Tumblr 
Written by 
Written by",Mridul Kesharwani,2018-10-20T13:42:26.556Z
Light Final Process. I updated my progress in this google… | by Danqi Qian | 2020 Spring | Light and Interactivity | Medium,"I updated my progress in this google doc 
This week, I exprimented a little around my home, found a floor lamp in roi’s room(she went back to china). 
I like this kind of gredient. 
I add a diffuser with milk box to the top one, using tiny screw. It looks fine, and when i put more milk boxed around it, it looked really close to what i want. But after I tried to insert 3 hanger wires to the base, thinking about skeleton, turned out that those wires are really hard and makes the fixture not stayable, don’t know why I did that, but then the bulb was broken, sadly. 
So then I added the diffuser to the lower lamp. 
And I cut the red cup in strips and inserted in, it has this kind of effect. 
It looks fun when there’s an extra light in front of and inside it a bit. 
It feels different when I don’t have control over materials i have, and i enjoyed the process more than what it looked in the end. I’ll explore more and buy more milk. Besides, thanks for also trying to use milk boxes, I didn’t have much motivation or ambition before you showed me the thing you made, sometimes i got grumpy for not having access to things I used to have, and also I cannot build stuff that I really get excited for (was thinking to make lots of butt and fill my room with them), but now i feel like, when life gives you lemons, make lemonade, and I actually can! 
I still cant believe I finished this with nothing but milk box, cola cups and ipad wrap paper… 
I first started with cutting the red cup with more straight side in the bottom. Then I just wrapped it around the bulb and tested real quick. I like the jagged shadow it created. So I decided to fix it with the bulb. 
I used another piece of milk box as the top, and drilled two holes, then used wire through milk box and two holes in the socket to make it more stayable. 
Testing testing. Looks like sea ​​urchin. 
Testing. This looks fun. I’ll do it. 
Since I used tiny screw to fix top piece and the diffuser, the shape is not a round cylinder anymore. And I also folded the cup inside. 
Then I added wrap paper around it. 
Written by 
Written by",Danqi Qian,2020-05-06T04:06:42.143Z
Attention Is All You Need — Transformer | by Sherwin Chen | Towards AI — Multidisciplinary Science Journal | Medium,"Recurrent Neural Networks(RNNs), Long Short-Term Memory(LSTM) and Gated Recurrent Units(GRU) in particular, have been firmly established as state-of-the-art approaches in sequence modeling and transduction problems. Such models typically rely on hidden states to maintain historical information. They are beneficial in that they allow the model to make predictions based on useful historical information distilled in the hidden state. On the other hand, this inherently sequential nature precludes parallelization, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Furthermore, in these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, which makes it more difficult to learn dependencies between distant positions. 
In this article, we will discuss a model named Transformer, proposed by Vaswani et al. at NIPS 2017, which utilizes self-attention to compute representations of its input and output without using sequence-aligned RNNs. In this way, it reduces the number of operations required to relate signals from two arbitrary positions to a constant number and achieves significantly more parallelization. In the rest of the article, we will focus on the main architecture of the model and the central idea of attention. For other details, please refer to [1] and [2] in References. 
One thing maybe worth keeping in mind is that the Transformer we introduce here maintains sequential information in a sample just as RNNs do. This suggests the input to the network is of the form [batch size, sequence length, embedding size]. 
The Transformer follows the encoder-decoder structure using stacked self-attention and fully connected layers for both the encoder and decoder, shown in the left and right halves of the following figure, respectively. 
In this work, we use sine and cosine functions of different frequencies to encode the position information: 
where pos is the position and i is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000⋅2π. The authors chose this function because they hypothesized it would allow the model to easily learn to attend by relative positions since for any fixed offset k, PE_{pos+k} can be represented as a linear function of PE_{pos}. 
Encoder 
The encoder is composed of a stack of N=6 identical layers. Each layer has two sublayers. The first is a multi-head self-attention mechanism(we will come back to it soon), and the second is a simple fully connected feed-forward network. Residual connections are employed around each of the two sub-layers, and layer normalization is applied in between. That is, the output of each sub-layer is x+Sublayer(LayerNorm(x)) (This one, adopted by [2], is slightly different from the one used in the paper, but follows the pattern recommended Kaiming He et al in [3]), where Sublayer(x) is the function implemented by the sub-layer itself. 
Decoder 
The decoder is also composed of a stack of N=6 identical layers. In addition to the two sub-layers in the encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack (i.e., where we have the output of the encoder as keys and values). Sub-layers in the decoder follows the same fashion as that in the encoder. 
Masking 
Masks are used before softmax in the self-attention layer in both encoder and decoder to prevent unwanted attention to out-of-sequence positions. Furthermore, in conjunction with the general mask, an additional mask is used in the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. Such a mask has a form of 
In practice, the two masks in the decoder can be blended via a bit-wise and operation. 
Scaled Dot-Product Attention 
An attention function can be described as a mapping from a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. 
More formally, the output is computed as 
where Q, K, V are queries, keys, and values, respectively; dₖ is the dimension of the keys; The compatibility function (softmax part) computes the weights assigned to each value in a row. The dot-product QK^T is scaled by 1\over \sqrt{dₖ} to avoid extremely small gradients for large values of dₖ, where the dot-product grows large in magnitude, pushing the softmax function into the edge region. 
Some takeaway: mathematically, attention is just focusing on the space where Q and K are similar(w.r.t. cosine similarity), given they are in the same magnitude — since (QK^T)_{i,j}=|Q_i||K_j|cosθ. An extreme thought exercise is a case where both Q and K are one-hot encoded. 
Multi-Head Attention 
Single attention head averages attention-weighted positions, reducing the effective resolution. To address this issue, multi-head attention is proposed to jointly attend to information from different representation subspaces at different positions. 
where the projections are parameter matrices 
For each head, we first apply a fully-connected layer to reduce the dimension, then we pass the result to a single attention function. At last, all heads are concatenated and once again projected, resulting in the final values. Since all heads run in parallel and the dimension of each head is reduced beforehand, the total computational cost is similar to that of single-head attention with full dimensionality. 
In practice, if we have hdₖ=hdᵥ=d_{model}, multi-head attention can be simply implemented using attention with four additional fully-connected layers, each of dimension d_{model}×d_{model} as follows 
Tensorflow Code 
We now provide Tensorflow code for multi-head attention. For simplicity, we further assume Q, K, V are all x. 
I hope you have developed a basic sense of Transformer. To see a complete example with code, you may further refer to [2] 
Towards AI publishes the best of tech, science, and engineering. Subscribe with us to receive our newsletter right on your inbox. For sponsorship opportunities, please email us at pub@towardsai.net Take a look 
Written by 
Written by",Sherwin Chen,2020-06-11T17:07:03.474Z
Explainable AI: Interpretation & Trust in Machine Learning Models - Lime | Towards Data Science,"Why should we trust a machine learning model blindly? Wouldn’t it be wonderful if we can get a better insight into model predictions and improve our decision making? With the advent of explainable AI techniques such as LIME and SHAP, it is no longer a challenge. Nowadays, machine learning models are ubiquitous and becoming a part of our lives more than ever. These models are usually a black box in essence that it’s hard for us to assess the model behavior. From smart speakers with inbuilt conversational agents to personalized recommendation systems, we use them daily, but do we understand why they behave in a certain way? Given their ability to influence our decision, it is of paramount and supreme importance that we should be able to trust them. Explainable AI systems help us understand the inner workings of such models. 
So, What’s Explainable AI? 
Explainable AI can be summed up as a process to understand the predictions of an ML model. The central idea is to make the model as interpretable as possible which will essentially help in testing its reliability and causality of features. Broadly speaking, there are two dimensions to interpretability: 
Typically, explainable AI systems provide an assessment of model input features and identify the features which are driving force of the model. It gives us a sense of control, as we can then decide if we can rely on the predictions of these models. For instance, we would probably trust a flu identification model more if it considers features like temperature, and cough more significant than other symptoms. 
Now that you have an idea of explainable systems, how do we explain model predictions? 
There are different ways to do that. LIME is one of them. Let’s squeeze it. 
LIME stands for:Local: Approximate locally in the neighborhood of prediction being explained,Interpretable: Explanations produced are human-readable,Model-Agnostic: Works for any model like SVM, Neural networks, etc Explanations: Provides explanations of model predictions.(Local linear explanation of model behaviour) 
Lime can be used to get more insights into model prediction like explaining why models take a particular decision for an individual observation. It can also be quite useful while selecting between different models. The central idea behind Lime is that it explains locally in the vicinity of the instance being explained by perturbating the different features rather than producing explanations at the entire model level. It does so by fitting a sparse model on the locally dispersed, noise-induced dataset. This helps convert a non-linear problem into a linear one. The indicator variables with the largest coefficients in the model are then returned as the drivers of the score. 
You can simply pip install it or can clone the Github repo: 
We will use Lime for explaining predictions of a random forest regressor model on the Diabetes dataset which is inbuilt in sci-kit learn. This post assumes that you already have some knowledge of Python, and Machine Learning. For the sake of simplicity, we will not cover all steps we usually follow in the model building pipeline like visualization and pre-processing. For the model building bit, You can clone the repo here. 
So, let’s cut to the chase and see how can we explain a certain instance using Lime. 
Understanding Model Behavior in predictions with Lime mainly comprises of two steps: 
The first step in explaining the model prediction is to create an explainer. We can use Lime Tabular explainer, which is the main explainer used for tabular data. Lime scales and generates new data using locality and computes statistics like mean for numerical data and frequency for categorical data, due to this we need to pass our training data as a parameter. 
In the second step, we simply need to call explain_instance for the instance in which you need explanations. You can use a different ‘i’ if you wish to understand a different instance. 
Finally, we can use the explainer to display the explanation for a particular prediction in the Jupyter Notebook. 
As we make our model complex, its interpretability decreases and vice-versa. A word of advice would be to take care of the trade-off between model complexity and its interpretability. 
You can optionally save your explanations as HTML file which makes it easier to share. 
Lime provides human-readable explanations and is a quick way to analyze the contribution of each feature and hence helps to gain a better insight into a Machine Learning model behavior. Once we understand, why the model predicted in a certain way, we can build trust with the model which is critical for interaction with machine learning. In this post, we used a Random Forest regression model to interpret its prediction on a particular instance. 
Interestingly, Lime also supports an explainer for images, textual data, and classification problems. You can explore Lime explanations further in more complex models such as Xgboost & LightGBM and compare predictions. Read more on Lime here. Also, here’s an interesting read on different tools for transparency and explainability in AI. 
I’d love to hear your thoughts about Lime, Machine learning, and Explainable AI in the comments below. If you found this useful and know anyone you think would benefit from this, please feel free to send it their way. 
Hands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look 
Written by 
Written by",Shashvat G,2020-07-26T11:13:06.834Z
Guide to Unsupervised Machine Learning | by Volodymyr Bilyk | Medium,"The effective use of information is one of the prime requirements for any kind of business operation. At some point, the amount of data produced goes beyond simple processing capacities. That’s where machine learning kicks in. 
However, before any of it could happen — the information needs to be explored and made sense of. That is what unsupervised machine learning is for in a nutshell. 
In this article, we will explain what unsupervised machine learning really is and explore its major applications. 
Unsupervised learning is a type of machine learning algorithm that brings order to the dataset and enables to make sense of data. 
Unsupervised machine learning algorithms are used to group unstructured data according to its similarities and distinct patterns in the dataset. 
The term “unsupervised” refers to the fact that the algorithm is not guided like a supervised learning algorithm. 
The unsupervised algorithm is handling data without prior training — it is a function that does its job with the data at its disposal. In a way, it is left at his own devices to sort things out as it sees fit. 
The unsupervised algorithm works with unlabeled data. Its purpose is exploration. If supervised machine learning works under clearly defines rules, unsupervised learning is working under the conditions of results being unknown and thus needed to be defined in the process. 
The unsupervised machine learning algorithm is used to: 
In other words, it describes information — go through the thick of it and identifies what it really is. 
In order to make that happen, unsupervised learning applies two major techniques — clustering and dimensionality reduction. 
Let’s take a look at both of them. 
“Clustering” is the term used to describe the exploration of data. The clustering operation is twofold. The catch is that both parts of the process are performed at the same time. 
Clustering involves: 
Clustering techniques are simple yet effective. They require some intense work yet can often give us some valuable insight into the data. 
As such, it’s been used in many applications for decades including: 
In a nutshell, dimensionality reduction is the process of distilling the relevant information. It can be also reiterated as getting rid of the unnecessary stuff. 
The thing is — raw data is usually laced with a thick layer of data noise. It can be anything — missing values, erroneous data, muddled bits, simple something irrelevant to the cause. Because of that, before you start digging for insights — you need to clean it up first. 
That’s what dimensionality reduction is for. 
From the technical standpoint — dimensionality reduction is the process of decreasing the complexity of data while retaining the relevant parts of its structure to a certain degree. 
K-means clustering is the central algorithm in unsupervised machine learning operation. It is the algorithm that defines the features present in the dataset and groups certain bits with common elements into clusters. 
As such, k-means clustering is an indispensable tool in the data mining operation. 
In addition to that — it is used in the following operations: 
Hidden Markov Model is one of the more elaborate unsupervised machine learning algorithms. It is a statical model that analyzes the features of data and groups it accordingly. 
Hidden Markov Model is a variation of the simple Markov chain that includes observations over the state of data. This adds another perspective on the data gives the algorithm more points of reference. 
Hidden Markov Model major fields of use include: 
In addition to that, Hidden Markov Models are used in data analytics operations. In that field, HMM is used for clustering purposes. It finds the associations between the objects in the dataset and explores its structure. Usually, HMM are used for sound or video sources of information. 
DBSCAN Clustering AKA Density-based Spatial Clustering of Applications with Noise is another approach to clustering. It is commonly used in data wrangling and data mining for the following activities: 
Overall, DBSCAN operation looks like this: 
DBSCAN algorithms are used in the following fields: 
PCA is the dimensionality reduction algorithm for data visualization. It is a nice and simple algorithm that does its job and doesn’t mess around. In the majority of the cases is the best option. 
In its core, PCA is a linear feature extraction tool. It maps the data in a linear manner in relation to the low-dimensional space. 
PCA combines input features in a way that gathers the most important parts of data while leaving out the irrelevant bits. 
As a visualization tool — PCA is good for showing a bird’s eye view on the operation. It can be a good tool to: 
t-SNE AKA T-distributed Stochastic Neighbor Embedding is another go-to algorithm for data visualization. 
t-SNE uses dimensionality reduction to translate high-dimensional data into low-dimensional space. In other words, show the cream of the crop of the dataset. 
The whole process looks like this: 
As such, t-SNE is good for visualizing more complex types of data with many moving parts and everchanging characteristics. For example, t-SNE is good for: 
Singular value decomposition is a dimensionality reduction algorithm used for exploratory and interpreting purposes. 
Basically, it is an algorithm that highlights the significant features of the information in the dataset and puts them front and center for further operation. Case in point — making consumer suggestions, such as which kind of shirt and shoes are fitting best with those ragged vantablack Levi’s jeans. 
In a nutshell, it sharpens the edges and turns the rounds into the tightly fitting squares. In a way, SVD is reappropriating relevant elements of information to fit a specific cause. 
SVD can be used: 
Association rule is one of the cornerstone algorithms of unsupervised machine learning. 
It is a series of technique aimed at uncovering the relationships between objects. This provides a solid ground for making all sorts of predictions and calculating the probabilities of certain turns of events over the other. 
While association rules can be applied almost everywhere, the best way to describe what exactly they are doing are via eCommerce-related example. 
There are three major measure applied in association rule algorithms 
The secret of the gaining the competitive advantage on the specific market is in effective use of data. 
Unsupervised machine learning algorithms let you discover the real value of the particular and find its place in the subsequent business operations. operation. 
This article show how exactly this thing happens. 
Got any ideas regarding data that require unsupervised learning? Go here! 
Written by 
Written by",Volodymyr Bilyk,2020-05-26T12:16:49.278Z
Get started with NLP (Part I). This is the first part of a series of… | by sigmoider | Medium,"This is the first part of a series of Natural Language Processing tutorials for beginners. Next post gives an introduction to NLP workflows. 
Nowadays, language is getting a lot of importance due to the recent boom of the so-called chatbots or conversational agents in several industries. But, as it happens with other fields of human knowledge, the study of natural language has a long past and these agents are not the first important application. Having a deep understanding of language is very important since we use it everyday in different scenarios and with different behaviors. 
This series of tutorials have the purpose of serving as an introduction to the amazing field of study that constitutes Natural Language Processing. 
Appart from an approach to communication, personal development, and psychotherapy (Neuro Linguistic Programming), NLP stands for Natural Language Processing, which is defined as the application of computational techniques to the analysis and synthesis of natural language and speech. In other words: the use of different techniques from computer science (algorithms) to understand and manipulate human language and speech. 
Sometimes NLP is confused with Machine Learning (ML), but this is just because some tools from ML are applied to NLP and improve the field. We can think of ML as a set of tools such that some of this tools are useful to solve NLP tasks. 
Also, to make this clear, Deep Learning (DL) is a branch of ML that makes use of a specific type of architectures or models named Neural Networks to solve learning tasks. We can think of DL models as a subset of tools within the set of ML models. It is not the only existing branch (there are others, such as Genetic Algorithms), but it is obtaining a lot of importance in the ML community due to two important reasons: 
This is important, since most of the current state of the art of NLP is being obtained through applying DL models. We will explain key concepts of DL for NLP in a later post, but now it is better to focus on the NLP basics. 
Finally, in Machine Learning the source is known as dataset, but in NLP and in general when our dataset is a large collection of texts, we usually talk about the corpus. 
There’s an area which is closely related to NLP and sometimes confused with it, that is Computational Linguistics. As Jason Eisner points out, the difference is the following: 
Both fields make use of Computer Science, Linguistics, and Machine Learning. 
The development of NLP has its meaning because of some specific problems and phenomena that arrive when we study natural language. Most of the times, these problems are unique in comparison to the problems that emerge in other fields of computer science or engineering, and that is in part what makes NLP such an interesting and different area. 
Now that we know what is and is not NLP and what problems does it face, we can start to learn which are the most basic NLP tools. In the next post we will apply these techniques using a Python NLP library called SpaCy. This post focuses on the concepts. 
a. Stemming and Lemmatizing: this tasks consist of reducing different forms of a word to a common base form. For example: 
Stemming usually refers to a crude process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational units (the obtained element is known as the stem). 
On the other hand, lemmatization consists in doing things properly with the use of a vocabulary and morphological analysis of words, to return the base or dictionary form of a word, which is known as the lemma. 
If we stem the sentence “I saw an amazing thing ”we would obtain ‘s’ instead of ‘saw’, but if we lemmatize it we would obtain ‘see’, which is the lemma. 
As it was already mentioned, both techniques could remove important information but also help us to normalize our corpus (although lemmatization is the one that is usually applied). 
b. Coreference resolution: consists of solving the coreferences that are present in our corpus. This can also be thought as a normalizing or preprocessing task. 
c. Part-of-speech (POS) Tagging: a POS tagger marks each word in a corpus by assigning a syntactic category such as: 
For example, given the sentence “I want to play the piano” a POS tagger should return: 
d. Dependency Parsing: sometimes instead of the category (POS tag) of a word we want to know the role of that word in a specific sentence of our corpus, this is the task of dependency parsers. The objective is to obtain the dependencies or relations of words in the format of a dependency tree. 
The considered dependencies are in general terms subject, object, complement and modifier relations. 
As an example, given the sentence “I want to play the piano” a dependency parser would produce the following tree: 
Here we can see that the dependency parser that I use (SpaCy’s dependency parser) also outputs the POS tags. If you think about it, it makes sense because we first need to know the category of each word to extract dependencies. 
We will see in detail the types of dependencies, but in this case we have: 
where a — b: R means “b is R of a”. For example, “the piano is direct object of play” (which is play — the piano: direct object from above). 
e. Named Entity Recognition (NER): 
in the real world, in our daily conversations we don’t work directly with the categories of words. Instead, for example, if we want to build a Netflix chatbot we want it to recognize both ‘Batman’ and ‘Avatar’ as instances of the same group which we call ‘films’ , but ‘Steven Spielberg’ as a ‘director’. This concept of semantic field dependent of a context is what we define as entity. The role of a named entity recognizer is to detect relevant entities in our corpus. 
For example, if our NER knows the entities ‘film’, ‘location’ and ‘director’, given the sentence “James Cameron filmed part of Avatar in New Zealand”, it will output: 
Note that in the example instances of entities can be just a single word (‘Avatar’) or several ones (‘New Zealand’ or ‘James Cameron’). 
Congratulations! Now you know the very basics of NLP so we can move to action and start learning to use this tools. The next post gives an introduction to NLP workflows. This was my first Medium article so I hope that it was as useful and clear as possible to you. Thank you very much for reading it, I’m open to and will thank any (respectful) form of suggestions and questions. 
sigmoider 
(Twitter) 
Written by 
Written by",sigmoider,2018-05-03T16:05:59.108Z
"AI in Cyber Security: Curse or Blessing? | by Lili Török | Small Business, Big World | Medium","Artificial intelligence, or AI, is one of the loudest buzzwords of our century. Back in the 1990s, the term was mostly associated with blockbuster movies about robots trying to take over the world. Today, AI is a very promising tool to help make people’s lives easier. Or much more difficult. 
Almost no segment of our lives is free of AI. Retail, sales, and customer service are just a few examples of business uses where AI is already a major player. 
Not to mention our personal lives: in a decade where even Home Alone gets a Google makeover, you shouldn’t be surprised when your apartment turns up the air conditioning and starts your coffee five minutes before you step through the front door. 
No wonder cyber security gets its fair share of AI. But is it as good as it sounds? 
In general terms, AI systems can be trained to look for malware or other suspicious activities. One way of training is through machine learning (ML), where the system is shown massive amounts of code, both harmful and clean. This way, the system learns how to distinguish between the two and how to react when encountering either. 
AI can be used for preventing cyber attacks. 
Thanks to machine learning, AI is able to detect harmful code and separate it from safe algorithms. 
In addition, AI can enhance protection by regularly checking the system for any vulnerabilities. If it finds anything out of the ordinary, it can alert an operator, or even fix it by itself. 
Based on their preventive skills and steps, AI systems can detect any malicious or abnormal activity. With the power of machine learning, an AI system can teach itself what kind of red flags to look for. This function is especially important for systems that use a large number of connected devices, like a smart home that features many potential entry points for hackers. 
In case of a successful cyber attack, AI can enhance responsive measures as well. AI can check the system for signs of compromise, saving cyber security personnel a lot of time and manual labor. In addition, AI is able to isolate infected parts of the system or quarantine vulnerable data in a safe spot to prevent the attackers from accessing it. 
While that certainly looks promising, there are side-effects of AI in cyber security. 
We may think of AI as an infallible system. But just like any other system, AI is designed by humans and thus prone to error. 
A system can be compromised right from the start. A human error in the code can lead to unforeseeable consequences later on. This can happen even without malicious intent. But if hackers get to the system during training, they can introduce their malware as a clean program, compromising the entire process from then on. 
An additional danger lies in the very nature of AI: working without human intervention. An error can go undetected for a long time, allowing hackers access to the entire system without anybody noticing. 
A way of defense against this is to use several, separate AI systems alongside each other, each one constantly checking how the other is working. While this has definite advantages, there’s an undisputed drawback: the cost. 
The math is obvious: the more AI systems in use, the higher the cost. 
Unfortunately, any tool used for good causes can be used for bad ones just as well. AI controlled by hackers can learn the way anti-malware software (or even AI) detects suspicious activity and potential cyber attacks. Then, using its knowledge, it can alter its malware’s appearance or code to get past the defenses. 
Even more alarming is the fact that hackers may infiltrate an AI system and change the way it works. For example, if a factory’s AI that runs its machinery is infiltrated by malicious intent, the consequences may be dire. A simple malfunction can lead to grave injury and eventual factory shutdown within a short amount of time. 
With a coordinated AI cyber attack, whole industries and even national economies may be brought to their knees. 
Does this sound like the plot of a Hollywood blockbuster? Certainly. But unfortunately, life may start imitating art in this regard very soon. 
We don’t want to sound overly pessimistic. AI is a great opportunity and a field that’s certainly worth exploring for many businesses. However, today it’s more important than ever to employ sufficient cyber security measures so you can keep all your systems safe. 
Written by 
Written by",Lili Török,2019-03-11T13:43:07.134Z
An In-Depth Guide to Personal Cybersecurity | by Nick Rosener | Medium,"I spent a day this week on an annual overhaul of my digital security. Several friends and colleagues were interested in a guide to doing the same; so I thought I would write one up and share with all of you: my closest internet friends. 
Brands are getting hacked. 
Media organizations are getting hacked. 
Tech companies are getting hacked. 
“Dating” websites are getting hacked. 
Small companies are getting hacked. 
Critical infrastructure is getting hacked. 
National political parties are getting hacked. 
Elections are getting hacked. 
Everybody and their grandmother is getting hacked these days. 
It’s no wonder online security breaches are becoming so prolific. Digital is pervading every corner of our lives, yet most people are terrible about security. In a 2016 Pew Research survey on cybersecurity, a substantial majority of online adults were able to correctly answer just two of the thirteen questions. 
Let’s do something about that, by beefing up our own personal digital security. 
Over the past 5 years or so, I’ve made it a habit to do an annual overhaul of my personal digital security. Each year, I review all of my online life for security threats, and commit to improving every year. These are the practices that I use as a result of that effort. 
This guide isn’t going to cover things that I would consider to be “the basics,” or practices that are typically covered in “top 10 lists of best practices” for general audiences. I have, however, included a few here for reference: 
In general, I attempt to address three core questions: 
Let’s begin. 
Let’s face it, people are terrible at passwords. We use passwords that are easy to guess, we re-use them across sites, and we keep all sorts of terrible password practices. 
Given how much sensitive information of ours is kept in our online accounts, the first thing you can do to beef up your security is to secure the way you log in online. 
The risk: You use a password at a mom-and-pop website to create an account. That website gets hacked, and it turns out the company stored your password in plain text in their database. If you re-use that password for another sensitive account (bank, social media, email , etc.) an attacker can use it to access your other accounts. 
In general, I follow these rules when it comes to passwords: 
The first thing to do when securing your logins is to get a comprehensive list of all the places you have online accounts. This can be daunting, and can be upwards of 100+, but this is the true scale of our online profile. 
The risk: An online service account you no longer use has an old, insecure password and stores sensitive data. This account may also belong to a website that has poor security practices, and is vulnerable to hacking (especially if you’re not using it anymore). 
Places to look to get a list of all the places where you have passwords to secure: 
Don’t Forget to Check the Following: 
Adobe, Airlines (Delta, United, JetBlue, etc.), Apple, Banks / Credit Unions (Chase, Bank of America, etc.), Craigslist, Dropbox, eBay, eCommerce Stores, Eventbrite, Facebook, Github, Google, Groupon, Healthcare (Cigna, ZocDoc, etc.), Heroku, Hotel Loyalty (Hilton, SPG, etc.), Imgur, Internet Providers (GoDaddy, CloudFlare, etc.), Intuit, Kickstarter, LinkedIn, Lyft, Mailchimp, Meetup, Mint, Mobile Phone (Verizon, T-mobile, etc.), Netflix, Online Training Providers (Udacity, etc.), PayPal, Publications (WSJ, NYT, etc.), Reddit, Slack, Spotify, Square, Starbucks, Student Loans, Tableau, Tax Services (TurboTax, TaxAct, etc.), Ticketmaster, Trello, Tumblr, Twilio, Twitter, Uber, University Email, UPS, Vimeo, Yahoo… 
Once you’re done, run a security challenge through your password manger. This will tell you: 
Most people don’t consider just how much personal data is sitting in their pocket, which can potentially be compromised. In this section, I go over several common topics that come into play when securing an iPhone (though many of these topics have similar processes for Android and other operating systems). 
Many in the security community point out that using TouchID (using your thumbprint to log in) is a bad idea for several reasons: 
This is an area where convenience conflicts with security: each person should make an informed choice on what they’re comfortable with. 
Location services are the systems on your phone which provide GPS location access to the apps on your phone. We often don’t consider the different ways that applications use our location data, but if unchecked, this can leak more information than we intend to tech companies who track our location, or through social media posts that attach location information to what we share. 
The risk: Your location data can leak your home or work address. 
Another risk: Publicly shared location can signal to potential thieves that your home is unoccupied. 
Yet another risk: Publicly sharing your location in real-time can signal people to come meet you in public venues when you don’t intend. 
Go to Settings -> Privacy -> Contacts to see which apps can access your contacts. For me, this was way more than I wanted. I removed most of them. Not so much a security concern as a privacy concern, but it’s personal preference. 
The risk: You start a social media account which you aren’t ready to publicly broadcast, but your social media profile is attached to your contact list, and the social network sends out a notification as soon as you set up the account to all other people who you know on the network. 
Another risk: The social media site who stores your contacts gets hacked, and your contact list becomes public. 
This is more of a privacy-related setting than security-related, but you can tweak the default ad tracking settings by going to Settings -> Privacy -> Advertising -> Limit Ad Tracking (Turn on). 
Check out what data is available when your phone is unlocked, and make sure you’re comfortable with it. 
I’ve found that using the iMessage apps on desktop and laptop leak more personal info than I feel comfortable. For example, iMessages have shown up on my computer’s notifications when not logged in, and my personal messages have come up on my computer during business presentations (unless I explicitly turn it off). I opted to log out of iMessage altogether on devices other than my phone. 
There’s a feature on iOS that allows you to ring multiple devices when your phone rings. For example, ringing your MacBook when your phone rings. I’m not personally very comfortable with this (it’s made it more obvious that I’m getting a phone call in business settings), so I disabled this at Settings -> Phone -> Calls on Other Devices. 
Many apps allow the option to add passcodes or TouchID inside the app. Imagine a situation where you give your phone to someone (like a curious 10-year-old nephew who wants to play a game) — is there any app you wouldn’t want that person to access? 
One of the main concepts in digital security is about not just preventing a breach, but minimizing the amount of data that is available in the event of a breach. In the case of iMessage, most people set their phones on the default of keeping their messages forever, but this offers a huge trove of potential data to an attacker that might access this data. 
You can set your phone to delete messages after a certain amount of time — I’ve set mine to delete messages after 30 days, in Settings -> Messages -> Keep Messages (set to 30 days). 
Personally, when I audited my messages, I was surprised at how much sensitive information I had sitting there. Setting the retention policy helps to keep this kind of information from persisting. 
I don’t probably have to tell you about how prevalent social media in our lives. According to Pew Research, 69% of all US adults use at least one social media site. It’s everywhere. 
Because social media use is so pervasive, most people I know are rather lax about the risks it can present. The social pressure to participate is strong. 
In my eyes, it’s possible to marry participation with security if you educate yourself about the risks. Below, I outline several common risks to using social media in general, as well as several tips for how to configure your privacy and security regimen for each platform. 
Some people create accounts for social media profiles that they want to be anonymous. Pay special attention to these accounts, because the platforms make it very difficult to remain anonymous. 
The risk: Your email is linked to your public profile, and the platform uses this in recommender algorithms to suggest your real friends. 
Another risk: You use the application on your phone which uploads your contact information, inviting your contacts to connect with your “anonymous” account. 
Yet another risk: The geolocation embedded in your posts, combined with other subtle cues, allows people to identify you. 
The practicalities of remaining anonymous in social media accounts are beyond the scope of this guide, but suffice it to say that it is very difficult. 
A cybersecurity audit isn’t complete without searching yourself to see what public information is available about you. There are two broad categories of information available to people searching for you: information you put out about yourself (through social media, your website, etc.) and information put out about you by third parties (news articles, data brokers, etc.). 
It’s a good practice to do a “background check” on yourself to see what you find. A couple places to try:1. Google2. Bing3. Pipl4. Spokeo 
Lastly, consider how the information you find about yourself could be used in a social engineering attack against you. The data you share here could be used to gain access to your accounts. For example, if you use your dog’s name as a recovery password, and post your dog’s name publicly, it could be used to guess a password. 
In the words of Andy Chen, “email is like a postcard.” Despite the imagery portrayed of emails being like a sealed envelope, unencrypted emails are often sent through multiple servers in plain text on their way to their destination. 
Once they get there, a pile of 10’s of 1000’s of emails can be a treasure trove of personal information to hackers. 
In this section, we explore some of the security practices around securing the data we keep in email and the cloud. 
Most of the security practices mentioned in the above sections are focused at preventing security breaches of your data. When it comes to email and cloud, these practices are especially important. If you haven’t already, make sure that you’ve hardened the logins for all your email and cloud file storage systems using the steps in Section 1 above. 
It’s not enough to assume that we’ll be perfect when it comes to preventing security breaches. The next level of security considers how to minimize the amount of data that would be compromised if your data were to be breached. 
This is where a “Data Retention Policy” comes in. 
The main idea in a data retention policy is to switch from a mindset of “do I need to keep this?” to a mindset of “why am I not destroying this?” 
The risk: Nearly any piece of personal data accessed by an attacker in a breach can be used to access other areas of your personal life, be used to gain access to other accounts, or be used in a social engineering attack. It can contribute to identity theft, be used to damage your reputation, be used as blackmail material, be released to the public directly, or be sold to third parties. 
This way, if your data is ever breached, the amount of data that is compromised will be much less than if you had emails going back several years. 
Important: People working in certain industries may be prohibited from doing this for legal compliance reasons. You may want to check with an attorney if you’re doing this for other than personal email. 
Once implementing a data retention policy for the data kept in email, apply the same idea to all the places your data is stored in the cloud. 
A personal anecdote: 
When I first began doing this audit for myself, I was shocked to find how much more sensitive information was stored in insecure places than I thought. As I was reviewing my files, I found old client passwords, credit card numbers, employee personal information and more in places I didn’t expect them. This was shocking for me, since I had thought I was keeping a strict security practice in my company while I was running it. 
It was definitely an eye-opening experience, and made me realize how easy it is to leave sensitive data unguarded. 
It’s a good practice to make sure that you would easily survive any of your devices being stolen or lost — not just things in the cloud. This entails two major areas: 
The browsing history and cookies in your browser can sometimes be a security risk. It’s a good practice to clear these regularly. To do this: 
Not everyone has the same level of digital security threats as others do. The advice above is what I would consider appropriate for general internet users to follow. 
However, there are some situations which can expose people to increased threats that aren’t typical to members of the general public. 
In CyberSecurity, “Threat Model” is a term used to represent the different types of attacks you want to consider when assessing security risk. 
For this guide, I’m breaking down “personal threat models” into common archetypes of increased risk. While not perfect, considering the following special cases that present higher risk can help you plan for specific types of threats that may not be present for the general public. 
The risk: Your laptop is stolen, and you didn’t yet have a chance to implement full-disk encryption on your drive. You run a social media agency, and the attacker finds a database of client financial information and web account logins on your computer. Now, not only is your own personal information subject to compromise, but so too is your clients’. 
If you run a small business, most of the above applies to you, but the risk is significantly higher. Not only do you have your data to protect, but you may have: 
Housing this data opens you up to additional liability and reputational damage if it is compromised through you or anyone on your team. I definitely recommend implementing the security procedures mentioned above (and more, where appropriate) to safeguard the data. 
The risk: Your website (or a client’s website) gets hacked. The compromised website is used to spread malware to visitors, promote advertising for unsavory products, and the site is blacklisted by Google and Chrome until the hack is mitigated. 
Having a website exposes your servers to the full hacking power of the global internet. Hardening a website to resist attackers is beyond the scope of this guide, but it’s worth researching / following up on if you haven’t considered it recently. At the very least, make sure your site’s data is backed up and it’s software is up to date. 
Another risk: Your eCommerce site gets hacked and isn’t properly secured, exposing private financial information of customers to attackers. 
Taking credit cards over the internet can put developers and business owners at huge additional liability in case of a breach. My recommendation is to attempt to have another organization handle the credit card transactions so customer financial info never touches your servers. 
If you must house the data on your servers, make sure you’re following PCI compliance guidelines, and consider insurance to cover data breaches. 
If you work for a large organization (especially one that’s well-known), it’s very likely that you have access to all kinds of juicy data that hackers or competitors would love to get their hands on. 
Corporate InfoSec is outside the scope of this article, but I will offer three general guidelines: 
I don’t have have children myself, so this is a difficult topic for me to comment on directly, but I found a few articles on this topic to consider if you’re a parent: 
Again, not a parent — mostly a thought experiment here. 
Most teens aren’t very worried about getting hacked. Many are more worried about their parents finding out sensitive details about their personal lives than they are about data breaches. Just think about that for a second — you may be a part of your child’s personal security threat model. In my opinion, it’s a big part of the appeal of Snapchat and other similar platforms. 
Teenagers are the worst. 
The risk: Becoming the victim of online harassment or stalking. 
The risk: Involvement with controversial issues can raise your profile and attract increased attention from hackers and other attackers. 
Other risks: If your work involves activism directed at governments, law enforcement, or other entities capable of surveillance, your online profile may be subject to increased surveillance. 
Hardening your personal profile and organization against sophisticated attackers is beyond the scope of this guide, but a few items to keep in mind: 
If this is you, Matt Mitchell is a great source for more info. 
The risk: Explicit pictures could be published, used as blackmail material, or shared in other ways without your consent. 
We’ve all heard about the risks of sending NSFW pictures to others. There’s more to keep in mind on this issue than the security concerns, however. 
Side note: please have compassion for people whose private images are made public without their consent. We live in a culture that is often quicker to shame a victim of this kind of sharing than to blame the sharer of the images. Always ask for consent before sharing any images of anyone (explicit or not) in any capacity, and always ask for consent before sending photos (especially explicit ones) to others. For more info, see Amy Adele Hasinoff’s TED talk on the topic. 
So far, this article has focused on a platform-centric approach to security: how to secure your cloud data, your email, your passwords, etc. Another way to approach security is by analyzing common “attack vectors” that we are often vulnerable to, in order to consider our own preparedness for these situations. 
I’ve outlined several below. 
The risk: Your computer is stolen, and attackers mine the data on the hard drive for personal information. 
Another risk: An attacker accessing your device while it is left unlocked and unattended. 
How to prepare: 
The risk: A vulnerability in an application you use is discovered, and can be used to exploit your device, apps, computer, web browser, or data. 
Always keep your software up to date on your phone, computer, and laptop. 
The risk: Unless you’re on a VPN, you should essentially assume that all your browsing activity and unencrypted credentials can be read by others on the network you’re using. 
The risk: Your flash drive gets stolen, and it has sensitive data on it. 
Flash drives are a huge deal for data security. You should essentially assume that anything in a flash drive could get stolen at any time. If possible, encrypt the drive to protect the data. At the very least, only keep the minimum amount of data on the drive that you need in order to do what you need to do — and wipe it often. 
The risk: An attacker can pose as you to contact a friend or family member while you’re traveling. The attacker can invent a fake emergency to convince people close to you to send money or other sensitive personal documents. 
How to prepare: 
Develop an authentication protocol to share with all your family members, and instruct them to use it whenever they receive a message from someone claiming to be you asking for money or personal documents. 
The protocol should be robust to: 
If you have a practice that I missed, leave it in a comment below. 
Written by 
Written by",Nick Rosener,2017-07-09T22:35:28.244Z
"Project ‘Gredient’ Identifies Food Allergy Risks | by Berkeley I School | BerkeleyISchool | Aug, 2020 | Medium","32 million Americans have a food allergy and millions of others have dietary restrictions. To eliminate the cumbersome chore of inspecting ingredient lists for allergens, 5th Year MIDS Students Isa Chau, Silvia Miramontes, Emma Russon, JJ Sahabu, and Chelsea Shu created Gredient, an iOS app that does the reading for you. 
Isa: Gredient is an award-winning iOS app that helps users check whether a product contains ingredients the user wants to avoid. The backend is powered by an optical character recognition (OCR) model hosted in an AWS serverless cloud infrastructure. With an OCR model, Gredient can handle even the rarest ingredients that users need to avoid, instead of relying on previously compiled ingredient or product databases. 
JJ: As consumers are becoming more health-conscious and allergy rates are increasing, shoppers can spend hours at a grocery store checking for harmful ingredients on product labels. Health-oriented, our team was inspired to address this issue and create a product that helps individuals maintain a safe and healthy lifestyle, with an app that would reduce the pains of watching what ingredients are in processed foods. 
Silvia: We completed the product within a 14-week time frame. From the start, we noted the ambitiousness of our project and understood that time would be our largest constraint. Our greatest challenges in this time frame were deciding and achieving a workable cloud infrastructure, as well as learning how to properly construct a mobile application for iOS devices. 
Since we had a good sense of our technical weaknesses and strengths, during our very first meeting, we devised a strategy to maintain momentum by setting deadlines to meet every week and stay on track. We also developed a set of guidelines to follow within the team to maintain accountability. The guidelines also indicated what to do whenever any team member encountered an issue. We believe that the final product would not have been possible without the team’s perseverance to work through the hurdles of app-development. 
Emma: With five members, we split our group into sub-teams to tackle front-end development and back-end modeling and infrastructure. These sub-teams spent the majority of the time constructing their respective components of Gredient, and worked to combine the front and back ends in the last couple of weeks of capstone. 
Silvia: During the early stages of the course, we spent approximately 2 weeks investigating optimal ways to create our cloud infrastructure. After researching and seeking advice through our network, we decided to utilize Amazon Web Services (AWS). 
Isa: Once we decided on the general architecture of our project, we split our team to specialize on the front-end and back-end, with one person as a go-between to make sure the two halves of the project were compatible and also to help wherever extra hands were needed. We also had two weekly video meetings, a goal-setting/task-delegation meeting, and a second check-in meeting halfway through the week. Our weekly goals were a big part of keeping our momentum up throughout the semester. We would also have informal “work-meetings” that any member could start or join to work together. 
Emma: We found W266: Natural Language Processing with Deep Learning and W207: Applied Machine Learning to be most helpful in preparing us for the development of the OCR and NLP models used in Gredient’s backend. With a serverless architecture, W205: Fundamentals of Data Engineering also proved to be helpful with the construction of data pipelines. Additionally, skills gained in W201 and W209 were helpful in designing engaging and effective presentations. 
Isa: We would love to release Gredient on the App Store, but for liability reasons, we need to seek legal counsel before doing so. In the future, we hope to develop a more sophisticated language mode to improve Gredient’s accuracy, improve the user interface; and also to develop Gredient for Android phones. 
JJ: We also are interested in adding premium features such as a scan history page, the creation of multiple profiles, and a harmful ingredients list. We will eventually seek partnerships with food and grocery delivery services to give users peace of mind when ordering dinner or buying groceries that their allergens are protected. 
Chelsea: There are 32 million people who have a food allergy in America, of which 200,000 require emergency medical care. Furthermore, grocery shopping while taking allergies into consideration can be time-consuming, nerve-wracking, and cumbersome. Long ingredient labels with fine print can make it difficult for the visually-impaired to read, and it can also cause uncertainty among people with allergies or caretakers of whether they truly read the label correctly. Our app, Gredient, aims to change the shopping experience by helping users to check ingredient labels for allergens quickly and accurately, so they can shop confidently and safely, and ultimately live a healthier, safer, and easier life. 
Written by 
Written by",Berkeley I School,2020-08-31T16:06:02.432Z
Understanding Genetic Algorithms in the Artificial Intelligence Spectrum | by Manish Kumar | Analytics Vidhya | Medium,"The field of genetics is seeing a lot of attention in AI these days. We have seen breakthroughs happening in scientific research lately but most people cannot make head or tails of how to even begin understanding this field. 
So in this article I will give you a tour of how the Genetic Algorithm works and why you should consider it the next time you are building a neural network model. Let’s dig in! 
Infinite Monkey Theorem 
The infinite monkey theorem states that if a monkey starts hitting keys at random on a keyboard for an infinite amount of time, he will almost surely type a given text, such as the complete works of William Shakespeare. In fact, the monkey would almost surely type every possible finite text an infinite number of times. However, the probability of this event is so tiny that it will require more time than the estimated age of the universe, but chances of occurrence of this event is not zero. 
Proof 
Suppose the typewriter has 50 keys, and the word to be typed is “banana”. If the keys are pressed randomly and independently, it means that each key has an equal chance of being pressed. Then, the chance that the first letter typed is ‘b’ is 1/50, and the chance that the second letter typed is ‘a’ is also 1/50, and so on. Therefore, the chance of the first six letters spelling “banana” is: 
(1/50) × (1/50) × (1/50) × (1/50) × (1/50) × (1/50) = (1/50)6 = 1/15 625 000 000, i.e., less than one in 15 billion. But still not zero, hence an outcome is still possible. 
So, the monkey will type the word ‘banana’ 1 out of 15,625,000,000 times. Now let us suppose the monkey hits a key per second the amount of time being taken for this event to occur in the worst case is 495 years approx. 
Now if I simulate a computer program for the above problem and do a Brute Force search for the word “banana”, the amount of computation and time involved is going to be huge. 
But, if I want to type the same, it will take me less than 6 seconds to do it. Why? Because I know letters, and I know the word banana and its spelling. 
So, can I use Evolution Theory and improve my program significantly? Yes, and this is thanks to the concept of Genetic Algorithms. 
Understanding Genetic Algorithms 
It is an algorithm that is inspired by Darwin’s theory of Natural Selection to solve optimization problems. It is a good solution especially with incomplete or imperfect information, or even limited computational capacity. 
In Darwin’s theory of Natural Selection, the three main principles necessary for evolution to happen are : 
1) Heredity — There must be a process in place by which children receive the property of their parent 
2) Variation — There must be a variety of traits present in the population or a means with which to introduce a variation 
3) Selection — There must be a mechanism by which some members of thr population can be parents and pass down their genetic information and some do not (survival for the fittest) 
There are Five phases in a genetic algorithm: 
1. Creating an Initial population 
2. Defining a Fitness function 
3. Selecting the parents 
4. Making a Crossover 
5. Mutation 
Creating an Initial Population 
In this step, we create a set of n elements which is called a Population. Each element from the population is a solution to the problem you want to solve. 
In our case, let this population be: 
Defining a Fitness Function 
The fitness function determines how likely an individual is fit to be selected for reproduction, and this is based on its fitness score. Let’s suppose our fitness function will assign a fitness score or a probability percentage to each element from the population, for each character matching our target word banana. 
In our case, let this population be: 
Elements, Fitness Score 
Selecting the Parents 
The idea behind this step is to select the fittest individuals and let them pass their genes to the next generation. Two elements of the population are selected based on their fitness scores. In our case, we select individuals with high fitness scores. 
In our case, we selected these elements as these words have a high fitness score from the given population. 
Elements, Fitness Score 
Making a Crossover 
It is the most significant phase in a genetic algorithm. In this step, we reproduce a new population of n elements from the selected elements. In this step, we have to permute and combine as many possible words from the characters obtained from the two parent words that were selected in the previous step. In our case, the parent words are ‘banyan’ and ‘cabana’. 
For example, we can pick the last 3 words from the word ‘banyan’ and first 3 words from the word ‘cabana’ and form a new word as ‘cabyan’, 
After applying all possible combinations from the word ‘banyan’ and ‘cabana’, we get a new population set. 
In our case the new reproduced elements are: 
Reproduced n Elements 
Making a Mutation 
There are chances that from the crossover phase, we might get a population which will not contribute to the evolution of a new diverse population and our algorithm will converge prematurely. So we need to alter the sequence of words from 1% of the newly created population to maintain this diversity. We can choose any sort of alteration. 
For example, suppose from 1% of the previous population we get words like ‘banyan’, and ‘yanbac’. Now we select these elements for creating a new population as these words have good fitness scores of 5 and 4 respectively, and thus have a high probability of being parents. Now if we pick the last 3 and first 3 letters from these two words and combine them, we will get ‘yanyan’ and this word is no longer productive enough to get any new diverse elements. 
But if we mutate our 1% of the previous population and alter the letters of ‘banyan’ and ‘yanbac’ by simply flipping the first and last letters in both words, we get ‘nanyab’ and ‘canbay’. Now if we apply the same combination of thelast 3 and first 3 letters of the mutated elements, we get ‘yabcan’ which is quite diverse from ‘yanyan’. (Note that in mutation you can alter elements in as many possible ways as you like. Flipping the first and last element is just a random way used in this example). 
When does this process stop? 
Our population has a fixed size. As new elements are formed, old elements with low fitness score are removed. When the population has converged, i.e., no new elements are reproduced which are significantly different from the previous population, then we may say that the genetic algorithm has provided a set of solutions to our problem. 
In our case when we find that all the population has a fitness score of 6 having a combination of all letters from word banana. 
Convergence 
We have a converged set, i.e., no matter how many times we repeat the entire above process, we are going to get only these set of elements. In our final set there must be the word banana, and so our simulated Infinite Monkey program has typed the word banana in a significantly less time as compared to brute force. 
Pseudocode 
Great algorithm but why should it be used in Artificial Intelligence? 
We can implement Genetic Algorithms to learn the best hyper-parameters for a Neural Network. To learn the hyper-parameters, we apply Genetic Algorithms as described in the steps below: 
• Create a population of several Neural Networks 
• Assign hyper-parameters randomly to all the Neural Networks 
• Repeat the following 
1. Train all the Neural Networks. 
2. Calculate their training cost (Ex- training error and regularization terms) 
3. From the cost of previous Neural Networks, calculate a fitness score from that set of hyper-parameters. The best Neural Networks will have the lowest cost. So, its inverse will give a high fitness value 
4. Select the two best Neural Networks based on their fitness 
5. Reproduce new Neural Networks from these 
6. Mutate the genes of the child 
7. Perform steps 5–7 for all the Neural Networks in the population. At the end of the latest generation, we have the optimum hyper-parameters 
Conclusion 
Genetic Algorithms can be used to solve various types of optimization problems. And we saw how to work with hyper-parameters in Artificial Intelligence with Genetic Algorithm. It’s a good alternative and worth checking out for your next project! 
Written by 
Written by",Manish Kumar,2018-09-05T05:51:01.172Z
CyberGRX – Medium,"Always know which third parties pose the most risk to your enterprise. Spot data risk sooner, respond to threats from third parties faster. 
Third-party risk management and wilderness risk management have many similarities… 
When I started my software quality career a little more than twenty years ago, we… 
I have 25k vendors. How do I know which vendors to assess for cyber risk?",NA,NA
Detecting real-time and unsupervised anomalies in streaming data: a starting point | by Jesus L. Lobo | Towards Data Science,"Sensors enable the Internet of Things (IoT) by collecting the data for smarter decisions in all kinds of systems. Data are usually produced in a real-time fashion, and then we may find ourselves forced to make a real-time processing (stream data mining [1]). The behaviour of the system is not always constant and unalterable, but may exhibit an unusual and significantly different from previous normal behaviour (anomaly [2]). Anomaly detection is valuable, yet it can turn into a difficult task to be executed reliably in practice. 
This article does not claim to be an exhaustive list of methods and solutions, but yes to be an easy entry point for those practitioners who tackle this problem for first time and need easy, understandable, and scalable solutions. Anomalies may be present in real-world applications such as fraud prevention, finance, energy demand or consumption, e-commerce, cybersecurity, medical diagnosis, social media, predictive maintenance, or fault detection among others. In this article we analyse some algorithms to deal with anomalies. They are Welford’s algorithm, a quartiles-based solution, a z-score metric-based solution, and a machine learning-based solution called Half-Space Trees (HST). The first 3 solutions are based on statistic indicators/metrics, while the last one comes from the machine learning field. 
In contrast with batch learning where data are assumed to be at rest (historical data is available), and where models do not continuously integrate new information into already constructed models, stream learning imposes constrained restrictions for the real-time processing: 
This set of constraints is the reason why most of existing anomaly detection algorithms for bacth processing are not applicable to streaming applications. 
In most practical cases, data consists of a sequential and univariate dataset, where supervised information about the anomalies (how many are, where they are) is not available. 
Welford’s method is a usable single-pass method for computing the running variance or the running standard deviation. It can be derived by looking at the differences between the sums of squared differences for N and N-1 instances. The data do not need to be stored for a second pass [3]. In order to used the Welford’s method for anomaly detection problems, I suggest to incorporate the following simple modification. 
We create an upper limit (UL) and a lower limit (LL). When the online mean consumption (orange solid line) overcomes any of these limits (or X times the online standard deviation), then we classify this reading (instance) as anomaly. The limits (black dashed lines) can be calculated as: 
UP=online mean consumption+X*online standard deviation 
LL=online mean consumption-X*online standard deviation 
The higher X is, the more false negatives (FNs) we will assume, and the lower X is the more false positives (FPs) we will obtain. So the choice of X is not trivial and there is a trade-off between this parameter X and FPs and FNs. Then, you need to define X=1, 2, 3, 4,… etc. depending on this decision. As you see, both online mean consumption and the limits are updated online every time a new instance arrives. 
Code: A simple implementation of the original method can be found here. 
Notes: readings=0 should be considered an anomaly or not. You have also noticed that the LL may be under 0 depending on the readings and X. This fact can be easily adjusted if it is a problem for your graphs. Please note that, if you obtain many consecutive anomalies, it is possible that you need to consider them as unique anomaly, or even as a “anomalous” period. Finally, I would like to mention the possibility of using a sliding window of size w, and calculate all the metrics over it. 
A boxplot is a popular way of representing the distribution of a dataset based on a set of number summaries: the minimum, the first quartile (Q1/25th Percentile), the median, the third quartile (Q3/75th Percentile), and the maximum. This representation can tell you about your outliers and their values. 
The interquartile range (IQR) goes from Q1 to Q3, and we can calculate the maximum and the minimum as: 
maximum=Q3+1.5*IQR 
minimum=Q3-1.5*IQR 
(For more detail on boxplots and quartiles, I recommend you to check the following article: https://towardsdatascience.com/understanding-boxplots-5e2df7bcbd51) 
Then, we can consider as outliers all those points that go above maximum or below minimum. And we can calculate in an online manner these number summaries. 
Code: as you see, this solution can be easily implemented. For lazy people, there are many available implementations of this method in known repositories or other websites. 
Notes: here we could also used a sliding window as previously mentioned. 
The standard score or z-score (z) gives an idea of how far from the mean a data instance is, i.e. how many standard deviations above or below the population mean a raw score is. 
The z-score for an instance x can be calculated as: z = (x — μ) / σ 
The standard score can be also calculated in an online manner. But this time, we have used a sliding window, then being known as running or moving z-score. Given this window size w, the moving z-score will be the number of standard deviations that each instance is away from the mean, where the mean and standard deviation are computed this time only over the previous w instances. 
Code: as you see, this solution can be easily implemented. For lazy people, there are many available implementations of this method in well-known source code repositories. 
Half-Space Trees (HST) [4] is a a fast one-class anomaly detector for evolving data streams. It requires only normal data for training and works well when anomalies are spread out in time. It does not work well if anomalies are packed together in windows of time. This technique is an ensemble of random HSTs, where each tree structure is constructed without any data, making the technique highly efficient because it does not require model restructuring when adapting to evolving data streams. 
This technique is incrementally trained, and uses an sliding window w. Other relevant parameters are the number of trees in the ensemble (nt), and the threshold for declaring anomalies (th). Any instance prediction probability above this threshold will be declared as an anomaly: the lower the score, the more likely it is that the current instance is an anomaly. To know more about all parameters please check the Code section below, or this paper. 
Code: this solution can be found in Creme or in scikit-multiflow, both frameworks implemented in Python. 
It also deserves special attention some other known approaches, among many others: 
[1] Bifet, A., Holmes, G., Kirkby, R., & Pfahringer, B. (2010). Moa: Massive online analysis. Journal of Machine Learning Research, 11(May), 1601–1604. 
[2] Chandola, V., Mithal, V., & Kumar, V. (2008, December). Comparative evaluation of anomaly detection techniques for sequence data. In 2008 Eighth IEEE international conference on data mining (pp. 743–748). IEEE. 
[3] Knuth, D. E. (2014). Art of computer programming, volume 2: Seminumerical algorithms. Addison-Wesley Professional. 
[4] S.C.Tan, K.M.Ting, and T.F.Liu, “Fast anomaly detection for streaming data,” in IJCAI Proceedings — International Joint Conference on Artificial Intelligence, 2011, vol. 22, no. 1, pp. 1511–1516. 
[5] Ahmad, S., Lavin, A., Purdy, S., & Agha, Z. (2017). Unsupervised real-time anomaly detection for streaming data. Neurocomputing, 262, 134–147. 
Hands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look 
Written by 
Written by",Jesus L. Lobo,2020-02-12T08:52:08.596Z
Distance Metric – Analytics Vidhya – Medium,,NA,NA
Word2Vec (Skip-Gram model) Explained | by n0obcoder | Data Driven Investor | Medium,"Word2Vec, as you might have guessed from the name itself, has something to do with words and vectors. But what exactly is it? Where do we use it? And most importantly, why do we even need it? 
We will try to find answers to all these questions in this blog and then we will try to wrap our heads around the idea of Skip-Gram model, which is a fairly interesting yet a very simple concept to understand. 
So let’s get started !!! 
word2vec is a class of models that represents a word in a large text corpus as a vector in n-dimensional space(or n-dimensional feature space) bringing similar words closer to each other. One such model is the Skip-Gram model. 
Skip-gram model is one of the most important concepts in NLP (Natural Language Processing) and we must understand how exactly does it work, rather than just using an already implemented model or pre-trained embeddings (got no idea what embeddings mean ? Don’t worry we will get to that very soon !), which by the way is very easily available. 
First, we need to understand that computers don’t understand words. All that a computer understands is numbers! Computers love numbers more than anything else. Well, maybe electricity comes first on the list. I think you guys got my point! 
So we need to find a way to convert every given English word present in the dictionary, to number(s). There are two ways of doing that actually. 
It is a way of representing categorical variables into vector forms(consisting of numbers, something that computers understand). The first thing to do when we are making One-Hot Encodings for a word present in our dictionary is to assign unique indices to all the words. This can be done by sorting the words in alphabetically ascending or descending order or maybe by arranging them in any random order. All we are interested in is mapping all the words to a unique index. 
Once the word-to-index mapping is done, to make the One-Hot Encoding of a word, we make a vector of length equal to the total number of words present in our dictionary, and we put 0 everywhere but at the index of that particular word. We put 1 at that index. So our vector ends up having tons of zeros and only a single ‘1'. 
For example, imagine having a dictionary of 10,000 words. 
Its has ‘a’ at it’s 0th index, ‘aaron’ at 2nd index ‘zulu’ at 999th index. 
One-Hot Encoding of ‘a’ will look like 
One-Hot Encoding of ‘zulu’ will look like 
I hope that you guys can come up with the One-Hot Encoding of any word, if you are given that word’s index in the word dictionary! 😄 
This is where Continuous Vectors come into play !!! 
These are the vector representation of words which consist of real continuous numbers (not just 0 or 1). In this representation, there is no fixed rule for the length of such vectors. You can pick any length (any number of features) to represent words present in a given dictionary. 
I am going to explain this to you by taking an example of words like ‘Batman’, ‘Joker’, ‘Spiderman’ and ‘Thanos’. Interesting selection of example words huh? I have made up 3-dimensional (3 features) continuous vectors of these words to explain to you, what the real numbers in the word vectors might represent. 
I hope that you have now understood, how different dimensions in a continuous word vector might capture different semantic meaning/features of that word, something that One-Hot Encoding fails to capture! 
The word vectors/embeddings don’t come with the right numbers making up the vector. The embeddings capture semantic meaning only when they are trained on a huge text corpus, using some word2vec model. Before training, the word embeddings are randomly initialized and they don’t make any sense at all. It’s only when the model is trained, that the word embeddings have captured the semantic meaning of all the words. 
The word embeddings can be thought of as a child’s understanding of the words. Initially, the word embeddings are randomly initialized and they don’t make any sense, just like the baby has no understanding of different words. It’s only after the model has started getting trained, the word vectors/embeddings start to capture the meaning of the words, just like the baby hears and learns different words. 
The whole idea of Deep Learning has been inspired by a human brain. The more it sees, the more it understands and learns. 
Skip-Gram model, like all the other word2vec models, uses a trick which is also used in a lot of other Machine Learning algorithms. Since we don’t have the labels associated with the words, learning word embeddings is not an example of supervised learning. This is semi-supervised learning because we don’t have the direct labels associated with the words but we use the neighboring words (of a context word in a sentence) as the labels. 
Word2vec takes a large corpus of text as it’s input and produces a vector space (feature space), typically of several hundred dimensions, with each unique word in the corpus being assigned a corresponding vector in the space. This is done by making context and target word pairs which further depends on the window size you take. 
Let’s understand this in more detail. 
The image above shows how context-target word pairs are made to be used for the training of Skip-Gram model. We use a window_size parameter (window_size is 2 in this case) which looks to the left and right of the context word for as many as window_size(=2) words. 
Let me walk you through the process of making context-target word pairs. 
Are you guys able to connect the dots now? : D 
So far we have our dictionary of 10,000 words ready and we also have the context-target word pairs. 
We take the One-Hot Encoding of the context word and feed it into the shallow neural network. This input is multiplied with the weights of the hidden layers and the end, we get the output vector (also with 10,000 components). 
Notice that the output layer has softmax applied to it which essentially converts the output vector of the neural network to a probability vector, with it’s each component representing the probability of the word at its index, being the target word. 
Then a loss function (Cross-Entropy Loss, because we treat this as a classification problem) is applied to compute the loss and back-propagation updates the weights (the word embeddings). 
This is how word embeddings are trained !!! 
Once our context and target pairs are ready, we train a neural network with a very few hidden layers(maybe only a single hidden layer) to perform a certain task, but we are never going to use this neural network to perform this task that it was trained for. Instead, we are interested in the weights of the hidden layers that have been learned during the training process. These weights, are the word embeddings that we have been discussing from the beginning of this post! 
This is called a ‘Fake Task’ because we are not interested in the prediction of the model, but the by-product(word vectors/embeddings) of the model. 
For the people, who want to get their hands dirty, here is a very simple implementation of word2vec (Skip-Gram-Model) in both, PyTorch and TensorFlow. 
In these implementations, I used a corpus from gensim library. I did some preprocessing on it, made a word dictionary, generated context-target word pairs and trained a single layer neural network. 
Once the embeddings were trained, I used TSNE to reduce the dimensionality of the embeddings for a set of words, so that the embeddings could be visualized on a 2D plot. And this is what I got! 
You can notice that words like ‘sun’ and ‘earth’ , with similar semantic meanings, have their 2D embeddings very close to each other. 
Also the words like ‘energy’ and ‘exploit’ , share some context which, my model was able to capture, and that’s why these two words have their 2D embeddings placed close to each other. 
This is a decent result, considering the small size of the text corpus that the model was trained on. 
I am writing this blog because I have learned a lot by reading other’s blogs and I feel that I should also write and share whatever I know as much as I can. So please leave your feedback in the comments section down below. Also, I am new to writing blogs, so any suggestions on how to improve my writing would be appreciated! :D 
In each issue we share the best stories from the Data-Driven Investor's expert community. Take a look 
Written by 
Written by",n0obcoder,2020-09-06T12:00:43.060Z
Algorithmic Trading in Crypto. We explore the design and… | by Kevin Zhou | Galois Capital | Medium,"We explore the design and implementation of trading algorithms in the crypto space. In particular, we focus on execution algos, market making algos, and several market microstructure considerations. We also investigate where practice diverges from theory, especially in handling the idiosyncrasies of the crypto markets. 
The objective of an execution algo is to transition a portfolio state into a different one while minimizing the costs of doing so. For example, if you wanted to increase your BTCUSD exposure by 1000, you might not want to instantly slam a market order into the BitMEX book, incurring a significant amount of slippage. Instead you might consider slowly getting into the desired position over time with a combination of market and limit orders over a number of different exchanges. 
An execution algo usually has 3 layers: the macrotrader, the microtrader, and the smart router. 
The macrotrader layer breaks up a large meta-order or parent order into smaller child orders spread across time. This is effectively the scheduling piece of the entire algo. VWAP, TWAP, and POV are common and simple examples of macrotrader algorithms. Generally there are many different market impact models that can be used in designing a sophisticated macrotrader layer. Market impact models look at how the market reacts to an execution. Does the market stay where it is after an execution? Or does it move further away? Or does it come back to some degree? The two most seminal market impact models are the Almgren-Chriss (1999, 2000) permanent market impact model and the Obizhaeva-Wang (2013) transient market impact model. Given that, in practice, market impact is not permanent, Obizhaeva-Wang seems to coincide with reality better. Since then, many new models have been formulated to deal with its deficiencies. 
The microtrader layer decides for each child order, whether to execute it as a market order or a limit order and, if as a limit order, what price should be specified. Much less literature exists on microtrader design. This is because the size of a child order is usually such a small part of the entire market that it doesn’t really matter how you execute it. However, crypto is different since liquidity is very thin and slippage is significant even for commonly-sized child orders in practice. Microtrader design generally focuses on order arrival distributions against time and depth, queue position, and other features of market microstructure. Market orders (and crossing limit orders if we ignore latency) guarantee execution while resting limit orders have no such guarantees. If execution is not guaranteed, you risk falling behind on the schedule set by the macrotrader. 
The smart router layer decides how to route executions to different exchanges/venues. For example, if Kraken has 60% of the liquidity and GDAX (Coinbase Pro/Prime) has 40% of the liquidity up to a given price level, then any market order decided upon by the microtrader should be routed 60–40 to Kraken-GDAX. Now you could make the argument that arbitragers and market makers in the market will transport liquidity from one exchange to another so if you execute half your order on Kraken and wait a few seconds, some of that liquidity would replenish from arbers and stat arbers moving over GDAX liquidity to Kraken and you would be able to get the rest done at a similar price. However, even in that case the arber would charge you something extra for their own profit as well as pass on their own hedging costs like Kraken’s maker fee. Moreover, some market participants post more than the size they want done across multiple venues and race to cancel excess size once they are hit. Ultimately, it’s best to have your own native smart routing. Native smart routing also has a latency advantage against third party smart routing services. In the former case, you can route directly to exchanges while in the latter case, you first need to send a message to the third party service and then they will route your order to exchanges (plus you have to pay the third party a routing fee). The sum of any two legs of a triangle is greater than the third leg. 
Market making is about providing immediate liquidity to other participants in the market and being compensated for it. You take on inventory risk in return for positive expected value. Ultimately, the market maker is compensated for two reasons. First, the market takers have high time preference and want immediacy. Market makers who facilitate liquidity to takers are, in turn, compensated for their lower time preference and patience. Second, the market maker PnL profile is left-skewed and generally most people have right-skew preference. In other words, market makers are analogous to bookies in betting markets, casinos, insurance companies, and state lotteries. They win small frequently and lose big infrequently. In return for taking on this undesirable return profile, market makers are compensated with expected value. 
At a high level, limit orders are free options written to the rest of the market. The rest of the market has the right but not the obligation to buy or sell an asset at the limit price of a limit order. In a perfectly informed market, no one would sell free options. It is only because the market is, in aggregate, not perfectly informed that it makes sense to sell free options. On the flip side, if the market was perfectly uninformed, a risk-neutral market maker would be willing to sell these free limit order options at even an infinitesimal spread since all trading would be noise. Obviously, real markets have a mix of participants, each with a unique level of informedness. 
In designing a market making algo, there are three perspectives to consider: the market maker’s, the market takers’, and the other market makers’. 
The market maker’s own perspective is represented by their inventory. If you already have too much asset exposure, you will probably lean/skew your quotes down and vice versa for having too little asset exposure. You do this for two distinct reasons. First, as a firm, you have some level of risk aversion (likely less than an individual but your utility for money is still concave). There are many constructions for the shape of this utility function (e.g. CARA, CRRA, more generally HARA, etc.). Second, as a passive liquidity provider in the market, you are subject to adverse selection risk. Active liquidity takers could know something you don’t or just be smarter than you. It’s basically the problem of selling free options into the market. Also, even at a mechanical level, a market order that hits your bid ticks the price down in a mark-to-market way while a market order lifting your offer ticks the mark-to-market price up. At the exact instant of any trade, you are always on the wrong side. Beyond that, a market maker’s quotes create passive market impact. In other words, the act of posting an order into the book has at least a slight effect of moving the market away from you. 
The market takers’ perspectives are represented by the order flow. The volume-weighted frequency of order arrival as a function of depth from the top of the book should have a few key properties. The function should be 1) decreasing, 2) convex (the intuition here is difficult to explain but this is unambiguously the case empirically), 3) asymptotically approaching 0 as depth becomes infinite. Some formulations require that this intensity function to be continuously twice differentiable for tractability which is a fine and reasonable assumption but ultimately unnecessary as well. Also, there are different formulations for how to calculate “depth or distance from top of the book”. You can generally use either some “fair mid price” or the best bid and best offer for each respective side. There are different tradeoffs between the two approaches which we won’t get into here. And beyond that, there is still a rabbit hole to go down on what a “fair mid price” should be. To add some color here, the mid price equidistant between the best bid and best offer is susceptible to noisiness when dust orders are posted and canceled. Also, given two cases with the same book shape, the last print being at the best bid would suggest a lower fair price than the last print being at the best offer. And there is another question of whether the history of the prints matter and if so should we look at it with respect to clock time or volume time? So where is the optimal limit order placement for a market maker given the characteristics of the flow in the market? If you post tight quotes near the top of the book, you will get filled often but make very little each time. If you post deep quotes, you will get filled less often but make “more” each time you are. This is effectively a convex optimization problem with a unique global maximum. Another consideration is order flow arrival across time which looks a bit like a Poisson process. Some suggest that it is closer to a Hawkes process. Moreover, bid-ask bounce, which a market maker tries to capture, is the shortest-term version of mean-reversion. Since this ultra short-term mean-reversion is scaled by local volatility, it makes sense for market makers to widen their quotes when vol is high and tighten their quotes when vol is low. 
The other market makers’ perspectives are represented by the book. The book reveals some private information of other market makers. More offers than bids near the top of the book suggest other makers are more willing to sell than to buy an asset. It’s possible that these makers already have large positive imbalances in inventory or they simply believe that the price is more likely to go down than up in the short-term. In either case, you, as a market maker, can adjust your quotes based on the skew of the book. Moreover, when market makers compete with each other, if the tick size is small, you often see this “penny jumping” behavior. Market makers compete for fill priority by jumping each other and laddering up on the book until some capitulation point is reached and there is a sole “winner” for fill priority. After a winner has been determined, the runner-up often drops back down to one tick in front of of the next best bid or offer. If you lost fill priority, you might as well get second fill priority and pay only just enough for it. This causes a regress whereby the winner now drops back to one tick in front of the runner up and the laddering game restarts. You can see this laddering game in real crypto market data. 
Finally, long-term directional signals can be overlaid on market making algos where the goal of the market making algo is no longer to keep inventory flat or constant but with some long-term target in mind and the corresponding skew to make it happen. 
There are two main reasons speed matters. First, you are able to hit orders resting in the order book before they are canceled. Second, you are able to cancel orders resting in the order book before they are hit. In other words, you want to pick off stale orders and you want to avoid getting your orders picked off. Arbitrage algos (active) and execution algos (active) care more about the former while market making algos (passive) care more about the latter. 
Generally the strategies that benefit the most from speed are the most basic. Any complex logic will necessarily slow down the roundtrip time. These types of algo strategies are the F1 cars of the trading world. Data validation, safety checks, instrumentation, orchestration, etc. might all be stripped away in favor of speed. Skip the OMS, EMS, and PMS (Portfolio Management System), and directly connect the computation logic on your GPUs to a colocated exchange’s binary API. A fast and dangerous game. 
Another class of speed-sensitive strategies, relativistic statistical arbitrage strategies, require servers to be physically positioned between multiple exchanges rather than colocated with an single exchange. While they will not be the fastest to data from any individual exchange, they will get and can act on correlation and cointegration data before any other strategy. 
In the speed game, winner takes most. In the simplest example, if an arbitrage opportunity exists, whoever can get to it first will claim the profit. Second place gets crumbs and third place gets nothing. The payoffs are likely power law distributed. 
The speed game is also a race to the bottom. Once everyone upgrades from fiber optics to microwave/laser networks, everyone is back on an even playing field and any initial advantage is commoditized away. 
Most matching engines obey price-time priority (pro-rata matching is a less common alternative but we don’t consider these for now). Limit orders at better prices get filled before limit orders at worse prices. For limit orders at the same price, the orders posted earlier get filled before orders posted later. 
Binance discretizes their order books down to a maximum of 8 decimal places. If a symbol has a price of .000001, a tick of .00000001 is 1% of the price. If a symbol has a price of .0001, a tick of .00000001 is 1 bps of the price. This is a huge difference. In the former case, jumping ahead of a large order costs a full point so time priority matters more while in the latter case, it is 100x cheaper so price priority matters more. In other words, if you have to pay up a full 1% to get fill priority, it may not be worth it because you are paying up a relatively large amount while increasing your probability of getting filled by a relatively small amount and it’s probably better just to wait in line but if you only have to pay up 1 bps to get fill priority, you might as well do that because you decrease your edge by a relatively small amount while increasing your probability of getting filled by a relatively large amount. Smaller ticks favor price priority and larger ticks favor time priority. 
This naturally leads to the next question: What is the value of your queue position? 
There are only two ways for a price level in the order book to decrement in quantity: either a trade crossed or a resting limit order was canceled. If a decrement was caused by a trade crossing, then all other price levels better than that would also cross and thus decrement. We can line up the ticker tape with tick-by-tick order opens and cancels and label each decrement as either a trade or a cancel. Intuitively, a trade means that two parties agreed to transact at a certain price while a cancel means that one party decided that it was no longer willing to buy or sell an asset at a certain price. Thus, at face value, we might say that a cancel at the best bid is a stronger signal that the market will move down in the short-term than a trade hitting that bid. 
On the other hand, there is only one way for a price level in the order book to increment in quantity: a resting limit order gets posted. 
Both increments and decrements in the order book reveal private information of market participants and thus provide short-term price signal. 
Right now most price indices take trade data from multiple exchanges and aggregate them together to get a volume-weighted average price. Tradeblock indices, in particular, also add penalties to exchange weights for inactivity and for price deviations away from the rest of the pack. But is there something more we can do? 
On GDAX, with a 0 bps maker fee and 30 bps taker fee, a printed trade at $4000/BTC hitting the offer side is effectively a seller selling at $4000/BTC and a buyer buying at $4012/BTC. The “fair price” of this trade should be closer to $4006/BTC rather than what was actually printed. On the other hand, because Bittrex’s fees of 25bps are symmetrically applied to both makers and takers, the fair price is the printed price. In other words, a print of $4000/BTC is effectively a buyer buying at $4010/BTC and a seller selling at $3990/BTC which averages out to the print itself. 
So, from a price discovery standpoint, ticker tapes are not directly comparable between exchanges and instead should be netted of fees and standardized when constructing a price index. Of course, there are some complications here because of volume-based fee tiers, which may increase or decrease the maker-taker fee asymmetry as they are climbed so we can’t know for sure where buyers bought and where sellers sold. This also suggests two interesting corollaries. 
First, price discovery is limited and in some ways schizophrenic on exchanges with strong maker-taker fee asymmetry. Assuming most accounts on GDAX are at the 0/30 bps maker-taker fee tier and noticing that GDAX often has 1 penny spreads on their BTCUSD book, each trade printing at the bid is approximately “fair value” trading at 15bps below spot and each trade printing at the offer is approximately “fair value” trading at 15bps above spot. So the “fair price” during calm times is rapidly oscillating between those two points with no further granularity for price discovery between them. 
Second, like tax incidence between producers and consumers, there is some equivalency between the maker and the taker on their fee incidence. If you charge makers relatively more, they widen out the book and pass on some of the fee incidence to takers. If you charge the takers relatively more, the makers tighten in the book and absorb some of the fee incidence from takers. The special edge case here is where you favor makers so much that the book squeezes to a single tick on the spread (like we often see on GDAX) and the book can’t get any tighter. Beyond this point, any extra fee incidence now falls on the exchange itself in terms of lost revenue. Outside of this special case, we can see that it doesn’t really matter to which side the fee is charged but rather it’s the sum of the maker and taker fee that matters. Ultimately, like the Laffer Curve in tax policy, exchanges face a revenue optimization problem in fee policy. We can see that the edge cases are the same as with tax policy. If an exchange charges no fees, they make no revenue. If the exchanges charge a 100% fee, there would be no trades and thus they would also make no revenue. With some additional thought, it’s clear that exchange revenue with respect to total fee level is a concave function with a unique maximum. 
Every OTC desk has semi-unique labeled graph data of each of its counterparties addresses and the coinflows between them and known exchange addresses. Labeled data provides a good starting point for many kinds of machine learning algorithms. 
Each miner has proprietary data on their marginal cost of minting a coin (in PoW). If they also have a sense of where they stand with respect to the rest of the miners in the world in terms of efficiency, they can derive unique insight on short-term supply gluts and shortages. 
Everyone knows black boxes are bad. It’s hard, if not impossible, to tell what’s going on and when something goes wrong, it’s profoundly difficult to diagnose why. Yet, many of the best hedge funds and prop shops eventually create black boxes. There’s a couple of very good reasons for this. First, people come and go at firms and badly documented legacy code will be difficult for newcomers to understand. Second, competition in the market means that any strategy a single mind can understand in it’s entirety will eventually lose to a strategy made in collaboration by experts and specialists in their own narrow field. Lastly, merging strategies is often better than running them separately. For example, suppose you had a long-term momentum strategy (S1) as well as a short-term mean-reversion strategy (S2). Surely, S1 could benefit from the short-term execution advantages of S2 and surely, S2 could benefit from the long-term drift predictions of S1. So naturally, we can combine them into a merged strategy which is more efficient than either of its constituents. Ultimately, strategies become black boxes, not because black boxes are desirable, but in spite of black boxes being undesirable. 
Suppose we had a model which predicted the frequency of Uber rides using a binary indicator of whether the ground was wet and it performed extremely well. Obviously, the ground being wet directly has nothing to do with Uber rides but, indirectly, rain causes the ground to be wet and rain also causes people to want to take Uber more. Even though our spurious model performs well, it is susceptible to tail-risk. If a water pipe bursts in a section of the city, causing the ground to be wet or there is natural flooding, we would wrongly predict that Uber rides should increase in frequency in that area. 
In general, when A implies B (A=>B) and A implies C (A=>C), a model of B=>C might work but only incidentally. So it is imperative that predictive relationships conform with intuition and common sense. It is not enough to blindly data mine and find strong predictive signals, but we should aim to unravel any confounding factors from them before the signals aggregate into a black box, upon which, these factors will then be increasingly difficult to unravel. 
Take a different example, say A=>B and B=>C. A model of A=>C will work but is inferior to a model of B=>C. First, A=>C leaves some money on the table because A may not be the only thing which causes (in the Granger causal sense) B; maybe A’ also causes B. Second, if the relationship A=>B breaks down, the A=>C model also breaks down but the B=>C model still works. 
Moving to multi-factor models, features should ideally be as orthogonal as possible to each other. For example, suppose we were investigating ice cream price as a function of sugar price and milk price. Perhaps a better model would be to use sugar price and season (spring, summer, fall, winter). The former model features are linked by inflation, are both of the category “food primitives/inputs” and “consumables”, and are both from the supply side of ice cream production while the latter model has 2 features which are much more orthogonal (one from the supply side and one from the demand side). Obviously using the 3 features of sugar price, milk price, and season would make a more accurate model but as the dimensionality of the model increases, calibrations will take at least super-linearly longer if not exponentially longer. By the time you have 20 features, it becomes intractable to run certain optimization methods like gradient descent so feature selection is key. We should drop correlated features in favor of more orthogonal features. 
Both empiricism and deductive reasoning are valuable in the context of designing quantitative models. 
One flaw of a purely empirical approach is that we cannot run controlled experiments in the markets. We cannot fix a point in time and try two different actions to see which performed better. In other words, there are no true counterfactuals in the soft/social sciences, unlike in the hard sciences. In trading, in particular, we are also unable to measure precisely the effect of our own actions on the market. In other words, during a historical time when we were not actively trading, we cannot know how the order book and flow would have behaved had we been actively trading and during a historical time when we were actively trading, we cannot know how the order book and flow would have behaved had we not been in the market. Another flaw of empiricism is that for any given historic pattern, there are an infinite number of models which would conform to the pattern but each could make an entirely different prediction of the future (i.e. a version of the black swan problem). Hume, Wittgenstein, Kripke, Quine, Popper, and Taleb all have many critiques and defenses of logical empiricism and the problem of induction that expounds further on these ideas. 
One issue with pure deductive reasoning is that we as humans are error-prone. Any mistake of logic along a chain of deduction would immediately void the result. Furthermore, soundness of an conclusion requires not just that each logical step along the way is valid but that the premises we assume are true themselves and in accordance with reality. Since models must be tractable to be useful, they are often simplifications of the world and make assumptions which do not hold against reality. 
Let’s look at an example. Suppose you were looking to run a Monte Carlo simulation for the trajectory of an asset price. If you take historic data on the asset returns and sample from them directly for your simulation paths, you run into the problem of 1) the data is sparse in the tails which represent extreme events and 2) you have some noisiness in the data away from some unknown true return probability distribution. Now, let’s say, instead of that, you fit the historic data to a normal distribution and then sample from it for your simulation paths. Now you run into a problem where returns are not actually normally distributed in reality (i.e. it’s leptokurtic; fat tails). So instead of all of that, you now fit historic returns to a Cauchy distribution or Levy distribution or even more generally to a Levy alpha-stable distribution. Now at this point, the model is getting more complex and you accidentally write a bug in the code. After a few days of toil, you figure out the problem and fix it. The code gets pushed to production and you have a working model… for about 2 years. 2 years later, it turns out that 5th moments matter and your Levy alpha-stable distribution does not capture this feature of reality. That’s basically how the game goes. 
Lastly, here’s two heuristics I generally use: 1) When in doubt, default to common sense. 2) All else equal, simplicity and parsimony are better than complexity and bloat. 
Having a theoretically profitable algo is one thing but dealing with the frictions of reality are another. 
Suppose you send a request to an exchange to post an order and normally you get a callback confirming that the order was posted or that there was an error and the order failed to be posted. Say one day, you don’t get a callback on your post request. Do you consider this Schrodinger order posted or failed? You are susceptible to both type 1 (false positive) and type 2 (false negative) errors by misclassifying the order. Is one error type less costly than the other? 
Suppose you are running an arbitrage strategy between two different exchanges. What do you do if one exchange’s API goes down in the middle of doing a pair of trades on both exchanges. One could have gone through but the other may have failed. Now you have unwanted inventory exposure. What’s the best way to handle this? 
How do you handle post and cancel delays when an exchange is getting DDoS’d or the matching engine is stressed under load? 
What about when exchanges make undocumented, unannounced changes to their APIs? 
Suppose an exchange does balance updates for its customer balances in parallel to their matching engine executing trades so balances queried on the same millisecond or microsecond as a trade printing could report conflicting balance states to the client where it looks like a trade executed but balances have not yet changed. How can you design your own systems to synchronize to a consistent state of the world even if the exchange reports conflicting states to you? 
Suppose the fees on an exchange are too high for you to place your limit orders at the model-derived optimal price. Or worse, some of your competitors were grandfathered into sweetheart deals with an exchange on fees. How would that change your behavior? 
How do you handle fiat rebalancing if your bank doesn’t operate over the weekend but crypto trading is 24/7? 
Arguably, an asset on one exchange is not perfectly fungible with the same asset on another exchange. First, each exchange’s counterparty risk is different, meaning the assets effectively bleed off differing negative interest rates. Second, because most exchanges have deposit/withdrawal limits, running into your limits means you can no longer physically rebalance assets between exchanges for some period of time. 
In your accounting systems, how do you handle forks, airdrops, dust attacks, and other situations which you cannot opt out of? 
Here’s a few heuristics that we generally follow. 1) Anything that can go wrong, will go wrong, even things you can’t currently think of, so build things to fail gracefully. 2) You and all third parties you connect with, like exchanges, will make mistakes. 3) Unlike consumer tech, don’t break things and iterate fast; rather, if you lose too much money, you won’t have a second chance. 4) Create system and data backups everywhere possible and create redundancies in your operational processes to account for human error. 5) Don’t use floating point types as precision loss could be very punishing for symbols with prices that are very small (e.g. KINBTC). 6) Reserve enough API calls from the API rate limit to burst cancel all open orders at all times. 
Trading is one of the only jobs in the world where the direct goal is to turn capital into more capital. Couple that with Wild West nature of crypto and you get a cesspool of get-rich-quick types. Everyone wants the easy answers but no one wants to learn the craft. People want talking heads on TV to tell them about price targets, when to buy and when to sell. They want to buy that online trading course for $99.99 or that TA-based algo strategy for $19.99 per month. But no one would actually sell you a magic box that prints money; they would keep it for themselves. There are no magic formulas for generating PnL. The markets continuously evolve and the game only gets harder over time. The only way to stay ahead of the game is to hire the best talent, who can adapt and continuously outdo themselves. 
The market is a giant poker table with millions of players sitting down, each of whom believes he or she can outplay his or her neighbor. And there is already a bit of self-selection for the people who sit at this table. Winning means playing better than a little bit more than half the capital at the table which, in turn, means you need to be better than 90% of the players since capital accrues to winners in power law fashion. 
Culturally, the trading game is different from VC investing. In Silicon Valley, it pays to be what Peter Thiel calls a definite optimist. You need to believe that new technology will change the world and that you can and will chart a course to make that happen. Over in Chicago where the prop shops are, the culture is much different. There, it pays to be a highly adversarial thinker. Everyone wants to win as badly as you do and every time you make a trade, you have that nagging thought in the back of your mind that maybe the person on the other side knows something you don’t. A startup in the Valley must first battle the indifference of the world before they face any real competition in the market. A trading shop, on the other hand, while not having customers to deal with, cannot avoid competition even from the very beginning. The very best shops, shroud themselves in secrecy. Crypto trading is the intersection of those two worlds and there are no clear winners in this nascent space. We, at Galois Capital, aspire toward that goal. 
If you are interested in the problems we are tackling and are willing to get down to the nitty gritty of algo trading, check out our careers page at galois.capital/careers and give us a ping at contact@galois.capital. 
Kevin Zhou | galois.capital 
Written by 
Written by",Kevin Zhou,2019-04-23T23:37:05.685Z
Object Modeling for Designers: An Introduction | by Heidi Adkisson | Medium,"As a user experience designer, it can feel as though I am waging an on-going, never-ending battle against complexity, especially when working on enterprise systems. 
A popular route to simplify an experience is to implement a design system. A number of full-fledged design systems are accessible on the web, and it doesn’t take much of a discerning eye to see the influence these systems have had on application design (an influence I’ll say more about later). 
Design systems are essential, but what I hope to show in this article is that a design system alone — particularly if it’s primarily a component level design system — does not guarantee a simple, consistent experience. I believe this because I see it regularly with the enterprise clients I work with. Most (though not all) have moved to a design system. And yet when I look at the result, there are still significant inconsistencies across screens that should be fundamentally similar. 
Design systems also don’t prevent what I call “feature tack-on,” where new functionality is merely tacked on to what is already there. Over time, this can create a labyrinth-like application structure. Feature tack-on is particularly endemic with enterprise systems, where customer requests can drive the development of highly specialized features. 
Why haven’t design systems proven to be a silver bullet? One problem is that design systems are often focused at the component level (or use the component level as a starting point). The systems then fail to mature beyond a collection of components. 
This is where an object model can help. 
An object model is a visual representation of a system’s objects, actions, and associated attributes. An object model can be used, in conjunction with a design system, to create a consistent experience across a system’s higher-level constructs. 
What exactly is a higher-level system construct? This idea is illustrated by Brad Frost’s Atomic Design Methodology, which uses a biology metaphor to describe how a design system can be organized. The smallest possible system components (such as an individual button) are considered atoms; atoms, in turn, can be arranged into molecules (for example, a group of components used for global search). Organisms, as a higher-level construct, are a grouping of molecules. The metaphor isn’t perfect, but it does convey the general scheme. 
I believe that design systems often struggle to represent higher-level system constructs because there isn’t enough of a framework to identify what should be included there. This issue is what object modeling can address. 
The whole idea of interface objects stems from the development of the graphical user interface (GUI). With a GUI, users could directly interact with objects represented on-screen; it was a radical departure from the command-line interfaces in use at the time. The Xerox Star system, released in 1981, was the first commercially-available system with a GUI — and served as inspiration for the Apple Lisa (in 1983) and later the Apple Macintosh (in 1984). 
In the 1990s as software began to proliferate, development processes became more formalized, and there was interest in representing software requirements in a consistent manner. UML (Unified Modeling Language) was developed to meet this need. The UML diagrams that are most likely to be familiar to UX-ers even today are the Use Case Model and the Class Diagram. 
The Use Case Model is a behavioral model and depicts the system from a task perspective (similar to user stories in Agile). 
The Class Diagram is a structural model, which shows a system’s objects and the relationship between them. Each object rectangle includes the object’s associated actions (in the lower pane) and attributes (in the middle pane). 
Within development processes reliant on UML, there was an emphasis on object-oriented user interface design, using the Class Diagram as the basis. And, indeed, in the 1990s whole books were written on this methodology. 
Admittedly, a complete Class Diagram can be time-consuming to complete and requires a set of analysis skills more typical of an engineer or analyst. It’s not something a UX professional today would typically prepare. 
I would argue, however, that object-orientation is a lost and necessary part of UX design, particularly for more complex applications. 
What I am proposing is a simplified object model I call a Narrative Object Model — narrative because it substantially replaces the UML notation with plain-English narrative. Also, while the Class Diagram is highly structured, following the rules of UML, the Narrative Object Model is lighter-weight and more flexible. 
In this next section, we’ll walk through the process of creating a model, breaking the process into three steps: 
1) Identifying the Objects 
2) Characterizing the Relationship Between Objects 
3) Identifying each Object’s Actions and Attributes 
Creating the model starts with identifying the source materials you have available. What sorts of artifacts do you have that describe or otherwise depict intended functionality? 
In my consulting work, I usually am starting with an existing system that we will redesign or re-envision in some way. Starting the model by inspecting that system’s existing interface accomplishes two things: 1) it gets me intimately familiar with that system in its current state 2) it provides a baseline object model that can then be extended to depict the desired future-state. 
The other primary source for an object model is user stories or other requirements-oriented artifacts. For greenfield design work, these may be the only sources available. 
We’ll begin an example model by looking at an existing interface, using the relatively simple example of Twitter.com. 
First, however, it’s important to define two central concepts: an object and an action. Simply stated, objects are the nouns in a system and actions are the verbs. In an existing interface, menus and button bars often contain clues to a system’s objects and actions. 
When working with objects, it’s also important to remember the difference between the object, which represents the concept, and an instance of an object. For example, most systems have the concept of an Account, which would be an object. However, there are many instances of an Account: the account of Louise Hughes, the account of Ramon Woods, etc. 
Below is a screen capture of a Tweet from Twitter.com. We have a button bar below the Tweet and a menu (shown expanded) to the right. We’ll use these to identify some nouns (aka objects). 
Looking at the menu labels, I can see that one noun is a Tweet. We have for example: 
Additional nouns I can identify are: 
The marked-up screen capture below shows the objects we’ve identified thus far: 
In our model, the objects become titled rectangles that we will connect together to show relationships. Here are our object rectangles so far: 
We definitely can (and will) do more with our objects. However, sometimes if I’m trying to make sense of a poorly organized system, it’s enough, at least initially, to get the objects identified and laid out on a page. 
I find the most logical next step, once I’ve identified the objects, is to identify the relationships between objects. 
Revisiting the UML Class Diagram, you’ll note there is a somewhat cryptic notation scheme for depicting relationships between objects. This scheme involves notating multiplicity of the relationship and showing relationship types using different line-end treatments. 
Rather than rely on notation to characterize the relationship between objects, the Narrative Object Model uses descriptive text. The diagram below shows the objects we’ve identified thus far with their relationships described. 
The descriptions help illuminate the purpose of each object in context of the larger whole: 
Again you could stop your model at this point, after the relationships have been defined, if you feel the model serves your purpose. However, it can be helpful to fill in more information about each object…specifically its associated actions and attributes. 
Next, let’s identify some Actions associated with a Tweet. We can again use the button bar and menu as a starting point: 
In an object, the actions are listed in the bottom pane of the object rectangle, as shown below. 
A stylistic note: I retain the traditional notation for actions in an object model, appending each action with parens. This notation is a preference on my part (completely unnecessary), but it does help visually distinguish actions, which can be helpful on larger, more complex object models. 
Note also that I’ve added some actions that aren’t depicted in the screen capture of the Tweet, but are found elsewhere in the interface — for example, the ability to remove a Tweet from a Moment. 
We’ll now fast-forward and assume that I’ve repeated identifying actions for our remaining objects. Here is what the model looks like at this point: 
It’s easiest to think of attributes as the data fields associated with each object. Attributes are what characterize each instance of an object; they listed are in the middle pane of the object rectangle, as shown below for the Tweet object: 
Here again, I’ve retained some notation from a Class Diagram. The colon after the attribute Text indicates the type of data — in this case, the fact that Text is limited in length (280 characters or less). I’ve only included data type for the Text attribute because I’ve deemed it to be especially meaningful to the user experience. 
Next, let’s look at our model more fully built-out, with some additional elements added to it. 
In addition to including more objects, I’ve added some visual elements to help me interpret the model more easily. These include: 
Not every object is of equal importance in a system. Some objects represent a system’s unique value proposition or are otherwise central to the experience. It usually makes sense to design conventions for the core objects first and then extend the conventions out to other objects in the system. 
For Twitter, I’ve indicated the Tweet and the Timeline objects as core, capitalizing the object name and using a bold outline treatment. 
In the model, it can be helpful to represent specific types of relationships. Relationship types illuminate more about the objects and how they might be treated from a design perspective. 
The four main types of relationships are: 
The simplest type of relationship is an association. Here we are showing the relationship between an Account and a Tweet with a plain line. In our Narrative Object Model, the line contains a verbal description of the relationship; the description includes reference to both objects (in bold). 
An aggregation relationship indicates an object that is merely a collection or list of other objects. For example, in Twitter users can add Tweets to a collection called a Moment. A Moment is a kind of collection of Tweets. 
An unfilled diamond notates an aggregation relationship. 
A component relationship is a type of dependency relationship where one object is a component of another. For Twitter, a Hashtag is an important component of a Tweet. The concept of a Hashtag is entirely dependent upon the idea of a Tweet — if you remove the concept of a Tweet from the system, the concept of Hashtag necessarily ceases to exist. 
A component relationship is notated by a filled-in diamond. 
Last but not least is the inheritance relationship. This is a parent-child relationship where the children objects inherit all or some of the parent object’s characteristics. 
Our example model for Twitter isn’t showing any inheritance relationships. However, if I was building a full object model for Twitter, I might use inheritance to depict different types of accounts, for example, a User Account and an Administrator Account. These objects, which share a subset of characteristics, are best treated as children of an Account. 
An inheritance relationship is shown using an unfilled arrow pointed at the parent object. The parent object contains the shared characteristics; only the unique characteristics are shown for the children. 
Note, however, in this example Account isn’t an actual object in the system — it’s an abstraction we used to create a parent. Here again, we can use traditional Class Diagram notation to indicate the abstract object <<Account>>. 
Though inheritance may seem a bit fussy to include in the model, it’s also one of the most useful relationships to understand. When designing, you want to make sure parent functions are presented consistently across all the children. 
Large models can become visually complex. To make them easier to read, it’s helpful to color-code objects that share a similar purpose. For example, for Twitter I’ve used three categories of objects: 
I’ve used Twitter to create an example model since it’s a relatively simple, well-known system. For simple systems, the objects and their relationships might be self-evident. However, that’s not usually the case with more complex systems, where the object model can provide a concise representation of an otherwise incoherent underlying structure. 
Below is a more real-world model for an enterprise system. This model is based on an actual workflow management system, though some details have been changed for presentation here. You can see the more extensive use of inheritance, component, and aggregation relationships — and expanded use of the verbal descriptions to help “tell the structural story” of the system. 
Object modeling takes some practice: the best way to learn is by doing. If you are working on a current project, go back and create an object model, even if you’re well along in the design process. If you don’t have a current real-world project, it’s easy to model existing systems on the web, as we did here for Twitter. 
Tips for Creating a Model: 
Of course, we aren’t creating models solely as an analytical exercise. We want to use the model to improve design outcomes. 
To begin, I want to emphasize that the Narrative Object Model provides a single, specific view of a system — a structural view. It’s not designed to replace other artifacts such as storyboards, journey maps, user stories, and use cases that represent the user experience from a task perspective. The object model is always a compliment to other design artifacts. 
Let’s look at how the object model fits into the design process. The model can be used to identify: 
I also use the object model when doing screen-level design to make sure there is a clear relationship between an object and all its associated actions. 
The rise of ready-made design systems such as Google Material Design has meant it’s easier than ever for an organization to get started on their design system. The risk with using publicly available components (or deriving directly from them) is that your application looks and feels like all the other ones out there. One way to avoid this is by first developing conventions for your core objects — those that are central to the experience — making the design of those objects distinctive. These collectively become the system’s signature elements, extended and built upon for the remaining design conventions. This approach has been proposed both by Dan Mall in “Distinct Design Systems” and by Emmet Connolly in “The Full Stack Design System.” 
Another way to view the model — and drive design conventions — is to identify those objects with a similar function or purpose and design those objects as a unit. For example, aggregate objects are often lists or collections that are inherently similar in function. By designing conventions for these objects concurrently, you can avoid the case where you’ve created a design based on one object but then discover it doesn’t appropriately extend to a substantially similar object. 
It’s also helpful to identify objects that have a similar purpose. In the previous section, we used color-coding to group objects into categories. 
In the Twitter model, there are three object categories: 
In the enterprise workflow system model, there are four categories: 
Here again, it can make sense to design the objects within a category concurrently so that the experience feels consistent. 
It’s also useful to look at cases of inheritance (parent-child relationships). You want to make sure parent functions are presented consistently across all the children. 
When designing new functionality, the object model can provide a helpful cross-check that all objects are functionally complete — and that actions, in particular, haven’t been overlooked. 
A starting point is looking for CRUD actions (create, read, update, and delete). For example, Twitter famously doesn’t allow users to update a Tweet once it’s been posted. As I am modeling Twitter and checking that objects are complete, I would notice this omission. In this case, the lack of an update function is intentional, but it could also have been overlooked. In my experience overlooked basic functions are not uncommon. 
It can also be helpful to look at objects that serve a similar function and compare them. For example, in Twitter, a Tweet and a Direct Message serve a similar function — to communicate with others. You can favorite a Tweet, but you can’t favorite a Direct Message. Should users be able to favorite a Direct Message as well? 
A common problem with systems that have grown over time (or have functions built by different teams) is inconsistent vocabulary. Sometimes inconsistencies are relatively minor (Edit vs. Update) but they can also be foundational (Task vs. Assignment). 
One reason to more completely fill out the actions and attributes for each object rectangle is to do a thorough vocabulary cross-check. The model can ultimately serve as a source for a controlled system vocabulary. 
In addition to checking vocabulary across objects, it’s also handy to review terms within an object. In Twitter, the word “Tweet” is used both as a noun and a verb. It’s evident that for Twitter this makes sense, but it could be a legitimate question to raise. 
During screen-level design work, the object model can serve as a reference to make sure there is a clear association between an object and its associated actions. Though this may sound like remedial advice, I’ve encountered many a system with illogically dispersed actions. Typically, this happens over time where new functions are placed where there’s available screen real estate rather than where they would more logically fit into the experience. 
In this article, I’ve attempted to lay out the case for object modeling. I consider object modeling a skill in my UX tool belt, along with conducting research, facilitating workshops, journey mapping, persona development, and the like. These are all inputs which, depending upon the needs of the project, contribute to successful design outcomes. 
For additional perspectives on object modeling for design, I invite you to explore these other resources: 
Happy Object Modeling! 
Written by 
Written by",Heidi Adkisson,2019-04-03T14:33:22.077Z
Natural Language Processing in Python | by Paco Nathan | derwen | Medium,"Practical techniques for preparing text for building knowledge graphs, custom search, content recommenders, and other areas of AI applications… 
I’ve been teaching a sequence of courses which take Python programmers through an introduction to popular NLP tools and techniques — based on spaCy — on through more advanced group projects incorporating NLP and ML, as an intro to AI applications. 
If you’re new to NLP, this course will provide you with initial hands-on work: the confidence to explore much further into use of deep learning with text, natural language generation, chatbots, etc. First, however, we’ll show you how to prepare text for parsing, how to extract key phrases, prepare text for indexing in search, calculate similarity between documents, etc. 
This first course in my new series is for you because… 
Upcoming: (will keep updated) 
Previous: 
Hands-on course materials are based on Jupyter notebooks and include instructions for installing Python libraries, although latter versions of the course run directly on Google Colab with no installations required. 
See the repo https://github.com/DerwenAI/spaCy_tuTorial and related material in the https://github.com/DerwenAI/pytextrank library. 
I’ve also been coordinating a machine learning competition that uses NLP, see https://github.com/Coleridge-Initiative/rclc for more details. 
In late November 2016, we launched a new program at O’Reilly Media to introduce a live online training program, as a new business unit which I’d helped to create. Although we’d produced similar online courses through O’Reilly for 14 months prior, moving that exclusively into a membership model was new territory. After months of preparation, research, planning, contingencies, position papers, etc., we took a deep breath and jumped in with both feet. The result? Thousands of people registered and waitlisted for courses within the first two days. Years later, demand has only increased. 
Keep in mind, these courses are the opposite of MOOCs. We realized how the industry had swung too far in the wrong direction with Ed Tech, how VC-backed tech startups had taken seriously detrimental short-cuts to attempt scale in learning, how current trends in “education” at scale opposed our ethos and experience at O’Reilly. Our origin story as a company was about peer teaching, with Tim and Dale active at Unix user group meetings. We’ve always been about peer teaching — that’s one reason I was eager to lead this program, calling back to my teaching fellowship many years ago at Stanford, where I’d helped establish a popular peer teaching program there. 
Written by 
Written by",Paco Nathan,2019-11-01T19:24:01.707Z
One Zero trust Architecture to rule them all? | by Andre Camillo | Medium,"Kings sitting behind fortified Castle walls and moats are a thing of the past. though I’m sure some our current politicians would love to be siloed in such buildings. 
But much like Monarchy eventually transitioned to a more people-centric government model (read Democracy) — something similar is happening to technology. Firewalls (our old, cosy and comfortable IT castles) are not enough to keep IT environments safe. Perimeter-based security was great when Data resided on On-Premises Data-centers. 
However, ever since the Cloud revolution ignited the adoption of cloud-based services and hybrid cloud, the data migrated too — then initiating this new “decentralized” model in which the data resides anywhere, and anyone with the right credentials can access it. 
But how do we effectively protect data wherever it is? Some people believe the journey is through a path they’re calling “Zero-Trust”. 
./why 
Zero-Trust is about access: least privilege, authenticated, authorized and contextualized. 
The Security benefits of it are: 
Business benefits include: 
./origins 
There are many Zero-Trust Models around. Consulting organizations such as Gartner and Forrester each has their own approach to this new security framework — and NIST still doesnt even has a proposed model (they released a draft recently, still under analysis). 
However, the foundations are the same, a security Architecture that is Data and Access Centric, instead of Perimeter-based. 
Although the roots of least privilege access to networks dates back to 1990s, the conception of the Zero-Trust Architecture is seen as dating back to 2010, from a Forrester report. 
Notoriously, the first implementation was done by Google, and they called it BeyondCorp. 
They defined Zero-Trust as the following: 
“a new model that dispenses with a privileged corporate network. Instead, access depends solely on device and user credentials, regardless of a user’s network location — be it an enterprise location, a home network, or a hotel or coffee shop. All access to enterprise resources is fully authenticated, fully authorized, and fully encrypted based upon device state and user credentials. We can enforce fine-grained access to different parts of enterprise resources. As a result, all Google employees can work successfully from any network, and without the need for a traditional VPN connection into the privileged network. The user experience between local and remote access to enterprise resources is effectively identical, apart from potential differences in latency.” 
Later on, new concepts were released such as Gartner’s Continuous Adaptive Risk and Trust Assessment — CARTA in short (2017). 
And then Forrester improved their original model and called it Zero-Trust Extended — ZTX (2018). 
As of 2019, NIST released the first draft for their own Zero-Trust Architecture (ZTA), proposal. 
Vendors started proposing their own versions of it around the same time. 
An important note — before any of these ones, The Jericho forum was a set of proposed standards for de-perimeterized networks issued by “The Open Group” organization, back in 2007. 
./what 
These are the pillars for each Zero-Trust model mentioned before: 
From Forrester’s Zero Trust model (2010) 
Goal: Make security ubiquitous throughout the network, not just at the perimeter because attackers will penetrate threat-centric defenses. 
Designed by John Kindervag. 
From Google’s BeyondCorp Implementation (2013) 
Gartner’s CARTA (2017) 
Forrester’s Zero Trust eXtended (2018) 
Expand the original model to adapt modern networks, while network segmentation and visibility remain critical, people access data and workloads outside the perimeter. 
Led by Chase Cunningham. 
*bold = Key pillars of this model 
NIST Zero Trust Architecture (2019 Draft) 
According to NIST, these are the Zero-Trust Architecture tenets: 
You gotta love their article on this- check the link for it in the “sources” section — it is very specific, and techie. 
./how 
This is something that I can’t answer. With so many options to achieve such architecture, though, the best one will depend on each business model. 
I personally believe that a working ZT architecture model is the one that will make your network: 
These are based on recommendations from the The Jericho Forum. 
But how to deploy all this is a much more complex and longer conversation. 
./conclusion 
Many options, proposals and there’s a very simple conclusion for all of this: 
Whatever Zero-Trust model you decide to go with, I can tell your Security Posture is heading to the right place. 
Get in touch if you need ideas/help with anything discussed here. Cheers. 
./sources 
Written by 
Written by",Andre Camillo,2019-10-22T10:16:01.591Z
Responses – Medium,,NA,NA
Shifting Peaks in Signal Separation and Curve Fitting | by Ossi Myllymäki | Medium,"This blog post describes algorithm(s) to handle shifting peaks in signal separation and curve fitting problems. 
All the related code can be found from my GitHub repo. 
The basic problem of signal separation problem here can be formulated as follows: given the pure component signals and mixture signal that is a linear combination of the pure component signals, one needs to solve the contributions of individual components. This problem appears in many different signal processing applications. 
In many cases, the problem is easy to solve using good old classical least squares fit method. The method works pretty well in many cases. To be more specific, it can handle random amplitude noise well. The problems appear when the mixture is not a linear combination of pure components (when it was assumed to be so), the mixture signal contains unknown components or there is error also in independent variable (also called x). 
This blog post concentrates on one type of problem where independent variable contains an error that is systematic by nature. This means that errors change from one sample to sample another but they follow some certain model. 
This blog post presents a method to handle this kind of errors. This method requires the user to specify a model for x-axis errors. In the ideal case, the model is known beforehand or it can be approximated somehow, using either theory or measurements. If the model is not known, the method can still be used by using some flexible error model, e.g. polynomial. 
Shifting of peaks causes problems in many applications, e.g. in NMR spectroscopy and chromatography. 
Parameters 
Input 
Output 
Description 
Make an initial guess for error model parameters 
While True: 
The update of error model parameters can be done in different ways. In this project, three different algorithms were implemented. 
Grid search 
All candidates are tested and the best option is returned as the final solution. 
Gauss-Newton 
The parameter estimates are updated using the Gauss-Newton optimization method. This method uses gradient to find direction and step size for the update. 
Evolutionary algorithm 
Parameter candidates are updated using an evolutionary algorithm. This method uses a population of random parameter combinations, evaluates candidates in population, and then generates a new population, based on results of the previous population. 
Combinations 
These three different update algorithms can be used separately or they can be combined to produce better results. For example, one can use the grid search to make the first rough estimate for parameters and then use more accurate Gauss-Newton to optimize the solution from there. Using the grid search first will reduce the risk that gradient-based Gauss-Newton optimization ends up to the local minimum instead of the global minimum. The evolutionary algorithm is much slower than Gauss-Newton but can handle local minima better. 
For testing and demonstration purposes, a synthetic data test set was generated. This sample set contained 100 samples with 3 components. Errors to the independent variable were generated using a quadratic polynomial model. To make the simulation more realistic, also amplitude noise was added to mixture samples. The pure components and generate mixtures are plotted below. 
For the analysis, the same error model (quadratic polynomial) was used as for the data generation. All the 100 samples were analyzed with and without correction using non-negative least squares as the fitting method. For x-axis correction, the parameter update was done using Gauss-Newton. 
In the figure below, fitting of one sample is illustrated. At the first iteration, the algorithm does regular NNLS fit with the initial guess (in this case, this means no x-axis correction at all). The error is large, as indicated by the large residual between measured and estimated signal. After that, the algorithm makes error model parameter update, using Gauss-Newton, and proceeds to the next iteration. In the next iteration, the x-axis correction is made with updated parameters and the residual is considerably smaller. This process is repeated until RMSE cannot be decreased anymore. For this sample, the algorithm converged after five iterations. 
The figure below shows analysis results for all the 100 generated samples. As can be seen, the errors without x-axis correction are large. The errors with x-axis correction are due to generated amplitude noise. 
This blog post described the problem that is related to shifting peaks in curve fitting and signal separation. The post described one type of algorithm that can be used to solve problems where shifting of peaks (errors in independent variable) follow some model. 
To use the algorithm, the user needs to provide an error model that is followed by errors of independent variable. The algorithm works by looking minimum RMSE by changing error model parameters and doing a regular curve fit after this. This process is repeated iteratively. The accepted solution is the one that produces minimum RMSE. Error model parameter updates can be made using different algorithms, e.g. Gauss-Newton, grid search, or evolutionary algorithm. In this algorithm, correction of the x-axis and the actual curve fitting are separated; this means that x-axis correction can be used in conjunction with any curve fitting method. 
Written by 
Written by",Ossi Myllymäki,2020-06-28T05:32:54.884Z
Geoffrey Hinton & Google Brain Unsupervised Learning Algorithm Improves SOTA Accuracy on ImageNet by 7% | by Synced | SyncedReview | Medium,"Geoffrey Hinton is once again in the AI spotlight, this time with new research that achieves a tremendous performance leap in image recognition using unsupervised learning. The AI pioneer and Turing Award honouree also made a rare appearance on Twitter promoting the research, “Unsupervised learning of representations is beginning to work quite well without requiring reconstruction.” 
Hinton’s comment regarding data types and model training echoes his speech at last week’s AAAI 2020 Conference in New York. Introducing his most recent work on Stacked Capsule Auto-encoders, Hinton quipped “I always knew unsupervised learning was the right thing to do.” 
Appearing on the same AAAI stage, fellow Turing Award winner Yann LeCun agreed that unsupervised learning may be a game-changer for AI moving forward: “We read a lot about the limitations of deep learning today, but most of those are actually limitations of supervised learning… This is an argument that Geoff [Hinton] has been making for decades. I was skeptical for a long time but changed my mind.” Unsupervised learning, which LeCun prefers to call “self-supervised learning” and which overlaps with the term “semi-supervised learning,” generally refers to model training that does not require manual data labelling. 
In the paper A Simple Framework for Contrastive Learning of Visual Representations, a team of Google Brain researchers including Hinton propose a simple but powerful “SimCLR” framework for contrastive learning of visual representations. The team concludes “A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over the previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100x fewer labels.” 
The impressive experiment results have made the paper topic a hot topic across the machine learning community. 
How to learn visual representation effectively without human supervision has been a longstanding problem for AI researchers, with generative approaches and discriminative approaches the two main existing methods. As discriminative approaches based on contrastive learning in the latent space have recently shown promising results, this is where the team applied their efforts. 
Contrastive visual representation learning was first introduced to learn representation by contrasting positive pairs against negative pairs. While previous researches used a memory bank to store the instance class representation vector, the team developed simplified contrastive self-supervised learning algorithms that do not require specialized architectures or a memory bank. The SimCLR framework learns representations by maximizing agreement between differently augmented views of the same data example through a contrastive loss in the latent space. 
The team observed that data augmentation has played an important part in yielding effective representations, and believe that conducting multiple data augmentation operations — random cropping, colour distortion, Gaussian blur, etc. — is crucial in defining the contrastive prediction tasks that yield effective representations. And compared to supervised learning, unsupervised contrastive learning shows greater benefits from stronger data augmentation. 
Currently, supervised learning and unsupervised learning are the two main machine learning methods. Traditional supervised learning however requires labelled data for algorithm training, and correctly labelled datasets are not always accessible. Unsupervised learning thus represents something of an ideal solution, as it allows researchers to feed unlabelled data directly to a deep learning model, which then attempts to extract features and patterns and essentially make sense of it. Semi-supervised learning meanwhile uses training datasets comprising both labelled and unlabelled data (usually much more of the latter than the former). This method performs particularly well when labelling the data is prohibitively time-consuming, and extracting relevant features from the data is difficult — for example with medical images such as CT scans and MRIs. 
The main machine learning methods are examined closely in the new Google Brain study, with researchers proposing that some previous methods for unsupervised or self-supervised learning may have been unnecessarily complicated. The researchers say the strength and performance of their new simple framework suggest that “despite a recent surge in interest, self-supervised learning remains undervalued.” 
The paper A Simple Framework for Contrastive Learning of Visual Representations is on arXiv. 
Journalist: Fangyu Cai | Editor: Michael Sarazen 
Thinking of contributing to Synced Review? Synced’s new column Share My Research welcomes scholars to share their own research breakthroughs with global AI enthusiasts. 
We know you don’t want to miss any story. Subscribe to our popular Synced Global AI Weekly to get weekly AI updates. 
Need a comprehensive review of the past, present and future of modern AI research development? Trends of AI Technology Development Report is out! 
2018 Fortune Global 500 Public Company AI Adaptivity Report is out!Purchase a Kindle-formatted report on Amazon.Apply for Insight Partner Program to get a complimentary full PDF report. 
Written by 
Written by",Synced,2020-02-19T17:35:36.417Z
Similarity Queries and Text Summarization in NLP | by Aravind CR | The Startup | Medium,"Before diving directly into similarity queries it is important to know what a similarity metric is. 
Similarity metrics are mathematical constructs which is particularly useful in NLP — especially information retrieval. We can understand a metric as a function that defines the distance between each pair of elements of a set or vectors. We can see how this can be useful — we can compare between how similar 2 documents would be based on the distance. If a lower value is returned by a distance function then it is known that 2 documents are similar and vice-versa. 
We can technically compare any 2 elements in the set — this also means that we compare 2 sets of topics created by topic model (If you don’t know what a topic model is please do checkout my previous blog Topic modeling-LDA) 
Most of them would be aware of one distance metric — Eucledian metric . It a distance metric we come across in high school mathematics, and we would have likely seen it being used to calculate the distance between two points in a 2-dimensional space (XY). 
Gensim, scikit-learn and most other ML packages recognize the importance of distance metric and have them implemented in their package. 
Its demostrated in the below implementation that we have the capability to compare between two documents, now its possible for us to set up an algorithm to extract out the most similar documents for an input query. 
Index each of the documents, then search for the lowest distance value returned between the corpus and the query, and return the documents with the lowest distance values — these would be most similar. Gensim has in-built structures to do this document similarity task. 
The tutorial on the Gensim website performs a similar experiment, but on the Wikipedia corpus — it is a useful demonstration on how to conduct similarity queries on much larger corpuses and is worth checking if you are dealing with a very large corpus. 
In the examples, we used LDA models for both distance calculation and to generate the index for the similarities. We can, however, use any vector representation of documents to generate this — it’s up to us to decide which one would be most effective for our use case. 
Below attached notebook contains samples for both similarity metrics and queries and has detailed explanation for each and every step, run the cells in the notebook before moving to next topic. 
Summarization is a process of distiling most important information form source or sources to produce an abridged version for a particular users and tasks. 
Text Summarization is a task of condensing piece of text into shortened form while preserving the meaning of the content and overall meaning. Text summarization is important because of the amount of textual data generated is increasing on another level day by day and it becomes difficult to read textual information and often people get bored to read large blocks of text. This is when text summarization came to rescue to make work easier for the readers. 
The idea of text summarization is to find the subset of data which contains information of the entire set, but sometimes it results in loss of information. 
Main idea — 
2 approaches for text summarization are — 
In text analysis, it is useful to summarize large bodies of text — either to have brief overlook of the text before deeply analyzing it or identifying keywords from the text. 
We will not be working on building our own text summarization pipeline , but rather focus on using the built in summarization API wich gensim offers. 
Its also important to know that gensim dosen’t create it own sentences, but rather extracts key sentences from the text which we run the algoirthm on (Extractive summarization). The summarization is based on the text rank algorithm(A graph based ranking model for text processing). 
Graph-based ranking algorithms are essentially a way of deciding the importance of a vertex within a graph, based on global information recursively drawn from the entire graph. The basic idea implemented by a graph-based ranking model is that of “voting” or “recommendation”. When one vertex links to another one, it is basically casting a vote for that other vertex. The higher the number of votes that are cast for a vertex, the higher the importance of the vertex. Moreover, the importance of the vertex casting the vote determines how important the vote itself is, and this information is also taken into account by the ranking model. Hence, the score associated with a vertex is determined based on the votes that are cast for it, and the score of the vertices casting these votes(you can find this explanation in research paper). 
Run the cells in the notebook for clear understanding . 
Above code sample contains extractive summarization of text. There are deep learning approaches and pretained models(which have achieved state-of-art-results) available for abstractive summarization of text, check out their implementation to dig-deep into the concept. 
One of the recent research for abstractive text summarization where the model predicts the masked sentence for summarization. 
Paper: https://lnkd.in/gtaNRF7 
GitHub: https://lnkd.in/gKJJ2c9 
Read more here: https://lnkd.in/g--hiY6 
Throught the blog we saw how basic mathematical and information retrieval methods can be used to help identify how similar or dissimilar 2 documents are and how text summarization can come in handy for various tasks(Financial research, Question answering and bots, Medical cases, Books and literature, Email overload, Science and R&D,Patent research, Helping disabled people, Programming languages,Automated content creation). Checkout the applications of automatic summarization in enterprise. 
— — — — — — — — — — — Thank you — — — — — — — — — — — 
Sometimes later becomes never. So do it now. 
Keep Learning…………………… 
Written by 
Written by",Aravind CR,2020-06-21T04:39:16.438Z
Applications of Graph Neural Networks | by Aishwarya Jadhav | Towards Data Science,"Graphs and their study have received a lot of attention since ages due to their ability of representing the real world in a fashion that can be analysed objectively. Indeed, graphs can be used to represent a lot of useful, real world datasets such as social networks, web link data, molecular structures, geographical maps, etc. Apart from these cases which have a natural structure to them, non-structured data such as images and text can also be modelled in the form of graphs in order to perform graph analysis on them. Due to the expressiveness of graphs and a tremendous increase in the available computational power in recent times, a good amount of attention has been directed towards the machine learning way of analysing graphs, i.e. Graph Neural Networks. 
According to this paper, Graph neural networks (GNNs) are connectionist models that capture the dependence of graphs via message passing between the nodes of graphs. They are extensions of the neural network model to capture the information represented as graphs. However, unlike the standard neural nets, GNNs maintain state information to capture the neighbouurhood properties of the nodes. These states of the nodes of a graph can then be used to produce output labels such as a classification of the node or an arbitrary function value computed by the node. The network tries to learn these encodings(hv) for each of the nodes through a mutual sharing of data among the nodes’ neighbours in an iterative manner until convergence. 
Given the wide range of domains in which GNNs can be applied, most of them could either be classified as structural applications or as non-structural applications based on the type of data. The structural case represents data that has an inherent structure to it such social interaction networks or molecular networks. Consequently, the non-structural scenarios represent cases where the structure in the data is not explicit such as text and images. In these non-structural scenarios then, the general approach is to try to transform the data to a structured format and then apply GNN. There are a few other applications as well that do not fall into the purview of either of the classes mentioned above. We will discuss some interesting applications from each of these categories here. 
The human brain is said to aid its reasoning process through the creation of graphs learned from daily experiences of the real world. Hence great importance is being assigned to the modelling of real-world entities and their interactions as graphs. This is deemed as a first steps towards human-like artificial intelligence. By considering objects as nodes and the relations or interactions between them as edges, simple physical systems can be modelled as graphs and can be effectively analysed using GNN. 
Interaction networks can be trained to reason about the interactions of objects in a complex physical system. It can make predictions and inferences about various system properties in domains such as collision dynamics (rigid and non-rigid). It simulates these systems using object and relation centric reasonings using deep neural networks on graphs. These GNNs have been shown to predict the trajectories of different objects thousands of time steps into the future. 
Visual interaction networks make use of the above interaction network to go a step further and learn about the dynamics of a system from just its raw visual observation or to put it simply, with as little as six video frames of the system in action. Apart from predicting the future trajectories of a range of physical systems just like its parent network, this model can also infer the mass and future states of unknown objects from their influence on the visible ones! This is achieved through the co-training of a perceptual front-end convolutional neural network that parses a visual scene to provide a set of object node representations to a back-end interaction network. 
The nano-scale molecules have an inherent graph like structure with the ions or the atoms being the nodes and the bonds between them, edges. GNNs can be applied in both scenarios: learning about existing molecular structures as well as discovering new chemical structures. This has had a significant impact in computer aided drug design. 
Molecular fingerprints are feature vectors that represent molecules. Machine learning models to predict the properties of a new molecule by learning from example molecules use fixed length fingerprints as inputs. GNNs can replace the traditional means that give a fixed encoding of the molecule to allow the generation of differentiable fingerprints adapted to the task for which they are required. Furthermore, these molecular fingerprints learned by the GNN from the graph structure of the molecule need not be as large as the fixed fingerprints which must encode all possible substructures in a single feature vector. The differentiable fingerprints can be optimised to encode only relevant features, reducing downstream computation and regularisation requirements. 
Image classification, a classic computer vision problem, has outstanding solutions from a number of state-of-the-art machine learning mechanisms, the most popular being convolutional neural networks (CNN). GNN, which drive their motivation out of CNN, have also been applied in this domain. Most of these models, including GNN, do provide attractive results when given a huge training set of labelled classes. The focus now is towards getting these models to perform well on zero-shot and few-shot learning tasks. Zero shot learning (ZSL) refers to trying to learn to recognise classes that the model has not encountered in its training. ZSL recognition relies on the existence of a labelled training set of seen classes and the knowledge about how each unseen class is semantically related to the seen ones. One approach is to leverage structural information, in the form of graphs, in ZSL image classification. GNN, consequently, appear quite appealing in this respect. Knowledge graphs can provide the necessary information to guide the ZSL task. Different approaches differ in the kind of information they represent in the knowledge graph. These graphs may be based on the similarities between the images themselves or those of the objects in the images extracted through object detection. The graphs may also incorporate semantic information from word embeddings of the category labels of the images. GNNs can then be applied to this structured data to aid the ZSL image classification-recognition task. 
Like images, the relation within text is also not explicit, i.e. text cannot be considered structured data to which GNN can be applied directly. However, there are ways to convert a text document into structured data such as a graph of words or of sentences and then use graph convolution to convolve the word graphs. Another approach to structure the text data is to use Sentence LSTM which views an entire sentence as a single state (node) consisting of sub-states i.e. words. Document citation relations can also be used to construct graphs with documents as the nodes. Text GNNs can then be used to learn embeddings for words and documents. These approaches can be used for various NLP tasks such as text classification, sequence labelling, machine translation, relation and event extraction. 
One interesting area of application is reading comprehension; given a text passage, the model must be able to answer questions based on it by consolidating information in the passage. 
Reading comprehension is one of the complex reasoning tasks performed by humans; the answer to a question may not be located in any single part of the extract but may need global inferencing. Representing the passage in the form of a graph using a Sentence LSTM helps in better connecting the global evidences. 
Combinatorial optimization problems over graphs are a set of NP-hard problems. Some of these can be solved by heuristic methods. In recent times, attempt is being made to solve them using deep neural networks. Consequently, GNNs are also being leveraged to operate over these graph structured datasets. 
Interestingly, a general GNN based framework can be applied to a number of optimisation problems over graphs such the minimum vertex cover problem, maximum cut, the travelling salesman problem, minimum spanning tree, etc. A few approaches combine GNNs with reinforcement learning to iteratively learn a solution given an input graph. One of the instances in which GNNs outperform traditional methods is the Quadratic Assignment Problem. It aims to measure the similarity of two graphs. The comparison of the graphs using GNN-learned node embeddings offers better performance than standard relaxation-based techniques. 
Graph Neural Networks are increasingly gaining popularity, given their expressive power and explicit representation of graphical data. Hence, they have a wide range of applications in domains that can harness graph structures out of their data. Presented above is just the tip of the iceberg. As newer architectures continue to crop up, GNNs will continue to foray into diverse domains. 
Graph Neural Networks have now evolved into Graph Convolution Networks which, as the name suggests, are inspired by Convolution Neural Networks. These are much more efficient and powerful and form the baseline for other complex Graph Neural Network architectures such as Graph Attention Networks, Graph Auto-Encoders, Graph Generative Networks and Graph Spatio-temporal Networks. Explore these advanced architectures in this post: Beyond Graph Convolution Networks. 
Hands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look 
Written by 
Written by",Aishwarya Jadhav,2019-07-09T09:27:14.529Z
Genetic Algorithms. How human evolution paved the way for… | by Amit Naik | Data Driven Investor | Medium,"How human evolution paved the way for genetic algorithms 
Charles Darwin developed the theory of biological evolution called Darwinism. This states that: 
“All species of organisms arise and develop through natural selection of small, inherited variations that increase he individual’s ability to compete, survive, and reproduce.” 
What this means is that if a population wants to survive in its environment, it must adapt to the changes in the environment. In other words ‘survival of the fittest’. For the population to grow, we can’t always take the best species. The reason for this is that there would be very little room for the species to adapt to the new changes in the environment. Selecting individuals of the species which are not the best (fittest), along with a few of the less fit individuals would allow a bit of diversity in the offsprings which would, in turn, give rise for easy adaption. This is exactly how a genetic algorithm would work. 
Let us take an example of matching a target word by using a genetic algorithm. 
To recreate the evolution and natural selection process, we need to define a metric which will allow us to differentiate between the less competent species from the more fitter ones. The goal here is that later on there will be more chances of the fitter species to get picked to form the next generation. For this, we define a fitness function which will calculate the fitness score. The fitness score can be defined as a ratio which gives a value between 0 and 1, where 0 indicates the lowest possible fitness and 1 indicates a perfect specimen. We can then scale this score from a range of 0 to 100. 
To generate the population we need to know the characteristics of our individual. This will help us decide which factors need to be altered and which need to be kept the same. In genetics, the term ‘allele’ is used to define the variant form of a particular gene. In our case, we know that there is a fixed size (length of the target word) within which we have to generate all the possibilities of the population. 
The trick here is to make sure that we do not take only the solution that seems good, but make the population as diverse as possible so as to cover all the variants. The perfect first population should cover every existing allele. 
For this example, we generate random words of the same size as the target word. This will give us our population of individuals. Once this is done, we then need to create the next set of individuals. This will form the next generation. We need to select a part of the current generation. The algorithm then combines the breeders to create the next set of individuals. 
While going about this stage, it is important to keep in mind that although we need to select the best individuals from the previous generation, we cannot completely neglect the others. If we do not take this into consideration, this will lead the population to converge to a local minimum and towards the best solution. We can do this by taking the ‘X’ best samples and randomly select ‘Y’ samples without any reference to their fitness. 
For our example of getting the matching word, we just have to pick up random letters from two different individuals and combine them to form new individuals or children. The total number of individuals in the current generation equals the number of individuals in the next generation. We have to make sure that we limit the number of children produced. This would otherwise make the population unstable. 
We use the previous generation of individuals to create the next set of children. These children are then mutated to create the next generation of individuals or species. This process repeats till the number of generations defined in the algorithm. 
The final step in this algorithm is the mutation of each individual. The main goal of this step is to not allow the algorithm to converge too quickly and on the other hand, not allow the algorithm to go on for long periods of time. The rate of mutation can vary depending on the population size. This article gives a good detailing on how mutation rates can influence the population size and efficiency of the algorithm. The population size, in turn, seems to have a negative correlation with algorithm efficiency. 
The fitness function is called at this time since we want to get a set of the fittest population and then breed those individuals with those who are not the fittest. This will allow for more diversity and adaptability. 
Pseudo code for our example is given below for the mutation process. 
While generating the population after mutation, we make sure that we only randomize if it falls within the chance of mutation defined. 
Intragenic mutation is when there is a choice of whether a mutation process will take place or not. If so, we change exactly only one of its genes. In intergenic mutation, every single gene of that individual can mutate. This gives us a larger number of mutated genes than when compared to the intragenic mutation process. Intergenic mutation is how mutation occurs in real life. 
How do we know when to stop the continuous process of evolution and mutation of the individuals in the algorithm? The main goal while approaching the termination condition is to make sure that our solution is close to the best solution at the end of the run. Listed below are the most common conditions for termination: 
Of course, there could be other better ways to terminate the algorithm based on the application and need of the developers. 
Some of the places where genetic algorithms have been applied are: 
Clap! Share! Follow! 
In each issue we share the best stories from the Data-Driven Investor's expert community. Take a look 
Written by 
Written by",Amit Naik,2019-01-18T10:36:57.702Z
GluonNLP 0.6: Closing the Gap in Reproducible Research with BERT | by Eric Haibin Lin | Apache MXNet | Medium,"BERT (Bidirectional Encoder Representations from Transformers) is arguably the most notable pre-training model in natural language processing (NLP). For instance, BERT lifts the score from 72.8 to 80.5 in the GLUE benchmark for 9 different NLP tasks — this is the biggest recent advancement[6]. 
Although BERT is exciting, unfortunately there have been no open source implementations that simultaneously 
Thus, we release GluonNLP 0.6 to address such pain points by i) pre-training BERT with 8 GPUs in 6.5 days; ii) reproducing multiple natural language understanding results; iii) streamlining deployment. 
In case you live under a rock and have not heard of BERT, here is how it works. It 1) uses stacked bidirectional transformer encoders, 2) learns parameters by masked language modeling and next sentence prediction on large corpora with self-supervision, and 3) transfers these learnt text representations to specific downstream NLP tasks with a small set of labeled data by fine-tuning. 
You may wonder: the official BERT repository has released multiple pre-trained models for free from the result of many TPU hours, why should we still care about pre-training BERT? This is because the choice of corpus for pre-training is very important. Like any other transfer learning setting, the model is more likely to perform well if the pre-trained data source is close to the task at hand. For example, pre-training on Wikipedia may not help us do better on tweets due to the difference in language style. 
We pre-trained the BERT Base model from scratch. We used an English Wikipedia data dump that contains 2.0 billion words after removing images and tables, and a books corpus dataset which contains 579.5 million words after de-duplication. With mixed precision training and gradient accumulation, the BERT Base model takes 6.5 days using 8 Volta 100 GPUs and achieves the following results on validation sets. 
Promoting reproducible research is one of the important goals of GluonNLP. In GluonNLP, we provide both training scripts and logs that replicate state-of-the-art results on RTE[6], MNLI[8], SST-2, MRPC[10], SQuAD 1.1[9] and SQuAD 2.0[11]. Our code is modularized to facilitate BERT on many tasks in one framework. 
We report F1 and exact match scores on the validation set for question answering datasets: 
Below please find accuracies on validation sets for the following sentence classification tasks with BERT Base model: 
With the power of MXNet, we provide BERT model that can be serialized into json format and deployed in in C++, Java, Scala, and many other languages. With float16 support, we see approximately 2 times speed up for BERT inference on GPUs. We are also working on int8 quantization on CPUs. 
To get started with BERT using GluonNLP, visit our tutorial that walks through the code for fine-tuning BERT for sentence classification. You can also check out our BERT model zoo for BERT pre-training scripts, and fine-tuning scripts for SQuAD and GLUE benchmarks. 
For other new features added in GluonNLP, please read our release notes. We are also working on BERT distributed training enhancements, and bringing GPT-2, BiDAF[12], QANet[3], BERT for NER/parsing, and many more to GluonNLP. 
Happy BERTing with GluonNLP 0.6! 
We thank great contributions from the GluonNLP community: @haven-jeon @fiercex @kenjewu @imgarylai @TaoLv @Ishitori @szha @astonzhang @cgraywang 
[1] Peters, Matthew E., et al. “Deep contextualized word representations.” arXiv preprint arXiv:1802.05365 (2018). 
[2] Howard, Jeremy, and Sebastian Ruder. “Universal language model fine-tuning for text classification.” arXiv preprint arXiv:1801.06146 (2018). 
[3] Yu, Adams Wei, et al. “Qanet: Combining local convolution with global self-attention for reading comprehension.” arXiv preprint arXiv:1804.09541 (2018). 
[4] Devlin, Jacob, et al. “Bert: Pre-training of deep bidirectional transformers for language understanding.” arXiv preprint arXiv:1810.04805 (2018). 
[5] Sebastian Ruder. NLP’s ImageNet moment has arrived, 2018 (accessed November 1, 2018). URL https://thegradient.pub/nlp-imagenet/. 
[6] Wang, Alex, et al. “Glue: A multi-task benchmark and analysis platform for natural language understanding.” arXiv preprint arXiv:1804.07461 (2018). 
[7] Liu, Xiaodong, et al. “Multi-Task Deep Neural Networks for Natural Language Understanding.” arXiv preprint arXiv:1901.11504 (2019). 
[8] Williams, Adina, Nikita Nangia, and Samuel R. Bowman. “A broad-coverage challenge corpus for sentence understanding through inference.” arXiv preprint arXiv:1704.05426 (2017). 
[9] Rajpurkar, Pranav, et al. “Squad: 100,000+ questions for machine comprehension of text.” arXiv preprint arXiv:1606.05250 (2016). 
[10] Dolan, Bill, Chris Brockett, and Chris Quirk. “Microsoft research paraphrase corpus.” Retrieved March 29 (2005): 2008 
[11] Rajpurkar, Pranav, Robin Jia, and Percy Liang. “Know What You Don’t Know: Unanswerable Questions for SQuAD.” arXiv preprint arXiv:1806.03822 (2018). 
[12] Tuason, Ramon, Daniel Grazian, and Genki Kondo. “BiDAF Model for Question Answering.” Table III EVALUATION ON MRC MODELS (TEST SET). Search Zhidao All. 
Written by 
Written by",Eric Haibin Lin,2019-03-19T14:01:00.901Z
Transformer Architecture: Attention Is All You Need | by Aditya Thiruvengadam | Medium,"In this post, we are going to explore the concept of attention and look at how it powers the “Transformer Architecture” which thus demonstrates why “Attention Is All You Need!” 
Introduction: 
Whenever long-term dependencies (natural language processing problems) are involved, we know that RNNs (even with using hacks like bi-directional, multi-layer, memory-based gates — LSTMs/GRUs) suffer from vanishing gradient problem. Also, they handle the sequence of inputs 1 by 1, word by word this resulting in an obstacle towards parallelization of the process. 
Especially when it comes to seq2seq models, is one hidden state really enough to capture global information pertaining to the translation? 
The problem with this approach was (as famously said at the ACL 2014 workshop): 
Here’s where attention comes in! 
Attention, in general, can be thought of as follows: 
The idea here is to learn a context vector (say U), which gives us global level information on all the inputs and tells us about the most important information (this could be done by taking a cosine similarity of this context vector U w.r.t the input hidden states from the fully connected layer. We do this for each input x_i and thus obtain a theta_i (attention weights). 
i.e. : theta_i = cosine_similarity(U, x_i) 
For each of the input hidden states x_1 … x_k, we learn a set of weights theta_1 to theta_k which measures how much of the inputs answer the query and this generates an output 
For an encoder-decoder architecture: 
For every single target decoder output ( say, t_j ), all hidden state source inputs (say s_i’s) are taken into account to compute the cosine similarity with the source inputs s_i, to generate the theta_i’s (attention weights) for every s_i. 
i.e. theta_i = cosine_similarity(t_j, s_i) 
The context vector (out — refer to the above equation) is now computed for every source input s_i and theta_i (generated for the corresponding target decoder word t_j). The context vector (out) and target word (t_j) are used to predict the output in the decoder architecture, which is then daisy chained and continued from here on in the above manner using attention. 
Attention mechanism solves this problem by allowing the decoder to “look-back” at the encoder’s hidden states based on its current state. This allows the decoder to extract only relevant information about the input tokens at each decoding, thus learning more complicated dependencies between the input and the output. 
This allows the decoder to capture global information rather than to rely solely based on one hidden state! 
Here, we see that the dependencies are learned between the inputs and outputs. 
But, in the Transformer architecture this idea is extended to learn intra-input and intra-output dependencies as well (we’ll get to that soon!) 
Attention Definition according to the Transformer paper: 
An attention function can be described as mapping a query (Q) and a set of key-value pairs (K, V) to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. 
According to our original definition: 
The attention weights are the relevance scores of the input encoder hidden states (values), in processing the decoder state (query). This is calculated using the encoder hidden states (keys) and the decoder hidden state (query). 
In the case of Neural Machine Translation, both keys and values are encoder hidden states (leading to Self-Attention — we’ll get to that soon!) 
But, Why do we need the Transformer? 
RNN: 
CNN: 
Objective or goal for the architecture: 
One solution to this is Hierarchical Convolution Seq2Seq architecture (https://arxiv.org/abs/1705.03122) 
The intuition here is that close input elements interact in the lower layers, while long-term dependencies are captured at the higher layers. 
However, in those CNN-based approaches, the number of calculations in the parallel computation of the hidden representation, for input → output position in the sequence, grows with the distance between those positions (architecture grows in height). The complexity of O(n) for ConvS2S and O(nlogn) for ByteNet makes it harder to learn dependencies on distant positions. 
As an alternative to convolutions, a new approach is presented by the Transformer. It proposes to encode each position and applying the attention mechanism, to relate two distant words of both the inputs and outputs w.r.t itself, which then can be parallelized, thus accelerating the training. 
The Transformer reduces the number of sequential operations to relate two symbols from input/output sequences to a constant O(1) number of operations. Transformer achieves this with the multi-head attention mechanism that allows to model dependencies regardless of their distance in input or output sentence. 
Now, Let’s Breakdown the Transformer’s Encoder and Decoder Architecture: 
The encoder and decoder both are both composed of a stack of identical layers, whose main components are as follows: 
In contrast to the CNN based approach, which tackles the sequence problem and the position problem in one go by encoding absolute positional information along with the embeddings, here the Transformer uses 2 different NNs to capture this information (as described below). 
The way this attention is integrated makes this architecture special! 
Let’s look at the Multi-Head Attention and Positional Encoding which forms the basis of this Architecture: 
1.Multi-Head Self-Attention Attention: 
The transformer adopts the Scaled Dot-Product Attention: 
The output is a weighted sum of the values, where the weight assigned to each value is determined by the dot-product of the query with all the keys: 
Unlike the commonly used additive based attention function (first definition above), this architecture uses the multiplicative attention function. 
Even though both have the same theoretical complexity, the Scaled Dot-Product is chosen due to it being much faster and space-efficient, as it uses an optimized matrix multiplication code. 
The intuition behind self-attention is as follows: 
Rather than computing single attention (weighted sum of values), the “Multi-Head” Attention computes multiple attention weighted sums, hence the name. 
Each of these “Multiple-Heads” is a linear transformation of the input representation. This is done so that different parts of the input representations could interact with different parts of the other representation to which it is compared to in the vector space. 
This provides the model to capture various different aspects of the input and improve its expressive ability. 
Essentially, the Multi-Head Attention is just several attention layers stacked in parallel, with different linear transformations of the same input. 
2.Position-Encoding and Position-Wise Feed Forward NN: 
With no recurrence or convolution present, for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence to the embeddings. 
The positional encodings have the same dimensions of the embeddings (say, d), so that they can be summed up. Here, 2 sinusoids (sine, cosine functions) of different frequencies are used: 
Where pos is the position of the token and i is the dimension. 
The wavelengths form a geometric progression from 2π to 10000⋅2π. We chose this function because they hypothesized it would allow the model to easily learn to attend by relative positions, since, for any fixed offset k, PE(pos+k) can be represented as a linear function of PE(pos). 
Instead of fixing said positional encodings, a learned set of representation is also providing the same result as the above. 
The most important part here is the “Residual Connections” around the layers. This is very important in retaining the position related information which we are adding to the input representation/embedding across the network. The network displayed catastrophic results on removing the Residual Connections. 
The authors have also discussed concatenation of the positional embeddings instead of adding them (ref: Allen NLP podcast). They were in the process of doing said experiments, but their initial results seem to say that the residual connections there can be mainly applied to the concatenated positional encoding section to propagate it through. 
Let’s now look at the architecture: 
Encoder: 
In the encoder phase, the Transformer first generates Initial Inputs (Input Embedding + Position Encoding) for each word in the input sentence. 
For each word, self-attention aggregates information from all other words (pairwise) in the context of the sentence, thus creating a new representation for each word — which is an attended representation of all other words in the sequence. 
This is repeated for each word in a sentence successively building newer representations on top of previous ones multiple times. 
Decoder: 
The decoder generates one word at a time from left to right. The first word is based on the final representation of the encoder (offset by 1 position) 
Every word predicted subsequently attends to the previously generated words of the decoder at that layer and the final representation of the encoder (Multi-Head Attention) — similar to a typical encoder-decoder architecture. 
Best way to understand the flow : 
It is worth noting how this self-attention strategy tackles the issue of co-reference resolution where e.g. word “it” in the sentence “The animal didn’t cross the street because it was too tired.” can refer to different noun (animal or street) of the sentence depending on context. 
The Transformer uses Multi-Head Attention in three different ways: 
Types of problems the algorithm well suited? 
Future Work: 
Resources to check out — References: 
Written by 
Written by",Aditya Thiruvengadam,2019-03-26T17:58:20.716Z
The Secret to Advancing the Science of Cyber Risk: A Q&A with Gregory Falco | by FSI Stanford | Freeman Spogli Institute for International Studies | Medium,"The science of cyber risk looks at a broad spectrum of risks across a variety of digital platforms. Often though, the work done within the field is limited by a failure to explore the knowledge of other fields, such as behavioral science, economics, law, management science, and political science. In a new Science Magazine article, “Cyber Risk Research Impeded by Disciplinary Barriers,” cyber risk experts and researchers at Stanford University make a compelling case for the importance of a cross-disciplinary approach. Gregory Falco, security researcher at the Program on Geopolitics, Technology, and Governance, and lead author of the paper, talked recently with the Cyber Policy Center about the need for a holistic approach, both within the study of cyber risk, and at a company level when an attack occurs. 
CPC: Your recent perspective paper in Science Magazine highlights the issue of terminology when it comes to how organizations and institutions define a cyber attack. Why is it so important to have consistent naming when we are talking about cyber risk? 
Falco: With any scientific discipline or field, there is a language for engaging with other experts. If there’s no consistent language or at least dialect for communication around cyber risk, it’s difficult to engage with scholars from different disciplines. For example: The phrase “cyber event” is contested and the threshold for what an organization considers to be a cyber event varies substantially. Some organizations consider someone pinging their network as a cyber event, others only consider something a cyber event once an intrusion has been publicly disclosed. So there’s a disparity when comparing metrics of cyber events from organization to organization because of the different thresholds of what’s considered an event. 
CPC: We’ve all been sent one of those emails letting us know our data may have been compromised and your paper points out it’s nearly impossible to put foolproof protections into place; attacks are inevitable. Given that, how should companies weigh the various ways they can protect themselves? 
Falco: The first exercise each organization should go through when they decide to be serious about cyber risk is to prioritize their assets. What is business critical? What is safety critical? Then, like all other risks, a cost-benefit analysis must be done for each asset based on its priority. If the asset is safety-critical, then resources should be allocated to help protect that asset or at least ensure its resilience. Trade-offs are inevitable, no company has unlimited resources. But starting with an understanding of where the priorities are, is critical. 
CPC: In companies, cyber security often falls entirely to the Chief Information Security Officer (CISO). Your paper argues that’s shortsighted. What is gained when a company takes a more holistic approach? 
Falco: Distributing responsibility across the organization catalyzes a security culture. A security culture is one where there is a constant vigilance or at least broad awareness of cybersecurity concerns throughout the organization. Fostering a security culture is often suggested as a mechanism to help reduce cyber risk in organizations. The problem with not distributing responsibility is that when something happens, it’s too easy to resort to finger-pointing at the CISO, and that’s counterproductive. Efforts after an attack should be on responding and being resilient, not finding the scapegoat. 
CPC: Cyber risk largely focuses on prevention, but your paper argues that it’s what happens after an attack in that needs greater attention. Why is that? 
Falco: Every organization will be attacked. However organizations can differentiate themselves from a cyber risk standpoint by appropriately managing the situation after an attack. Some of the most significant damages to organizations can be reputational if communication after an attack is unclear or botched. Poor communication after an attack can result in major regulatory fines or valuation adjustments as seen in cases like Yahoo and that can have major business implications. Communications aren’t the only important element of post-attack response. A thorough post-mortem of the organization’s response to the attack can be an important learning experience and a way to plan for future attacks. 
CPC: Protecting against cyber attacks and the losses that go with them can obviously be costly for companies. You make a case for collaboration among different fields, say among data scientists and economists. How can that be encouraged? 
Falco: We argue that cross-disciplinary collaboration rarely happens organically. Therefore, we call on funding agencies like the NSF or DARPA to specify a preference for cross disciplinary research when funding cyber risk projects. Typically, this isn’t currently a feature of calls for proposals, but for cyber risk programs it should be. We encourage researchers to explore cyber risk questions at the margins of their discipline. Those questions may lend themselves to potential overlap with other disciplines and foster a starting point for cross-disciplinary collaboration. 
For more on these topics, see a full list of recent publications from the Cyber Policy Center and the Program on Geopolitics, Technology, and Governance. 
Written by 
Written by",FSI Stanford,2019-12-17T17:19:22.504Z
Recommendation Systems. Diving into recommendation systems | by Bindhu Balu | Towards AI — Multidisciplinary Science Journal | Medium,"Wiki Definition: Recommendation Engines are a subclass of information filtering systems that seeks to predict the ‘rating’ or ‘preference’ that the user would give to an item. 
My Definition: Recommendation Engine is a black box which analyzes some set of users and shows the items which a single user may like. 
— Your family and friends as clothes recommendation engines 
— Your Professors as book recommendation engines: 
— Your friends as movie recommendation engines: 
Notice that all of these “offline recommenders” know something about you. They know your style, taste or area of study, and thus can make more informed decisions about what to recommend that would benefit you most. It is this personalization- based on getting to “know” you- that online recommenders aim to emulate. 
Facebook: “People You May Know” 
Facebook users a recommender system to suggest Facebook users you may know offline. The system is trained on personal data mutual friends, where you went to school, places of work and mutual networks (pages, groups, etc.), to learn who might be in your offline & offline network. 
Netflix: “Other Movies You Might Enjoy” 
When you fill out your Taste Preferences or rate movies and TV shows, you’re helping Netflix to filter through the thousands of selections to get a better idea of what you might like to watch. Factors that the Netflix algorithm uses to make such recommendations include: 
LinkedIn: “Jobs You Maybe Interested In” 
The Jobs You May Be Interested In feature shows jobs posted on LinkedIn that match your profile in some way. These recommendations are shown based on the titles and descriptions in your previous experience, and the skills other users have “endorsed”. 
Amazon: “Customers Who Bought This Item Also Bought… 
Amazon’s algorithm crunches data on all of its millions of customer baskets, to figure out which items are frequently bought together. This can lead to huge returns- for example, if you’re buying an electrical item, and see a recommendation for the cables or batteries it requires beneath it, you’re very likely to purchase both the core product and the accessories from Amazon. 
In the immortal words of Steve Jobs: “A lot of times, people don’t know what they want until you show it to them.” Customers may love your movie, your product, your job opening- but they may not know it exists. The job of the recommender system is to open the customer/user up to whole new products and possibilities, which they would not think to directly search for themselves. 
Let me introduce you to three very important types of recommender systems: 
Collaborative Filtering 
Collaborative filtering methods are based on collecting and analyzing a large amount of information on users’ behaviors, activities or preferences and predicting what users will like based on their similarity to other users. A key advantage of the collaborative filtering approach is that it does not rely on machine analyzable content and therefore it is capable of accurately recommending complex items such as movies without requiring an “understanding” of the item itself. Many algorithms have been used in measuring user similarity or item similarity in recommender systems. For example, the k-nearest neighbor (k-NN) approach and the Pearson Correlation. 
Content-Based Filtering 
Content-based filtering methods are based on a description of the item and a profile of the user’s preference. In a content-based recommendation system, keywords are used to describe the items; besides, a user profile is built to indicate the type of item this user likes. In other words, these algorithms try to recommend items that are similar to those that a user liked in the past (or is examining in the present). In particular, various candidate items are compared with items previously rated by the user and the best-matching items are recommended. This approach has its roots in information retrieval and information filtering research. 
Hybrid Recommendation Systems 
Recent research has demonstrated that a hybrid approach, combining collaborative filtering and content-based filtering could be more effective in some cases. Hybrid approaches can be implemented in several ways, by making content-based and collaborative-based predictions separately and then combining them, by adding content-based capabilities to a collaborative-based approach (and vice versa), or by unifying the approaches into one model. Several studies empirically compare the performance of the hybrid with pure collaborative and content-based methods and demonstrate that hybrid methods can provide more accurate recommendations than pure approaches. These methods can also be used to overcome some of the common problems in recommendation systems such as cold start and the sparsity problem. 
Netflix is a good example of a hybrid system. They make recommendations by comparing the watching and searching habits of similar users (i.e. collaborative filtering) as well as by offering movies that share characteristics with films that a user has rated highly (content-based filtering). 
I hope you liked this post. In the next installment, I will detail these three recommendation systems in the bigger picture, and learn how to implement them. Any questions? Leave a comment below. 
Towards AI publishes the best of tech, science, and engineering. Subscribe with us to receive our newsletter right on your inbox. For sponsorship opportunities, please email us at pub@towardsai.net Take a look 
Written by 
Written by",Bindhu Balu,2019-10-17T04:41:21.697Z
Free #CyberSecurity: 6 Basic Tips | by Leonard Burger | Medium,"Originally published on LinkedIn 
Our vastly connected digital society has seen dozens of terms and words added to dictionaries and glossaries over the past two decades. We live in a world where in the sharing economy zillions of human interactions take place online every day. From finding a date through swiping left or right to live tracking your Uber ride, we are digital, we are online and we are (well, most of us) are loving it. It doesn’t just stop there, the constant expansion of the Internet of Things portfolio means we can interact with a lot of the ‘smart things’ that are part of our lives as well. With IoT on the rise Artificial Intelligence is being discussed more and more, and AI has come along way since Arnold uttered the words ‘Hasta la vista, baby’. It will, however, take a bit more time before Arnold can actually be ‘replaced’ in a modern Terminator sequel. 
That said, the on-board voice controller in your new car, will soon understand the tiredly pronounced sentence ‘I just want to go home…’ and reply ‘sorry to hear you had a dreadful day, I have found the fastest route home for you’. It will then communicate with its fellow cars in the network, and the road beneath it, so as to prevent any accidents from happening during its drive to take you home. 
As the likes of Steve Hawkins, Bill Gates and Elon Musk have all ‘warned’ about the dangers of AI and given I would not (yet) know how to advice you to protect yourself from robots that have gone rogue, I would like to address the more current and pertinent issue of your online security both in a professional* and personal context. 
UK parliament’s Cyber Security month (February 2017) has come to an end today and was aimed at the continued raising of public awareness regarding the existence of cyber threats. It is not a secret that cyber security is an important topic for individuals, companies and governments alike. And while most larger businesses and organisations will usually be at the forefront of implementing/developing the latest cyber security measures, often lead by financial organisations as these become victims of increasingly sophisticated malware (e.g. fileless malware), too many individuals are not so ‘private’ about their privacy online. 
Your online or digital footprint has multiple layers and is extremely vast, what is more it increases at an exponential rate, and while most of these ‘footprints’ can only be tied to either metadata or one/multiple ‘numbers’ due to privacy laws†, your online identity used on a myriad of digital services harbours a lot of personal data that I am sure you would want to keep from falling into the wrong hands. As our lives increasingly depend on such digital services, keeping usage and personal data secure becomes more and more pertinent. Problem is that as long as any cyber crime has not affected us directly the opposite of the ‘backyard-syndrome’ or NYMBY syndrome kicks in. Hence, most of us do too little to take caution when it comes to our own cyber security. 
Below are some tips, including advice on how to implement, without too much exertion, basic safety measures using free tools (majority of these are open source); 
Please do take your cyber security serious, it will save a lot of headache from when trouble does find your ‘backyard’. This blog post could have included a lot more details, however I’ve aimed to keep it as basic as possible. The information that is available on Cybersecurity is quite extensive, yet I am sure the above will give you a head-start in terms of the basics. For those who are coders, github fanatics, blockchain lovers or anyone who simply loves to keep up with the latest, please do add in any suggestions for easy implementable steps regarding privacy and/or online security in the comments section. As always I’m also interested in hearing feedback on my posts. 
PS: If you are a small business owner do give this article a read, it includes 5 useful tips for keeping your business safe from such things like ransomware. 
If you are interested in learning more about Cybersecurity, check out this free course on FutureLearn, study the fundamentals on Coursera or perhaps you are more interested in the economics of Cybersecurity, available on EdX. 
Please do leave a comment or drop me a message with any feedback you might have. 
Footnotes: 
* = Please always check with your employer before acting upon implementing any of your own steps to online security in your capacity as a professional/employee, especially in regard of downloading anything on company owned devices. 
† = I would like to add a quick side note on privacy laws which often have to be fought for by NGOs you have likely not heard off (e.g.Privacy International and Electronic Frontier Foundation). I believe privacy and security can go hand in hand, if we build a strong ethical framework around both AI and Data, check out dathethics.eu for useful blog posts on the ethics of it all. 
‡= Importance of using differing passwords, more details; As we have countless private online accounts, not to mention work related accounts, a lot of inspiration is needed to get a good set of passwords. One way to go about it is to rank your accounts on importance, in terms of which other important accounts these are connected to, what personal info these hold and to create a set of strong passwords for these particular accounts (also do not forget to then rotate these passwords every once in a while). That said, always check if the site uses cross-site SLL security (usually the case for more popular services) before deciding on what password to set, as the password database might not be encrypted, if unsure set a password and click ‘forgot password’ upon your second login, if an email containing your full password is send to you it’s safe to say the site does not use encryption. Hence, any personal data on that site that is tied to other online accounts could leave hackers with a clue as to what passwords you might use on those other sites. 
Disclaimer: Given the sensitivity of this blog post’s topic please note that the I, the author, can not give any guaranties as to the accuracy of information given on the third party links provided. The author can not be held responsible for any loss of data, harm to devices and/or individuals and/or for anything else in relation to actions that are derived from the above provided information in any way or form. 
#AI #infosec #cybersecurity #datasec #dataethics #IoT #CISO #bitcoin #blockchain # Ethereum 
Written by 
Written by",Leonard Burger,2017-06-03T16:50:34.317Z
Real World Cyber Security. Blog Introduction and  Index | by RealWorldCyberSecurity | Medium,"Einstein allegedly defined insanity as “doing the same thing over and over again and expecting different results.” Well, in information security and cybersecurity, we’ve sure been doing a lot of “the same old thing” over and over again, but we continue to get hacked. I guess we’re insane then, because we expect that what we’re doing will keep us from getting breached, but it doesn’t. Yet, we keep doing more and more of it and expecting different results. Insanity? You betcha! 
A few years ago, what we consider today to be a minor breach would have been headline news. Breaches have become so common that most major ones are relegated to being buried somewhere deep inside the business section of the newspaper, with only the truly monumental ones making headlines. And, the breaches keep coming, and coming at a seemingly increasing pace, each one more significant, more damaging, and more costly than the previous. Yet, we keep doing the same old things to try to prevent the hacks. Either we are actually insane, or there is something seriously wrong with what we are doing and our thought processes behind it. I choose to believe the latter. 
That’s what this blog is about: What we are doing wrong in security and how we need to fix it. Looking at the problem from 65,000 feet, I see two fundamental problems: First and foremost, we are trying to treat cybersecurity problems as though they are information security problems; second, we are basing much of our security thought processes on outdated premises. Yes, there are numerous other issues we face, but until we address those fundamentals, we have no hope. 
So, let’s briefly think about those two points. 
First, too many security problems are caused by too narrow of a view of security, such as treating all security problems as information security problems, resulting in the failure to identify the actual security gaps associated with threats. Information security focuses on the protection of information, whereas cybersecurity focuses on the protection of everything connected by some network to something else on some network, including the networks themselves. Cybersecurity requires both a much broader focus and a somewhat different mindset than information security. These differences in focus and mindset are critical issues that are usually left unaddressed in most organizations. 
And, second, most of the fundamental principles on which we base all security today have changed very little since the 1980s (or earlier). Meanwhile, the scope of what must be secured has vastly increased, and changed from its original information security focus upon which these fundamental principles were developed. Thus, it is long past time that we change our thinking regarding our approach to security — both in terms of what must be protected and how we should go about protecting it. 
We also have one big practical issue we need to address as well: Simply too much of the security we have in place today is “security theater” — that is, measures intended to give the illusion of security while actually doing little or nothing to secure the assets intended to be protected, and potentially making those assets less secure. Every organization has this problem — the only question is how much of its security is real and effective, versus simply theater? 
The objective of this blog is to provide some thoughts on what should be considered “security done right.” That is, how do you reduce risk in a cost-effective manner? The objective of both information and cybersecurity should be to first reduce risk to the greatest extent practical. Then, when a breach does occur, to detect it and shut it down as rapidly as possible, and in the process to have collected the information required to determine how the breach occurred, what was infiltrated, what was exfiltrated, and how to prevent a future reoccurrence. 
If you are reading this blog in hopes of discovering how to prevent getting hacked, you’re not going to find that information here. In fact, I will go so far as to state that anyone who claims he or she can prevent an organization from getting hacked is either terribly naive or a liar! 
This blog focuses on cybersecurity but also covers many aspects of traditional information security, and even touches on traditional corporate security as well. It includes both technical and non-technical content, and is oriented towards two different corporate audiences: 
This blog is written at a level that should be easily understood by anyone with an interest in either information security or cybersecurity, from students and interns through corporate executives and board members, and every security practitioner and manager in between. Please feel free to leave me a note if you have any questions. 
The Blog Index follows in the next sections. 
Thanks for reading! 
Check back regularly for updates. 
Okay, maybe not everything, but a whole lot of what you think you know about security is probably either wrong, out-of-date, or both. 
The biggest problem with cybersecurity is that everything in the field is changing so fast that it is nearly impossible to keep current. In fact, the insider-joke in the industry is that security follows the inverse of Moore’s Law — that is, every eighteen months, half of everything you know is now obsolete. Thus, if you are not continually reading and following security news feeds, and attending courses and conferences at least twice yearly, you are probably falling behind. Security is simply changing that fast, and a lot of what you think you know is now obsolete. 
Welcome to the real world of cybersecurity! 
Staying current is one problem, and it is definitely a very big problem in the industry. However, probably the most significant problem is that many of the industry’s most fundamental beliefs and principals — which most people in the industry simply take as gospel, and are what I call “security mantras” — are simply incomplete, and/or incorrect, and/or out-of-date. Worse, many of the security industry’s most sacred security mantras are flat out wrong! 
“What’s wrong with what we think we know about security,” is the focus of this section of the blog. 
If You Can’t Properly Define Cybersecurity, How Can You Know What It Is?It’s clear that the cybersecurity industry hasn’t been able to agree upon what cybersecurity is and isn’t. Even NIST, who is responsible for the definition of technical terms used by the U.S. Federal Government, has four different definitions of cybersecurity! At a minimum, there are dozens of different definitions of cybersecurity currently in use. Nearly all are incomplete in scope, some are horridly wrong, and nearly all fail to differentiate between cybersecurity and its information security cousin. 
The CIA Triad Is Dangerously Obsolete and IncompleteThe CIA Triad (Confidentiality, Integrity, Availability) purports to define the services that are provided by security to defend against threats to an asset being secured. Yet, it only provides defenses for three of the seven widely-recognized categories of security threats. An incomplete definition of the security’s fundamental services means we are also dangerously incomplete in the proper securing of our assets. 
Why Biometrics Are Not Valid AuthenticatorsMost security courses teach there are three ways to authenticate: “What you know,” “What you have,” and “What you are.” However, authenticators must be revocable and deterministic. Biometrics (“What you are”) are probabilistic and non-revocable. Thus, biometrics cannot serve as a means of authentication. 
(coming soon!)Access control systems are often focused on only authentication, authorization, and accounting, and neglect identification and audit. Even identity and access management systems often neglect the audit aspect of access control. But, this incomplete view of access control can lead to critical security weaknesses. 
Seriously! A Password Should Only Be Changed If There Are Indications Of Its CompromiseThe decades-old practice of changing your password every 30 (or 60 or 90 or whatever) days is lousy security. You should pick a strong password and not change it without a good reason to do so. Passwords shouldn’t be the gatekeeper for logins; rather, it should be a password in combination with a “second factor,” such as an app-generated code or a hardware security token. 
(coming soon!)The good, bad, and ugly of two-factor authentication. 
(coming soon!)Single sign-on (SSO) without two-factor authentication (TFA) is handing the keys to your kingdom to a hacker. Even with TFA, a bad implementation of SSO can substantially increase your organization’s exposure to identity hijacking. 
(coming soon!) 
Proper Requirements Are The First Step To Verifiable SecurityAll too often, organizations lack any appropriate definition of their security requirements. And, the alleged requirements documents that do exist are most likely design specifications, not requirements specifications. Serious security breaches are unavoidable without a proper understanding of what is to be secured and why. That is, serious security breaches are unavoidable without proper security requirements. 
(coming soon!)The most data that an asymmetric cypher can encrypt is several bytes less than the length of its key. What actually occurs when using asymmetric cyphers to encrypt messages is that the asymmetric cypher is used to encrypt the key of a symmetric cypher used to encrypt the message. This blog post gives an inside look at that process. 
RFC1122 Specifies Only Four Layers in The Internet Protocol StackIt is a common misperception that the Internet is based upon the ISO 7-Layer Model. It is not. It is based upon a software protocol stack defined in RFC1122 that has several differences from the ISO specification. 
(coming soon!) 
This section of the blog covers those threats which may not be headline issues today, but are bound to be headlines in the near future. It also includes some threats that are here today but are not in the headlines. But, they are serious threats you need to know about. 
A New Tool Detects If Your ISP Has Implemented Route Hijacking MitigationsThe Internet runs on a protocol called BGP, which determines how your data is routed from your ISP to its destination, such as Apple or Netflix. However, BGP, in its default configuration, is insecure and subject to hijacking attacks. There are mitigations for such attacks, but your ISP must explicitly implement them. A new tool from Cloudflare lets you check your ISP, and name and shame them if they haven’t implemented appropriate fixes. 
Your Processor Remains ExploitableThe common perception is that if you update your processor’s microcode, your processor is “fixed.” Well, it isn’t. Every time you reset your processor (e.g., reboot), the microcode patches are wiped. This leads to exploitable security holes in your system. 
Not Actual Protection Rings, But Conceptual Privilege Levels Susceptible To ExploitationMost likely, you’re aware of the hardware “protection rings” in Intel Architecture processors — the familiar “Ring 0” for the kernel through “Ring 3” for userland. But, have you ever heard of “rings” “minus one” through “minus three”? If not, you’re missing out on three entire levels of processor vulnerabilities. 
Have an Intel Processor? Then you’re a user!MINIX: It’s the world’s most widely used operating system and another security threat that you’ve never heard of! Like all operating systems, it has bugs. Only you can’t patch the bugs in MINIX! 
(coming soon!)The Management Engine (ME) is the “Ring -3” processor on your IA chipsets which you can’t turn off. “Powering Off” your computer does not power off the ME. The only way to power off the ME is to remove all power from the processor. Thus, even when “power is off” to your computer, but line or battery power is connected to the computer’s mainboard, the ME continues to run. And, the ME has access to everything accessible by your computer. And, that’s just the tip of this iceberg. 
(coming soon!) 
(coming soon!) 
Ah, politics. Everyone’s favorite subject today. Or, maybe not. 
Regardless, politics pay a critical role in all policy decisions. Thus, I include both topics under this one single heading. 
This section discusses how policies impact security, and how politics often result in insecurity. 
Why The Cryptographic Backdoors Law Enforcement Seeks Are Worthless Against Any Minimally-Determined Adversary.Their purported “need” for encryption backdoors is purely and simply a barefaced lie. There’s no other civilized way of putting it. Backdoors are neither necessary, nor will they solve the alleged “encryption problem.” Worse, backdoors will critically compromise everyone’s security. 
(coming soon!)The lack of a formal corporate security organization creates costly gaps and overlaps in an organization’s security. 
(coming soon!)Where does a hacker who wishes to target your organization begin her recognizance? Most likely, on LinkedIn. 
A Guide To Learning How Well A Candidate Understands SecurityInterviews for security roles tend to come in three flavors: How have you solved a given security problem in the past? How would you configure a particular security tool to solve a specific problem? Or, tell us about your previous experience (as though they hadn’t bothered to read my résumé). None of these approaches provide insight into a candidate’s actual understanding of basic security principals and their application. This blog post presents a guide to interviewing security candidates with a focus on whether they actually understand security fundamentals. 
(coming soon!)I am an architect-level and executive-level security consultant. It’s unbelievable the number of recruiters who contact me for a “Security Architect” position which only requires 5 years of security experience. When I see such a job description, it tells me two things about the organization: (1) They are most likely clueless when it comes to security, and (2) They are only willing to pay for a security engineer, and not a security architect. In this blog post, I explain security roles, responsibilities, experience, and appropriate job titles. 
(coming soon!) 
(coming soon!) 
About The Blogger 
(and You Should Too) 
All of the following blogs and newsfeeds support RSS. 
The following mailing lists are worth subscribing. Choose wisely, as some are very high volume. 
(and You Should Too) 
Featured Image Credit: NASA 
Written by 
Written by",RealWorldCyberSecurity,2020-06-09T23:00:19.536Z
LDA Topic Modeling: An Explanation | by Tyler Doll | Towards Data Science,"Topic modeling is the process of identifying topics in a set of documents. This can be useful for search engines, customer service automation, and any other instance where knowing the topics of documents is important. There are multiple methods of going about doing this, but here I will explain one: Latent Dirichlet Allocation (LDA). 
LDA is a form of unsupervised learning that views documents as bags of words (ie order does not matter). LDA works by first making a key assumption: the way a document was generated was by picking a set of topics and then for each topic picking a set of words. Now you may be asking “ok so how does it find topics?” Well the answer is simple: it reverse engineers this process. To do this it does the following for each document m: 
Above is what is known as a plate diagram of an LDA model where:α is the per-document topic distributions,β is the per-topic word distribution,θ is the topic distribution for document m,φ is the word distribution for topic k,z is the topic for the n-th word in document m, andw is the specific word 
In the plate model diagram above, you can see that w is grayed out. This is because it is the only observable variable in the system while the others are latent. Because of this, to tweak the model there are a few things you can mess with and below I focus on two. 
α is a matrix where each row is a document and each column represents a topic. A value in row i and column j represents how likely document i contains topic j. A symmetric distribution would mean that each topic is evenly distributed throughout the document while an asymmetric distribution favors certain topics over others. This affects the starting point of the model and can be used when you have a rough idea of how the topics are distributed to improve results. 
β is a matrix where each row represents a topic and each column represents a word. A value in row i and column j represents how likely that topic i contains word j. Usually each word is distributed evenly throughout the topic such that no topic is biased towards certain words. This can be exploited though in order to bias certain topics to favor certain words. For example if you know you have a topic about Apple products it can be helpful to bias words like “iphone” and “ipad” for one of the topics in order to push the model towards finding that particular topic. 
This article is not meant to be a full-blown LDA tutorial, but rather to give an overview of how LDA models work and how to use them. There are many implementations out there such as Gensim that are easy to use and very effective. A good tutorial on using the Gensim library for LDA modeling can be found here. 
Have any thoughts or find something I missed? Let me know! 
Happy topic modeling! 
Hands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look 
Written by 
Written by",Tyler Doll,2019-03-11T04:03:36.795Z
Introduction to Genetic Algorithms — Including Example Code | by Vijini Mallawaarachchi | Towards Data Science,"A genetic algorithm is a search heuristic that is inspired by Charles Darwin’s theory of natural evolution. This algorithm reflects the process of natural selection where the fittest individuals are selected for reproduction in order to produce offspring of the next generation. 
The process of natural selection starts with the selection of fittest individuals from a population. They produce offspring which inherit the characteristics of the parents and will be added to the next generation. If parents have better fitness, their offspring will be better than parents and have a better chance at surviving. This process keeps on iterating and at the end, a generation with the fittest individuals will be found. 
This notion can be applied for a search problem. We consider a set of solutions for a problem and select the set of best ones out of them. 
Five phases are considered in a genetic algorithm. 
The process begins with a set of individuals which is called a Population. Each individual is a solution to the problem you want to solve. 
An individual is characterized by a set of parameters (variables) known as Genes. Genes are joined into a string to form a Chromosome (solution). 
In a genetic algorithm, the set of genes of an individual is represented using a string, in terms of an alphabet. Usually, binary values are used (string of 1s and 0s). We say that we encode the genes in a chromosome. 
The fitness function determines how fit an individual is (the ability of an individual to compete with other individuals). It gives a fitness score to each individual. The probability that an individual will be selected for reproduction is based on its fitness score. 
The idea of selection phase is to select the fittest individuals and let them pass their genes to the next generation. 
Two pairs of individuals (parents) are selected based on their fitness scores. Individuals with high fitness have more chance to be selected for reproduction. 
Crossover is the most significant phase in a genetic algorithm. For each pair of parents to be mated, a crossover point is chosen at random from within the genes. 
For example, consider the crossover point to be 3 as shown below. 
Offspring are created by exchanging the genes of parents among themselves until the crossover point is reached. 
The new offspring are added to the population. 
In certain new offspring formed, some of their genes can be subjected to a mutation with a low random probability. This implies that some of the bits in the bit string can be flipped. 
Mutation occurs to maintain diversity within the population and prevent premature convergence. 
The algorithm terminates if the population has converged (does not produce offspring which are significantly different from the previous generation). Then it is said that the genetic algorithm has provided a set of solutions to our problem. 
The population has a fixed size. As new generations are formed, individuals with least fitness die, providing space for new offspring. 
The sequence of phases is repeated to produce individuals in each new generation which are better than the previous generation. 
Given below is an example implementation of a genetic algorithm in Java. Feel free to play around with the code. 
Given a set of 5 genes, each gene can hold one of the binary values 0 and 1. 
The fitness value is calculated as the number of 1s present in the genome. If there are five 1s, then it is having maximum fitness. If there are no 1s, then it has the minimum fitness. 
This genetic algorithm tries to maximize the fitness function to provide a population consisting of the fittest individual, i.e. individuals with five 1s. 
Note: In this example, after crossover and mutation, the least fit individual is replaced from the new fittest offspring. 
Check out this awesome implementation of genetic algorithms with visualizations of the gene pool in each generation at https://github.com/memento/GeneticAlgorithm by mem ento. 
Thank you very much mem ento for sharing this repo with me and letting me add the link to the article. 
Hands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look 
Written by 
Written by",Vijini Mallawaarachchi,2020-03-01T06:15:40.326Z
"Google Page Rank and Markov Chains | by RAvi TeJA GUNdimeDA | Analytics Vidhya | Aug, 2020 | Medium","Whenever you give a query on Google, you will get the web pages in an order based on their Page Rank. PageRank algorithm is the one, which is behind this ordering of search results. 
Google page rank is the objective way of rating web pages. We will compute a vector called PageRank Vector defined as the Eigen Vector of the Google Class matrix. 
PageRank Vector: Is the vector that contains PageRank Values. 
Markov Chain: 
Page rank is based on the random surfer. Let's say we are surfing on the internet by clicking on random links. This can be interpreted as a Markov Chain. 
Markov Chain helps in predicting the behavior of the system which is in transition from one state to another by considering only the current state. At each time ‘t’ the system moves from state ‘i’ to ‘j’ with probability Pij. Pij is called as the transition probability. 
The transition probability helps to find out what is the next state of the object by considering only the current state and not any previous ones. 
A Markov chain contains: 
i) ‘n’ number of states 
ii) ‘n x n’ matrix formed from transition probability. 
Every matrix entry Pij in transition probability matrix(T) tells us, P(j|i) the probability of ‘j’ being the next state from the current state ‘i’. 
Transition Matrix Properties: 
i) Each entry in the matrix must be between 0 and 1. 
ii) The matrix has to be a square and non-negative matrix. 
iii) The Sum of entries in a row has to be equal to 1. This is called as stochastic matrix. 
Google Page Rank : 
It’s a popular link-based ranking algorithm. Rather than going into the content and ranking the pages, Page rank makes use of the linked structure to rank the pages. 
World Wide Web(WWW) is represented as a directed graph W in which all the nodes are pages and edges are hyperlinks. This directed graph is called a Web graph. 
Page rank is computed based on the incoming and outgoing links to the page. Link from a high reputed page has a higher weight compared to that of the link from a low reputed page. 
Example: 
The formula given in the original paper for calculating the page rank is as follows: 
PR(A)= PageRank of page A. 
PR(B),PR(C),…. = PageRank of pages [B,C,D….] which link to page A. 
L(B),L(C),L(D)…. = Number of outbound links of page . 
d = Teleporting (or) damping factor that lies between 0 and 1. 
Page rank computation is continued until the page rank gets converged. 
Sometimes random surfing leads to dead ends i.e., a page with no outgoing links. These kinds of pages are called dangling pages and these pages create storage, computational issues. 
How to identify the dangling pages in the transition matrix? 
The row corresponding to the dangling page contains all zeros. 
Handling the Dangling Pages: 
Method_1: Connect every dangling page to the hypothetical node of the web graph, construct a self-loop on the hypothetical node. 
Constructing the hypothetical node makes a matrix to stochastic, which is necessary for computing the page rank vector because Markov chain is defined only for the stochastic matrix. 
Method_2: Replace the row corresponding to the dangling page by Pij=1/n for all ‘j’ instead of all zeroes. 
By implementing any of the above handling methods we can achieve the stochastic property but it won't guarantee that the Markov model will converge and steady-state vector exists. 
The Markov is irreducible(which means every node has to be connected to every other node) but in the real web, every page is not connected to every other page. In order to achieve the irreducibility, all the entries in the transition matrix were made non-zeros i.e., 0<Pij<1 to make it regular. This ensures the convergence. 
Google Class Matrix: 
Google matrix is extremely large, it is almost billion by billion in size. So computing the eigen vector for this matrix would be a Herculean task! But there are some numerical methods to compute these eigen vectors as fast as possible. 
The google matrix ‘G’ is represented as follows: 
(1-α) is the probability that a random surfer may jump to a random page. α is the probability of clicking the forward link on the current page. Google uses α =0.85 for the PageRank algorithm. 
Numerical Methods used for Eigen Vector computation is as follows: 
After continuously performing some iterations we get the stationary vector. 
Facts of Google PageRank: 
It is developed by Google founders Larry Page and Sergey Brin in 1996. Google has further developed this method. The current model details were kept confidential but still the current method is based on the original one. 
PageRank algorithm became the Heart of the Google Search Engine!!! 
Other Works Include: 
Written by 
Written by",RAvi TeJA GUNdimeDA,2020-08-16T12:39:03.494Z
Word2Vector using Gensim. Intro : The goal is to build Word2Vec… | by Gaurav Padawe | Analytics Vidhya | Medium,"What is Word2Vec ? 
But Why Word2Vec ? 
Advantages : 
Disadvantages : 
Here are few parameters which one could play with : 
array([-2.7726305 , -1.0800452 , -1.788979 , 2.340867 , -0.32861072, 0.0651653 , 1.6166486 , -3.2617207 , -3.6233435 , -3.32576 , -1.7012835 , 1.0813012 , -0.24166667, -1.1136819 , 1.7357157 , -2.852496 , 0.2456542 , 0.9012077 , -0.8035166 , 1.7389616 , 1.5673314 , 2.0869598 , 3.3215692 , 0.8369672 , 0.07051245, 2.9767258 , -0.92073804, 0.6535899 , 2.716228 , 2.5288267 , 0.18343763, 1.5990931 , -2.1080818 , 1.5348029 , 0.19268313, -1.7983583 , -0.22839952, -0.22228098, 4.939321 , 2.071981 , -0.2585357 , -1.6617067 , 1.3812392 , -3.7641723 , 1.650655 , -1.4870547 , 2.4975944 , 2.2064195 , -2.383971 , -1.3767233 ], dtype=float32) 
Note : Please help me in correcting mistakes, If any. Do Share if you find it helpful. 
Thanks a Lot. 
Written by 
Written by",Gaurav Padawe,2019-11-21T05:35:17.658Z
Responses – Nathan Johnson – Medium,"Hi! I work at Stanford in the Office of Development, and am interested in finding applications of data science in higher education and fundraising. 
Thank you for the article, Tom. I think there is a lot of wisdom in these words. 
I still don’t understand why Zoom is getting such negative press. In my experience it is leaps better than alternative products. What an important product to have during this time! 
Ahh, okay got it. It has been interesting to see how the hype has developed around “data science” and the related buzz words. 
Why for losers? lol 
Thank you very much Sandeep! This was very helpful.",NA,NA
Getting to Know Natural Language Understanding | by ODSC - Open Data Science | Medium,"We like to imagine talking to computers the way Picard spoke to Data in Next Generation, but in reality, natural language processing is more than just teaching a computer to understand words. The subtext of how and why we use the words we do is notoriously difficult for computers to comprehend. Instead of Data, we get frustrations with our assistants and endless SNL jokes. 
[Related article: An Introduction to Natural Language Processing (NLP)] 
Natural Language Understanding (NLU) is a subfield of NLP concerned with teaching computers to comprehend the deeper contextual meanings of human communication. It’s considered an AI-hard problem for a few notable reasons. Let’s take a look at why computers can win chess matches against world champions and calculate billions of bits of data in seconds but can’t seem to grasp sarcasm. 
The first obstacle is teaching a computer to understand despite typos and misspellings. Humans aren’t always accurate in what they write, but a simple typo that you could skip right over without missing a beat could be enough to trip up the filters for computer understanding. 
We mentioned sarcasm above, but understanding the true meaning of utterances requires a strong understanding of context. Not only do sarcastic replies affect the outcome but not every negative utterance involves the presence of an explicitly negative word. To ask “How was lunch?” and receive a reply “I spend the entire time waiting at the doctor” is clear to you (lunch was bad) but not necessarily to a computer trained to search for negative words (no, not for example). 
Language understanding also requires input from variances in the same language. British English and American English have overall similarities, but a few things different, including spelling and meaning, can trip up a computer. And those are just two of the many, many versions of English, which in itself is a non-standard language and still remains the most parsed language in all of NLP. What about the others? 
[Related article: The Promise of Retrofitting: Building Better Models for Natural Language Processing] 
Natural Language Processing is the system we use to handle machine/human interactions, but NLU is a bit more narrow than that. When you’re in doubt, use NLU to refer to the simple act of machines understanding what we say. 
NLU is post-processing. Once your algorithms have scrubbed the text, adding part of speech tagging, for example, you begin to work with the real context of what’s going on. This post-processing is what starts to reveal to the computer the true meanings of text and not just surface understanding. 
NLU is a huge problem and an ongoing research area because the ability of computers to recognize and process human language at human-like accuracy has an enormous possibility. Computers could finally stand in for low paid customer service agents, capable of understanding human speech and its intent. 
In language teaching, students often complain that they can understand their teacher’s language, but that understanding doesn’t transfer when they walk outside the classroom. Computers are similar to these language students. When researchers formulate test texts, for example, they may unconsciously formulate them in ways that avoid those three common problems above, a luxury not afforded in a real-world context. A Twitter user isn’t going to scrub tweets of misspellings and ambiguous language before publishing, but that’s precisely what the computer must understand. 
The subfield relies heavily on both training lexicons and semantic theory. We can quantify semantics to an extent as long as we have large amounts of training data to provide context. As computers consume this training data, deep learning begins to make sense of intent. 
The biggest draw for NLU is a computer’s ability to interact with humans unsupervised. The algorithms classify speech into a structured ontology, but AI takes over to organize the intent behind the words. This method of deep learning allows computers to learn context and create rules based on more substantial amounts of input through training. 
Aside from everyone having their very own Data? Cracking Natural Language Understanding is the key piece of computers learning to understand human language without extraordinary intervention from humans themselves. 
NLU can be used to provide predictive insights for businesses by analyzing the unstructured data feeds of things like news reports, for example. This capability is especially true in areas such as high-frequency trading where trades are handled by automated systems. 
Unlocking NLU also rockets our AI assistants like Siri and Alexa into what finally counts as real human interaction. Siri still contains numerous errors exploited for humor by places like SNL, and those errors plague developers in search of human-like accuracy. If developers want off the SNL joke series, cracking AI is the key. 
Humans are still reigning champions for understanding language despite roadblocks (mispronunciations, misspellings, colloquialisms, implicit meaning), but the NLU problem could unlock the final door we need for machines to step up to our level. 
Original post here. 
Read more data science articles on OpenDataScience.com, including tutorials and guides from beginner to advanced levels! Subscribe to our weekly newsletter here and receive the latest news every Thursday. 
Written by 
Written by",ODSC - Open Data Science,2019-05-22T18:01:01.539Z
Analyzing historical speeches using Amazon Transcribe and Comprehend | by Gabriel dos Santos Goncalves | Towards Data Science,"Trying to make computers understand our language was one of the first challenges for researchers in Artificial Intelligence (AI) more than half a century ago. In recent years it has exploded in popularity in a field widely known as Natural Language Processing (NLP). Translation, speech recognition, boots, and many other tools and applications are part of our lives and somehow utilizes some kind of algorithm or model related to NLP. Chances are that if you are working on a software project you might need to use tools to deal with NLP. Learning the basic theory and how to use traditional libraries can take time, so we present here a straightforward approach for analyzing audio files using services offered by AWS. 
The goal of this article is to present two AWS NLP tools, Amazon Transcribe, and Amazon Comprehend using Python and boto3. We are going to download some historical speeches from Youtube using Youtube-dl, transcribe it to text and perform sentiment analysis. We also are going to compare the audio transcription with the original transcription to evaluate the similarity and consistency of the model used by Amazon Transcribe. 
We will be using five historical speeches downloaded from Youtube to illustrate the potential of the aforementioned tools. You can find the history and the original transcription of the speeches on this great article published on the Art of Manliness by Brett & Kate McKay. The selected speeches are the following: 
In July the 4th of 1940 Winston Churchill went to the House of Commons to deliver one of the most courageous and important speeches in history. Europe was being rapidly conquered by the German army and France has already fallen into Hitler’s control, and the British army had to retreat from the battle of Dunkirk. With this speech, Churchill invokes the British people to fight the Nazi menace discarding any chance of surrender, becoming the point of inflection in the history of World War II. 
Former US president Ronald Reagan showed the world his sense of leadership and freedom when by the Brandeburg Gate in June the 12th of 1987, side by side with the Soviet leader Mikhail Gorbachov, he calls for an end to Cold War. 
Minister Martin Luther King Jr. was the main leader of the Civil Rights movement that happened in the 50's and 60's in the US. During the March on Washington in 1963, he delivered his famous “I have a dream” speech claiming for peace and respect between whites and blacks. 
Few Americans in history have come close to General Douglas MacArthur in terms of military life achievements. Having fought in three wars, he embodied the figure of duty and honor for a military leader. His final speech at West Point Academy is an ode to patriotism and respect to soldiers that have died in the name of freedom. 
Former US President John F. Kennedy was one of the greatest speakers in modern history and during an event in May the 25th of 1961 in Houston he communicated the nation and the world the goal for America and the humanity to go to the moon. Few times in our history projects have impacted so much science and modern society as the space race and moon landing. 
To follow this tutorial you need the following: 
A) Python3 with the following libraries installed: boto3, pandas, matplotlib, seaborn, spacy, and wmd. 
B) An active account on AWS; 
C) AWS command-line interface (awscli) installed and configured; 
D) Youtube-DL installed. 
To increase readability and comprehension of this article I decided to show only the most important blocks of code of the analysis, but you can find all the details on the Jupyter notebook on my GitHub repository, including the code for doing the plots. 
To download the audio files from Youtube, we are going to use a command-line tool called Youtube-DL. After installing it, you can simply type the commands described below on the terminal with the URL of any Youtube video to download it. Youtube-DL was developed in Python and is in active development by the community on GitHub. 
Boto3 is the Python SDK developed by AWS for users to access its services. It calls AWS API and has two types of service access: Client and Resources. Client is a low-level service access and the response is in JSON format (Python dictionary). Resources is a high-level service access structured in an object-oriented way. You can basically use any of two types of service access independent of the task you need to perform on AWS, but in general, Resource can be used with fewer lines of code. For more information about boto3 please checkout Ralu Bolovan great article. We are going to do all of our analysis using Python on a Jupyter notebook and boto3 is going to be essential for the workflow. 
In order to convert our audio to a text, we are going to use Amazon Transcribe, an AWS service that offers automatic speech recognition for audio files located on an S3 bucket. It supports 9 different languages and a few variations: US English, British English, Australian English, French, Canadian French, US Spanish, ES Spanish, Italian, Brazilian Portuguese, German, Korean, Hindi, Indian-accented English, and Modern Standard Arabic. Amazon transcribe accepts files in 4 formats (mp3, mp4, wav, and flac) and has the capability of identifying multiple speakers on the audio, returning a respective transcription for each on one. If you have an audio file on an S3 bucket you can simply use its URL to create a transcription job on the AWS Transcribe console. But as we are doing our analysis in Python, we are going to use boto3 to call the AWS API to invoke the Transcribe job. 
Amazon Comprehend is the service found on the AWS ML/AI suite that offers a wide variety of functions for you to get insights from your text, like sentiment analysis, tokenization and identification of entities and classification of documents. We are going to use the tool for sentiment analysis on our transcriptions to try to capture the global message found on each of the speeches cited above. 
Spacy is an NLP library for Python that offers a great set of models already trained for 9 different languages. It was built on top of Cython and integrates easily with other Machine and Deep Learning libraries like TensorFlow, PyTorch, scikit-learn and Gensim, so it is really efficient and easy to use. Spacy is widely used by different companies and projects and has an increasing number of users worldwide. 
The first step in our analysis is to download the audio files from the five speeches mentioned in the introduction from Youtube. To organize our analysis and make it reproducible we are going to create a Pandas Dataframe to store the data generated during the analysis. 
To start, we created a dictionary with the name of each speaker as key and the respective Youtube URLs video as value (1). Next, we iterated over the values and executed youtube-dl command using os.system to download each file to our local folder (2). On the steps (3) we create the Dataframe from the dictionary and a new column with the name of each downloaded file (4). 
Now that we have the audio files for the analysis, we just need to upload it to an S3 bucket to make the transcription to text. On step (5) we used boto3 to create a new S3 bucket and uploaded the audio files to it (6). In order to start the automatic speech recognition, we need the path to each file located on the new S3 bucket. So by using the S3 path convention we defined the path for each file and save it to our Dataframe (7). On step (8), we create a function called start_transcription to call AWS API using boto3, and invoked it iteratively for each speech (9). We also defined on the function start_transcription to output the results as a JSON file on our S3 bucket. 
When the Amazon Transcribe jobs are done, you can access the results by reading the JSON file that was saved on the S3 bucket (10). In order to evaluate the quality of the transcription generated by Amazon Transcribe, we are going to use the original texts stored on the JSON file (11). The original transcripts were copied from the link provided on the Art of Manliness article, and new line characters were removed. Some parts of the original text were removed to match exactly the content of the audio, as some were not the complete speech. Before doing any text comparison we are going to remove the stop words, punctuations, pronouns and perform lemmatization on tokens (12). To establish the similarity between the texts we are going to use spaCy method for semantic similarity (13). It’s not a literal comparison of texts, as it is based on Word2vec, meaning we are comparing the percentage of each word over the whole sentence/text. 
The results for word vector similarity are plotted in Figure 7. We can see that based on this metric we can say that Amazon Transcribe is doing a good job transcribing the audio to text, as all the transcription were more than 99% similar to the original one. 
As word vectors similarity is based on the representation of each word on text, it can give us a false perception that two texts are alike. In order to have a better metric on the text difference, we are going to use Word Movers Distance to compare the original transcriptions to the one generated using Amazon Transcribe. To estimate this distance we need the library wmd that can be integrated with spaCy (14). For more information on estimating text similarities, please take a look at Adrien Sieg great article. 
As we can see in Figure 8, there are some transcriptions that show a larger distance between the original and the audio transcription, with Churchill’s and MacArthur’s speeches being the most evident. 
In order to compare the structure of the original transcripts to the ones generated by Amazon Transcribe, we are going to use a spaCy visualizer for entity recognition called displacy (15). 
Displacy offers visual identification of words and their respective class, making it easier to compare the structure of your text (Figure 9). 
By looking at the visualizations created with displacy, we spotted a few inconsistencies especially on the audio transcriptions from Churchill’s and MacArthur’s speeches. It seems that the quality of the audio and the diction of the speaker have an impact on the quality of the transcription. When we listen to Reagan’s and Kennedy’s speech, we can notice that the quality of the audio is much higher and the words and more clear, so the WMD is smaller between the original and the automatic transcription, meaning Amazon Transcribe model could show a better performance. 
On the last part of our analysis we are going to use Amazon Comprehend for sentiment analysis of the speeches. As mentioned before, AWS offers a pre-trained model that you can use to return the percentage of 4 different sentiments: positive, negative, mixed or neutral. To perform the sentiment analysis we simply need to provide the text as a string and the language. One limitation imposed by Amazon Comprehend is the size of the text that can be analyzed to 5000 bytes (which translates as a string containing 5000 characters). As we are dealing with texts transcripts that are larger than this limit, we created the start_comprehend_job function that split the input text into smaller chunks and calls the sentiment analysis using boto3 for each independent part. The results are then aggregated and returned as a dictionary (16). So we used this function on the audio transcription for each speaker and saved on our Dataframe for further analysis (17). 
Figures 10 (bar plot) and 11 (radar plot) show the results for the sentiment analysis for each selected speech. It seems that neutral sentences are the most prevalent in all speeches, followed by positive, mixed and negative. Churchill’s speech seems to be the one with the highest negative percentage, and Kennedy’s the most positive. 
AWS offers a set of ML/AI tools that can be easily accessed and integrated using boto3 for API call. 
Amazon Transcribe seems to be consistent when the audio quality is good, like with Reagan’s and Kennedy’s speeches, but tend to perform not as well when audio is not clear, like in the case of Churchill’s speech. 
Amazon Comprehend offers an easy way to extract sentiment from text, and it would be great to see its results against other NLP tools and other models. 
Hands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look 
Written by 
Written by",Gabriel dos Santos Goncalves,2019-11-14T19:32:47.518Z
Graph-based Deep Learning: Approaching a True “Neural” Network | by Mark Cleverley | Medium,"Graph networks (or network graphs, or just graphs) are data structures that model relationships between data. They’re comprised of a set of nodes and edges: points and relationships linking them together. I’ve done a brief introduction on them, and modeled US lobbying using a directed acyclic graph. They’re neat. 
However, the world of graph theory is huge, extending into multiple scientific disciplines. Like everything in data science, someone eventually asked: “How can I use this in machine learning?”. As it turns out, that was a fine question to ask. Recently, large developments have been made in the area of graph-based neural networks, or GNNs. 
In 2018, Zhou et. al published a large review of various GNN methods & applications that I’m quite fond of. It highlights some interesting points about the development and refinement of learning procedures. 
Graph neural nets are rooted in convolutional neural nets. CNNs are well-known for their ability to derive spatial features and craft highly useful matrix representations. They’re widely used in image-based learning because of their use filter maps to “generalize” and learn on similarity instead of exact binary matches (much like the human brain’s tendency to store and recognize patterns). 
Three central aspects of CNNs lend themselves naturally to GNNs: shared weights, local connections and multiple layers. Shared weights reduce computational cost, graphs are usually quite locally connected, and multi-layer approaches help solve hierarchical data patterns (when features vary in size). 
But where CNNs require Euclidean data in the form of images or text, graphs don’t. It’s tough to find convolutional filters and pooling functions to transform a 2D image to the non-Euclidean domain, but an image can be thought of a limited instance of a graph. Graphs are naturally non-Euclidean (most data is, though) because triangle inequality doesn’t hold up in mathematical space; similar things aren’t necessarily closer together. 
Any real-world system with defined structure can, in theory, be modeled by a graph. Take enough of these systems (or a large enough system to afford subdivision) and you can feed the data into a GNN that can learn the “habits” of the system, predicting tendencies and subtleties that the human eye would miss.A prevalent example is social network analysis: 
Wolfram visualized a graph of Facebook users as nodes and friend connections as edges, and was able to detect several distinct communities within one network. 
The spacing and distribution of nodes on such graphs is commonly determined by ‘number of connections’. If you were to add a new node to the graph and create edges to 20 “online friends” and 3 “family” nodes, it would end up somewhere in the red. In graphs where edges contain properties (numerical weight, etc) spacing becomes more precise. 
Social networks grow by the day, and nobody predicts online connections slowing down (a pandemic only increased the number). Qiu et. al created DeepInf, a framework that extracts a user’s local network and feeds it into a GNN to predict “latent social influence”. It’s a more complicated metric than you might imagine: you may be connected to many people, but are those people highly-connected? Are there specific qualities about those people (age, occupation, location) that influence connectivity? “Influence” is somewhere in between. 
Conventional social network analysis has to rely on custom rules and domain knowledge to account for the intricacies of a certain platform (what constitutes “influence” on twitter may differ on WeChat). However, DeepInf’s representation learning approach eliminates the need for labor-intensive labeling, and generally outperforms Linear Regression, Support Vector Machines and PCSN at predicting influence. 
Beyond abstract social relationships, graphs also have use in concrete physical sciences. There’s huge potential in biology and chemistry to map molecules to graphs for computational analysis. 
Since nodes and edges can hold any property or label (‘covalent’, ‘oxygen’, ‘5’), complex molecules can be effectively represented as a graph of atoms and bonds. This allows GNNs to learn and predict molecular qualities associated with structural patterns. 
For example, these nets have found use in Protein Inferface Prediction, a very challenging task necessary for drug discovery. Fout et. al (Colorado State) propose a Graph Convolutional Network that learns ligand and receptor residue markers and merges them for pairwise classification. They found that neighborhood-based GCN methods outperformed state-of-the-art diffusion and SVM methods at predicting interactions between proteins based solely on structure. 
In between abstract social networks and concrete molecular structure, hybrid semi-real systems are finding use in medical studies. Parisot et. al applied GCNs to predict neural diseases based on brain imaging and phenotype encodings. They model a medical population where nodes are brain-image feature vectors and edges encode phenotypic connections. They then apply the model to ABIDE and ADNI databases for Autism and Alzheimers prediction, outperforming contemporary non-graph models at 70% and 80% respectively. 
Neural networks are ostensibly designed after the connected neurons of the human brain, but a densely connected neural net is more like a series of fully-connected layers of neurons (which don’t connect to same-layer neurons). 
A graph network comes closer to representing the entangled complexity of our own carbon-based networks. Though it’s rather unscientific to jump to grand conclusions, it makes some intuitive sense that models closer in physical structure to the real thing will best approximate the subtleties of reality. 
If you found GNNs interesting, Tsinghua university aggregated many relevant papers into a categorized index. 
Written by 
Written by",Mark Cleverley,2020-04-19T15:10:07.420Z
BerkeleyISchool – Medium,,NA,NA
Recommendation Engine Explained. Github source code examples can be… | by Mohamed Fawzy | tajawal | Medium,"Github source code examples can be founded here: 
https://github.com/MohamedFawzy/recommendation-engine 
Have you wondered how amazon recommend items to you ? or netflix recommend content for you , spotify and youtube Here i will summarize things as much as possible . 
Recommendation engine a branch of information retrieval and artificial intelligence , are powerful tools and techniques to analyze huge volumes of data , especially product information and user information , and then provide relevant suggestions based on data-mining approaches . 
In tech terms recommendation engine problem is to develop mathematical model or objective function which can predict how much user will like an item . 
Get how much user will like this item 
Recommender systems collect informations on the preferences of it’s users for set of items (e.g movies , songs , books, jokes, gadgets) .The information can be acquired explicitly ( typically by collecting user’s ratings ) .The information can be acquired implicit ( typically by monitoring users’ behavior such as song heard , website visited and books read ). 
Recommender system may use demo-graphic features of user such as (age, nationality, gender, etc ) .Social information like followers , tweets, posts is commonly used in recommender systems . 
There’s growing tend toward the use of information from internet of things ( e.g GPS location , RFID, real-time signals ).Recommender system make use of different sources of information for providing users with predictions and recommendations of items . they try to balance factors like accuracy , novelty, dispersity and stability . 
Collaborative filtering (CF) methods play an important role in the recommendation also they’re often used along with other techniques like content-based, knowledge-based or social ones . 
Cf is based on the way which humans have made decisions throughout history.The most common research papers focused on movie recommendation studies however a great volume of literatures for RS is centered on different topics such as music , e-commerce, books, web search and others. 
Collaborative filtering recommender systems are basic forms of recommender engines. In this type of recommendation engine, filtering items from a large set of alternative is done collaborative by users preferences. 
The basic assumption in collaborative filtering is that of two users shared the same interest in the past they will also have similar taste in the future for e.g user A and user B have similar movie preferences , and user A recently watched titanic which user B has not yet seen, then the idea is to recommend this unseen movie to user B . 
User-based . 
Item-Based 
User-based collaborative filtering: in user based CF recommendations are generated by considering the preferences in the user’s neighborhood user-based CF is done in two steps: 
1- Identify similar users based on similar user preferences. 
2- Recommend new items to an active user based on rating given by similar user on the items not rated by active user . 
Item-based collaborative filtering : in item based CF , the recommendation engine are generated using the neighborhood of items unlike user-based, we first find similarities between items and then recommend non-rated items which are similar to the items the active user has rated in the past . Item-based recommender system are constructed in two steps : 
1- Calculate the item similarity based on the item preferences . 
2- Find the top similar items to the non-rated items by active user and recommend them . 
Pros: 
Cons: 
Example : 
Content-Based: 
In this type of recommender systems instated of consider only user-based item-based preferences though it’s accurate.It make more sense if we consider user properties and item properties while building the recommendation engine.Using content information of the items for building the recommender model. 
A content recommender system typically contains a user- profile-generation step, item-profile-generation step and model building-generation step to generate recommendation for an active user. 
The content-based recommender system recommends items to users by taking the content or features of items and user profiles. As an example, if you have searched for videos of Lionel Messi on YouTube, then the content-based recommender system will learn your preference and recommend other videos related to Lionel Messi and other videos related to football. 
In simpler terms, the system recommends items similar to those that the user has liked in the past. The similarity of items is calculated based on the features associated with the other compared items and is matched with the user’s historical preferences. 
e.g of recommender system for movies using content-based 
Hybrid system: 
Built by combining various recommender systems to build more robust system. By combining various recommender systems, we can replace disadvantage of one of system with the advantages of another system and thus build a more robust system. For example by combining collaborative filtering methods where model fail with cold start problem with content-based systems, where features information about the items are available , new items can be recommended more accurately and efficiently. 
For example, if you are a frequent reader of news on Google News, the underlying recommendation engine recommends news articles to you by combining popular news articles read by people similar to you and using your personal preferences, calculated using your previous click information. 
With this type of recommendation system, collaborative filtering recommendations are combined with content-based recommendations before pushing recommendations. 
Context-aware: 
Personalized recommender systems such as content-based recommender systems , are inefficient , they fail to suggest recommendations with respect to context .For example assume lady is very fond of ice-cream. Also assume this lady goes to cold place. 
Now here is high chance that a personalized recommender systems suggest a popular ice-cream brand . 
Is it make any sense to suggest ice-cream to here in cold place or system should suggest coffee ? This type of recommender systems which is personalized and context-aware called context-aware . 
User preferences may differ with the context such as: 
This section present the most relevant concepts on which the traditional recommender systems based .Provide general descriptions on the classical taxonomies, algorithms , methods, filtering approaches , database , etc . 
After that will describe cold start problem which will illustrate the difficulty of making collaborative filtering recommendation when the recommender system have small amount of data . 
Fundamentals: 
The process for generating an RS recommendation is based on combinations of the following considerations : 
Datasets: 
Through these databases, the scientific community can replicate experiments to validate and improve their techniques the current public databases referenced most often in research are 
The internal functions for recommender system are characterized by the filtering algorithm the most widely used classifications divides the filtering into: 
Content-based filtering : makes recommendations based on user choice made in the past (e.g web-based , e-commerce ) 
Demographic-filtering : is justified on the principle that individuals with certain common personal attributes (sex, age, country, etc.) will also have common preferences. 
Collaborative filtering allows users to give ratings about a set of elements (e.g videos, songs , films , etc ) . 
The most widely used algorithm for collaborative filtering is the k nearest neighbor (KNN) algorithm . 
In the user to user version, kNN executes the following three tasks to generate recommendations for an active user: 
Recommendation categories based on model: 
Foundation — Cold start: 
The cold start problem occurs when it’s not possible to make reliable recommendation due to an initial lack of ratings . 
Types of cold start: 
New community : problem refers to the difficulty when starting a new RS. In obtaining a sufficient amount of data ratings for making reliable recommendations, two commons way are used for tackling this problem 
- Encourage users to make rating through gamification model . 
- To take CF when there are enough data for users and ratings . 
New item : arises because new items added to RS don’t usually have initial ratings , and therefore they are not likely to be recommended . Then a lot of users will never seen this items . A common solution for this problem is to have a set of motivated users who are responsible for rating each new item in the system . 
New user: one of greatest difficulties faced by RS . Since new user has not provided any ratings yet in RS.They cannot receive any personalized recommendation based on memory-based CF.The common strategy to tackle this problem consist of turning to additional information to the set of ratings in order to be able to make recommendation based on the data available for each user . 
A metric or a Similarity Measure (SM) determines the similarity between pairs of users (user to user CF) or the similarity between pairs of items (item to item CF). 
For this purpose, we compare the ratings of all the items rated by two users (user to user) or the ratings of all users who have rated two items (item to item) . 
The KNN algorithm is based essentially on the use of traditional similarity metrics of statistical origin. These metrics require, as the only source of information, the set of votes made by the users on the items (memory-based CF). 
Techniques used to measure Similarity: 
The most commonly used quality measurement are the following 
Evolution metrics can be classified as: 
Papers: 
Books: 
Links: 
Written by 
Written by",Mohamed Fawzy,2019-08-28T07:55:01.458Z
Natural Language Processing: Experimenting Entity Recognition ( Part 2 AWS Comprehend NLP API) | by Patrick Rotzetter | Analytics Vidhya | Medium,"In the first article of the series we have shown how to use the Google NLP API to extract words from a person profile and compare it to a job description. In this second article, let us do the same experiment using AWS ( Amazon Web Services) Comprehend API. We will be using R again, but the same can be easily done in Python as well and we will show how at the end of the article. 
Before anything, you have to have a registered account with AWS, as it is not recommended to use your root user to access AWS services, let us create a specific user for our experimentation. 
On the next page, you will be asked to assign the user to a pre-defined group ( or to create a specific group). For example I have defined a group with full access to AWS Comprehend and I will assign the test user to this group. 
Once the user is successfully created, you will have the ability to download the user credentials, this is critical information in order to be able to access the AWS API later on, so keep it somewhere safe. 
In order to experiment the AWS API with R, we will use the aws.comprehend R package and library: 
Note that the library has an issue in the detect entities function if you wnat to use it, so check in github for the fix if you want to use it. 
Now it is time to use your previously saved access key ID and secret access key that should be defined in an environment variable for aws.comprehend library to find the credentials: 
Exactly like in the previous article https://medium.com/@patrick.rotzetter/natural-language-processing-experimenting-entity-recognition-with-google-amazon-nltk-and-maybe-b1fe673efe46, we are using following code to read the PDF files and do some pre-processing 
And now we are ready to call the AWS NLP API, in this case we will be using the ‘detect syntax’ functionality, the text will be analyzed and words will be classified as noun, verb, adverb and so on. 
We will filter all nouns from the AWS result and group them by noun and count them: 
Compared to the Google API results, we can see some significant differences in the number of times a word has been identified, for example we had 144 times the word ‘projects’ in the previous experiment and 14 times using AWS. 
To be able to interact with AWS Comprehend APiyou will need to dowload the AWS command line interface (CLI) and configure it using your previously stored credentials. ( more on this under https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html) 
The boto3 library is the is the Amazon Web Services (AWS) SDK for Python. So first import the library 
Then you instantiate a comprehend object with ‘comprehend’ as parameter for the boto3.client call 
You can read your PDF file using any popular PDF library, for example like this 
Once you have the text you want to analyze, just call the detect entities function and that’s it 
Written by 
Written by",Patrick Rotzetter,2020-04-03T08:20:39.434Z
Exfiltrating data via GET Headers (X-CSRF-Token) | by Ramon Martinez | byhardest | Medium,"In Infosec, Data Exfiltration is one of main threats and concerns nowadays when fighting against cybercrime, both from malware perspective and also insiders. According to MITRE ATT&CK Exfiltration is; 
Exfiltration refers to techniques and attributes that result or aid in the adversary removing files and information from a target network 
The reason could be many since from non-bad intentional co-workers only trying to be more productive on weekends ignoring security risks up to an APT espionage attack that hacked your network for a couple weeks or months. 
The reason I decided to do this poc, is to bring attention on how difficult it is to monitor for these exfiltration attempts, even for protocols that are very well consolidated on market and has known controls such as HTTP which on it background has severals security solutions such as web content controls, DLPs, sandbox analysis and so on.. 
Many of the above tools are able to detect and block uploads and some C2 via HTTP/FTP in fact with the most common HTTP/FTP protocol basicly via POST and PUT. 
But, what if we use the GET method to send the file cut inside a header and at the server side remount this disassembled data reconstructing as binary leaving it intact (integrity remaining the same?) 
3 Most reasons I figured why would it be hard to detect. 
So how does it work. 
The scope contains two parts; 
- ToBase64 .NET component- Invoke-WebRequest 
I have choose the data to sent over X-CSRF-Token, but you can create your own list with other headers and also modify the user-agent string to look like a true navigator one easily becoming more stealthy. 
2. Server *The one will receive the file as strings and remount it.* 
SimpleHTTPServer listening for GET requests over a specific pattern to trigger the incomiing files. 
I left it as default the “200 (OK)” response and also a message “Under Maintenance” when replying back for any request, but this can be modified. 
DISCLAIMER 
“Exfiltrating and leaking not authorized data may suffer law and consequences is at your own risk. Educational purposes only.” 
DEMO: 
What could we still do to mitigate this: 
Web-Content: Block newly recent created websites or uncategorized for web navigation, this will break most of the attacks, since attackers change their webserver domains often and in the meantime these websites are not yet classified. Many of the web-content solutions already have the most popular and business classified, this should be equivalent to 5% of your traffic, but the one that may save you. Golden tip. 
Extra tip: Since this can be easily bypassed using amazon ec2 e.g. Have with you a specific rule in your SIEM for monitoring ec2 instances access, since the URL can be easily changed powering on/off your instance and therefore being categorized as Information Technology as you can see some examples below: 
Network Behaviour Traffic Analysis solutions based on netflow could be a good investment when you realize in this kind of behavior; even if changing from POST to GET bytes_out are still higher than bytes_in and requests to that server. Also many of the market solutions would help to create a previous baseline and alert any abnormal high traffic regardless the method or protocol could be considered suspicious, resilience is the key. 
Using wireshark capture & traffic analysis you can see below; 
ip.addr==192.168.1.16 && ip.addr==18.228.44.81 && http 
Statistics -> I/O Graph then filter; 
ip.src ==192.168.1.16 && ip.dst==18.228.44.81 && http ip.dst ==192.168.1.16 && ip.src ==18.228.44.81 && http 
Change color style for better view. 
Below you can see the red line in a peak of 9000 bytes OUT compared to a peak of 1500 bytes IN, 6x more DATA SENT via GET. This could be a good (or bad hehe) indicator of abnomaly for both C2 & Data Exfiltration. 
Endpoint Controls 
1) Upgrade the powershell to version 5 collect it logs and monitor for any abnormal scripts. There are good papers around there for how to create these rules. 
These scripts will certainly leave evidences and if you analyze the code would be very easy to figure out what’s doing, also if the actor encode the script will also trigger else from other tools as EDRs or SIEMs. 
2) Whenever is possible control the software being run on workstation with softwares as Applocker, would prevent against not sanctioned code. 
Source code: https://github.com/byhardest/getexfiltration 
If you want to contribute, or you know any other tip let me know, also feel free to correct me at any point if I am wrong, surely will earn my trust. :-) 
Thank you, 
Ramon Martinez 
http://twitter.com/byhardesthttps://github.com/byhardesthttps://www.linkedin.com/in/ramon-eduardo-m-715b764a/ 
Written by 
Written by",Ramon Martinez,2019-07-17T17:28:52.061Z
Algorithm selection for Anomaly Detection | by Sahil Garg | Analytics Vidhya | Medium,"Anomalies can be defined as observations which deviate sufficiently from most observations in the data set to consider that they were generated by a different, not normal, generative process. Anomaly is any observation which deviates so much from other observations in the data set so as to arouse suspicion. In brief, anomalies are rare and significantly different observations within a data set. 
Anomaly detection algorithms are now used in many application domains for Intrusion detection, Fraud Detection, Data leakage prevention, Data Quality, Surveillance & Monitoring. As we see these are wide variety of applications, some require very fast, near real time anomaly detection whereas some require very high performance due to high cost of missing an anomaly. Anomaly detection techniques are most commonly used to detect fraud, where malicious attempts/transactions often differ from most nominal cases. Outlined below are the different types of anomalies: 
Point anomaly: Single anomalous instances in a larger dataset 
Collective anomaly: If an anomalous situation is represented as a set of many instances, this is called a collective anomaly. 
Contextual anomaly: In contextual anomalies, point can be seen as normal but when a given context is taken into account, the point turns out to be an anomaly. 
The solution to anomaly detection can be framed in all three types of machine learning methods — Supervised, Semi-supervised and Unsupervised, depending on the type of data available. Supervised learning algorithms can be used for anomaly detection when anomalies are already known and labelled data is available. These methods are particularly expensive when the labeling has to be done manually. Unbalanced classification algorithms such as Support Vector Machines (SVM) or Artificial Neural Networks (ANN) can be used for supervised anomaly detection. 
Semi-supervised anomaly Detection uses labelled data consisting only of normal data without any anomalies. The basic idea is, that a model of the normal class is learned and any deviations from that model can be said to be anomalies. Popular algorithms: Auto-Encoders, Gaussian Mixture Models, Kernel Density Estimation. 
Unsupervised learning methods are most commonly used to detect anomalies, the following chart outlines major families of algorithms and algorithms which can be used for anomaly detection. 
K-Nearest Neighbor (kNN): kNN is a neighbor based method which was primarily designed to identify outliers. For each data point, the whole set of data points is examined to extract the k items that have the most similar feature values: these are the k nearest neighbors (NN). Then, the data point is classified as anomalous if the majority of NN was previously classified as anomalous. 
Local Outlier Factor(LOF): Local Outlier Factor is a density-based method designed to find local anomalies. For each data point, the NN are computed. Then, using the computed neighborhood, the local density is computed as the Local Reachability Density (LRD). Finally, the LOF score is computed by comparing the LRD of a data point with the LRD of the previously computed NN. 
Connectivity Based Outlier factor (COF): Connectivity-based Outlier Factor (COF) differs from LOF in the computation of the density of the data points, since it also considers links between data points. To such extent, this method adopts a shortest-path approach that calculates a chaining distance, using a minimum spanning tree 
K-Means: K-means Clustering is a popular clustering algorithm that groups data points into k clusters by their feature values. Scores of each data point inside a cluster are calculated as the distance to its centroid. Data points which are far from the centroid of their clusters are labeled as anomalies. 
Robust Principal Component Analysis(rPCA): Principal component analysis is a commonly used technique for detecting sub-spaces in datasets. It also serves as an anomaly detection technique, such that deviations from the normal sub-spaces may indicate anomalous instances. Once the principal components are determined major components show global deviations from the majority of the data whereas using minor components can indicate smaller local deviations. 
One Class SVM: One-class Support Vector Machine algorithm aims at learning a decision boundary to group the data points. It can be used for unsupervised anomaly detection, the one-class SVM is trained with the dataset and then each data point is classified considering the normalized distance of the data point from the determined decision boundary 
Isolation Forest: Isolation Forest structures data points as nodes of an isolation tree, assuming that anomalies are rare events with feature values that differ a lot from expected data points. Therefore, anomalies are more susceptible to isolation than the expected data points, since they are isolated closer to the root of the tree instead of the leaves. It follows that a data point can be isolated and then classified according to its distance from the root of the tree. 
Angle Based Outlier detection (ABOD): Angle Based Outlier detection (ABOD) relates data to high-dimensional spaces, using the variance in the angles between a data point to the other points as anomaly score. The angle-based outlier detection (ABOD) method provides an good alternative in identifying outliers in high-dimensional spaces 
Mahalanobis Distance: Mahalanobis method solely the distance space to flag outliers. The Mahalanobis distance is suitable for anomaly detection tasks targeting multivariate datasets composed of a single Gaussian-shaped cluster. The model parameters are the mean and inverse covariance matrix of the data. 
Neural Networks such as Self Organizing Maps: Also called Grow When Required (GWR) network, it is a reconstruction based non parametric neural network. It fits a graph of adaptive topology lying in the input space to a dataset. Severe outliers and dense clouds of outliers are correctly identified with this technique. 
Gaussian Mixture Models: Gaussian Mixture Model (GMM) fits a given number of Gaussian distributions to a dataset. The model is trained using the Expectation-Maximization algorithm which maximizes a lower bound of the likelihood iteratively. Assessing the number of components of the mixture by data exploration can be complex 
Auto-Encoders: An autoencoder is a special type of neural network that copies the input values to the output values. The key idea is to train a set of autoencoders to learn the normal behavior of the data and, after training, use them to identify abnormal conditions or anomalies. 
Anomaly Detection algorithm selection is complex activity with multiple considerations: type of anomaly, data available, performance, memory consumption, scalability and robustness. 
Performance metrics for anomaly detection models are mainly based on Boolean anomaly/expected labels assigned to a given data point such as Precision, Recall, F-score, Accuracy and AUC. The image below, from research paper: Quantitative Comparison of Unsupervised Anomaly Detection Algorithms for Intrusion Detection, shows relative performance of algorithms families against the performance metrics. 
Training & Prediction time: Scalability and consumption of the model can be judged by computation and prediction time required by the different methods when increasing the data set size and dimensionality. The graphs below from paper: A comparative evaluation of outlier detection algorithms: Experiments and analyses, highlights relative performance of the algorithms with respect to increasing number of features and training time and prediction time. 
Robustness & Scalability: Many anomaly detection methods suffer from the curse of dimensionality. It is important to check resistance of each algorithm to the curse of dimensionality, where we keep a fixed level of background noise while increasing the data set dimensionality. The graph below from paper: A comparative evaluation of outlier detection algorithms: Experiments and analyses, highlights relative performance of the algorithms with respect to increasing number of features and average precision. 
Memory usage: Several algorithms have high memory requirements, thus choice of algorithm should be done carefully with due consideration to available hardware and scalability requirements. The graph below from paper: A comparative evaluation of outlier detection algorithms: Experiments and analyses, highlights relative performance of the algorithms with respect to increasing number of features and memory usage. 
Similarly, another paper ,A Comparative Evaluation of Unsupervised Anomaly Detection Algorithms for Multivariate Data, published multiple findings for comparative universal evaluation of anomaly detection algorithms on publicly available datasets. The following table contains recommendations from this paper, where 19 different unsupervised anomaly detection algorithms are evaluated on 10 different datasets from multiple application domains. 
Written by 
Written by",Sahil Garg,2020-06-13T16:15:44.618Z
Interpreting and validating topic models | by Patrick van Kessel | Pew Research Center: Decoded | Medium,"(Related posts: An intro to topic models for text analysis, Making sense of topic models and Overcoming the limitations of topic models with a semi-supervised approach) 
My previous post in this series showed how a semi-supervised topic modeling approach can allow researchers to manually refine topic models to produce topics that are cleaner and more interpretable than those produced by completely unsupervised models. The particular algorithm we used was called CorEx, which provides users with the ability to expand particular topics with “anchor words” that the model may have missed. Using this semi-supervised approach, we were able to train models on a collection of open-ended Pew Research Center survey responses about sources of meaning in life and arrive at a set of topics that seemed to clearly map onto key themes and concepts in our data. 
Next, we faced the task of interpreting the topics and assessing whether our interpretations were valid. Here are four topics that seemed coherent after we improved them using our semi-supervised approach: 
Interpreting topics from a model can be more difficult than it may initially seem. Understanding the exact meaning of a set of words requires an intimate understanding of how the words are used in your data and the meaning they’re likely intended to convey in context. 
Topic 37 above appears to be a classic example of an “overcooked” topic, consisting of little more than the words “health,” “healthy,” and a collection of phrases that contain them. At first, we were unsure whether we’d be able to use this topic to measure a useful concept. While we were hoping to use it to identify survey responses that mentioned the concept of health, we suspected that even our best attempts to brainstorm additional anchor words for the topic might still leave relevant terms missing. Depending on how common these missing terms were in our data, the topic model could seriously understate the number of people who mentioned health-related concepts. 
To investigate, we read through a sample of responses and looked for ones that mentioned our desired theme. To our surprise, we realized that this particular “overcooked” topic was not a problem in the context of our particular dataset. Of the responses we read (many of which used a wide variety of terms related to health), we found very few that mentioned the theme of health without using a variant of the word itself. In fact, the overwhelming majority of responses that used the term appeared to refer specifically to the theme of being in good health. Responses that mentioned health problems or poor health were far less frequent and typically used more specific terms like “medical condition,” “medication,” or “surgery.” 
Based on the specific nature of our documents and the context of the survey prompts we used to collect them, we decided that we could not only use this “overcooked” topic, but we could also assign it a more specific interpretation — “being in good health” — than might otherwise have been possible with different data. 
However, this turned out to be a unique case. For other topics, separating out both positive and negative mentions was often impossible. 
For example, the language our survey responses used to describe both financial security and financial difficulties was so diverse and overlapping that we realized we wouldn’t be able to develop separate positive and negative anchor lists and train a model with two separate topics related to finances. Instead, we had to group all of our money-related anchor words together into Topic 44, which we could only interpret as being about money or finances in general. We manually coded a sample of these responses and found that 77% mentioned money in a positive light, compared with 23% that brought it up in a neutral or negative manner. But even our manually-refined semi-supervised topic model couldn’t be used to tell the difference. 
Clearly, context matters when using unsupervised (or even semi-supervised) methods. Depending on how they’re used, words found in one set of survey responses can mean something entirely different in another set, and interpretations assigned to topics from a model trained on one set of data may not be transferable to another. Since algorithms like topic models don’t understand the context of our documents — including how they were collected and what they mean — it falls on researchers to adjust how we interpret the output based on our own nuanced understanding of the language used. 
Between the two semi-supervised CorEx topic models that we trained, we identified 92 potentially interesting and interpretable topics. To test our ability to interpret them, we gave each one a short description, including some additional caveats based on what we knew about the context of each topic’s words in our corpus: 
For each topic, we first drew a small exploratory sample consisting of some responses that contained the topic’s top words and others that didn’t. A member of the research team then coded each response based on whether or not it matched the label we had given the topic. After coding all of the samples, we used Cohen’s Kappa, a common measure of inter-rater reliability, to test how well the topic models agreed with the descriptions that we had given the topics. 
Some topics resulted in particularly poor agreement between the model and our own interpretation, often because the words in the topic were used in so many different contexts that its definition would have to be expanded to the point that it would no longer be meaningful or useful for analysis. 
For example, one of the topics we had to abandon dealt with opinions about politics, society and the state of the world. Some of the words in this topic were straightforward: “politics,” “government,” “news,” “media,” etc. While there were a handful of false positives for these words — responses in which someone wrote about their career in government, or recently receiving good news — the vast majority of responses that mentioned these words contained opinions about the state of the world, in line with our interpretation. But a single word, “world” itself wound up posing a critical problem that forced us to give up on refining this topic. 
In our particular dataset, there were a lot of respondents that opined about the state of the world using only general references to “the world” — but there were also many respondents that used “world” in a non-political, personal context, such as describing how they wanted to “make the world a better place” or “travel the world.” As a result, including “world” as an anchor term for this topic produced numerous false positives, but excluding it produced many false negatives. Either way, our topic model would either be overstating or understating the topic’s prevalence to an unacceptable degree. 
In this case, we were able to narrow our list of anchor terms to focus on the more specific concept of politics, but other topics presented similar challenges and we were forced to set some of them aside. As we continued reviewing our exploratory samples, we also noticed that some topics that initially seemed interesting — like “spending time doing something” and “thinking about the future” — turned out to be too abstract to be analytically useful, so we set these aside, too. 
From our initial 92 topics, we were left with 31 that seemed analytically interesting, could be given a clear and specific definition, and had encouraging levels of initial reliability, at least based on our non-random exploratory samples. Drawing on the insights we’d gained from viewing these topics in context, we refined them further and added or removed words from our anchor lists where it seemed useful. 
For our final selection of topics, we drew new random samples of 100 documents for each topic, this time to be coded by two different researchers to determine whether our tentative topic labels were defined coherently enough to be understood and replicated by humans. 
Unfortunately, we found that seven of these 31 topics resulted in unacceptable inter-rater reliability. Though they had seemed clear on paper, our labels turned out to be too vague or confusing and we couldn’t consistently agree on which responses mentioned the topics and which did not. Fortunately, we had acceptable rates of agreement on the remaining 24 topics — but for a few of the rarer ones, that wasn’t good enough. In an upcoming post, I’ll explain how we used a method called keyword oversampling to salvage these topics. 
Patrick van Kessel is a senior data scientist at Pew Research Center. 
Written by 
Written by",Patrick van Kessel,2019-08-01T18:31:02.071Z
Automated Keyword Extraction from Articles using NLP | by Sowmya Vivek | Analytics Vidhya | Medium,"In research & news articles, keywords form an important component since they provide a concise representation of the article’s content. Keywords also play a crucial role in locating the article from information retrieval systems, bibliographic databases and for search engine optimization. Keywords also help to categorize the article into the relevant subject or discipline. 
Conventional approaches of extracting keywords involve manual assignment of keywords based on the article content and the authors’ judgment. This involves a lot of time & effort and also may not be accurate in terms of selecting the appropriate keywords. With the emergence of Natural Language Processing (NLP), keyword extraction has evolved into being effective as well as efficient. 
And in this article, we will combine the two — we’ll be applying NLP on a collection of articles (more on this below) to extract keywords. 
In this article, we will be extracting keywords from a dataset that contains about 3,800 abstracts. The original dataset is from Kaggle — NIPS Paper. Neural Information Processing Systems (NIPS) is one of the top machine learning conferences in the world. This dataset includes the title and abstracts for all NIPS papers to date (ranging from the first 1987 conference to the current 2016 conference). 
The original dataset also contains the article text. However, since the focus is on understanding the concept of keyword extraction and using the full article text could be computationally intensive, only abstracts have been used for NLP modelling. The same code block can be used on the full article text to get a better and enhanced keyword extraction. 
The dataset used for this article is a subset of the papers.csv dataset provided in the NIPS paper datasets on Kaggle. Only those rows that contain an abstract have been used. The title and abstract have been concatenated after which the file is saved as a tab separated *.txt file. 
As we can see, the dataset contains the article ID, year of publication and the abstract. 
Before we proceed with any text pre-processing, it is advisable to quickly explore the dataset in terms of word counts, most common and most uncommon words. 
The average word count is about 156 words per abstract. The word count ranges from a minimum of 27 to a maximum of 325. The word count is important to give us an indication of the size of the dataset that we are handling as well as the variation in word counts across the rows. 
A peek into the most common words gives insights not only on the frequently used words but also words that could also be potential data specific stop words. A comparison of the most common words and the default English stop words will give us a list of words that need to be added to a custom stop word list. 
Sparsity: In text mining, huge matrices are created based on word frequencies with many cells having zero values. This problem is called sparsity and is minimized using various techniques. 
Text pre-processing can be divided into two broad categories — noise removal & normalization. Data components that are redundant to the core text analytics can be considered as noise. 
Handling multiple occurrences / representations of the same word is called normalization. There are two types of normalization — stemming and lemmatization. Let us consider an example of various versions of the word learn — learn, learned, learning, learner. Normalisation will convert all these words to a single normalised version — “learn”. 
Stemming normalizes text by removing suffixes. 
Lemmatisation is a more advanced technique which works based on the root of the word. 
The following example illustrates the way stemming and lemmatisation work: 
To carry out text pre-processing on our dataset, we will first import the required libraries. 
Removing stopwords: Stop words include the large number of prepositions, pronouns, conjunctions etc in sentences. These words need to be removed before we analyse the text, so that the frequently used words are mainly the words relevant to the context and not common words used in the text. 
There is a default list of stopwords in python nltk library. In addition, we might want to add context specific stopwords for which the “most common words” that we listed in the beginning will be helpful. We will now see how to create a list of stopwords and how to add custom stopwords: 
We will now carry out the pre-processing tasks step-by-step to get a cleaned and normalised text corpus: 
Let us now view an item from the corpus: 
We will now visualize the text corpus that we created after pre-processing to get insights on the most frequently used words. 
Text in the corpus needs to be converted to a format that can be interpreted by the machine learning algorithms. There are 2 parts of this conversion — Tokenisation and Vectorisation. 
Tokenisation is the process of converting the continuous text into a list of words. The list of words is then converted to a matrix of integers by the process of vectorisation. Vectorisation is also called feature extraction. 
For text preparation we use the bag of words model which ignores the sequence of the words and only considers word frequencies. 
As the first step of conversion, we will use the CountVectoriser to tokenise the text and build a vocabulary of known words. We first create a variable “cv” of the CountVectoriser class, and then evoke the fit_transform function to learn and build the vocabulary. 
Let us now understand the parameters passed into the function: 
An encoded vector is returned with a length of the entire vocabulary. 
We can use the CountVectoriser to visualise the top 20 unigrams, bi-grams and tri-grams. 
The next step of refining the word counts is using the TF-IDF vectoriser. The deficiency of a mere word count obtained from the countVectoriser is that, large counts of certain common words may dilute the impact of more context specific words in the corpus. This is overcome by the TF-IDF vectoriser which penalizes words that appear several times across the document. TF-IDF are word frequency scores that highlight words that are more important to the context rather than those that appear frequently across documents. 
TF-IDF consists of 2 components: 
Based on the TF-IDF scores, we can extract the words with the highest scores to get the keywords for a document. 
Ideally for the IDF calculation to be effective, it should be based on a large corpora and a good representative of the text for which the keywords need to be extracted. In our example, if we use the full article text instead of the abstracts, the IDF extraction would be much more effective. However, considering the size of the dataset, I have limited the corpora to just the abstracts for the purpose of demonstration. 
This is a fairly simple approach to understand fundamental concepts of NLP and to provide a good hands-on practice with some python codes on a real-life use case. The same approach can be used to extract keywords from news feeds & social media feeds. 
Written by 
Written by",Sowmya Vivek,2018-12-17T08:09:47.306Z
Introduction to Active Learning. What is Active Learning? | by Michelle Zhao | Towards Data Science,"The goal of this post is to help demystify active learning and show how it differs from standard supervised machine learning. 
First, what is active learning? Active learning is a machine learning framework in which the learning algorithm can interactively query a user (teacher or oracle) to label new data points with the true labels. The process of active learning is also referred to as optimal experimental design. 
The motivation for active learning is a scenario in which we have a large pool of unlabelled data. Consider the problem of training an image classification model to distinguish between cats and dogs. There are millions of images out there of each, but not all of them are needed to train a good model. Some images may offer more clarity and information than others. Another similar application is classifying the content of Youtube videos, where the data is inherently dense and there exists a lot of it. 
Passive learning, the standard framework in which a large quantity of labelled data is passed to the algorithm, requires significant effort in labelling the entire set of data. 
By using active learning, we can selectively leverage a system like crowd-sourcing, to ask human experts to selectively label some items in the data set, but not have to label the entirety. The algorithm iteratively selects the most informative examples based on some value metric and sends those unlabelled examples to a labelling oracle, who returns the true labels for those queried examples back to the algorithm. 
In several cases, active learning performs better than random sampling. The below image shows a motivating example of active learning’s improvement over random selection. The entire set of data points (union of the sets of red triangles and green circles) is not linearly separable. 
Active learning is motivated by the understanding that not all labelled examples are equally important. With uniform random sampling over all of the examples, the learned model doesn’t quite represent the division between classes. However, active learning selects examples near the class boundary, and is able to find a more representative classifier. Previous research has also shown that active learning offers improvement over standard random selection for tasks like multi-class image classification [1, 2, 3, 4]. 
The active learning framework reduces the selection of data to a problem of determining which are the most informative data points in the set? In active learning, the most informative data points are generally the ones that the model is most uncertain about. This necessitates various metrics to quantify and compare uncertainty of examples. 
Active learning is considered to be a semi-supervised learning method, between unsupervised being using 0% of the learning examples and fully supervised being using 100% of the examples. By iteratively increasing the size of our labelled training set, we can achieve greater performance, near fully-supervised performance, with a fraction of the cost or time to train using all of the data. 
In pool-based sampling, training examples are chosen from a large pool of unlabelled data. Selected training examples from this pool are labelled by the oracle. 
In stream-based active learning, the set of all training examples is presented to the algorithm as a stream. Each example is sent individually to the algorithm for consideration. The algorithm must make an immediate decision on whether to label or not label this example. Selected training examples from this pool are labelled by the oracle, and the label is immediately received by the algorithm before the next example is shown for consideration. 
The decision for selecting the most informative data points is dependent on the uncertainty measure used in selection. In pool-based sampling, the active learning algorithm selects examples to add to the growing training set that are the most informative. 
The most informative examples are the ones that the classifier is the least certain about. 
The intuition here is that the examples for which the model has the least certainty will likely be the most difficult examples — specifically the examples that lie near the class boundaries. The learning algorithm will gain the most information about the class boundaries by observing the difficult examples. 
Below are four common uncertainty measures used in active learning to select the most informative examples. 
The smallest margin uncertainty is a best-versus-second-best uncertainty comparison. The smallest margin uncertainty (SMU) is the classification probability of the most likely class minus the classification probability of the second most likely class [1]. The intuition behind this metric is that if the probability of the most likely class is significantly greater than the probability of the second most likely class, then the classifier is more certain about the example’s class membership. Likewise, if the probability of the most likely class is not much greater than the probability of the second most likely class, then the classifier is less certain about the example’s class membership. The active learning algorithm will select the example with the minimum SMU value. 
Least confidence uncertainty (LCU) is selecting the example for which the classifier is least certain about the selected class. LCU selection only looks at the most likely class, and selects the example that has the lowest probability assigned to that class. 
Entropy is the measure of the uncertainty of a random variable. In this experiment, we use Shannon Entropy. Shannon entropy has several basic properties, including (1) uniform distributions have maximum uncertainty, (2) uncertainty is additive for independent events, and (3) adding an outcome with zero probability has no effect, and (4) events with a certain outcome have zero effect [6, 7]. Considering class predictions as outcomes, we can measure Shannon entropy of the predicted class probabilities. 
Higher values of entropy indicate greater uncertainty in the probability distribution [1]. In each active learning step, for every unlabelled example in the training set, the active learning algorithm computes the entropy over the predicted class probabilities, and selects the example with the highest entropy. The example with the highest entropy is the example for which the classifier is least certain about its class membership. 
The largest margin uncertainty is a best-versus-worst uncertainty comparison [5]. The largest margin uncertainty (LMU) is the classification probability of the most likely class minus the classification probability of the least likely class. The intuition behind this metric is that if the probability of the most likely class is significantly greater than the probability of the least likely class, then the classifier is more certain about the example’s class membership. Likewise, if the probability of the most likely class is not much greater than the probability of the least likely class, then the classifier is less certain about the example’s class membership. The active learning algorithm will select the example with the minimum LMU value. 
The algorithm below is one for pool-based active learning [8]. Stream-based active learning can be similarly written. 
A principle bottleneck in large-scale classification tasks is the large number of training examples needed for training a classifier. Using active learning, we can reduce the number of training examples needed to teach a classifier by strategically selecting particular examples. 
[1] A. J. Joshi, F. Porikli and N. Papanikolopoulos, “Multi-class active learning for image classification,” 2009 IEEE Conference on Computer Vision and Pattern Recognition, Miami, FL, 2009, pp. 2372–2379. 
[2] Guo-Jun Qi, Xian-Sheng Hua, Yong Rui, Jinhui Tang and Hong-Jiang Zhang, “Two-Dimensional Active Learning for image classification,” 2008 IEEE Conference on Computer Vision and Pattern Recognition, Anchorage, AK, 2008, pp. 1–8. 
[3] E. Y. Chang, S. Tong, K. Goh, and C. Chang. “Support vector machine concept-dependent active learning for image retrieval,” IEEE Transaction on Multimedia, 2005. 
[4] A. Kapoor, K. Grauman, R. Urtasun and T. Darrell, “Active Learning with Gaussian Processes for Object Categorization,” 2007 IEEE 11th International Conference on Computer Vision, Rio de Janeiro, 2007, pp. 1–8. 
[5] https://becominghuman.ai/accelerate-machine-learning-with-active-learning-96cea4b72fdb 
[6] https://towardsdatascience.com/entropy-is-a-measure-of-uncertainty-e2c000301c2c 
[7] L. M. Tiwari, S. Agrawal, S. Kapoor and A. Chauhan, “Entropy as a measure of uncertainty in queueing system,” 2011 National Postgraduate Conference, Kuala Lumpur, 2011, pp. 1–4. 
[8] https://towardsdatascience.com/active-learning-tutorial-57c3398e34d 
Hands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look 
Written by 
Written by",Michelle Zhao,2020-03-18T23:01:07.880Z
How to Build a Recommendation System for Purchase Data (Step-by-Step) | by Moorissa Tjokro | Data Driven Investor | Medium,"Whether you are responsible for user experience and product strategy in a customer centric company, or sitting in your couch watching movies with loved ones, chances are you are already aware of some ways that recommendation technology is used to personalize your content and offers. 
Recommendation systems are one of the most common, easily comprehendible applications of big data and machine learning. Among the most known applications are Amazon’s recommendation engine that provides us with a personalized webpage when we visit the site, and Spotify’s recommendation list of songs when we listen using their app. 
Last time, we got to build a Spotify’s Discover Weekly with a bulk of audio data using Spark. This time, we’ll build a recommendation engine for more tangible items. 
If you look up online, there are many ways to build recommendation systems for rating-based data, such as movies and songs. The problem with rating-based models is that they couldn’t be standardized easily for data with non-scaled target values, such as purchase or frequency data. For example, ratings are usually from 0–5 or 0–10 across songs and movies. However, purchase data is continuous and without an upper bound. 
A lot of online resources unfortunately provide results without evaluating their models. For most data scientists and engineers, this is a dangerous area when you’re involving millions of data! For industries, results alone won’t get your tools anywhere without any evaluation. 
In solving these problems, we will build collaborative filtering models for recommending products to customers using purchase data. In particular, we’ll cover in details the step-by-step process in constructing a recommendation system with Python and machine learning module Turicreate. These steps include: 
Imagine a grocery chain releases a new mobile app allowing its customers to place orders before they even have to walk into the store. 
There is an opportunity for the app to show recommendations: When a customer first taps on the “order” page, we may recommend top 10 items to be added to their basket, e.g. disposable utensils, fresh meat, chips, and and so on. 
The tool will also be able to search for a recommendation list based on a specified user, such that: 
Two datasets in .csv format are used below, which can be found in data folder here: 
Our goal here is to break down each list of items in the products column into rows and count the number of products bought by a user 
The above steps can be combined to a function defined below: 
In this step, we have normalized the their purchase history, from 0–1 (with 1 being the most number of purchase for an item and 0 being 0 purchase count for that item). 
Let’s define a splitting function below. 
Now that we have three datasets with purchase counts, purchase dummy, and scaled purchase counts, we would like to split each for modeling. 
Before running a more complicated approach such as collaborative filtering, we should run a baseline model to compare and evaluate models. Since baseline typically uses a very simple approach, techniques used beyond this approach should be chosen if they show relatively better accuracy and complexity. In this case, we will be using popularity model. 
A more complicated but common approach to predict purchase items is collaborative filtering. I will discuss more about the popularity model and collaborative filtering in the later section. For now, let’s first define our variables to use in the models: 
Turicreate has made it super easy for us to call a modeling technique, so let’s define our function for all models as follows: 
While I wrote python scripts for all the above process including finding similarity using python scripts (which can be found here, we use turicreate library for now to capture different measures faster and evaluate models. 
i. Using purchase count 
ii. Using purchase dummy 
iii. Using scaled purchase count 
In collaborative filtering, we would recommend items based on how similar users purchase items. For instance, if customer 1 and customer 2 bought similar items, e.g. 1 bought X, Y, Z and 2 bought X, Y, we would recommend an item Z to customer 2. 
To define similarity across users, we use the following steps: 
1. Create a user-item matrix, where index values represent unique customer IDs and column values represent unique product IDs 
2. Create an item-to-item similarity matrix. The idea is to calculate how similar a product is to another product. There are a number of ways of calculating this. In steps 7.2 and 7.3, we use cosine or pearson similarity measure, respectively. 
3. For each customer, we then predict his likelihood to buy a product (or his purchase counts) for products that he had not bought. 
i. Using purchase count 
ii. Using purchase dummy 
iii. Using scaled purchase count 
i. Using purchase count 
ii. Using purchase dummy 
iii. Using scaled purchase count 
For evaluating recommendation engines, we can use the concept of RMSE and precision-recall. 
i. RMSE (Root Mean Squared Errors) 
ii. Recall 
iii. Precision 
Why are both recall and precision important? 
Let’s first create initial callable variables for model evaluation: 
Lets compare all the models we have built based on RMSE and precision-recall characteristics: 
Therefore, we select the Cosine similarity on Purchase Dummy approach as our final model. 
Finally, we would like to manipulate format for recommendation output to one we can export to csv, and also a function that will return recommendation list given a customer ID. 
We need to first rerun the model using the whole dataset, as we came to a final model using train data and evaluated with test set. 
Here we want to manipulate our result to a csv output. Let’s see what we have: 
Let’s define a function to create a desired output: 
Lets print the output below and setprint_csv to true, this way we could literally print out our output file in csv, which you can also find it here. 
Let’s define a function that will return recommendation list given a customer ID: 
Bingo! 
In this article, we were able to traverse a step-by-step process for making recommendations to customers. We used Collaborative Filtering approaches with Cosine and Pearson measure and compared the models with our baseline popularity model. 
We also prepared three sets of data that include regular buying count, buying dummy, as well as normalized purchase frequency as our target variable. Using RMSE, precision and recall, we evaluated our models and observed the impact of personalization. Finally, we selected the Cosine approach using dummy data as our best recommendation system model. 
Hope you enjoy reading this article and are now ready to create your own “add-to-cart” button. Please give 50 claps and comment down below if you want more reads like this :) Enjoy hacking! 
Moorissa is a mission-driven data scientist and social enterprise enthusiast. In December 2017, she graduated from Columbia University with a study in data science and machine learning. She hopes to always leverage her skills for making the world a better place, one day at a time. 
In each issue we share the best stories from the Data-Driven Investor's expert community. Take a look 
Written by 
Written by",Moorissa Tjokro,2018-10-15T23:58:14.919Z
A collection of must known resources for every Natural Language Processing (NLP) practitioner | by Nikhil Jaiswal | Towards Data Science,"Hey, are you a fresher wondering to dive into the world of NLP or a regular NLP practitioner who is confused with the vast amount of information available on the web and don’t know where to start from? Relax, I was the same until I decided to spend a good amount of time gathering all the required resources at one single place. 
After thorough readings from multiple sources since last one year, here are my compiled versions of the best sources of learnings which can help anyone to start their journey into the fascinating world of NLP. There are a variety of tasks which comes under the broader area of NLP such as Machine Translation, Question Answering, Text Summarization, Dialogue Systems, Speech Recognition, etc. However to work in any of these fields, the underlying must known pre-requisite knowledge is the same which I am going to discuss briefly in this blog. (Note: If any of the links have expired, please do let me know in the comments section.) 
Just a quick disclaimer about the contents:1. The contents which I am going to discuss mostly belongs to modern NLP and not that of the classical NLP techniques.2. It’s impossible for anyone to go through all the available resources. I have applied my uttermost effort in whatever way I could do. 3. I assume that the reader is comfortable with at least a decent amount of knowledge regarding Machine Learning (ML) and Deep Learning(DL) algorithms.4. For all the topics that I am going to cover, I have mainly cited the best resources in terms of blogs or videos. Readers can easily find the research papers for each individual topics. I feel the mentioned blogs are more than sufficient for anyone to fully understand the respective topics. 
Here is my roadmap to the NLP world:1. Word Embeddings — Word2Vec, GloVe, FastText2. Language Models & RNN3. Contextual Word Embeddings — ELMo4. Transfer Learning in NLP — ULMFiT5. Sentence Embeddings 6. Seq2Seq & Attention Mechanism7. Transformers 8. OpenAI GPT & BERT9. GPT-2, XLNet 10. Summary 
Let’s briefly summarize the above 10 topics: 
Well the first point that comes into mind when we start studying NLP is that how can we represent the words into numbers so that any ML or DL algorithm can be applied to it. That’s where the word vectors/embeddings come into play. As the name suggests, the aim here is to take as input any given word and outputs a meaningful vector representation that characterizes this word.There exist different approaches to obtain this representation based on the underlying techniques such as Word2Vec, GloVe, FastText 
To begin with this topic, I would suggest the reader watch lecture 1 & 2 of Stanford CS224N: NLP with Deep Learning | Winter 2019 freely available on YouTube. 
https://www.youtube.com/watch?v=8rXD5-xhemo&list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&index=1 
These two lectures form a solid background regarding semantic word representation. Apart from that, you also get to know the detailed mathematics involved in the working of both Word2Vec and GloVe model. Once you are comfortable with this, I would like to refer you to some of the blogs that I found most useful on this topic. In these blogs, you can find some of the examples and visualization that helps you gain a better understanding. 
http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/http://jalammar.github.io/illustrated-word2vec/ 
I hope these readings are more than sufficient to give you a solid understanding of Word2Vec. Let’s move ahead. 
GloVe was much better explained in lecture 3 of Stanford Natural Language Processing with Deep Learning (Winter 2017) 
https://www.youtube.com/watch?v=ASn7ExxLZws 
Apart from this, the following blogs help you obtain a clear picture of the topic and the mathematics behind this. 
[slideshare id=229369562&doc=paperdissectedgloveglobalvectorsforwordrepresentationexplainedmachinelearningexplained-200228052056&type=d] 
[slideshare id=229369559&doc=emnlpwhatisgloveparti-towardsdatascience-200228052054&type=d] 
[slideshare id=229369555&doc=emnlpwhatisglovepartii-towardsdatascience-200228052050&type=d] 
[slideshare id=229369551&doc=emnlpwhatisglovepartiii-towardsdatascience-200228052047&type=d] 
I hope you must have till now understood how GloVe takes advantage of the global statistics information unlike Word2Vec and optimizes a completely different objective. 
FastText is a library created by Facebook Research Team for efficient learning of word representations and sentence classification. It supports training CBOW or Skip Gram models similar to that of Word2Vec but it operates on n-gram representation of a word. By doing so, it helps in finding vector representation of the rare words by making use of character-level information. 
Kindly refer following links for a better understanding: 
https://towardsdatascience.com/fasttext-under-the-hood-11efc57b2b3https://arxiv.org/pdf/1607.04606v1.pdfhttps://arxiv.org/pdf/1607.01759.pdf 
If you are done with the above-mentioned pointers, you must be now at least having a deeper understanding of the word embeddings approach. It’s time to move into the backbone of NLP — Language Models. 
Language models are what we use on a daily basis. One such scenario occurs while texting a message be it on your cellphone or Gmail or LinkedIn. LM provides you with the most probable suggestions that you would like to type further. In simple words, LM is a task of predicting what word comes next. And my argument about LM being the backbone of NLP is because all the current state of the art transfer learning models depends on LM as the underlying task. You will get to know further about these in your upcoming journey. But before that, let’s look at the resources to understand LM. 
As usual, my first go-to suggestion over here is to go through some of the wonderful lectures from Stanford on this particular topic.Lecture 6 of CS224N covers this topic beautifully. It gives you a glimpse of how LM was developed prior to neural networks and what advantages does neural networks basically RNN brings to this. Also, if you would like to re brush your knowledge regarding RNNs, kindly refer lecture 7 for the same. 
https://www.youtube.com/watch?v=iWea12EAu6U&list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&index=6https://www.youtube.com/watch?v=QEw0qEa0E50&list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&index=7 
Also, if you feel less knowledgeable about the inner working of RNNs, you can go for a wonderful course that I studied on Udemy- 
Deep Learning: Advanced NLP and RNNs https://www.udemy.com/course/deep-learning-advanced-nlp/ 
This is one of the best courses that I found useful in the vast collection of online courses available on the web. In this course, you can understand the working of RNNs by unrolling them, turning them to bidirectional etc. Also, you can learn to code these models in Keras — one of the simplest deep learning framework to get started with. 
Guess what word embeddings are back !! Wait for a second, there is a term ‘contextual’ which makes it different from our previous studied approaches. OK, then why did not we study this topic together with the first topic. Hmmm, only because we need the knowledge of LM to understand this topic. And yeah, as I had mentioned earlier, we have come across the first application of LM in our ongoing journey. Trust me by the end, you will agree with me with awarding LM as the backbone of NLP. Enough said, let’s jump into our current topic — Contextual Word Embeddings 
Embeddings from Language Models (ELMo) uses LM to obtain embeddings of individual words. Until now we were having a single embedding of any input word, for example, say a bank. Now suppose I have 2 different sentences — I went to withdraw money from the bank and I was standing near the bank of a river. In both these sentences, the meaning of the word bank are completely different and therefore they must be having different vector representation. This is what contextual embeddings aim at. ELMo is one such approach based on the multi-layer bidirectional LSTM models for obtaining contextual word embeddings. Please go through following blogs to learn about them. 
https://mlexplained.com/2018/06/15/paper-dissected-deep-contextualizedword-representations-explained/https://www.slideshare.net/shuntaroy/a-review-of-deep-contextualized-word-representations-peters-2018 
I hope that the above two resources are sufficient enough to help you get a better understanding of ELMo. It’s time to move ahead … 
Transfer Learning has completely revolutionized NLP domain in the last one year. Most of the current state of the art algorithms that are being developed makes use of this technique. After making a significant contribution to the field of Computer Vision, Transfer Learning has finally rejoiced NLP practitioners. Universal Language Model Fine-tuning for Text Classification (ULMFiT) is one such approach that should be credited for this wonderful change.ULMFiT introduced methods to effectively utilize a lot of what the model learns during pre-training — more than just embeddings and more thancontextualized embeddings. ULMFiT introduced a language model and a process to effectively fine-tune that language model for various tasks.Finally, pre-training and fine-tuning concepts started showing its magic power in the NLP field. ULMFiT paper also introduced different techniques such as Discriminative Fine Tuning and Slanted Triangular Learning rates that helped in improving the way the transfer learning approach could be utilized. 
Ready to explore these exciting terms, then keep calm and refer the following blogs: 
http://nlp.fast.ai/classification/2018/05/15/introducing-ulmfit.htmlhttps://ahmedhanibrahim.wordpress.com/2019/07/01/a-study-on-cove-context2vec-elmo-ulmfit-and-bert/https://yashuseth.blog/2018/09/12/awd-lstm-explanation-understanding-language-model/ 
Well by now, you must be quite familiar with ULMFiT. Next in our journey comes the Sentence Embedding. 
Learnt enough about the word embeddings. What about the sentence? Can we obtain some representation of a sentence similar to that of a word? One very naive but a strong baseline approach would be to average the sentence’s word vectors (so-called Bag-of-Word approach). Apart from this, there can be different approaches based on unsupervised, supervised and multi-task learning set up. 
Unsupervised schemes learn sentence embeddings as a byproduct of learning to predict a coherent succession of sentences. The main advantage over here is that you can get plenty of unsupervised data since the internet is full of text kinds of stuff. Skip Thought Vectors and Quick Thought Vectors are two such successful approaches which have been developed in the unsupervised setting.On the other hand, supervised learning requires a labelled dataset annotated for a given task. Accomplishing this task lets you learn a good sentence embedding. InferSent is one such interesting approach by Facebook Research Team. Now to resolve the conflict between unsupervised and supervised embeddings, multi-task learning set up comes into the picture. Several proposals for multi-task learning were published such as MILA/MSR’s General Purpose Sentence Representation, Google’s Universal Sentence Encoder etc. 
Excited to enter this world. Explore & explore the mentioned links: 
https://medium.com/huggingface/universal-word-sentence-embeddings-ce48ddc8fc3ahttps://ai.googleblog.com/2018/05/advances-in-semantic-textual-similarity.htmlhttps://towardsdatascience.com/deep-transfer-learning-for-natural-language-processing-text-classification-with-universal-1a2c69e5baa9 
Having learnt the variants of RNN Models as well as having a good understanding of word and sentence embeddings, it’s time to move ahead to an exciting NLP architecture known as Sequence 2 Sequence models(Seq2Seq). This architecture is used in a variety of NLP tasks such as Neural Machine Translation, Text Summarization, Conversational Systems, Image Captioning, etc. A sequence-to-sequence model is a model that takes a sequence of items (words, letters, features of an image …etc) and outputs another sequence of items. The best way to understand these models is with the help of visualization, and this is where I would like to refer you to one of my most loved NLP author’s blog. He is none other than Jay Alammar. Believe me, you would love to go through each of his blogs. The efforts he uses to explain these terms are outstanding. Click below link to enter into this beautiful world. 
http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/ 
I think I do need to explain you further regarding Seq2Seq, as by now you must be well conversed with it.However, now I would like to refer you again to the Stanford lectures to learn more about Statistical and Neural Machine Translation. Having knowledge of Seq2Seq would help you move fluently with these lectures. Also, Attention which is one of the most important topics is discussed in detail there. Together with that, you will also get to know about the Beam Search Decoding & BLEU metric used for evaluating NMT models. 
Kindly refer CS224N Lecture — 8 https://www.youtube.com/watch?v=XXtpJxZBa2c&list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&index=8 
It’s time for the beast — Transformers. While LSTM models were revolutionizing the NLP industry, it was Transformers that was developed out of the box as an improved replacement of the RNN models. The Transformer is a model that uses attention to boost the speed with which these models can be trained.The Transformer lends itself to parallelization. The Transformer was proposed in the paper Attention is All You Need. Due to the parallelization nature, it frees us from the recurrence connections involved in the RNN models. Not only it helps in reducing the training time, but also in improving the accuracy by a great margin on various NLP tasks. It is similar to Seq2Seq architecture but it depends only on the attention mechanism along with their variants. Again, the best blog to understand this topic is by Jay Alammar. In fact, as mentioned earlier you can follow all his blogs to learn about these advanced NLP techniques. 
http://jalammar.github.io/illustrated-transformer/ 
Apart from this if you want to understand this paper in terms of implementation point of view, then please refer this awesome annotated blog by Harvard NLP group. 
https://nlp.seas.harvard.edu/2018/04/03/attention.html 
If you have successfully understood the above two blogs, then give yourself a big thumbs up!! Believe me, it was not an easy task.Let’s explore now regarding how researchers have utilized this newer architecture to build State of the Art models like BERT, GPT-2, etc. 
Transfer learning is back but now of course with Transformers. It’s as simple as follows: utilize Transformer decoder’s stack to build a newer model called GPT or utilize the encoder part of the Transformer to build an amazing model named BERT.Believe me, even if you are very new to NLP field and have been just listening to NLP buzzwords in the last one year then BERT and GPT are the toppers in this list. 
Generative Pre-Training(GPT) goal is similar to that of ULMFit i.e. to apply transfer learning in NLP. But there is a major difference. Yeah, you got it right — using Transformer instead of LSTM. Apart from that, there are also some of the difference in the training objective which you can learn about after going through the below-mentioned blogs. To summarize, the overall idea of GPT is to train the transformer decoder for language modelling task also known as pre-training. Once it is pre-trained, we can start to use it for downstream tasks. There can be a number of input transformations to handle a variety of such tasks. 
Here comes the most buzzing word of NLP — BERT. 
The main objective was to build a transformer-based model whose language model was conditioned on both left as well as right context. This was the limitation of GPT since GPT only trains a forward language model. Now to achieve the objective of bidirectional conditioning, BERT made use of encoder part of the Transformer. And in order to not see the future words while calculating attention scores, it uses a special technique called masking. According to the authors of this paper, this masking technique was the greatest contribution of this paper. Apart from the masking objective to handle relationships between multiple sentences, the pre-training process includes an additional task: Given two sentences (A and B), is B likely to be the sentence that follows A, or not? 
Well, if you are feeling burdened by some of the above terms and want a piece of deeper knowledge about them, just be relaxed. All these terms are beautifully explained in the following blogs: 
http://jalammar.github.io/illustrated-bert/http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/https://mlexplained.com/2019/01/07/paper-dissected-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding-explained/https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/https://medium.com/@_init_/why-bert-has-3-embedding-layers-and-their-implementation-details-9c261108e28a 
GPT-2 was nothing but a successor to GPT with more than 10X the parameters and trained on more than 10X the amount of data. Due to the concerns about malicious applications of the technology, the authors initially did not release the larger trained model which became a debatable topic.XLNet is a generalized autoregressive model. It outperforms BERT on 20 tasks, often by a large margin. It is the new go-to technique for transfer learning in NLP. For a broader understanding of GPT-2 and XLNet, please refer the below blogs. 
http://jalammar.github.io/illustrated-gpt2/https://openai.com/blog/better-language-models/https://towardsdatascience.com/openai-gpt-2-understanding-language-generation-through-visualization-8252f683b2f8https://towardsdatascience.com/what-is-xlnet-and-why-it-outperformsbert-8d8fce710335 
Finally, you have covered the entire journey of learning NLP pre-requisite as per our proposed plan. Kudos to you !!! Models keep on involving due to a large number of researchers working actively in this field. And so, within almost every month, you come across a new paper which beats the previous state of the art. So, the only way to move ahead with this fast-changing world is to remain updated with the latest knowledge by going through the research papers and these informative blogs on a regular basis. 
If you want to recollect the entire journey, go through the following mentioned blog once. https://lilianweng.github.io/lil-log/2019/01/31/generalized-language-models.html 
Here, I am also listing below some of the best blogs which I find most useful while learning about new topics in the field of NLP: 
https://medium.com/huggingfacehttp://jalammar.github.io/https://ruder.io/https://mlexplained.com/https://mccormickml.com/ 
I hope my resources were helpful to the reader. As said earlier, it’s impossible for anyone to cover the entire topics. Suggestions are always welcome citing some of the better blogs or the important topics which I have missed. I hope anyone who is thorough with these pre-requisites can work in any of the NLP tasks. Till then, cheers, enjoy !!! 
Hands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look 
Written by 
Written by",Nikhil Jaiswal,2020-05-18T07:34:10.555Z
Natural Language Processing and Social Media | by Paldesk | Medium,"In this article, we will talk about natural language processing. We will define what it is, where we use programming languages, what is their contribution, the most popular applications of it, and their usage in different fields. In the end, we will discuss if NLP could ever replace human support. 
Natural Language Processing (NLP) is a subfield of Computer Science, Information Engineering and Artificial Intelligence concerned with the interactions between computers and human language. It focuses on processing and analyzing large amounts of natural language data. Also, it focuses on how to get computers closer to a human-level understanding of language. 
The main aim is to make the computer as intelligent as a human being in understanding the language. The developer can perform a task such as sentiment analysis, speech recognition, and relationship extraction. Challenges in natural language processing involve speech recognition, natural language understanding, and natural language generation. 
Certain programming languages help to program NLP. 
Some of them are R, Python, and Java. 
Programming language R is used for statistical learning. 
It understands and explores your data using statistical methods and graphs. R plays an important role in investigating big data, supporting the researcher, and also it is useful for intense learning analytics. It has an enormous number of natural language processing algorithms. 
Another one is Python, a high-level object-oriented scientific programming language. 
Python is easy to learn syntax readability and reduce the cost of maintenance. It contains a lot of packages using this you can do code reusability. 
Also, it can extract information from unstructured text, either to guess the topics and identity named entity. Using Python, parsing and semantic you can analyze language structure. 
The third one and the most popular programming language for Android Smartphone is Java. 
Java helps you to organize text using full-text search, clustering, tagging, and information extraction. It is a platform independent language because of this feature the processing of information becomes easy. 
Some of the applications of this new technique are as an enhancement for grammar checking software such as Grammarly or a writing platform like Twinword Writer. 
Another application is used to translate from one human language to another. Translating with computer help would cut down the time needed for translating documents. 
More demanding applications of NLP are chatbots and humanoid robots. Chatbots or intelligent agents answer all types of questions and inquiries from their users. Humanoid robots, such as gynoids, use NLP as their base of understanding and responding to different stimulants. 
The development of social media has revolutionized the amount and types of information available today to NLP researchers. 
Data available from social media such as Twitter, Facebook, YouTube, blogs, and discussion forums make it possible to find relations between demographic information, language use, and social interaction. 
Using statistical and ML techniques, researchers can learn to identify demographic information, language, track trending topics, and predict disease spreading. 
For instance, with Google Flu Trends it recognizes deception in fake reviews from symptoms mentioned in a tweet or food-related illnesses. 
Social media has changed the ways we can use big data. Product reviews can help to predict pricing trends and plan future advertising campaigns. Political forums help to predict success in elections. 
Equally important, social networks can be used to determine indicators of influence among different groups. Similarly, medical forums can contribute to the discovery of questions about patients suffering from a particular medical condition. 
What is more important, much of the work in NLP has focused on sentiment analysis. The sentimental analysis determines opinions, committed or uncommitted beliefs, emotions, positive or negative orientation. In the same way, it determines the connotation of textual language or speaking language. 
All mentioned is based on lexical and syntactic information. Positive and negative orientation signal by words sentimental attitudes. 
For example, words with a negative connotation are “sad,” “worried,” “difficult,” and “weak”. In contrast, “comfortable,” “important,” “successful,” and “interesting” convey a positive sentiment. 
Online sentiment dictionaries, such as Whissel’s Dictionary of Affect, can assess positive and negative sentiment in text. However, there are new practices for identifying emotions based on six basic ones. 
There has also been researching using features that can recognize classic emotions to identify speaker medical conditions. 
For instance, it can recognize autism and Parkinson’s disease. In addition to medical conditions, it can recognize speaker characteristics such as age, gender, likeability, pathology, and personality. 
Moreover, it can recognize the speaker’s conditions like cognitive load, drunkenness, sleepiness, interest, and trust. 
Recent advances in Machine Learning (ML) together with deep learning have enabled computers to do quite a lot of useful things with NP. 
Furthermore, it has enabled us to write programs to perform things like language translation, semantic understanding, and recognizing emotions. Although there is a disadvantage where computers don’t yet have the same intuitive understanding of natural language that humans do. 
Because of that, it’s impossible for them to “read between the lines”. That’s why it’s justifiable to doubt that they won’t be able to do a better job than humans. 
But, if we reconsider that humans also often are not able to “read between the lines”, can handle less information and are slower — NLP is a great opportunity to take over some humans responsibilities. 
Originally published at www.paldesk.com on February 19, 2019. 
Written by 
Written by",Paldesk,2019-02-26T14:12:42.642Z
I see you have extracted the keywords but what are the scores in the extract key word function? | by Shengyu Chen | Medium,"The other columns also need cleaning. Everything needs to be lowercase to avoid duplications, and I also decided to merge all first and l… 
I see you have extracted the keywords but what are the scores in the extract key word function? I have been trying to see if they have any documentation on that but I couldn’t find any. 
Written by 
Written by",Shengyu Chen,2019-02-09T03:09:25.288Z
"cyBERT. Neural network, that’s the tech; To… | by Bartley Richardson | RAPIDS AI | Medium","Authors: Rachel Allen, Bartley Richardson 
Since the dawn of time, humans have been struggling with and overcoming their problems with logs. Tools to fell trees that first built simple lean-to structures were inefficient for growing populations, and civilizations invented new ways to harvest logs, mill them, and erect larger and more complex buildings, outpacing traditional log cabins. Humans discovered new ways to use logs as fuel, combining them with a spark to maintain fires that provided warmth and power. It comes as no surprise that with the rise of computers and network communication, a different type of log became important and, unfortunately, more difficult to manage than ever before. 
Cybersecurity logs are generated across an organization and cover endpoints (e.g, computers, laptops, servers), network communications, and perimeter devices (e.g., VPN nodes, firewalls). Using a conservative estimate for a company of 1000 employee devices, a small organization can expect to generate over 100 GB/day in log traffic with a peak EPS (Event Per Second) of over 22,000¹. Some of these logs are generated by users and activity on the network, and other logs are generated by network and security appliances deployed throughout the environment. 
It began simple enough. Logs needed to be kept for important events. Bank transactions needed to be recorded for verification and auditing purposes. As communications systems became more complex, additional logs were kept to ensure systems were reliable and robust. The Internet ushered in a new age of communication, commerce, and information exchange. Valuable information was being passed, and we started to need logs to verify communications were authentic and permitted. As the price of storage fell, security professionals urged their organizations to collect more logs, to collect more data. And they were successful. 
Today, organizations collect, store, and (attempt to) analyze more data than ever before. Logs are heterogeneous in source, format, and time. In order to analyze the data, it first needs to be parsed. Actionable fields must be extracted from raw logs. Today, logs are parsed using complex heuristics and regular expressions. These heuristics are inflexible and prone to failure if a log deviates at all from its specific format. Consider the below situations. 
Put simply, there has to be a better way of parsing logs in a more flexible, resilient way. Let’s look at what’s available. Most organizations keep a large history of logs, and it’s straightforward to keep raw logs and parsed versions of those logs. Access to a lot of data examples sounds like something well-poised for deep learning — specifically, a deep neural network (DNN). But there are so many to choose from, so where do we start? 
There are many ways to process logs and cybersecurity data. In this case, we focus on parsing logs that are typically defined by humans to record data that captures machine-to-machine exchanges. Turning to a technique like Natural Language Processing (NLP) is worth exploring. NLP is traditionally used for applications such as text translation, interactive chatbots, and virtual assistants. The first step for modern NLP techniques is transforming text or speech into a mathematical representation. These representations can be as straight-forward as a look-up that converts characters to numbers, or they can be much more complex, like using the output from a previously trained neural-network (e.g. Word2vec, GloVe, BERT, GPT-2). These neural-network representations learn relationships between words in an unsupervised method based on their occurrences with other words in a very large training corpus, like all of english wikipedia. Machine learning models are then developed using these representations to achieve the desired output, such as clustering or classification. Previous² work³ shows that viewing cybersecurity data as a type of natural language can be successful. 
Given their functionality, there is no shortage of pre-trained word representations created for NLP. Older neural-network word representations like Word2vec are context-free. They create a single word-embedding for each word in the vocabulary and are unable to distinguish words with multiple meanings (e.g. the file on disk vs. single file line). More recent models (e.g., ULMFit and ELMo) have multiple representations for words based on context. They achieve this by using the word plus the previous words in the sentence to create the representations. 
BERT (Bidirectional Encoder Representations from Transformers) also creates contextual representations, but it takes into account the surrounding context in both directions — both before and after a word. Encoding this contextual information is important for understanding cyber logs because of their ordered nature. For example, across multiple log types a source address occurs before a destination address. An additional challenge of applying a natural language model to cyber logs is that many “words” in a cyber log are not English language words; they include things like file paths, hexadecimal values, and IP addresses. Other language models return an “out-of-dictionary” entry when faced with an unknown word, but BERT breaks down the words in our cyber logs into in-dictionary WordPieces. For example, ProcessID becomes two in-dictionary WordPieces — Process and ##ID. Additionally, BERT is an attractive model for our use case because it was open sourced by Google in late 2018, and the HuggingFace transformer library contains an easy to use pre-trained model implemented in PyTorch. The transformer library can easily add fine-tuning layers to the representation layers for our specific downstream classification task of Named Entity Recognition (NER). A final benefit of selecting the BERT model for cyber log parsing is that we can take advantage of the epic portmanteau — cyBERT. 
cyBERT is an ongoing experiment to train and optimize transformer networks for the task of flexibly and robustly parsing logs of heterogeneous cybersecurity data. It’s part of CLX (read our overview blog about CLX), a set of cyber-specific applications built using RAPIDS. Since BERT was designed for natural human language and more traditional NLP tasks like question answering, we have overcome several challenges in our implementation. Unlike the flexible sentence organization of human language, the rigid order of some cyber logs can cause our model to learn the absolute positions of the fields rather than their relative positions. Another challenge is that many of our logs exceed the maximum number of 512 tokens, also called WordPieces, that can be input as one sequence into BERT. Additionally, longer sequences are disproportionately expensive because the time of the attention mechanism of the network is quadratic to the sequence length. To achieve more robustness and flexibility, we fine-tuned our model on log pieces of varying lengths and starting positions. Before inference, we split the logs into overlapping pieces to accommodate the input size of the model; labeled logs are recombined in post-processing. Thus far, we’ve experimented with input sequences of varying lengths, training data sizes, numbers of log types, and the number of training epochs. 
For example, inference for a BERT model of 512 is 20.3ms. However, this does not tell the entire story. In order to parse a log with a WordPiece sequence size of 256, more than 2 parts must be fed into the model. This is to account for overlap between the log pieces. To achieve the same effect as parsing a log with one 512 length WordPiece sequence, it is necessary to run 3 sequences through a 256 WordPiece sequence model. Figure 1 illustrates the performance characteristics (lines) and timings (bars) of across various WordPiece sequence sizes when parsing an entire log. 
For large log sizes with an average number of tokens over 512, it makes sense to use the largest possible WordPiece size. This gives not only the fastest performance but also near top performance on all evaluation metrics. However, in the real-world, a Security Operations Center (SOC) may not actually approach these large amounts of tokens in their logs. In this case, a balance could be struck between the maximum number of tokens and performance criteria. 
Consider a WordPiece size of 64. While parsing an entire log of multiple sequences requires 15 sequences in our experiment (compared with a single sequence at 512), the time required increases by ~5ms. If logs are typically smaller though, inference time on a single sequence with 64 tokens is 18.9ms. Even with a reduced number of tokens, performance across all metrics is still high. What all of this means is that there isn’t a single off-the-shelf way to implement cyBERT that will work for every organization. Attention must be given to the type of logs and their general composition. Our code for cyBERT with the parameters that worked best for our data can be found in the CLX repo. 
Fine-tuning the pre-trained base BERT model to label the entries of cyber logs with their field names is quite powerful. We initially trained and tested our model on whole logs that were all small enough to fit in one input sequence and achieved a micro-F1 score of 0.9995. However, this model cannot parse logs larger than the maximum model input sequence, and its performance suffered when the logs from the same testing set were changed to have variable starting positions (micro-F1: 0.9634) or were cut into smaller pieces (micro-F1: 0.9456). To stop the model from learning the absolute positions of the fields, we moved to training on log pieces. This training results in similar accuracy to the fixed starting positions and performs well on log pieces of variable starting positions (micro-F1: 0.9938). 
We achieve the best results when we train our model on log pieces, and measure our testing accuracy by splitting each log before inference into overlapping log pieces, then recombining and taking the predictions from the middle half of each log piece. This allows the model to have the most context in both directions for inference. One of the most exciting features of cyBERT is its ability to parse log types outside the training set. When trained on just 1000 examples of each of nine different Windows event log types, it can accurately (micro-F1: 0.9645, see Figure 2) parse a never seen before Windows event log type. 
After an encouraging start with the high accuracy of the BERT base model our next steps work to make the cyBERT more robust and flexible. The current model is trained only on Windows event logs; we plan to collect a more diverse set of logs for training including additional Windows event logs and apache web logs. The language of cyber logs is not the same as the English language corpus the BERT tokenizer and neural-network were trained on. We believe our model will improve both speed and accuracy if we move to a custom tokenizer and representation trained from scratch on a large corpus of cyber logs. For example, the current BERT WordPiece tokenizer breaks down AccountDomain into A ##cco ##unt ##D ##oma ##in which we believe is more granular than the meaningful WordPieces of AccountDomain in the cyber log language. Our parser needs to move at network to speed to keep up with the high volume of generated logs. In the future we will move all preprocessing, tokenization, and post-processing to the GPU for faster parsing without the need to communicate back and forth with host memory. 
cyBERT is off to a promising start in the long standing battle of man versus logs. In this post, we’ve shown how interpreting synthetic cybersecurity logs as a natural language has the potential to render traditional, regex-based parsing mechanisms obsolete and introduce flexibility and resilience at a new level to typical log parsing architectures. Parsing logs efficiently and correctly is critical to any security operations center, and cyBERT allows users to accomplish this without the need to develop extensive regex libraries. Further, as we increase the speed of pre- and post-processing with cyBERT, the ability to replay archived logs through new parsers will be possible, allowing security analysts the ability to quickly extract new information from older logs as needed. We’re excited about the future of cyBERT and sharing our work with the larger cybersecurity community! 
Rachel Allen is a Senior InfoSec Data Scientist in the AI Infrastructure team at NVIDIA. Rachel’s focus at NVIDIA is the research and application of GPU-accelerated methods to help solve information security and cybersecurity challenges. Her primary research interests involve the application of NLP and Bayesian statistical modeling to cybersecurity challenges, with cyBERT being her latest contribution. Prior to joining NVIDIA, Rachel was a lead data scientist at Booz Allen Hamilton where she designed a variety of capabilities for advanced threat hunting and network defense with their commercial cybersecurity group, DarkLabs. She is a former fellow and instructor at The Data Incubator, a data science training program. Rachel holds a bachelor’s degree in cognitive science and a PhD in neuroscience from the University of Virginia. 
Bartley Richardson is an AI Infrastructure manager and Senior Data Scientist at NVIDIA. His focus at NVIDIA is the research and application of GPU-accelerated methods and GPU architectures that can help solve today’s information security and cybersecurity challenges. Prior to joining NVIDIA, Bartley was a technical lead and performer on multiple DARPA research projects, where he applied data science and machine learning algorithms at-scale to solve large cybersecurity problems. He was also the principal investigator of an Internet of Things research project which focused on applying machine and deep learning techniques to large amounts of IoT data. His primary research areas involve NLP and sequence-based methods applied to cyber network datasets as well as cross-domain applications of machine and deep learning solutions to tackle the growing number of cybersecurity threats. Bartley holds a PhD in Computer Science and Engineering from the University of Cincinnati with a focus on loosely- and un-structured query optimization. His BS is in Computer Engineering with a focus on software design and AI. 
Written by 
Written by",Bartley Richardson,2019-12-06T17:15:33.826Z
Cyber Risk Management. Digital Risk Management | by Mohammed Mahboubi | Medium,"Digital Risk Management 
The rationale behind IT is to make life easier and safer for the people (individuals and businesses). In the digital universe, we deal with data (model assets of the real world), softwares (define how to deal with the data and how to generate added value) and devices (offers capabilities to store, deliver and compute). So, to manage the inherent risk, we should manage the risk related to these three kinds of assets. 
Before to continue, I noted that usually people confuse between a threat, vulnerability, a risk, an exploit, a zero-day, and an attack. 
We can formulate the risk with the formula: Risk + Confidence = 100 % 
In the past, information security (InfoSec) programs and policies were designed to protect the IT asset (data, application, system or network) through the triad of confidentiality, integrity, and availability within the confines of an organization. 
But with the advent of nomadism and mobility InfoSec needed to adapt and evolve to meet new requirements entailed by that technological and behavioral shift. 
Nowadays, organizations are rarely self-contained and the price of inter-connectivity is exposure to attacks. 
So, we evolved from InfoSec to Cybersecurity… 
Digital transformation 
In less than a decade, the IT world shifted from a fortress approach; in which the data was almost confined within an organization premises to a nomad-based and mobile-based interaction with data; that data became accessible and disseminated across many locations. 
So, cybersecurity allows organizations to protect every connection as well as every relevant data wherever it is stored, transmitted, or processed. 
Hence, cybersecurity programs and policies develop upon traditional information security programs, but also include the following: 
Threat taxonomy 
Cyber risk management should address the three kinds of threats: 
The first two kinds of threats are managed using a compliance-based approach. 
The last kind of threats is managed using a scenario-based approach. 
Compliance-based approach is used to set the security baseline on which scenario-based approach will be developed using targeted and sophisticated risk scenarios. The aim of the latter approach is to assess the cyber risk. 
First of all, inventory of all the assets (on premises and on the cloud) should be performed to ensure a holistic view of the process. Then, behavioral baselines should be defined to define what a normal use of an asset looks like; this is necessary to ease anomalies detection. 
Now, we can proceed with the risk management process… 
Security compliance is attained by vetting the information system (IS) against cyber hygiene basic guidelines. 
Then, the information system is vetted against regulations and frameworks. 
As you may guess, security compliance is used to protect against attacks having simple or average level of sophistication while cyber risk assessment is used to protect against advanced attacks. 
As a rule of thumb, the more an attack is widespread, the less it is sophisticated. 
Going Further 
COVID-19 is a turning point for digital transformation. It was a momentum for teleworking. 
But, to leverage the best of technology, strong teleworking security policies have to be developed and enforced to secure the data in transit, in processing and in storage. 
Furthermore, monitoring, acquiring threat intelligence, documenting changes to the information system, conducting security impact analyses for each change and conducting security assessments should be considered on an ongoing basis. 
To ensure that the digital transformation will not become a cyber disaster, the boards need to initiate sustainable programs to manage the cyber risk. That risk can never be fully neutralized. Businesses need to assume that they will be cyber-attacked. 
For instance, FireEye’s M-Trends states that the median time for a business to discover they’ve been compromised was 146 days in 2015. The global median dwell time was 56 days in 2019. Fortunately, that number dropped over the years. 
But, there will be always a gap to detect an unknown attack. Hence, we need to adopt new approaches to reduce the dwell time. One promising approach is to implement behavioral-based methodologies to improve detection capabilities. 
Written by 
Written by",Mohammed Mahboubi,2020-07-23T17:22:20.376Z
Similarity Measures — Scoring Textual Articles | by Saif | Towards Data Science,"Many real-world applications make use of similarity measures to see how two objects are related together. We can use these measures in the applications involving Computer vision and Natural Language Processing, for example, to find and map similar documents. One important use case here for the business would be to match resumes with the Job Description saving a considerable amount of time for the recruiter. Another important use case would be to segment different customers for marketing campaigns using the K Means Clustering algorithm which also uses similarity measures. 
Similarities are usually positive ranging between 0 (No Similarity) and 1 (Complete Similarity). We will specifically discuss two important similarity metric namely euclidean and cosine along with the coding example to deal with Wikipedia articles. 
Do you remember Pythagoras Theorem?? Pythagoras Theorem is used to calculate the distance between two points as indicated in the figure below. 
In the figure, we have two data points (x1,y1) and (x2,y2) and we are interested in calculating the distance or closeness between these two points. To find the distance, we need to first go horizontally from x1 to x2 and then go vertically up from y1 to y2. This makes up a right-angled triangle. We are interested in calculating hypotenuse d which we can calculate easily using the Pythagoras theorem. 
where b is the base of the right-angled triangle and p is the perpendicular of the right-angled triangle. 
This completes our euclidean distance formula for two points in two-dimensional space. 
This defines the euclidean distance between two points in one, two, three or higher-dimensional space where n is the number of dimensions and x_k and y_k are components of x and y respectively. 
Python Function to define euclidean distance 
Here x and y are the two vectors. 
You can also use sklearn library to calculate the euclidean distance. This function is computationally more efficient. 
Greater the distance, lower the similarity between the two objects; Lower the distance, higher the similarity between the two objects. To convert this distance metric into the similarity metric, we can divide the distances of objects with the max distance, and then subtract it by 1 to score the similarity between 0 and 1. We will look at the example after discussing the cosine metric. 
This is another metric to find the similarity specifically for the documents. This metric is a measure of the angle between x and y as indicated in the diagram and is used when the magnitude of the vector does not matter. 
If the angle between v and w is 0 degree, then the cosine similarity =1 (Complete Similarity). 
Cosine Formula for dot Product: 
‖⋅‖ denotes vector length 
𝜃: Angle between 𝐯 and 𝐰. Here we are interested in measuring the similarity between v and w. 
If you want to know how to derive the equation for cosine formula for dot product, please refer to this page: https://proofwiki.org/wiki/Cosine_Formula_for_Dot_Product. This dot product formula is derived from the same Law of Cosine which you have studied in your schools. 
Let’s use the notation x and y instead of v and w. 
Example: 
These two vectors have low similarity explained by the value of 0.11. This value is close to 0 which means that the angle between x and y is close to 90 degrees. If the value would have been close to 1, then this would be very similar objects with an angle close to 0 degrees. 
Dividing x and y by their lengths normalizes its length to 1 making it what is known as Unit Vector. This tells that cosine similarity does not take into account the magnitude of x and y. When we need to take into account the magnitude, euclidean might be a better option. 
If we already have vectors with a length of 1, cosine similarity can be easily calculated using simple dot product. It is therefore recommended to normalize vectors first to have a unit length to reduce the computation time. 
Python Function to define Cosine Similarity 
The diagram summarizes both the Euclidean and Cosine Metric. 
Cosine looks at the angle between the two vectors ignoring magnitude while the Euclidean focuses on the straight line distance taking into account the magnitude of the vector. 
Text mining is one of those areas where we can make use of cosine similarity to map similar documents. We can also use it to rank documents with respect to the given vector of words, CV shortlisting is one of those use cases where we can make use of Cosine similarity. 
Let’s take the problem where we need to match similar documents, we first create the Document Term Matrix that contains the frequency of terms occurring in a collection of documents. We usually have documents of uneven lengths as in the case of Wikipedia articles. Let’s say the word math appeared more in Document 1 than it does in document 2, cosine similarity, in this case, would be a perfect choice as we are not concerned about the length of the document but the content it contains. If we were to consider the length, then the Euclidean might be a perfect option. 
Here we have just fetched contents from the 5 Wikipedia articles. 
Count Vectorizer just converts the collection of documents into the matrix of word counts. 
Lets now use the euclidean metric to find out which of our articles are similar. Here we have normalized these euclidean distances between 0 and 1 and then subtracted it by 1 to range it between 0 and 1. Hence, closer it is to 1, higher the similarity. 
How are these two articles(Data Mining Swimming) most similar? It does not make any sense. Lets now try Cosine Similarity. 
This is what makes sense to us. Data Mining is very close to Machine Learning. 
The Euclidean metric here seems to focus on length rather than the content while cosine concentrated on the content ignoring the magnitude. 
References 
[1] https://proofwiki.org/wiki/Cosine_Formula_for_Dot_Product?source=post_page-----e3dbd4e58660---------------------- 
[2] https://cmry.github.io/notes/euclidean-v-cosine?source=post_page-----e3dbd4e58660---------------------- 
Hands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look 
Written by 
Written by",Saif,2019-10-27T18:52:50.243Z
"Neural Networks Intuitions: 9. Distance Metric Learning | by Raghul Asokan | Aug, 2020 | Towards Data Science","Welcome back to my series Neural Networks Intuitions. In this ninth segment, we will be looking into deep distance metric learning, the motivation behind using it, wide range of methods proposed and its applications. 
Note: All techniques discussed in this article comes under Deep Metric Learning (DML) i.e distance metric learning using neural networks. 
Distance Metric Learning means learning a distance in a low dimensional space which is consistent with the notion of semantic similarity. (as given in [No Fuss Distance Metric Learning using Proxies]) 
What does the above statement mean w.r.t image domain? 
It means learning a distance in a low dimensional space(non-input space) such that similar images in the input space result in similar representation(low distance) and dissimilar images result in varied representation(high distance). 
Okay, this sounds exactly what a classifier does. Isn’t it? Yes. 
So how is this different from supervised image classification? Why different terminology? 
Metric learning addresses the problem of open-set setup in machine learning i.e generalize to new examples at test time. 
This is not possible by a feature-extractor followed by fully connected layer Classification network. 
Why? 
This is a very important question. The answer is as follows: 
And do not aim to minimize intra-class distance which results in the desirable discriminative features. 
This aspect of learning discriminative features is what metric learning achieves. 
Before looking into the most widely used methods in metric learning, let us look into its applications to make the problem statement more concrete and why a standard classification approach may not be suitable. 
Applications of metric learning are as follows: 
Great! 
Now let us see the prominent methods employed in metric learning: 
a. Siamese network with contrastive loss(pairs) 
b. Triple networks with triplet loss(triplets) 
c. Classification based methods. 
To produce embeddings which are close in euclidean space(assuming) for similar images and far apart for dissimilar images. 
The paper Dimensionality Reduction by Learning an Invariant Mapping solves the problem by taking two images as input and outputs whether the image pair is similar or not. 
Approach: 
So now we need a loss function(incorporating the euclidean distance) which is zero for similar pairs and one for dissimilar pairs. 
That’s exactly what the contrastive loss function does! 
Let (X1, X2) be the input image pair, Gw be the function mapping producing low dimensional representations(where w represents the parameters), Y be the ground truth label denoting similar or not and Dw represents the euclidean distance. 
Contrastive loss bear a resemblance to the traditional cross entropy loss. The first term in the loss function takes care of similar pairs(Y=0) such that Dw becomes 0. The second term takes care of dissimilar pairs(Y=1) such that Dw becomes at least m. 
Here m is the margin and m > 0. 
Why do we need to show dissimilar pairs during training? Why not simply minimize the loss function over a set of similar pairs? 
The authors answer that, “The contrastive term involving dissimilar pairs is crucial. Simply minimizing DW (X1, X2) over the set of all similar pairs will usually lead to a collapsed solution, since DW and the loss L could then be made zero by setting GW to a constant”. 
Now that we have understood what Contrastive Loss function is, what is a Siamsese Network then? 
This architecture where the same network(i.e sharing same set of parameters) is used for extracting low dimensional representations for both images in a pair is called the Siamese architecture. 
The paper FaceNet: A Unified Embedding for Face Recognition and Clustering takes a similar approach as contrastive loss — except instead of dealing with pairs at every step, it considers a triplet of images. 
A triplet consists of an anchor, positive(similar to anchor) and negative(dissimilar to anchor) image. 
Approach: 
Let f be the function mapping producing low dimensional representation, xa be the anchor image, xp be the positive image, xn be the negative image and α be the margin. 
Triplet loss ensures that the representation of an anchor image is closer to all images similar to it, than it is to any other negative image. 
What is a triplet network? 
The architecture where the same network(i.e sharing same set of parameters) is used for extracting low dimensional representations for all images in a triplet is called the Triplet architecture/network. 
So, how are the above problems tackled? 
By careful selection of training image pairs and triplets — offline or online and using a larger batch size. 
*Note: There are a wide range of techniques that helps in solving the sampling issue with contrastive and triplet methods which I hope to cover in my future articles :) 
The paper A Discriminative Feature Learning Approach for Deep Face Recognition solves the objective using the vanilla neural network classification by introducing a new loss called center loss in addition to the cross entropy loss i.e a joint supervision signal. 
This paper solves the problem mentioned at the start of the article — “Classifiers do not minimize intra-class distance but only maxmize inter-class distance resulting in linearly separable features but not discriminative features”. 
Center loss minimizes the distance between every class center and the class samples’ representation — which makes sure that representations of samples in the same class stay similar in addition to preserving inter-class distance(taken care by CE loss). 
Let x be the input sample and c be the class center for that sample. 
The distance between class center(embeddings) and the sample embeddings are computed for every iteration and the weights are updated. 
That’s all in this article on Deep Distance Metric Learning. I hope all of you got a good grasp on what the problem is and how it is being addressed through various interesting techniques :) 
Cheers! 
Hands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look 
Written by 
Written by",Raghul Asokan,2020-08-23T17:18:08.905Z
Topic Modeling. Optimizing for Human Interpretability | by Alyssa Wisdom | Square Corner Blog | Medium,"Heads up, we’ve moved! If you’d like to continue keeping up with the latest technical content from Square please visit us at our new home https://developer.squareup.com/blog 
Problem 
Have you ever skimmed through a large amount of text and wanted to quickly understand the general trends, topics, or themes that it contains? This ranges from that book you never finished for book club to parsing through user feedback and comments at work. Well, rest-assured you’re not alone and there are tools out there to help you accomplish this feat, such as topic modeling. 
Topic modeling is a type of unsupervised machine learning that makes use of clustering to find latent variables or hidden structures in your data. In other words, it’s an approach for finding topics in large amounts of text. Topic modeling is great for document clustering, information retrieval from unstructured text, and feature selection. 
Here at Square, we use topic modeling to parse through feedback provided by sellers in free-form text fields. One example is pictured below — a comment section for sellers to leave feedback about why they’ve decided to leave the product and how we can better serve them and other customers like them in the future. 
Although topic modeling is a powerful statistical method, it can be difficult to evaluate the effectiveness of a topic model or interpret the results. Two major questions data practitioners ask when using topic modeling are: 
1.) What is the best way to determine the number of topics (𝜅) in a topic model? 
2.) How do you evaluate and improve the interpretability of a model’s results? 
In this article, I’ll walk through a few techniques that can help you answer the above questions. 
Data & Methods 
When implementing a topic model, it’s important to clean your text data to ensure you get the most precise and meaningful results. This includes common data pre-processing techniques such as tokenization, removing stopwords, and stemming/lemmatization. These are ways to segment documents into their atomic elements of words, remove words that provide little to no meaning, and ensure that root words similar in meaning get their proper weightage of importance in the model. 
The next step after adequately cleaning and preparing your corpus is to construct a document-term matrix and train your model on it. This is where the first question of number of topics (𝜅) to choose arises since 𝜅 is a common model parameter for topic models. 
Number (𝜅) of Topics 
When determining how many topics to use, it’s important to consider both qualitative and quantitative factors. Qualitatively, you should have domain knowledge of the data you’re analyzing and be able to gauge a general ballpark of clusters your data will separate into. There should be enough topics to be able to distinguish between overarching themes in the text but not so many topics that they lose their interpretability. In the case of evaluating our sellers’ text responses, from a qualitative perspective, 10 topics seemed like a reasonable place to start because it gives enough room to capture actionable topics such as functionality and cost without getting to granular. 
From a quantitative perspective, some data practitioners use perplexity or predictive likelihood to help determine the optimal number of topics and then evaluate the model fit. Perplexity is calculated by taking the log likelihood of unseen text documents given the topics defined by a topic model. A good model will have a high likelihood and resultantly low perplexity. Usually you would plot these measures over a spectrum of topics and choose the topic that best optimizes for your measure of choice. But sometimes these metrics are not correlated with human interpretability of the model, which can be impractical in a business setting. 
Another quantitative solution you can use to evaluate a topic model that has better human interpretability is called topic coherence. Topic coherence looks at a set of words in generated topics and rates the interpretability of the topics. There are a number of measures that calculate coherence in various ways, but Cv proves to be the measure most aligned with human interpretability (see Figure 2 below). 
Because Cv is generally the most interpretable, I used this coherence measure to refine my qualitative selection of 10 topics for our sellers’ text responses using Latent Dirichlet Allocation (LDA). 
LDA is a generative statistical topic model used to find accurate sets of topics within a given document set. The model assumes that text documents are comprised of a mix of topics and each topic is comprised of a mix of words. From there, using probability distributions the model can determine which topics are in a given document and which words are in a given topic based on word prevalence across topics and topic prevalence across document. Looking at the topic coherence across a number of topics ranging from 0 to 15, seven was the optimal number of topics to use for this model to maximize topic coherence. 
Evaluate & Improve Interpretability 
Once you choose the optimal number of topics, the next question to tackle is how to best evaluate and improve the interpretability of those topics. One approach is to visualize the results of your topic model to ensure that they make sense for your use case. You can use the pyLDAvis tool to visualize the fit of your LDA model across topics and their top words. If the top words for each topic are not coherent enough to pass a word intrusion test ( i.e., if you cannot identify a spurious or fake word inserted into a topic), then your topic model can still benefit from interpretability improvements. 
Another approach is to look at the topic coherence across different models. For my analysis, I looked at how LDA performed in comparison to other topic models such as Latent Semantic Indexing (LSI), which gives you ranking order of topics using singular value decomposition, and Hierarchical Dirichlet Process (HDP), which uses posterior inference to determine the number of topics for you. Overall LDA performed better than LSI but lower than HDP on topic coherence scores. However, upon further inspection of the 20 topics the HDP model selected, some of the topics, while coherent, were too granular to derive generalizable meaning from for the use case at hand. 
Conclusion 
Overall, when choosing the number of topics and evaluating the interpretability of your topic model, it is important to look at both qualitative and quantitative factors. It’s also important to balance interpretability with other quantitative metrics, such as accuracy and perplexity, that help you gauge the model’s performance. 
This also holds true for how you evaluate the results of the model. In our use case of evaluating our sellers’ text responses, it’s quantitatively useful to have topic categories to understand general trends in how our sellers’ needs are growing and how we as a company can evolve to meet those needs. It’s also qualitatively useful to drill into those topics and understand the nuances of each of our sellers’ individual requests. 
No matter how you choose to use topic modeling, keeping these tips in mind will help you to best optimize your model and get results that are valid, useful and applicable for whatever your business needs may be. 
Written by 
Written by",Alyssa Wisdom,2019-04-18T22:29:34.971Z
"Cheat Sheet for AWS ML Specialty Certification | by Madikanti | The Startup | Aug, 2020 | Medium","a highly important and carefully crafted piece, * this will only be useful after completing the entire course on Udemy 
Recall is an important metric in situations where classifications are highly imbalanced, and the positive case is rare. Accuracy tends to be misleading in these cases. 
can’t be done out of the box 
End. 
Written by 
Written by",Madikanti,2020-09-23T14:27:42.767Z
Hidden Markov Models — Part 1: the Likelihood Problem | by Maria Burlando | Medium,"The traditional definition of HMM comes from Wikipedia’s unfailing support when it comes to searching a new topic: 
Hidden Markov Model (HMM) is a statistical Markov model in which the system being modeled is assumed to be a Markov process with unobserved (i.e. hidden) states. 
And again, the definition for a Markov model: 
In probability theory, a Markov model is a stochastic model used to model randomly changing systems. It is assumed that future states depend only on the current state, not on the events that occurred before it (that is, it assumes the Markov property). 
Hidden Markov Models application include reinforcement learning and temporal pattern recognition such as speech, handwriting, gesture recognition, part-of-speech tagging, musical score following, partial discharges and bioinformatics. 
Now that we’ve got the fancy descriptions out of the way, we’re going to be looking at a rather simplified version of an HMM, so that our brains don’t have to start frying. 
Let’s take Lea; Lea is our imaginary friend and during the day she does either of these four things: 
Not much of a life, is it? But anyway, this is a list of what are called observables. Now, let’s suppose that in four days Lea does the following: painting, cleaning, shopping, biking. 
From this observation sequence we want to know whether the day has been sunny or rainy. These two are going to be our hidden states. 
Here is a graph of our potential HMM: 
To start off, a Hidden Markov Model consists of the following properties: 
Under more mathematical terms, we would describe this model’s properties as such: 
This is all very nice, but immediately we are faced with three problems: 
In this first tutorial we’re going to analyse the first problem, which asks the likelihood of a certain observation sequence deriving from the HMM model that we have initialized. 
Let’s take the initial example about Lea’s activity in four days. The observation sequence is as follows: Paint, Clean, Shop and Bike. 
So, what is the likelihood that this observation sequence O can derive from our HMM λ? 
P(O|λ) = ??? 
There are two methods with which we can calculate this: the Forward Algorithm and the Backward Algorithm. 
The Forward algorithm comprises of three steps: 
The above equation means that the first forward variable is calculated by multiplying the initial probability of state i by the emission probability b of that state given the observable O at time 1. 
As can be seen the initial forward variable of the Sunny state is the initial probability of Sunny, 0.6, times the emission probability from Sunny to the observable Paint, 0.4. Hence, 0.24. 
While the initial forward variable of the Rainy state is the initial probability of Rainy, 0.4, times the emission probability from Rainy to the observable Paint, 0.3. Hence, 0.12. 
For t = 1, 2, …, T-1 we make use of the recursion equation which defines the forward variable of state j as the product of the previous forward variable of state i, multiplied by the transition probability a between the previous state i to state j, multiplied by the emission probability b from state j to the observable O. 
Mind frying, I know, but let’s have a look at the diagram below: 
Here we are calculating the forward variable of state Sunny at time 2 by summing the results of two multiplications: 
Then, according to the equation above, we sum these results and get our forward variable. 
α = 0.0192 +0.0048 = 0.024 
Similarly, for the next step we’ll have a forward variable of 0.054 for the Rainy state: 
And so on until we have all the forward variables: 
This final equation tells us that to find the probability of an observation sequence O deriving from an HMM model λ, we need to sum up all the forward variables at time T, i.e. all the variables of every state at the end of the observation sequence. Hence, in our example above, 
P(O|λ) = 0.0028512 + 0.0003048 = 0.003156 
Usually, to find the solution to the Likelihood problem we don’t really require to know the Backward Algorithm. However, its explanation and resolution is a good litmus test to show that the Forward algorithm works proficiently, and moreover, by understanding it now, we can be ready for when the time will come to use it for the resolution of the third problem of Learning. 
The Backward Algorithm is similar to the Forward algorithm, but like the name suggests it goes backward in time. There’s again an Initialization, a Recursion and a Termination. 
What this simple equation is telling us is that at time T (at the end of the observation sequence) the backward variables of every state is equal to 1. Simple as that. 
The equation above tells us to sum up all the multiplications of the transition probability from the current State i to the next State j, times the emission probability of the observable O at time t+1 from the next State j, times the backward variable of the next State j at time t+1. 
It might not look extremely clear, but let’s analyze the diagram below. 
We’re going back in time. At time T where the final observable in the observation sequence was Bike, the backward variables have been initialized as equal to 1. 
By going backwards we’re going to calculate the backward variable of the previous Sunny state. This consists of summing up two multiplications: 
Then, according to the equation above, we sum these results and get our backward variable. 
β = 0.24 +0.01 = 0.25 
Similarly for the Rainy state: 
Like before, this is what we’re going to sum up: 
β = 0.12 +0.03 = 0.15 
And so on until we have all the backward variables: 
What this equation wants us to do is to sum the multiplications of the initial probability π of state i, times the emission probability b of the observable O at time t=1 from state i, times the backward variable at time t=1 of state i. 
Let’s clarify this in the diagram below: 
As we can see, I have highlighted the probabilities we’re going to need to calculate our final probability. 
As per equation we’re going to sum the following multiplications: 
P(O|λ) = 0.001704 + 0.001452 = 0.003156 
And as we can see, the result is the same as that of the Forward Algorithm. 
To finalize what we’ve learned we can say that the probability that Lea has spent the last four days painting, cleaning, shopping and biking, is 0.003156 given the HMM model we have built for her. 
I know that these calculations are pretty hard to do by hand, and that is why I’m also offering you to play around the Lea example in JavaScript. I have created an npm package called mary-markov that solves all the problems of an HMM using the algorithms required. 
You can download the npm package here, and see how I wrote the JavaScript code here. 
Once you have installed it using npm install --save mary-markov you can write the following code in your index.js file and run it. This will solve you the Likelihood problem by calculating the Forward and Backward Algorithms. 
If you’ve managed to read up to this point, well done! This was certainly a hard one. 
Unfortunately, it’s only the beginning of the HMM saga. In the next article we’re going to be looking at the Decoding problem of HMMs and how to solve it using the Viterbi Algorithm. 
So, stick around, fellas! 
Written by 
Written by",Maria Burlando,2019-01-03T13:49:00.489Z
An intro to topic models for text analysis | by Patrick van Kessel | Pew Research Center: Decoded | Medium,"(Related posts: Making sense of topic models, Overcoming the limitations of topic models with a semi-supervised approach and Interpreting and validating topic models) 
If you’ve ever had to analyze a set of documents — such as social media posts, open-ended survey responses, focus group transcripts, software logs, etc. — you may have wondered about natural language processing (NLP). NLP is an umbrella term that encompasses a wide variety of algorithmic methods that can be used to automatically analyze large volumes of text. 
Some of these methods are “supervised,” which means that they require researchers to first classify a sample set of documents manually. Algorithms can then use these manual classifications to learn word associations well enough to apply the same system to a larger collection of documents. Supervised learning algorithms can save researchers a lot of time, but they still involve extensive and careful reading of documents and detailed categorization. 
Other NLP methods are “unsupervised,” which means they don’t require training data classified by people. Instead, these algorithms look at how words are used in the documents you’re examining. They pick up on patterns and provide an estimate of what the documents convey with little direct supervision from the researcher. One of the most popular forms of unsupervised learning for text analysis is topic models, which I’ll be addressing here and in future posts. 
A topic model is a type of algorithm that scans a set of documents (known in the NLP field as a corpus), examines how words and phrases co-occur in them, and automatically “learns” groups or clusters of words that best characterize those documents. These sets of words often appear to represent a coherent theme or topic. 
There are a variety of commonly used topic modeling algorithms — including non-negative matrix factorization, Latent Dirichlet Allocation (LDA), and Structural Topic Models. Ongoing research has produced other variants as well. At Pew Research Center, we’ve been testing some of these algorithms and exploring how topic models might be able to help us analyze large collections of text, such as open-ended survey responses and social media posts. Here, I’ll be exploring a dataset consisting of open-ended responses to a survey about what makes life feel fulfilling. All we need to train the model is a spreadsheet with a single column of text, one response (i.e. document) per row. 
In one way or another, every topic modeling algorithm starts with the assumption that your documents consist of a fixed number of topics. The model then assesses the underlying structure of the words within your data and attempts to find the groups of words that best “fit” your corpus based on that constraint. At the end, you’re left with two output tables: the term-topic matrix, which breaks topics down in terms of their word components, and the document-topic matrix, which describes documents in terms of their topics. Depending on the specific algorithm that you use, a word may be assigned to multiple topics in varying proportions, or assigned to a single topic exclusively. 
Let’s consider the below example of a term-topic matrix, which is based on topics from a model trained on our open-ended responses. (The topics are real but for simplicity the numbers and documents here are illustrative and not based on actual data.) The first column contains an arbitrary identifier for each topic so we can refer to each topic by a name, followed by a column for every possible word that each topic could contain (known as our “vocabulary”). The values in the cells indicate how much each word “belongs” to each topic. Their exact meaning will depend on the specific algorithm used, but usually the most common value in this table will be zero (or close to it), since only a fraction of our vocabulary will be at all relevant to any particular topic. Since these tables can become enormous, the example below shows just three topics and a few of their top words. 
Since this raw output can be difficult to read and interpret, researchers often sort through the words for each topic and pick out the top words based on some measure of importance. For models that assign words to topics proportionally, we can look at the words that have the highest weights for each topic — that is, those that make up the greatest share of the topic. We can also use other metrics, like mutual information, to compare the words in each topic against all of the other topics, and pick out the words that are most distinctive. Using one of these sorting methods, we can select the five or 10 most important words for each topic, making it easier for us to view and interpret them: 
If our goal is simply to get a better sense of what’s in our documents, then we may have already accomplished our mission. In cases where we have a large collection of text but don’t really know the nature of its contents, topic models can help us get a glimpse inside and identify the main themes in our corpus. Topic 4 shows us that some of our documents have something to do with the pursuit of hobbies and passions for fun. Topic 5 seems to have something to do with charity and giving back to society, and Topic 88 touches on a more general notion of spending time pursuing and contributing to more serious things, like work. But you may also have noticed some quirks or irregularities in these topics. While each group of words seems to be capturing a distinct theme, all three are somewhat difficult to interpret precisely, and some words look a little out of place. 
Despite those quirks, we can still use this topic model for a rough categorization of individual documents, which can be useful if we want to make comparisons between documents and analyze how different themes are distributed. To do so, we need to examine the other side of the model’s output, the document–topic matrix. 
Most topic models break down documents in terms of topic proportions — for example, a model might say that a particular document consists 70% of one topic and 30% of another — but other algorithms may simply classify whether or not a document contains each topic. When using a model that allocates topics to documents as proportions, researchers can analyze the topics as either continuous variables, or as discrete classifications by setting a cutoff threshold to decide whether a document contains a topic or not. Here’s an example of what the corresponding weights for our three example documents might look like, for the three different topics above: 
By setting a 50% cutoff threshold, we can also turn these into categorical variables, like those you might collect in a survey: 
When you first see topic model output, it can be inspiring. Having the ability to automatically identify and measure the main themes in a collection of documents opens the door to all kinds of exciting research questions, and you may find yourself eager to get started. However, it turns out that running the topic model is the easy part — and the first of several steps. Before we can use our model as a measurement tool, we also have to figure out what it’s measuring — an undertaking that can be surprisingly difficult. In my next post, I’ll explain why. 
Patrick van Kessel is a senior data scientist at Pew Research Center. 
Written by 
Written by",Patrick van Kessel,2019-11-21T22:51:20.529Z
"Understanding NLP- BERT & Transformer | by Nehatayade | Aug, 2020 | Medium","Bidirectional Encoder Representations from Transformers (BERT) is an unsupervised language representation model developed by Google to leverage an understanding behind users’ search queries. It’s structure follows an open-source neural-network for natural language processing that is pre-trained on contextual representations on building ways to return results on question-answer based systems or achieve high accuracies on other NLP tasks like sentiment analysis. This state-of-the-art model is one of the biggest leaps in the history of Searches to improve language understanding capabilities by deciding on how to formulate complex or conversational queries. 
BERT, for original English language, uses two plain-text corpora in pre-training: BookCorpus (800M words) and English Wikipedia (2,500M words). 
The breakthrough in the NLP modelling was through the research on the bidirectional training of Transformers (Google Brain)- they take into account the context of occurrence of all the given words in a sentence, rather than one-by-one word in order. Transformer applies skip connection (residual blocks in ResNet) to the output of the multi-head attention followed by a layer normalization. Both techniques make training easier and more stable. This eliminates the ambiguities in English language due to searches based on keyword-based intentions. Unlike previous models, BERT is deeply bidirectional to have a deeper sense of language context by capturing the essence of words in a text sentence from surroundings (left-to-right as well as right-to-left directions) and pushes the limits of what we can do using traditional hardware by using Cloud TPUs or a few hours using a single GPU to fetch information quickly. 
How are vector representations different from word embeddings? Context-free models like GloVe and Word2Vec generate word embeddings for each respective word in the vocabulary. A vector representation in a contextual model like BERT generates a representation of each word that is based on the other words in the sentence. For example, in the sentence “a bat is used to hit the ball,” a unidirectional contextual model would represent “bat” based on just “a” but not “is used to hit the ball.” However, BERT represents “bat” using both its previous and next context — “a bat …ball” — starting from the very bottom of a deep neural network, making it deeply bidirectional. 
Using pre-trained BERT model for a purpose-specific task like question-answering or text summarization shows the characteristics similar to that of transfer learning which fine-tunes the parameters of a unsupervised network to perform a well-defined task. Here, the vector representations can be used as features to train the neural network through relationships trained by attention weights. The Transformer uses this attention mechanism to learn contextual relationships between words (or sub-words) in a text. 
The Vanilla model defines two opposite mechanisms- 
The encoder accepts an input sequence of tokens which are embedded into fixed-length vectors before passing through the neural network. The output vectors correspond to input tokens with their respective indexes. 
Many important downstream tasks such as Question Answering (QA) and Natural Language Inference (NLI) are based on understanding the relationship between two sentences, which is not directly captured by language modeling. In order to train a model that understands sentence relationships, we pre-train the pairs of sentences for a binarized next sentence prediction task. 
BERT Pre-training (Unsupervised) strategies : 
In order to train a deep bidirectional representation to predict the target word through multi-layered context, we mask 15% of the word-piece input tokens at random before feeding word sequences into BERT. Masking creates a mismatch between pre-training and fine-tuning phases by using [MASK] tokens only during the training phase. If an iᵗʰ token is chosen, we replace the iᵗʰ token with - 
(1) the [MASK] token 80% of the time 
(2) a random token 10% of the time 
(3) the unchanged iᵗʰ token 10% of the time 
The non-masked words in the sequences try to predict the original value of the masked words based on the context they provide. The probability of each word in the vocabulary involves the processes of using a classifying layer with softmax on top of the encoder output to choose the most suitable prediction obtained by multiplying the output vectors by an embedding matrix, transforming them into the vocabulary dimension. 
BERT’s usage of self-attention mechanism by encoding a concatenated text pair with self-attention effectively increases bidirectional context awareness for downstream tasks. 
2. Next Sentence Prediction- NSP 
In NLP certain tasks are based on understanding the relationship between two sentences, we want to predict if the second sentence in the pair is the subsequent sentence in the original document. A binarized next sentence prediction task that can trivially handle this dependency which language modeling fails to capture. Specifically, when choosing the sentences A and B for each pre-training example, 50% of the time B is the actual next sentence that follows A (labeled as IsNext), and 50% of the time it is a random sentence from the corpus (labeled as NotNext). The assumption is that the random sentence will be disconnected from the first sentence. 
During pre-training, the sentences are integrated with context-sensitive features - 
Instead of using every single word as tokens, BERT breaks a word into word pieces to reduce the vocabulary size (30,000 token vocabularies). For example, the word “running” may decompose into “run” and “ing”. Then it applies an embedding matrix (V × H) to convert the one-hot vector Rⱽ to Rᴴ. 
The segment embeddings model which sequence that tokens belong to. Does it belong to the first sentence or the second sentence. So it has a vocabulary size of two (segment A or B). Intuitively, it adds a constant offset to the embedding with value based on whether it belongs to sequence A or B. Mathematically, we apply an embedding matrix (2 × H) to convert R² to Rᴴ. The last one is the position embedding in H-Dimension. It serves the same purpose in the Transformer in identifying the absolute or relative position of words. 
These input representations are constructed by summing the corresponding token, segment, and position embeddings and then fed to the Transformer model to get the probabilities of the second sentence following the first one. The flattened embedding samples are passed through a softmax classifier to map the output to a single predicted value. 
Both, Masked LM and Next Sentence Prediction approaches aim to minimize the loss obtained from the two unsupervised strategies. These two training tasks help BERT to train the vector representation of one or two word-sequences. Other than the context, it likely discovers other linguistics information including semantics and co-reference. 
BERT Fine-training (Supervised): 
Fine-tuning simply refines the model and the corresponding labels on task-specific parameters such as Question Answering, Sentiment analysis, Text classification, Text Translation, etc. 
Fine-tuning is relatively inexpensive and BERT can be used for a wide variety of language tasks by adding a shallow classifier for any NLP task or a decoder to the core model: 
From these approaches, we hypothesize that when the model is fine-tuned directly on the downstream tasks and uses only a very small number of randomly initialized additional parameters, the task-specific models can benefit from the larger, more expressive pre-trained representations even when downstream task data is very small. 
Recent empirical improvements due to transfer learning with language models have demonstrated that rich, unsupervised pre-training is an integral part of many language understanding systems and that BERT models are undoubtedly a breakthrough in the use of Machine Learning for Natural Language Processing. The flexibility, easy fine-tuning and a generic deep bidirectional architecture for diverse tasks allows BERT to be a baseline for several NLP practical applications. 
Written by 
Written by",Nehatayade,2020-08-23T00:28:01.636Z
PCA: Principal Component Analysis | by Kadir Yasar | Medium,"In this article, we will talk about Principal Component Analysis (PCA), what is it? how it works? where it is used? etc. First we will look at the technical details behind PCA and apply it on well-known iris dataset and discuss the results. However, reader should have knowledge about linear algebra and statistical analysis; at least matrix operations and variance/covariance. If you need to fresh your knowledge on them, you can see this article of me. So, if everything is good to you so far, let’s start.. 
PCA is a statistical technique to extract patterns in a dataset. Yes, it is. You maybe know it as dimensionality reduction method, yes it is; but it is actually more than that. PCA simply converts your dataset to identify hidden relationships, similarities or differences, then you can make dimension reduction, data compression or feature extraction over the output of it. However, PCA is the best known and used to reduce the dimensions of dataset, and that’s what we will do in this article: dimensionality reduction with PCA. 
You should ask this question here: Why do we need to reduce the dimensions in dataset? Isn’t that loosing information? Yes, we loose information when we discard some of the dimensions in our data. However, in some cases our data can have lots of features or variables to apply a machine learning technique to do classification or clustering. Think about user dataset of AmazonVideo, Youtube or Netflix, they can be in million-dimension where each video content is a variable or feature, and multiply it with the number of users they have when you need to extract similarities between users or videos and produce recommendations. For more info about recommender systems, see my other posts. 
Simply, the more dimensions data has, the harder to process it. Therefore, dimensionality reduction techniques like PCA, LDA are applied to extract new powerful features from data and these new features or components are used instead of original features. Although some of data is taken out, selected best components should be enough to process. 
To analyse and build new dataset (reduced in dimensions) from original one by PCA, Following steps are used in general: 
Let’s discuss and apply each step one-by-one to iris dataset. 
As we talked, iris dataset is our target dataset in this article. If you are not familiar with it, check this out. The data has 4 features or variables; or 4 dimensions in matrix algebra. And, 1 target vector shows the type of flower dependent to 4 features. So, the problem is in 4-Dimensional. 4D is not much but we will try to reduce it to 2D for illustration of PCA. Let’s start loading the data: 
We have loaded the data to a matrix called R having 150 samples (x-axes) and 4 features (y-axes) for each sample. 
Covariance matrix is just a matrix of covariance of features (dimensions). Covariance is the variance of 2 features; in other words, how 2 features vary from each other. It is a very useful information when you need to extract new patterns or features from existing features. Therefore, as 2nd step we need to calculate covariance matrix of our dataset. Since there are 4 features in data, we have 6 covariances to calculate, and 4 variances. 
Below, we have the result covariance matrix. Diagonal values are variances of each feature but we are not interested in them because we try to find new patterns/features among existing features. Therefore, other entries except for those in diagonal are essential for PCA. 
As cov(a, b) = cov(b, a), up and bottom sides of diagonal in covariance matrix are equal. If covariance of (a, b) is positive, a and b varies together; if negative, they vary in different directions. As an example, from the matrix below, petal_length and petal_width features of a flower in dataset has a positive covariance (1.296287), it means these 2 features increases or decreases together. When you check these 2 features of some of the samples from the dataset, you can see it. 
Eigenvalues and eigenvectors are the heart of PCA; well not only inPCA, but also in others like SVD, LDA. But why they are so important? Eigenvalues and Eigenvectors which are associated constitute the root characteristics of a matrix equations. I will leave the explanation of this to this source and will continue with important features of eigenvectors and eigenvalues for PCA. 
There are 3 simple features we need to know about them: First, we can only calculate eigenvalues/eigenvectors of a square matrix (n x n, covariance of matrix). Second, eigenvectors are perpendicular/orthogonal to each other. If we have n-dimensional matrix so we have n eigenvectors in n-space and all of them are perpendicular. This makes sense because all of them constitutes the data they represents. Lastly, length of eigenvectors are exactly 1 and each has a corresponding eigenvalue which represents the power of the vector. 
Since we are looking for new features to reduce the dimensionality in our data, the eigenvectors of covariance matrix of data are calculated to find patterns (eigenvectors) with their significance (eigenvalues). Eigenvectors of covariance matrix will represent new features and we will choose some of them according to their eigenvalue power or impact. Let’s do this on our iris example. We have already had covariance matrix and it’s a square matrix! 
From the first result, we have a eigenvalue for each dimension in data and a corresponding eigenvector in results as listed above. What we need to do know is to order eigenvalues in value from highest to lowest. And then, we pick some of the eigenvectors with highest value to build our new features. 
As we discussed earlier, eigenvalues represents the impact or power of a vector, so we must pick the eigenvectors whose eigenvalue is higher in value. In this case, since we want to reduce the dimensionality of iris data to 2, we will pick the first eigenvectors because their eigenvalues are the highest 2 in results. The selected highest value eigenvectors will be our principal components to build new featured and reduced dataset. And, we will call this matrix as new feature vector. 
By dropping some of the components or eigenvalues/eigenvectors we will loose some information. However, since we choose the components having highest values or importance, this loose will be reasonable. By dropping, we will have data in less dimension to work. 
Now, we are ready to build new reduced data from selected principal components from the previous step. To build new dataset, we need to multiply the transpose of original matrix (R) on the left of the transpose of new feature vector (selected principal components). 
Why we multiplied transposed of original dataset and principal components is to get new data in terms of eigenvectors we choose. I know it seems to be complicated but you can find visual explanation of these steps taken so far. 
And here is the code to produce new 2D data from selected components for iris dataset. 
At last, we have 2D reduced new dataset in our hands. Of course, it has lost some information but since we selected 2 major eigenvectors, new features we built from selected components should be enough to go further. If not, we can consider to increase the number of components to build new dataset. 
Now, let’s do some visualization. At first, our dataset had 4 dimensions and it was impossible to plot, but it is a 2D data and will be easy to plot. 
From the plot above, now it will be easy to classify or cluster the samples over 2 principal components. Although we lost some information by dropping other minor components, now we have much more interpretable data in our hand. The best thing is that it is still very close to original data. How? Remember the selection of eigenvalues our selected eigenvalues consists of %97.7. Think about it.. 
Well done! you have done with PCA! It reduced the dimensions in our original iris dataset to a desired size (2D) and produced new features. Now it will be more efficient to apply further techniques on new dataset. For this iris example, we didn’t gain so much but think about the datasets in huge companies like Amazon, Youtube and Netflix. 
From now on, we can continue applying our classification/clustering techniques to find the best MSE/MAE over new dataset or principal components. If the result does not satisfy us, we can try change number of components or can apply other similar dimensionality reduction techniques like SVD or LDA. 
Please do not hesitate to correct me or asking questions.. 
Thanks for reading.. 
Bye 
Written by 
Written by",Kadir Yasar,2018-07-25T22:37:48.976Z
An Introduction to Natural Language Processing (NLP) | by ODSC - Open Data Science | Medium,"Everything we express (either verbally or in written) carries huge amounts of information. The topic we choose, our tone, our selection of words, everything adds some type of information that can be interpreted and value can be extracted from it. In theory, we can understand and even predict human behavior using that information. 
But there is a problem: one person may generate hundreds or thousands of words in a declaration, each sentence with its corresponding complexity. If you want to scale and analyze several hundreds, thousands, or millions of people or declarations in a given geography, then the situation is unmanageable. 
Data generated from conversations, declarations, or even tweets are examples of unstructured data. Unstructured data doesn’t fit neatly into the traditional row and column structure of relational databases, and represent the vast majority of data available in the actual world. It is messy and hard to manipulate. Nevertheless, thanks to the advances in disciplines like machine learning, a big revolution is going on regarding this topic. Nowadays it is no longer about trying to interpret text or speech based on its keywords (the old fashioned mechanical way), but about understanding the meaning behind those words (the cognitive way). This way, it is possible to detect figures of speech like irony or even perform sentiment analysis. 
Natural Language Processing or NLP is a field of artificial intelligence that gives the machines the ability to read, understand and derive meaning from human languages. 
It is a discipline that focuses on the interaction between data science and human language, and is scaling to countless industries. Today, NLP is booming thanks to huge improvements in the access to data and increases in computational power, which are allowing practitioners to achieve meaningful results in areas like healthcare, media, finance, and human resources, among others. 
[Related article: Essential NLP Tools, Code, and Tips] 
Use Cases of NLP 
In simple terms, NLP represents the automatic handling of natural human language like speech or text, and although the concept itself is fascinating, the real value behind this technology comes from the use cases. 
NLP can help you with lots of tasks and the fields of application just seem to increase on a daily basis. Let’s mention some examples: 
NLP is particularly booming in the healthcare industry. This technology is improving care delivery, disease diagnosis, and bringing costs down while healthcare organizations are going through a growing adoption of electronic health records. The fact that clinical documentation can be improved means that patients can be better understood and benefited through better healthcare. The goal should be to optimize their experience, and several organizations are already working on this. 
Companies like Winterlight Labs are making huge improvements in the treatment of Alzheimer’s disease by monitoring cognitive impairment through speech and they can also support clinical trials and studies for a wide range of central nervous system disorders. Following a similar approach, Stanford University developed Woebot, a chatbot therapist with the aim of helping people with anxiety and other disorders. 
However, there is still serious controversy around the subject. A couple of years ago, Microsoft demonstrated that by analyzing large samples of search engine queries, they could identify internet users who were suffering from pancreatic cancer even before they have received a diagnosis of the disease. How would users react to such diagnosis? And what would happen if you were tested as a false positive? (meaning that you can be diagnosed with the disease even though you don’t have it). This recalls the case of Google Flu Trends which in 2009 was announced as being able to predict influenza but later on vanished due to its low accuracy and inability to meet its projected rates. 
NLP may be the key to effective clinical support in the future, but there are still many challenges to face in the short term. 
How does the future look like? 
At the moment NLP is battling to detect nuances in language meaning, whether due to lack of context, spelling errors, or dialectal differences. 
On March 2016 Microsoft launched Tay, an Artificial Intelligence (AI) chatbot released on Twitter as an NLP experiment. The idea was that as more users conversed with Tay, the smarter it would get. Well, the result was that after 16 hours Tay had to be removed due to its racist and abusive comments: 
Microsoft learned from its own experience and some months later released Zo, its second generation English-language chatbot that won’t be caught making the same mistakes as its predecessor. Zo uses a combination of innovative approaches to recognize and generate conversation, and other companies are exploring with bots that can remember details specific to an individual conversation. 
Although the future looks extremely challenging and full of threats for NLP, the discipline is developing at a very fast pace (probably like never before) and we are likely to reach a level of advancement in the coming years that will make complex applications look possible. 
Interested in these topics? Follow me on Twitter at @lopezyse where I post all about Data Science, Technology, and the most interesting advances! 
Read more data science articles on OpenDataScience.com, including tutorials and guides from beginner to advanced levels! Subscribe to our weekly newsletter here and receive the latest news every Thursday. 
Written by 
Written by",ODSC - Open Data Science,2019-02-25T15:51:00.742Z
How Netflix’s Recommendation Engine Works? | by Springboard India | Medium,"This is the question that pops into your mind once you are back home from the office and sitting in front of the TV with no remembrance of what kind of shows you watched recently. Today, everyone wants an intelligent streaming platform that can understand their preferences and tastes without merely running on autopilot. From Netflix to Amazon Prime — recommendation systems are gaining importance as they directly interact (usually behind the scenes) with users every day. 
With over 139 million paid subscribers(total viewer pool -300 million) across 190 countries, 15,400 titles across its regional libraries and 112 Emmy Award Nominations in 2018 — Netflix is the world’s leading Internet television network and the most-valued largest streaming service in the world. The amazing digital success story of Netflix is incomplete without the mention of its recommender systems that focus on personalization. 
Have you ever thought why the Netflix artwork changes for different shows when you login to the account? One day it might be an image of the entire bridge crew while the other day it is the Worf glaring at you judgingly. If you are Netflix user you might also have noticed that the platform shows really precise genres like Romantic Dramas where the leading character is left-handed. How does Netflix come up with such precise genres for its 100 million-plus subscriber base? How does Netflix artwork change? It’s machine learning, AI, and the creativity behind the scenes that guess what will make a user pick a particular show to watch. Machine learning and data science help Netflix personalize the experience for you based on your history of picking shows to watch. 
Netflix began using analytic tools in 2000 to recommend videos for users to rent. 
Netflix just has a 90-second window to help viewers find a movie or a TV show before they leave the platform and visit some other service. That’s one of the major reasons why Netflix is so obsessed with personalizing recommendations to hook users. 
Netflix’s personalized recommendation algorithms produce $1 billion a year in value from customer retention. 
Majority of Netflix users consider recommendations with 80% of Netflix views coming from the service’s recommendations. 
Netflix has set up 1300 recommendation clusters based on users viewing preferences. 
Netflix segments its viewers into over 2K taste groups. Based on the taste group a viewer falls, it dictates the recommendations. 
With over 7K TV shows and movies in the catalogue, it is actually impossible for a viewer to find movies they like to watch on their own. Netflix’s recommendation engine automates this search process for its users. 
Netflix’s chief content officer Ted Sarandos said – 
There’s no such thing as a ‘Netflix show’. Our brand is personalization. 
Personalization begins on Netflix’s homepage that shows group of videos arranged in horizontal rows. Each horizontal row has a title which relates to the videos in that group. Most of the personalized recommendations begin based on the way rows are selected and the order in which the items are placed. Recommender systems at Netflix span various algorithmic approaches like reinforcement learning, neural networks, causal modelling, probabilistic graphical models, matrix factorization, ensembles, bandits. 
Netflix’s recommendation systems have been developed by hundreds of engineers that analyse the habits of millions of users based on multiple factors. Whenever a user accesses Netflix services, the recommendations system estimates the probability of a user watching a particular title based on the following factors – 
For every new subscriber, Netflix asks them to choose titles they would like to watch. These titles are used as the first step for personalized recommendations. Later as viewers continue to watch over time the recommendations are powered by the titles they watched more recently along with other factors mentioned above. Netflix’s machine learning based recommendations learn from their own users. Every time a viewer spends time watching a movie or a show, it collects data that informs the machine learning algorithm behind the scenes and refreshes it. The more a viewer watches the more up-to-date and accurate the algorithm is. 
The main goal of Netflix is to provide personalized recommendations by showing the apt titles to each of the viewers at the right time. But, why should a viewer care about the titles Netflix recommends? How does Netflix convince a viewer that a title is worth watching? How does Netflix grab the attention of a viewer to a new and unfamiliar title? Answering these questions is important to understand how viewers discover great content, particularly for new and unfamiliar titles. Netflix tackles this challenge through artwork personalization or thumbnails personalization that portray the titles. 
Netflix differs from a hundred other media companies by personalizing the so-called artworks. They say an image is worth a thousand words and Netflix is tapping on to it with its new recommendation algorithm based on artwork. The artwork for a title is used to capture the attention of the viewer and gives them a visual evidence on why it could be a perfect choice for them to watch it. The thumbnail or artwork might highlight an exciting scene from a movie like a car chase, a famous actor that the viewer recognizes, or a dramatic scene that depicts the essence of the TV show or a movie. For every new title various images are assigned randomly to different subscribers based on the taste communities. Netflix then presents the image with highest likelihood on a user’s homepage so that they will give it a try. 
Netflix makes use of thousands of video frames from existing TV shows and movies for thumbnail generation. The images are then annotated and ranked to predict the highest likelihood of being clicked by a viewer. These calculations depends on what other viewers with similar taste and preferences have clicked on. For instance, viewers who like a particular actor are most likely to click on images with the actor. 
We have to thank machine learning and data science for having totally disrupted the way media and entertainment industries operate. It is pretty clear that Netflix’s amalgamation of data, algorithms, and personalization are likely to keep users glued to their screens. It will be interesting to see how the media and entertainment industry will reshape with machine learning and artificial intelligence. 
Written by 
Written by",Springboard India,2019-11-05T05:06:21.729Z
Overcoming the limitations of topic models with a semi-supervised approach | by Patrick van Kessel | Pew Research Center: Decoded | Medium,"(Related posts: An intro to topic models for text analysis, Making sense of topic models and Interpreting and validating topic models) 
In two earlier posts on this blog, I introduced topic models and explored some of the difficulties that can arise when researchers attempt to use them to measure content. In this post, I’ll show how to overcome some of these challenges with what’s known as a “semi-supervised” approach. 
To illustrate how this approach works, I’ll use a dataset of open-ended survey responses to the following question: “What makes your life feel meaningful, satisfying or fulfilling?” The dataset comes from a recent Pew Research Center report about where Americans find meaning in their lives. (You can read some of the actual responses here.) 
As my colleagues and I worked on this project over the past year, we tested a number of computational methods — including several topic modeling algorithms — to measure the themes that emerged from these survey responses. The image below shows a selection of four topics drawn from models trained on our dataset (the topics are shown in the rows). To arrive at these topics, we used three different topic modeling algorithms, two of which are shown in the columns below — Latent Dirichlet Allocation (LDA) and Non-Negative Matrix Factorization (NMF). (More on the third algorithm later.) 
In each of these models, we can clearly see topics related to four distinct themes: health, finances, career and religion (specifically Christianity). But it’s also clear that many of these topics suffer from a few of the problems that I discussed in my last post: Some topics appear to be “overcooked” or “undercooked,” and some contain what might be called “conceptually spurious” words, highlighted above in red. These are words that might apply across multiple contexts and can cause problems under some circumstances. 
For example, if our survey respondents only ever mention the word “insurance” when discussing health insurance, it might still fit with a topic intended to measure the concept of “health.” On the other hand, including the word “care” in such a topic might be particularly problematic if, in addition to talking about “health care,” our respondents also frequently use the word to talk about other themes unrelated to health, like how much they care about their families. If that were the case, our model might substantially overestimate the prevalence of the true topic of “health” in our dataset. 
For these reasons, our initial attempts at topic modeling Americans’ sources of meaning in life raised some concerns — while we were confident that we could use this method to identify the main themes in our documents, it was unclear whether it could reliably measure those themes. While topic models can be executed quickly, they don’t always make classification decisions as accurately as more complex supervised learning models would — and in some cases their results can be downright misleading. 
Fortunately, one recent innovation offered us a compromise between an unsupervised topic modeling approach and a more labor-intensive supervised classification modeling approach: semi-supervised topic models. While still largely automated, the new class of semi-supervised topic models allows researchers to draw on their domain knowledge to “nudge” the model in the right direction. 
Intrigued by this possibility, we tested a third algorithm called CorEx (short for Correlation Explanation), a semi-supervised topic model that — unlike LDA and NMF — allowed us to provide the model with “anchor words” that represented potential topics we thought the model should attempt to find. With CorEx, you can also tell the model how much weight it should give to these anchors. If you are less confident in your choices, the model may override your suggestions if they don’t fit the data well enough. Armed with this new ability to guide a topic model with our domain expertise, we set out to assess whether we could make topic models more useful by manually correcting some of the problems we discovered in the above topics. 
In training our initial CorEx models, we embraced the reality that no perfect number of topics exists. No matter which parameters you set, a topic model of any size almost always returns at least a handful of jumbled or uninterpretable topics. We decided to train two different models — one with a large number of topics and one with fewer topics — and found that some of the topics that were “undercooked” in the smaller model were successfully split apart in the larger one. 
By the same token, “overcooked” topics in the larger model were collapsed into more general and interpretable topics in the smaller one. We didn’t use any anchor words in this first run because we wanted to use the models to identify the main themes in our data and avoid imposing our own expectations about the prominent topics in the responses. 
We brought the anchors into play as we reviewed the topics produced by the two models. After getting a sense of the core themes the models were picking up, we selected words that seemed to correctly belong to each topic and “confirmed” them by setting them as anchors. Then, in addition to drawing on our own domain knowledge to expand our lists of anchors with words we knew were related, we also read through a sample of our survey responses and added any relevant words that we noticed the models had missed. 
In cases where the models were incorrectly adding “conceptually spurious” words to certain topics, we encouraged the models to pull the undesirable words out of our good topics by adding them to a list of anchor words for a separate “junk topic” that we had designated for this purpose. After re-running the models and repeating this process several times, our anchor lists had grown substantially, and our topics were looking much more interpretable and coherent: 
Our application of semi-supervised topic modeling was a game-changer. By guiding the models in the right direction over multiple iterations and careful revisions of our anchor lists, we were able to successfully remove many of the conceptually spurious and multi-context words that appeared in the initial unanchored models. Our topics were looking much cleaner, and we were now hopeful that we could use this approach not only to identify the main topics in our data, but also refine them enough to interpret them, give them clear labels and use them as reliable measurement instruments. In my upcoming posts, I’ll cover how we did just that. 
In the meantime, if you’d like to try out semi-supervised topic modeling yourself, here is some example code to get you started. 
Patrick van Kessel is a senior data scientist at Pew Research Center. 
Written by 
Written by",Patrick van Kessel,2019-08-01T18:43:35.250Z
NLP – BerkeleyISchool – Medium,,NA,NA
A dive into Natural Language Processing | by Jocelyn D'Souza | GreyAtom | Medium,"This post will take you a beginner's guide to Natural Language Processing. A language is a way we humans, communicate with each other. Each day we produce data from emails, SMS, tweets, etc. we must have methods to understand these type of data, just like we do for other types of data. We will learn some of the basic but important techniques in Natural Language Processing. 
As per Wikipedia: 
Natural-language processing (NLP) is an area of computer science and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to fruitfully process large amounts of natural language data. 
In simple terms, Natural language processing (NLP) is the ability of computers to understand human speech as it is spoken. NLP helps to analyze, understand, and derive meaning from human language in a smart and useful way. 
NLP algorithms are machine learning algorithms based. NLP learns by analyzing a set of examples (i.e. a large corpus, like a book, down to a collection of sentences), and making a statistical inference, instead of coding large sets of rules. We can organize the massive chunks of text data and solve a wide range of problems such as — automatic summarization, machine translation, named entity recognition, relationship extraction, sentiment analysis, speech recognition, and topic segmentation etc. 
As we all know, the text is the most unstructured form of all the available data. It is important to cleaning and standardize this text and make it noise free. The idea is to take the raw text and turn into something which can be utilized by an ML algorithm to carry out prediction. We will talk about few important techniques using NLTK. 
We break the articles into sentences. Often we have to do analysis at sentences level. For example, we want to check the number of sentences in an article and number of words in a sentence. 
Tokenization breaks unstructured data, text, into chunks of information which can be counted as discrete elements. This immediately turns an unstructured string (text document) into a more usable data, which can be further structured, and made more suitable for machine learning. Here we take the first sentence and we get each word as token. Below are two different ways i.e RegexpTokenizer & Word Tokenize. 
Consider words like a, an, the, be etc. These words don’t add any extra information in a sentence. Such words can often create noise while modelling. Such words are known as Stop Words. We filter each sentence by removing the stop words as shown below: 
Some words represent the same meaning. For example, Copy, copied, copying. The model might treat them differently, so we tend to strip such words to their core. We can do that by stemming or lemmatisation. Stemming and Lemmatization are the basic text processing methods for English text. 
It helps to create groups of words which have similar meanings and works based on a set of rules, such as remove “ing” if words are ending with “ing”. Different types of stemmers in NLTK are PorterStemmer, LancasterStemmer, SnowballStemmer. 
It uses a knowledgebase called WordNet. Because of knowledge, lemmatization can even convert words which are different and cant be solved by stemmers, for example converting “came” to “come”. 
These are few basic techniques used in NLP. I hope I’ve given cleared some of the basic and important concepts in NLP as this is the building block of many other NLP concepts. To learn more about NLTK, visit this link. 
Thanks for reading! ❤ 
Follow for more updates! 
Written by 
Written by",Jocelyn D'Souza,2018-03-27T12:37:00.275Z
Understanding Genetic Algorithms. Solving a Battleship Board Game as an… | by Adam C Dick | Towards Data Science,"A genetic algorithm is a prime example of technology imitating nature to solve complex problems, in this case, by adopting the concept of natural selection in an evolutionary algorithm. Genetic algorithms, introduced in 1960 by John Holland, extend Alan Turing’s concept of a “learning machine” and are best-suited for solving optimization problems such as the traveling salesman. 
To intuitively understand the practical implementation and fundamental requirements for employing genetic algorithms, we can set up a toy problem and solve the board of the classic guessing game, Battleship, first released by Milton Bradley in 1967. But rather than calling a sequence of individual shots, let’s ask our genetic algorithm to make a series of guesses of the entire board. 
Genetic algorithms can be applied to problems whose solutions can be expressed as genetic representations, which are simply arrays of ones and zeros. Each binary element is called a gene, while an array of multiple genes is referred to as a chromosome. The optimal solution of a given problem is the chromosome that results in the best fitness score of a performance metric. 
A Battleship board is composed of a 10 x 10 grid, upon which we can randomly place five ships of varying length. Our fleet includes a carrier (5), a battleship (4), a cruiser (3), a submarine (3) and a destroyer (2). We can express the board as a binary representation simply by denoting the squares occupied by our ships as ones and the unoccupied squares as zeros. 
We can randomly position each of our ships by following a few simple steps: 
This results in a two-dimensional binary matrix, which must be transformed into a one-dimensional array that conforms to the genetic form. We can reshape the board by stacking the rows to create a chromosome of 100 genes. Since each gene in the board solution can either be a one or a zero, our toy problem could have been arranged in 2¹⁰⁰ or 1.27e+30 possible ways! 
Board Solution in Genetic Form 
00001110000000000000000001111000000000000000010000…00010100000001010000000101000010000100001000000000 
We can randomly guess the entire board by generating a chromosome with any combination of 100 genes. By guessing that each gene has an equal probability of being occupied (red) or unoccupied (blue), a random sample should be expected to contain 50 occupied squares. By overlaying the guess and the solution (ones & zeros), we can visualize how well they match. 
The fitness of a chromosome can most simply be assessed by counting how many genes correctly match those in the genetic solution (green) and then dividing by the total number of genes (green & red). This accuracy rate of our random chromosome is already 52%! Even though there are only 17 occupied squares, any binomial distribution will approach a median accuracy of 50%. 
The genetic algorithm can now learn from this fitness metric to suggest an even higher performing chromosome. Fitness functions can be as sophisticated as desired to provide more informative feedback. For instance, we could calculate a confusion matrix, commonly used in classification problems to account for false positives and false negatives, to return precision and recall. 
Now that we know how to create a random chromosome and assess its fitness, let’s create a generation or gene pool of 10 random guesses and observe how well they do. We can sort the chromosomes by level of fitness and label the top performers, which are 57% accurate, as elite. Even though we have only a small sample, the median fitness of the sample distribution is still 52.5%. 
Now that we have established our gene pool, we can use the natural selection concept of survival of the fittest to create a new generation from an old one. The first step is to select the elite chromosomes from the old generation to be parents of the new generation. These parent chromosomes can then create children via two popular genetic operators, known as crossover and mutation. 
Elitism: The fittest chromosomes from the former generation that are selected to be parents of all of the chromosomes in a new generation. 
Crossover: Two chromosomes can be spliced at a random crossover gene and recombined to create two children, sharing gene chains from both parents. 
Mutation: Random genes of one chromosome can be inverted with bit flips to create a single child that exhibits a small genetic variation from its parent. 
Let’s create a second generation of 10 chromosomes with a 20% elite rate (two parents), a 20% crossover rate (one pair of spliced children) and a 60% mutation rate (six mutant children). Our bit flip rate is 1%, such that mutants vary from their parents by one gene on average. We can once again sort our generation by fitness and find that our two elites are now 58% and 60% fit! 
What happens if we iteratively repeat this process to create new generations? With each successive generation, we only retain the highest performers and discard non-optimal solutions. By carrying over elites, we guarantee that generational peak performance can potentially increase and never decrease. After 155 generations, a chromosome reaches the solution with 100% fitness! 
We have fixed the size of each generation to be 10 chromosomes, but the genetic algorithm incorporates randomness in many areas, from our random chromosomes to the crossover genes to the bit flip rate. Statistics of the first 10 generations show how natural selection gradually improves the population fitness, while dispersion is highest in the first random generation. 
Statistics of the last 10 generations show that each population is extremely fit with the least fit chromosome scoring 95%. As a high fitness chromosome approaches the genetic solution, it becomes more likely that its fitness can only improve by mutating a single gene. Thus, high mutation rates and low bit flip rates give fit generations the best chance of achieving the target solution. 
We can visualize the performance of the genetic algorithm by plotting fitness statistics across generations. Any random binomial distribution will empower the first generation with 50% fitness. Elitism guarantees that generational peak performance will be monotonically increasing, while crossover promotes substantial initial improvements and mutation pushes them to the top. 
The randomness of the genetic algorithm means that its performance and efficiency will vary over a series of repeated trials. By solving 1,000 games and plotting the distribution of total chromosomes created over all generations, we see that our genetic algorithm will typically find the optimal solution after creating only about 1,700 chromosomes out of 1.27e+30 possibilities! 
This performance distribution is specific to the hyper-parameters of our model, which are 10 chromosomes per generation, a 20% elite rate, a 20% crossover rate, a 60% mutation rate and a 1% bit flip rate. In the future, we could optimize the efficiency of our genetic algorithm for this problem by minimizing the convergence rate with respect to these feature parameters. 
The source code written for this analysis is available on GitHub, which includes a random generator of a Battleship board, implementation of a genetic algorithm and the examples presented in this article. While there are many more details in mastering genetic algorithms, we should feel confident that we understand the fundamentals of this optimization algorithm. 
Hands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look 
Written by 
Written by",Adam C Dick,2019-06-27T02:41:10.213Z
Various Implementations of Collaborative Filtering | by Prince Grover | Towards Data Science,"Comparison of different methods to build recommendation system using collaborative filtering 
We see the use of recommendation systems all around us. These systems are personalizing our web experience, telling us what to buy (Amazon), which movies to watch (Netflix), whom to be friends with (Facebook), which songs to listen (Spotify) etc. These recommendation systems leverage our shopping/ watching/ listening patterns and predict what we could like in future based on our behavior patterns. The most basic models for recommendations systems are collaborative filtering models which are based on assumption that people like things similar to other things they like, and things that are liked by other people with similar taste. 
It is easy to build targeted recommendation models, then why not just build one for your own customers. I have written this post to make it even more easier for you. Most of the content of this post is inspired from fast.ai Deep Learning part 1 v2 course. 
Here is the link to the notebook with implementations of techniques discussed below. 
In this post, I have discussed and compared different collaborative filtering algorithms to predict user rating for a movie. For comparison, I have used MovieLens data which has 100,004 ratings from 671 unique users on 9066 unique movies. The readers can treat this post as 1-stop source to know how to do collaborative filtering on python and test different techniques on their own dataset. (I have also provided my own recommendation about which technique to use based on my analysis). 
Before reading further, I hope that the you have basic understanding of collaborative filtering and its application in recommender systems. If not, I strongly recommend you to go through below blogposts which are written by a fellow student at USF: Shikhar Gupta 
Blogs: Collaborative filtering and embeddings — Part 1 and Part 2 
A lot of research has been done on collaborative filtering (CF), and most popular approaches are based on low-dimensional factor models (model based matrix factorization. I will discuss these in detail). The CF techniques are broadly divided into 2-types: 
Say we want to recommend a new item to our product users (e.g. movie to subscribers of Netflix or clothing to online buyer at Amazon). For that we can use one of many techniques as discussed below. 
Quoted by Agnes Johannsdottir in her blogpost 
Memory-Based Collaborative Filtering approaches can be divided into two main sections: user-item filtering and item-item filtering. A user-item filtering takes a particular user, find users that are similar to that user based on similarity of ratings, and recommend items that those similar users liked. In contrast, item-item filtering will take an item, find users who liked that item, and find other items that those users or similar users also liked. It takes items and outputs other items as recommendations. 
Item-Item Collaborative Filtering: “Users who liked this item also liked …”User-Item Collaborative Filtering: “Users who are similar to you also liked …” 
The key difference of memory-based approach from the model-based techniques (hang on, will be discussed in next paragraph) is that we are not learning any parameter using gradient descent (or any other optimization algorithm). The closest user or items are calculated only by using Cosine similarity or Pearson correlation coefficients, which are only based on arithmetic operations. Edit: As stated in above paragraph, the techniques where we don’t use parametric machine learning approach are classified as Memory based techniques. Therefore, non parametric ML approaches like KNN should also come under Memory based approach. When I had originally written this blog, I kept it under Model based approach, which is not right. 
Quoted in this blog 
A common distance metric is cosine similarity. The metric can be thought of geometrically if one treats a given user’s (item’s) row (column) of the ratings matrix as a vector. For user-based collaborative filtering, two users’ similarity is measured as the cosine of the angle between the two users’ vectors. For users u and u′, the cosine similarity is: 
We can predict user-u’s rating for movie-i by taking weighted sum of movie-i ratings from all other users (u′s) where weighting is similarity number between each user and user-u. 
We should also normalize the ratings by total number of u′ (other user’s) ratings. 
Final words on Memory-based approach: As no training or optimization is involved, it is an easy to use approach. But its performance decreases when we have sparse data which hinders scalability of this approach for most of the real-world problems. 
If you are interested to try this approach, below are the links of great posts showing step by step python implementation of the same. (I have not discussed the implementations here as I would personally use scalable model-based approaches) 
Link 1: Implementing your own recommender systems in PythonLink 2: Intro to Recommender Systems: Collaborative Filtering 
In this approach, CF models are developed using machine learning algorithms to predict user’s rating of unrated items. As per my understanding, the algorithms in this approach can further be broken down into 3 sub-types. 
Edit: Group name “Clustering based algorithm” based on above diagram is wrong. It’s not generating clusters. We are just finding k closest training examples in KNN. “Non parametric approach” is better terminology. 
Matrix decomposition can be reformulated as an optimization problem with loss functions and constraints. Now the constraints are chosen based on property of our model. For e.g. for Non negative matrix decomposition, we want non negative elements in resultant matrices. 
Embeddings:Intuitively, we can understand embeddings as low dimensional hidden factors for items and users. For e.g. say we have 5 dimensional (i.e. D or n_factors = 5 in above figure) embeddings for both items and users (# 5 chosen randomly). Then for user-X & movie-A, we can say the those 5 numbers might represent 5 different characteristics about the movie, like (i) how much movie-A is sci-fi intense (ii) how recent is the movie (iii) how much special effects are in movie A (iv) how dialogue driven is the movie (v) how CGI driven is the movie. Likewise, 5 numbers in user embedding matrix might represent, (i) how much does user-X like sci-fi movie (ii) how much does user-X like recent movies …and so on. In above figure, a higher number from dot product of user-X and movie-A matrix means that movie-A is a good recommendation for user-X. 
(I am not saying these numbers actually represent such information. We don’t actually know what these factors mean. This is just to build an intuition) 
Learn more about embeddings in this great blogpost by another fellow USF student: Kerem Turgutlu.Link: Structured Deep Learning 
Matrix factorization can be done by various methods and there are several research papers out there. In next section, there is python implementation for orthogonal factorization (SVD) or probabilistic factorization (PMF) or Non-negative factorization (NMF). 
Below is the visualization to explain what is happening when we are using neural nets for this problem. 
We can think of this as an extension to matrix factorization method. For SVD or PCA, we decompose our original sparse matrix into product of 2 low rank orthogonal matrices. For neural net implementation, we don’t need them to be orthogonal, we want our model to learn the values of embedding matrix itself. The user latent features and movie latent features are looked up from the embedding matrices for specific movie-user combination. These are the input values for further linear and non-linear layers. We can pass this input to multiple relu, linear or sigmoid layers and learn the corresponding weights by any optimization algorithm (Adam, SGD, etc.). 
___________________________________________________________________ 
Github repo link: here 
Let’s look at the python implementation of above discussed algorithms. I have explored 2 different python packages which give options of various algorithms to chose from. 
This package has been specially developed to make recommendation based on collaborative filtering easy. It has default implementation for a variety of CF algorithms. 
Step 1: Download MovieLens data and read it in pandas df http://files.grouplens.org/datasets/movielens/ml-latest-small.zip 
Step 2: Install Surprise package by pip install scikit-surprise. Load the data into Dataset class 
Step 3: Now implementing any MF algorithm after data preparation is as simple as running 1 line of code. Below are the codes and outputs of Singular Value Decomposition (SVD) and Non negative Matrix Factorization (NMF). The code can also be used for KNN by just changing algo = KNNBasic() in below code. (Please check wikipedia links for SVD and NMF ) 
Validation RMSE scores from SVD and NMF 
Best RMSE = 0.8967 (SVD) and corresponding MSE is 0.804 
fast.ai is massive python library that is making machine learning and deep learning easy for people with basic coding skills. 
Below are 5 lines of codes which implement probabilistic matrix factorization for CF. The implementation leverages the fact that the 2 factorized matrices are just embedding matrices which can be modeled by adding embedding layer in neural net (We can call it Shallow Learning). 
Training results: 
Best validation MSE obtained from fast.ai implementation of PMF is 0.801 which is close to what we got from SVD. 
We can add more linear and non linear layers to our neural net to make it deep neural net model. 
Steps to make a deep neural net for collaborative filtering using fast.ai 
Step 1: Load data into PyTorch data-loader. The fast.ai library is built on top of PyTorch. If you want to customize dataset class for specific format of data, learn it here. 
I used fast.ai’sColumnarModelData.from_data_frame function to load the dataset.You can also define your own data-loader function. X has data about userId, movieId and timestamp and Y has data about only ratings (target variable). 
Step 2: Defining custom neural net class (The syntax is specific to PyTorch, but same logic can be employed for Keras also). 
We have to create 2 functions. __init__ (), the constructor for the class and forward(), the forward pass function. 
A little more about layers that have been used in forward pass: 
Step 3: Model fitting and prediction. 
Results: 
Best validation MSE = 0.7906. This is best among all the models discussed above. 
Below is the plot of MSE obtained from different approaches on MovieLens 100k data. 
Neural net (DL) and SVD give the best results. Neural net implementation will also perform well on imbalanced data, with infrequent users unlike other MF algorithms. 
It can be useful to built customer targeted recommendation system for your products/ services. Most easiest and well-researched method out there is collaborative filtering. I have written this post with aim that rather than going through technical research papers and spending hours to learn about collaborative filtering, readers can find all the useful materials, together with implementation at just one place. 
Hands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look 
Written by 
Written by",Prince Grover,2020-03-31T00:47:15.010Z
The Graph – Medium,,NA,NA
Collaborative Filtering in Recommendation Systems | by Kunal Deshmukh | kunalrdeshmukh | Medium,"Recommendation systems describe the techniques used to predict ratings and opinions in which a user might have a propensity to express. As an example, think of the recommended content shown to users on Netflix or perhaps the products a user may want to purchase from Amazon. Recommendation systems are ubiquitous in the digital world. Users encounter these systems as they see ‘Suggested Friends’ on Facebook, ‘Suggested Videos’ on YouTube, or ‘Other Jobs for you’ on LinkedIn. 
There are two approaches through which recommendation system are designed:1. Content-based filtering2. Collaborative filtering 
Techniques which selectively make use of both approaches are called Hybrid recommendation systems. 
There are some advantages and disadvantages in both the systems : 
Content-based recommendation systems take into account the data provided by the user both directly and indirectly. For example, age can be used to determine classes of products or items reviewed and bought by the user. Since content-based recommendations are based on usage patterns of a user, a user is often able to speculate why he/she is being shown a recommendation. This type of recommendation system relies on characteristics of the object. New content can be quickly recommended to the user. E.g. if the user has a history of watching all action movies, a newly released action movie is recommended by this system. However, this system does not take into account behavior/data about other users in the system hence, if a particular action movie fetches very low rating / negative recommendations by other users, It will still be recommended to the user.TF-IDF ( Term Frequency — Inverse Document Frequency) and cosine similarity between space vector model techniques are used in content-based filtering. 
This technique is used in information retrieval and text mining. TF-IDF, as the name suggests has two terms. TF calculates normalized frequency at which a given term appears in the document. And IDF calculates importance of a term in general. Eg. terms ‘recommendation’, ‘system’, ‘movie’ conveys more information about the document than terms ‘the’, ‘and’, ‘are’ etc. 
TF: It measures the frequency of a term in the document. Since the size of a document may vary, It will be futile to use simple count. Hence this count is normalized.TF(w) = (Number of times term w appears in a document) / (Total number of terms in the document).IDF: It measures the overall importance of a given term.Since commonly used terms like ‘is’, ‘the’, ‘are’ doesn’t usually provide information about the document, IDF for these terms is low. IDF is calculated as :IDF(t) = log_e(Total number of documents / Number of documents with term t in it).In content-based filtering technique, TF-IDF can be useful in determining products which are similar to a given product. Since this technique is keyword based, it is more useful in the area with high textual data. Eg. book recommendation. In these areas generally, cosine similarity is used to calculate similarity between two items.TF-IDF method is used to calculate a vector in feature space. 
Cosine Similarity Matrix : 
As the name promises, this method calculates cosine value in vector calculation. This method provides the estimation of similarity between two objects as a measure of the angle between two vectors.Cosine similarity can be calculated by obtaining a dot product between two vectors. 
Cosine similarity is a measure of similarity between two vectors. 
It is evident in fig above, the similarity between two vectors in first image is the highest i.e. ~ 1. While cosine similarity in third plot would be around -1. Cosine similarity of vectors, orthogonal to each other will be 0. 
TfidfVectorizer module of scikit learn can be used to calculate tf-idf matrix.And Cosine_similarity method in sklearn.metrics.pairwise module can be used to calculate cosine similarity of a matrix. 
2. Deep drive in collaborative filtering 
Developers at Xerox first use collaborative filtering in document retrieval system[5]. PageRank algorithm used by Google is an example of document retrieval system using collaborative filtering. Collaborative filtering is used to tailor recommendations based on the behavior of persons with similar interests. Sometimes it can be based on an item bought by the user. Since this method does not require a person himself to always contribute to a data store, and voids can be filled by the actions of other persons/ actions by the same person on other items. Few approaches for User and Item-based collaborative recommendation techniques are as follow:1. Neighborhood-based approach2. Item-based approach3. Classification approach4. Neural Collaborative Filtering 
Let’s refer the user for which rating is to be predicted as ‘active user’. In the approach, users are selected based on their similarity to the active user. Correlation between two users can be found out using the formula below: 
Let the ratings to a product by person ‘A’ and ‘B’ be ‘Ak’ and ‘Bk’ . And mean values are represented as A,B 
Persons with higher correlation are considered as neighbors. 
Let Vi,j be the vote of user i on item j. Mean vote for i is 
Ii is items for which user i has voted. 
The vote of an ‘active user’ a is : 
Here, k is a constant used for normalization. w(a,i) is weights of ’n’ similar users. 
The simplest way to calculate w(a,i) is to using KNN algorithm where each point is represented by a user in n-dimensional space. However, there are more efficient algorithms which can be used to calculate weight w. 
w(a,i) is calculated by Pearson correlation coefficient : 
The most serious problem collaborative filtering techniques face in a real world is too few ratings by the users. Hence, In the real-world dataset, user vs items matrix may have some null values. 
w(a,i) is calculated in such cases as: 
Where fi = log(n/ni) is inverse user frequency. here, n is a number of users. Nj is a number of users voting for item j. is inverse user frequency. 
One approach for to reduce the effect of null values is by dimensionality reduction. Billsus and pazzani[7] suggest the use of the non-correlation based approach to make predictions using neural networks. 
2. Item — to — Item approach 
Instead of using ratings given by the users to calculate neighborhood, the ratings are used to find similarity between items. The same pearson coefficient can be used for this approach. 
3. Classification approach 
In classification approach, items are represented as vectors and they are classified and suggested to the user based on the ratings provided by the active user to each class of items. With this approach, collaborative filtering is visualized as classic classification approach. Classification techniques such as Support Vector Machines or Bayesian classifier can be used. Random Forest proves useful in case of the unbalanced dataset. 
4. Neural Collaborative Filtering 
Neural networks are being used increasingly for collaborative filtering. Xiangnan HE et al[8] explored the use of neural networks for collaborative filtering.In this use, User-item interaction matrix data is treated as an implicit data. While observed entries at least reflect users’ interest on items, the unobserved entries can be just missing data and there is in most cases, natural scarcity of negative feedback. The recommendation problem with implicit feedback is formulated as the problem of estimating the scores of unobserved entries. Matrix factorization is used to estimate predicted output. The missing data is replaced by using this input. Filled input space is then passed to a multi-layer perceptron network to estimate ratings for an active user. 
Youtube recommendation system[10] is a deep neural network based recommendation system. 
This system employs two separate neural networks. The first network is used in candidate generation while another network that follows is used for ranking. The candidate generation network only provides broad personalization via collaborative filtering. This network uses personal history of a user to suggest few hundred out of a huge corpus of videos to the user. The second filter uses a rich set of features including a description of a video and user to rank videos in suggestion.Collaborative filtering approach performs better than simple content-based recommendation technique when huge user data is available. This method does not prove helpful in case of very sparse data. This is known as ‘cold start problem’. 
3. Hybrid Recommendation System 
The hybrid recommendation system is a combination of collaborative and content-based filtering techniques. In this approach, content is used to infer ratings in case of the sparsity of ratings. This combination is used in most recommendation systems at present. Netflix movie recommendation system is an example of hybrid recommendation system.Scikit-surprise package is in python is useful to implementation of recommendation system. Since there is no single ‘correct’ way to implement recommendation system, various machine learning algorithms for classification can be explored. Typical benchmarks are available on for comparison[9]. 
How to measure success rate / Efficiency? 
Statistical accuracy metrics are used to evaluate the accuracy of a filtering technique by comparing the predicted ratings directly with the actual user rating. Most commonly used statistical metrics are:1.Mean Absolute Error (MAE)2. Root Mean Square Error (RMSE) and3. ROC / PR curve 
MAE :Mean absolute error is most popular metrics to measure the efficiency of recommendation system. It is a measure of the deviation of recommendation from user’s specific value. 
Here yi is actual user rating and yi(cap) is a predicted rating. 
RMSE :Root mean square error measures average magnitude or error. 
Precision, Recall measures :Precision recall values are useful for determining accuracy of a prediction system in case of unbalanced datasets. 
References :1. Content-based recommendation systems http://www.fxpal.com/publications/FXPAL-PR-06-383.pdf2. Recommender Systems http://recommender-systems.org3. Using collaborative filtering to weave an information Tapestry. https://www.ischool.utexas.edu/~i385d/readings/Goldberg_UsingCollaborative_92.pdf4. Item-based collaborative filtering http://www.cs.carleton.edu/cs_comps/0607/recommend/recommender/itembased.html5. Collaborative filtering tutorial: https://www.cs.cmu.edu/~wcohen/collab-filtering-tutorial.ppt6. Learning Collaborative Information Filters http://www.ics.uci.edu/~pazzani/Publications/MLC98.pdf7. Neural Collaborative Filtering [Xiangnan He et al] https://arxiv.org/pdf/1708.05031.pdf8. Surprise library http://surpriselib.com/9. Recommendation systems: principles, methods, and evaluation. https://www.sciencedirect.com/science/article/pii/S111086651500034110. Deep Neural networks in YouTube recommendations. https://static.googleusercontent.com/media/research.google.com/ru//pubs/archive/45530.pdf 
Written by 
Written by",Kunal Deshmukh,2019-03-25T06:15:28.578Z
Understanding BERT Transformer: Attention isn’t all you need | by Damien Sileo | synapse_dev | Medium,"BERT is a recent natural language processing model that has shown groundbreaking results in many tasks such as question answering, natural language inference and paraphrase detection. Since it is openly available, it has become popular in the research community. 
The following graph shows the evolution of scores for GLUE benchmark — the average of scores in various NLP evaluation tasks. 
While it’s not clear that all GLUE tasks are very meaningful, generic models based on an encoder named Transformer (Open-GPT, BERT and BigBird), closed the gap between task-dedicated models and human performance and within less than a year. 
However, as Yoav Goldberg notes it, we don’t fully undestand how the Transformer encodes sentences. 
[Transformers] in contrast to RNNs — relies purely on attention mechanisms, and does not have an explicit notion of word order beyond marking each word with its absolute-position embedding. This reliance on attention may lead one to expect decreased performance on syntax-sensitive tasks compared to RNN (LSTM) models that do model word order directly, and explicitly track states across the sentence. 
Several articles delve into the technicalities of BERT. Here, we will try to deliver some new insights and hypotheses that could explain BERT’s strong capabilities. 
The way humans are able to understand language has been a long-standing philosophical question. In the 20th century, two complementary principles shed light on this problem: 
Parsing hierarchical structures and deriving meaning from their components recursively until sentence level is reached is an appealing recipe for language understanding. Consider the sentence “Bart watched a squirrel with binoculars”. A good parsing component could yield the following parse tree: 
The meaning of the sentence could be derived from successive compositions (composing “a” and “squirrel”, “watched” with “a squirrel”, “watched a squirrel” and “with binoculars”) until the sentence meaning is obtained.Vector spaces (as in word embeddings) can be used to represent words, phrases, and other constituents. Composition could be framed as a function f which would compose (“a”,”squirrel”) into a meaningful vector representation of “a squirrel” = f(“a”,”squirrel”). [Baroni 2014] 
However, composition and parsing are both hard tasks, and they need one another. 
Obviously, composition relies on the result of parsing to determine what ought to be composed. But even with the right inputs, composition is a difficult problem. For example, the meaning of adjectives changes depending on the word they characterize: the color of “white wine” is actually yellow-ish while a white cat is actually rather white. This phenomenon is known as co-composition. [Pustejovsky 2017] 
A broader context can also be necessary for composition. For instance, the way the words in “green light” should be composed depends on the situation. A green light can denote an authorization or an actual green light. The meaning of some idiomatic expressions requires a form of memorization rather than composition per se. Thus, performing those compositions in the vector space requires powerful nonlinear functions like a deep neural network (that can also memorize [Arpit 2017]). 
Conversely, the parsing operation arguably needs composition in order to work in some cases. Consider the following parse tree of the same previous sentence “Bart watched a squirrel with binoculars”. 
While it is syntactically valid, this parse leads to an odd interpretation of the sentence where Bart watches (with his bare eyes) a squirrel holding binoculars. However, some form of composition must be used in order to figure out that a squirrel holding binoculars is an unlikely event.More generally, many disambiguations and integrations of background knowledge have to go on before the appropriate structures are derived. But this derivation might also be achieved with some forms of parsing and composition. 
Several models have tried to put the combination of parsing and composition in practice [Socher 2013], however they relied on a restrictive setup with manually annotated standard parse trees, and have been outperformed by much simpler models. 
We hypothesize that Transformers rely heavily on these two operations, in an innovative way: since composition needs parsing, and parsing needs composition, Transformers use an iterative process, with successive parsing and composition steps , in order to solve the interdependence problem. Indeed, Transformers are made of several stacked layers (also called blocks). Each block consists of an attention layer followed by a non-linear function applied at each token.We will try to highlight the link between those components and the parsing/composition framework. 
In BERT, an attention mechanism lets each token from the input sequence (e.g. sentences made of word or subwords tokens) focus on any other token. 
For illustration purposes, we use the visualization tool from this article to delve into the attention heads and test our hypothesis on the pre-trained BERT base uncased model. In the following illustration of an attention head, the word “it” attends to every other token and seems to focus on “street” and “animal”. 
BERT uses 12 separate attention mechanism for each layer. Therefore, at each layer, each token can focus on 12 distinct aspects of other tokens. Since Transformers use many distinct attention heads (12*12=144 for the base BERT model), each head can focus on a different kind of constituent combinations. 
We ignored the values of attention related to the “[CLS]” and “[SEP]” token. We tried using several sentences, and it’s hard not to overinterpret results, so you can feel free to test our hypothesis on this colab notebook with different sentences. Please note that in the figures, the left sequence attends to the right sequence. 
In the second layer, attention head #1 seems to form constituents based on relatedness. 
More interestingly, in the third layer, head #9 seem to show higher level constituents : some tokens attend to the same central words (if, keep, have) 
In the fifth layer, the matching performed by the attention head #6 seem to focus on specific combinations, notably involving verbs. The special tokens like [SEP] seem to be used to indicate the absence of matching. This could allow attention heads to detect specific structures where a composition would be appropriate. Such consistent structures could be fed to the composition function. 
Arbitrary trees could have been represented with successive layers of shallow parsing, as shown in the next figure: 
We didn’t find such clear cut tree structures upon examination of BERT attention heads, but still, it is possible for Transformers to represent them. We note that, since the encoding is performed simultaneously on all layers, it is hard to correctly interpret what BERT is doing. The analysis of a given layer only makes sense with respect to the next and previous layers. The parsing is also distributed across attention heads. 
The following illustration shows what BERT attention more realistically looks like for two attention heads: 
However, as we have seen previously, the parse tree is a high level representation, and it might build upon more complex “rhizomatic” [Deleuze 1987] structures . For instance, we might need to find out what pronouns refer to in order to encode the input (coreference resolution). In other cases, a global context can also be required for disambiguation. 
Surprisingly, we discovered an attention head (layer 6 head #0) that seems to actually perform coreference resolution. And, as noted here, some attention heads seem to feed a global context to each word (layer 0 head #0). 
In each layer, the outputs of all attention heads are concatenated and fed to a neural network that can represent complex nonlinear functions (needed for an expressive composition). 
Relying on structured input from the attention heads, this neural network could perform various compositions. In the previously shown layer 5, the attention head #6 could guide the model to perform the following compositions: (we, have), (if, we), (keep, up) (get, angry). The model could combine them non-linearly and return a composed representation. Thus, the many attention heads can be used as tools that pave the way for composition. 
While we didn’t find attention heads focusing on more consistent combinations such as adjective/nouns, there might be some common grounds between verb/adverb composition and other compositions that the model leverages. 
There are many possible relevant compositions (words-subwords, adjective-noun, verb-preposition, clause-clause). To go even further, we can see disambiguation as a composition of an ambiguous word (e.g. bank) with its relevant context words (e.g. river or cashier). Integration of background common sense knowledge related to concept given a context could also be performed during a composition phase. Such disambiguation could also occur at other levels (e.g. sentence-level, clause-level). 
Besides, the composition might also be involved in word order reasoning. It has been argued that the positional encoding might not be sufficient to encode order of the words correctly. However, the positional encoding is designed to encode coarse, fine, and possibly exact positions of each token. (The positional encoding is a vector that is averaged with the input embedding in order to yield a position-aware representation of each token in the input sequence). Therefore, based on two positional encodings, the non-linear composition could theoretically perform some relational reasoning based on word relative positions. 
We hypothesize that the composition phase also does the heavy lifting in BERT natural language understanding: Attention isn’t all you need. 
We proposed an insight on the inductive bias of Transformers. However, we have to keep in mind that our interpretation might be optimistic regarding the capabilities of Transformer. As a reminder, LSTM were shown to be able to deal implicitly with tree structures [Bowman 2015] and composition [Tai 2015]. But LSTM has some limitations, some being due to vanishing gradient [Hochreiter 1998]. Thus, further work is needed to shed light on the limitations of the transformer. 
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding [Devlin 2018]Attention Is All You Need [Vaswani 2017]Assessing BERT’s Syntactic Abilities [Goldberg 2019]Compositionality [Szabó 2017]Frege in Space [Baroi, 2014]Co-compositionality in Grammar [Pustejovsky 2017]A Closer Look at Memorization in Deep Networks [Arpit 2017]Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank [Socher 2013]Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks [Tai 2015]Tree-structured composition in neural networks without tree-structured architectures [Bowman 2015]A Thousand Plateaus [Deleuze 1987]The vanishing gradient problem during learning recurrent neural nets and problem solutions [Hochreiter 1998]Deconstructing BERT, Part 2: Visualizing the Inner Workings of Attention [Vig 2019] 
Written by 
Written by",Damien Sileo,2019-03-04T08:11:25.895Z
"AI in Cyber Security, an oversimplified introduction | by Sukant Khurana | Medium","Yousuf Rashid Uz Zaman (yousufruz@gmail.com), Apoorv and Sukant Khurana 
Cybersecurity is the main concern for today’s digital world, there are still uncertainties about the impact of AI. Not just the corporates but also the government sectors are also trying to master AI and Machine Learning for the protection of data and creating more opportunities. With the advancements in AI, many companies have started to use it as a powerful weapon against the significant cyber-attacks and trespasses. AI allows us to automate the detection of threat and combat even without the involvement of the humans, powering your data to stay more secure than ever. Malware and virus attacks are common in the cyber world. Highly skilled hackers know how to perpetrate the right attacks, leaving the company’s cyber cell no clues about what happened. AI comes to the rescue of cyber cell but AI learns from data, so it is a constant arms race. AI lets the defenders protect and stay strong even against the series of attacks. 
The other two related areas of cybersecurity that can be positively influenced by AI are the password protection and authenticity detection systems. Since passwords are much vulnerable, AI is implemented frequently in this segment. AI is being used for the detection of physical characteristics like fingerprints, retina scans etc., making the systems more safe and secure than ever. Current research in the public domain is limited to white hat hackers employing machine learning to identify vulnerabilities and suggest fixes. That said, it is always an arms race. Nothing, absolutely nothing is unbreakable or idiot proof. 
One Murphy’s law corollary claims “Nothing is foolproof to a sufficiently talented fool” and “If you make something idiot-proof, someone will just make a better idiot”. Similarly, Douglas Adams in Mostly Harmless wrote, “a common mistake that people make when trying to design something completely foolproof is to underestimate the ingenuity of complete fools” 
Coming back from the tangent of idiot proofing to arms race, at the speed AI is developing, however, it won’t be long before we see attackers using these capabilities on a mass scale, if they don’t already. Analyzing large data sets helps attackers prioritize their victims based on online behavior and estimated wealth. Predictive models can go further and determine the willingness to pay the ransom based on historical data and even adjust the size of pay-out to maximize the chances and, therefore, revenue for cyber criminals. 
Written by 
Written by",Sukant Khurana,2019-12-09T13:30:58.193Z
Reviewing UX Portfolios: 4 High-Risk Hiring Mistakes | by Jared M. Spool | Medium,"A UX portfolio seems like a no-brainer. You ask the candidate to give it to you. You review it. Then, if you like what you see, you interview them further and maybe you’ll offer them the opportunity to join your team. 
However, there’s more to reviewing a portfolio than that. And many teams make serious mistakes in the process. 
Hiring team members is hard enough. Let’s not make it harder. 
It’s risky to push a candidate away who, had we hired them, would’ve done a great job for us. Losing a viable candidate means we’ll need to spend more time finding and interviewing other candidates to fill the job. 
When we take too long to review their portfolio and give them feedback, they may decline to continue in our hiring process or refuse our job offer. This makes the hiring process more difficult. 
It’s even worse if we hire someone who we thought looked great on paper and interviewed well, but can’t do the work we need them to do. We then find ourselves with a messy management and morale problem. It’s hard to work with a team member who is ill-equipped for the position, no matter how otherwise awesome they are. 
Hiring mistakes put our team’s growth at risk. They can be avoided. Here are four hiring mistakes that teams make when reviewing UX portfolios, and how to avoid them. 
Yogi Berra once said, “If you don’t know where you’re going, you’ll never get there.” So many hiring managers and team members open up a portfolio without knowing what they need to learn from it. 
Treating a UX portfolio like a coffee-table picture book turns out to be a sure way to get a wrong understanding of the candidate. We can’t just flip through the portfolio and smile at the pretty pictures. 
We must have specific objectives and assessment criteria for assessing each candidate’s portfolio. Otherwise, we’ll risk making judgment calls that are more about the aesthetics of the work and less about the capability of each candidate. 
When we’re reviewing a portfolio, we’re on a hunt for direct evidence of the candidate’s comparable experience. Comparable experience is any work they’ve done in their previous jobs (or in school, when we’re considering an early-career candidate who is low on work experience) that matches the work we’ll need them to accomplish. We use the entire interview process, to collect all the evidence we can uncover that the candidate has similar work before. If they’ve done a great job in the past, there’s a good chance they’ll do a good job again, this time for us. 
The goal of reviewing a candidate’s portfolio is simple: Can we tell if there’s enough evidence here to suggest this candidate should be prioritized higher in our hiring process than other candidates? If we find lots of comparable experience evidence in the portfolio, we want to quickly bring this person in for interviews. If we don’t see any evidence, we might hold up and talk to other candidates first. 
To know what evidence to look for, we need to know what the new hire’s work will be. Will they be creating wireframes for a new design? Conducting research to identify user needs? Guiding a team to deliver a finely-designed mobile app? Rolling out a large design system to dozens of products? 
No two designers will do the same job. Even if they work on the same project, they’ll likely divide up the work based on their strengths and experience. Having a clear picture of what we hope our new hire will accomplish in, say, their first year, will let us focus our review of every candidate’s portfolio. 
When working with teams, we start them with a simple exercise of writing a thank you note to their future new hire. This is an easy way to start the conversation about what the new position will entail. 
To know what to look for in candidate portfolios, the thank you note won’t be enough. For that, we create a much more detailed performance profile. The profile describes the objectives we want our new hire to achieve in their first year. By listing out the objectives, we can create a wishlist of what we want to see in every candidate’s UX portfolio. 
We know that résumés don’t tell us a lot about a candidate. When a candidate gives us a portfolio, they get a richer opportunity to communicate how they may fit the position we’re looking to fill. 
Unfortunately, many candidates won’t have a portfolio ready when we start accepting applications. That’s not a reason to immediately disqualify them. There are several reasons why someone who could be a great fit might not have a portfolio ready and up-to-date. 
When we’re hiring, our goal is to identify those candidates who have the ideal skills, knowledge, and experience for the work we need done. Often, these candidates have gained their expertise because that’s what their current job is. And they may be very happy where they currently are. 
These candidates aren’t looking for a new job. At least, not until they saw our open position. Suddenly, they’re intrigued. 
Our job may be attractive because it gives that person a growth opportunity they wouldn’t get if they stayed where they currently are. The growth and challenges offered in our job look very appealing. 
We call these folks passive candidates since they aren’t actively looking for a new job. Passive candidates are often the best-suited candidates, because the work they do every day is the work we need doing. 
It’s likely a passive candidate doesn’t have a prepared UX portfolio ready. After all, they weren’t thinking about leaving their current job. Keeping a portfolio up to date hasn’t been high on their priority list. 
When we insist they give us a portfolio as part of their job application, they may decide it’s not worth the effort to apply. For passive candidates, we have to remove any friction in the hiring process. They’re happy in their current job and will drop out of consideration at the slightest hint of inconvenience. 
That would be a shame, because this candidate could be an ideal fit. We’ll never know, because we never had a chance to explore the job with them. 
Active candidates may also have problems producing a UX portfolio for consideration. Even though they’re actively looking for a new job, it may be hard for them to show what they’ve done. 
Many companies require every employee to sign a non-disclosure agreement when they start working. In those agreements, it’s common for the employer to refuse to give permission to show any of the work they’ve done. 
If you can’t show work you’ve done, what do you put into your portfolio? This becomes a difficult problem for those candidates who have major work covered under non-disclosure. 
(Pushing a candidate to violate their non-disclosure has real ethical issues. They made a promise and it’s not cool to ask them to break it, even when it’s inconvenient for your hiring process.) 
Even when a candidate has permission to show their work, for many UX professionals, there may not be an easy way to capture the essence of their achievements. The portfolio may be the wrong way for them to show their comparable experience. 
UX portfolios are direct descendants of portfolio books used in graphic design and industrial design. In those cases, the portfolio was intended to show the designer’s body of work. Since the work was mostly visual, their portfolios could exist solely as pictures of each design they created. 
In contrast, much UX work isn’t visual. It’s about communication and collaboration. Even when we create design artifacts, they are not very visually interesting. 
For example, we may need to hire someone to manage our content strategy, starting with a content audit and then building up a content model of our future publications across multiple platforms. While there are many artifacts produced, they are mostly word processing documents and spreadsheets, with the occasional slide deck. Not the kind of stunning visuals that will dazzle someone looking through the portfolio. 
Content strategy, information architecture, user research, and many types of interaction design are hard to capture in a portfolio that is mostly visual. (Exactly how do you show the designs of a voice user interface in a portfolio?) A list of web URLs in a content inventory won’t tell the candidate’s story about their work experience in any compelling way. 
Most UX work is collaborative. For many UX jobs, we’re very interested in a candidate’s ability to collaborate. 
Yet, a portfolio only shows an individual’s direct — often solo — contribution to a project. It’s not easy to represent a team effort within a UX portfolio, especially when trying to highlight the specific capability of the portfolio’s owner. How does a candidate show work that dozens of people contributed to, without laying claim to any work done by other people? 
Similarly, what should a design manager put into their portfolio? They are directly responsible for the work of all the people who report to them, should they show the work of their team in their own portfolio? 
If they don’t show the work of their direct reports, they might not have anything to put in the portfolio. Their comparable experience might be how they rallied team resources to produce great work. Yet, there’s nothing in that work for the candidate to point to and say, “I did that part.” 
They may be our perfect new manager, applying their very relevant skills, knowledge, and experience at the challenges our team faces. Their portfolio can’t easily show us any of that. 
It’s critical we don’t exclude an ideal candidate just because they couldn’t show us their work inside a UX portfolio. That’s an expensive mistake we can easily avoid. 
Even though not all candidates will have portfolios, we can encourage those that have one ready to submit it. However, we should give them hints as to what we want to see in it. 
After all, we’re on a hunt for specific comparable experience. We want to see the evidence that tells us the candidate has what’s needed for the job. 
Collecting evidence is a collaboration between the interview team and the candidate. They work together so the candidate has every opportunity to share their relevant comparable experience. 
Part of that collaboration starts with us saying what we’d like to see. In the job ad and wherever we ask candidates to apply, we’ll list out how we’ll assess if they’re a match for the position. For the applicants, this makes applying much easier, because they have an idea exactly what you’re looking for. 
For example, here’s an excerpt from a team hiring looking for someone to manage and deliver a new multi-product design system: 
We don’t require a portfolio, but we’d love to see one if you have it. In this position, you will be supporting the rollout of a design system. 
We’d love to see any case studies you have about the design systems you’ve rolled out. We are particularly interested in the scale of the system (how many teams and products did it support) and the timeline of the work. What challenges did you encounter? How did you overcome them? What might you do differently next time, based on what you learned? 
It’s ok if you don’t have a portfolio ready. Please apply without it. We’re excited to talk with you about your design system work, whether you have a portfolio or not. 
A few years back, I was sitting next to a hiring manager as we were reviewing job applications for new positions on his team. As we brought up one candidate’s portfolio, he leaned forward and proclaimed “Oh my god. They’ve implemented their portfolio in WIX. That’s an immediate No.” 
At first, I thought he was joking. He wasn’t. 
He was seriously rejecting this candidate because of the tools they chose to implement their portfolio. (Maybe if the position was for a front-end designer, there could be an argument that the portfolio’s implementation is important to consider. But the objectives for this position didn’t require any coding. It shouldn’t have been a consideration.) 
The purpose of any candidate’s UX portfolio is to reveal that candidate’s comparable experience. But an absence of relevant evidence doesn’t mean the candidate doesn’t have the skills, knowledge, and experience we’re seeking. 
A good portfolio gives us the information and confidence we need to move a high-potential candidate to the front of our pipeline. That way we can consider them faster and possibly get them an offer sooner, since they’re demonstrating they have what we need for the job. 
Our goal is to fill an open position. The faster we fill the position with a qualified candidate, the sooner we have a new team member contributing to our efforts. 
Yet, a UX portfolio that is missing evidence shouldn’t reflect poorly on the candidate’s ability or value. And if they have work that isn’t relevant or they implement work in a way that isn’t what we think is right, we shouldn’t eliminate them from consideration. 
Through the rest of the interview process, we can ask that candidate for more examples of their comparable experience. A portfolio will never provide enough evidence to tell us this is a candidate we should make the offer to. Similarly, it should, on its own, never disqualify a candidate from further consideration. At best, it just works as an accelerant, helping us quickly see the most promising candidates amongst all that applied. 
This article was originally published in our new UX Strategy with Jared Spool newsletter. If you’re passionate about driving your organization to deliver better-designed products and services, you’ll want to subscribe.Subscribe here. 
At our 2-day Creating a UX Strategy Playbook workshop, we look at every aspect of a solid UX strategy, including how we’ll build our team to be highly effective. You’ll balance hiring new members against training up the team you have, all to achieve the major objectives to grow your organization’s design capability. 
Bring your team to our next workshops. You’ll leave with an action plan that will drive your organization to deliver better-designed products and services immediately. Register today. 
Written by 
Written by",Jared M. Spool,2019-11-21T17:12:07.814Z
Deep learning: the final frontier for signal processing and time series analysis? | by Alexandr Honchar | Medium,"Hi everyone!People use deep learning almost for everything today, and the “sexiest” areas of applications are computer vision, natural language processing, speech and audio analysis, recommender systems and predictive analytics. But there is also one field that is unfairly forgotten in terms of machine learning — signal processing (and, of course, time series analysis). In this article, I want to show several areas where signals or time series are vital, after I will briefly review classical approaches and will move on to my experience with applying deep learning for biosignal analysis in Mawi Solutions and for algorithmic trading. I already gave a couple of talks on this topic in Barcelona and Lviv, but I would like to make the materials a bit more accessible. 
I am sure, that not only people working with time series data will benefit from this article. Computer vision specialists will learn how similar their domain expertise is to signal processing, NLP people will get some insights about sequential modeling and other professionals can have interesting takeaways as well. Enjoy! 
First of all, planet Earth and space bodies around it are sources of signals — we measure amount and intensity of sunspots, temperature changes in different regions, wind speed, asteroids speed and a lot of other things: 
Of course, most common examples of time series are the ones related to business and finance: stock prices and all possible derivatives, sales in big and small business, manufacturing, websites activity, energy production, political and sociological factors and many others: 
We can’t also forget about humans as great source of biosignals: brain activity (EEG), heart activity (ECG), muscle tension (EMG), data from wearables like pulse, activity based on accelerometer, sleep, stress indices — all these vital signals are getting very popular today and they need to be analyzed: 
Surely, there are also other examples like data from the vehicles, but I hope you can already see a very large range of applications. I personally am very tightly involved into biosignal analysis, in particular, cardiograms — I’m responsible for ML in Mawi Band — a company that developed our own medical grade portable cardiograph that measure first lead ECG. 
The coolest part is that ECG data shows not just the state of your heart as it is — you can track emotional state and stress level, physical state, drowsiness and energy, the influence of alcohol or smoking on your heart and a lot of other things. If you’re doing research and you need to gather biomedical data for further analysis of your hypotheses (even the craziest ones), we launched a platform to democratize data collection and analysis process. Check it out: 
Before machine learning and deep learning era, people were creating mathematical models and approaches for time series and signals analysis. Here is a summary of the most important of them: 
Deep learning is easy. From a practical point of view, you just need to stack layers in your favorite framework and be careful about overfitting. But everything is a bit more complex than that. Four years ago great researches thought of if this stacking layers is the best we can do not for general AI of course, but at least for the signal processing? Four years later we might have an answer: neural networks are extremely powerful tools for all the domains I showed in previous sections, they win Kaggle competitions like sales forecasting and web traffic forecasting, they surpass human accuracy in biosignals analysis, they trade better than us as well. In this section, I want to talk about main deep learning approaches that give state of the art results and why they work so well. 
The first thing that comes to our mind when we talk about any sequence analysis with neural networks (from time series to language) is the recurrent neural network. It was created especially for sequences with the ability to maintain its hidden state and learn dependencies over time, is Turing complete and is able to deal with sequences of any length. But as it has been shown in recent research, we barely use these benefits in practice. Moreover, we encounter numerous problems that won’t make them able to work with too long sequences (and that’s what we have with streaming signal processing with the high-frequency sampling rate, e.g. 500–100 Hz), for more details check the reading list in the conclusions of this article. From my personal experience, recurrent nets are good only when we deal with rather short sequences (10–100 time steps) with multiple variables on each time step (can be multivariate time series or word embeddings). In all other cases we better go with the next: 
CNNs are great for computer vision, because they’re able to catch the finest details (local patterns) in images or even 3D volumetric data. Why we don’t apply them for even more simple 1D data? And we definitely should do this, taking into account, that all we need to do — to take the open sourced state of the art deep learning architecture like ResNet or DenseNet and replace 2D convolutions with 1D ones (no joke!). They show great performance, are fast, can be optimized in parallel, work well both for classification and regression, since the combination of all local patterns in time series is what defines them. I have benchmarked them a lot of times and they’re mostly superior to RNNs. I can only add, that at the moment when I am working with signals I have two main baselines: logistic regression and 2–3 layers CNN. 
RNNs and CNNs are something that you might expect, but let’s consider more interesting models. Local patterns are good, but what if we still imply temporal dependence of these patterns (but RNNs over raw signal is not the best option)? We just should remember, that convolutional nets are good in dimension reduction using different pooling techniques and over these reduced representations we already can run recurrent neural nets and their “physical meaning” will be “checking dependency between local patterns over time”, which definitely has the sense for some applications. The visual illustration of the concept you can see above. 
What if we really want to avoid unnecessary difficulties related to recurrent neural networks? Is there a way to “emulate” somehow dependence from last N time steps and have this N quite large? Here is where WaveNet and similar architectures are coming into the game. More generally we can call them autoregressive feedforward models that model last N steps using dilated convolutions. You can check why they’re so good for sequential modeling in the links at the end of this article, but I also want to add, that the trend of switching from recurrent neural networks to (autoregressive) feedforward models touched not only speech recognition or time series analysis, but NLP as well (check Transformer architecture by Google) 
Apart from classification and regression using RNNs / CNNs / Autoregressive models we’re also interested in clustering time series into meaningful groups. We can do this using combination of specific for time series distances (like above mentioned DTW) and metric-based clustering algorithms like K-Means, but it’s rather slow and not optimal approach. We would like to have something that can work with signals of different lengths, but much more efficient. Of course, we can ask neural networks to provide us with an embedding space where we will perform clustering, for instance, with autoencoders. After we can cluster signals in that space or augment autoencoder with auxiliary tasks as it’s done in this research. 
Another important and popular task that has to be solved based on sequential data (often streaming data) is anomaly detection — finding situations in data that are “not what we’re expected to see at this moment”. Very often this problem can be solved using some threshold methods and distances (statistical distances sometimes), but not always we can rely on the Euclidean distance between time series time steps. As in previous task (clustering), we can rely on deep learning to embed our data into the new space with autoencoders or we can use GANs (generative adversarial networks) as anomaly detectors with exploiting discriminator network as anomaly detector (check more details and code here) 
Sometimes you already have your well defined mathematical model or features that clearly represent your data and it shows relatively good performance. But you still want to use the power of deep learning and combine your expert model with something that is learned by neural nets. You can learn from Uber Labs experience on how to make hybrid solutions. 
The idea is to combine a set of features from the left side of a picture above and embedding from the autoencoder from the right side and after train a joint model, where you can even control the importance of different feature and track what influence the final performance more. 
What are the main takeaways? 
Home reading list: 
The question from the header remains open. As for me, taking into account all modern time series theoretical foundations (from time series models to dynamical systems) we have neural nets that can efficiently approximate any of them. Why we are haven’t done this yet? It’s another question, but right now you can start implementing deep learning models for your own tasks, replacing your old algorithms or augmenting them with deep learning embeddings. I really hope you learned something new from this article. Stay tuned! 
P.S.Follow me also on Facebook for AI articles that are too short for Medium, Instagram for personal stuff and let’s connect on Linkedin! 
Written by 
Written by",Alexandr Honchar,2018-11-10T07:01:01.268Z
"The A to Z of Cyber Security. From BEC scams to DDoS attacks, and… | by Dick O'Brien | Threat Intel | Medium","Authentication is one of the fundamentals of cyber security and a core requirement for any kind of transaction or for access to private data. The standard form of authentication is what’s known as “single factor”, i.e. a username and password. However, the rise of online fraud has severely tested its limits. Usernames are easily guessed (usually a person’s name or email address) and, all too often, so are passwords (since many people pick weak, easy-to-guess passwords such as “abc123”, “123456” and… ummmm… “password”). Even a strong password has its limitations, since it can be stolen, either through a data breach or phishing (see below). As a result, we’ve seen a move towards more robust forms of authentication, including two factor authentication (2FA) where the user has to input an additional, private piece of information (such as a pin code) and biometric authentication, where a fingerprint or facial recognition (such as in the new iPhone X) is employed. 
As a form of fraud, Business Email Compromise (BEC) scams are among the most lucrative for cyber criminals. According to the FBI, roughly US$5 billion was lost to BEC scams between October 2013 and December 2016. Also known as “whaling” or “CEO fraud”, BEC scams don’t require too much technical sophistication and instead rely on social engineering. How do they work? Scammers will usually send an email purporting to come from the CEO, asking the recipient to initiate an urgent wire transfer. The emails are usually sent to a senior member of the finance department, often the CFO. If the recipient falls for the email, they could be duped into sending huge sums to accounts controlled by the scammers. Losses of hundreds of thousands of dollars have frequently been seen and, in some cases, millions have been stolen. 
The arrival of cryptocurrencies such as Bitcoin has had a major impact on cybersecurity. Cryptocurrencies have quickly become the default currency on the cyber underground, since they’re easily traded and offer a greater degree of anonymity than traditional financial systems. Their arrival played a central role in the growth of ransomware, providing a payment mechanism that was easily automated and hard to trace. A surge in the value of many cryptocurrencies over the past year has prompted a growing number of cyber criminals to get involved in it by distributing crypto-mining malware and effectively outsourcing their mining to unsuspecting victims. 
One of the biggest causes of online disruption is denial of service attacks, where websites or the networks of entire organizations are knocked offline, usually by flooding them with traffic. The most common form of attack these days is the Distributed Denial of Service (DDoS) attack, where attackers use botnets of thousands of infected devices to generate the sheer volume of traffic needed to knock even the most well-resourced organizations offline. DDoS attacks are carried out for a variety of reasons. In some cases, it’s simple malice, but in others the attackers are seeking to extract money from the targeted organization in exchange for calling off the attacks. Other motives may revolve around “hacktivism”, where an organization is attacked for political reasons. 
Exploit kits are one of the methods cyber criminals use to infect people with malware. What are they? Essentially they allow attackers to use a website to infect unsuspecting visitors. They work by exploiting vulnerabilities in software in order to install malware. Usually, exploit kit operators will compromise third-party web servers and inject malicious code into the web pages hosted on them. This code directs browsers to the servers hosting the exploit kit itself. The exploit kit will analyze the visiting computer in order to identify the most appropriate exploit for the victim’s computer. This exploit will then be used to attempt to deliver malware to their computer. 
The success of any cyber attack largely depends on the likelihood of discovery, and it goes without saying that attackers put considerable time and effort into minimizing that risk. The more evidence you leave on a computer, the greater that risk, and one of the main kinds of evidence we see are files installed on a computer. One tactic we’ve seen growing in popularity in recent years is fileless threats; malware that never writes itself to the hard disk but instead remains resident in memory, which could evade the attention of security software that only scans the hard disk. They aren’t undetectable though. Our way of combating this threat is memory exploit mitigation (MEM) techniques in our products, which can proactively block remote code execution exploits (RCE), along with our heuristic based memory scanning, which can detect memory-only threats. 
The divide between legitimate software and malware is often blurred. Grayware occupies the murky middle ground. Grayware is applications that may not have any recognizable malware concealed within them but can nevertheless be in some way harmful or annoying to the user. It could track their web browsing habits, serve up unwanted ads or dial up premium rate numbers. In many cases, grayware authors often maintain a veneer of legitimacy by outlining the application’s capabilities in the small print of the software license agreement. 
We’ve all heard of websites or companies being hacked. But what does “hacking” actually mean? In its original incarnation, hacking simply meant finding new ways of doing things with computers and software. That could be good (improving the performance of something) or bad (finding a way onto a computer you shouldn’t have access to). Nowadays hacking tends to be employed as a catch-all phrase for all sorts of malicious activity from data breaches, to web page defacement, to bank fraud. Often it doesn’t even involve any programming skill on the part of the attacker. It’s even used as an excuse for embarrassing behavior. Post something you regret saying on social media? “Sorry folks, my account was hacked”. 
There was once a time when it was just computers that were connected to the internet. Then it was cellphones. Now, it’s pretty much…. everything. Household appliances, security systems, home heating and lighting, and even cars are all becoming Internet-enabled. That makes the Internet of Things (IoT) one of the new frontiers for cyber security. We’ve long heard predictions about ransomware on home appliances and hijacking of cars, but threats against IoT have now gone from theory to reality. At the moment, attacks are less elaborate than some of the predicted scenarios. By and large, attackers have taken advantage of poor security on IoT devices to add them to botnets (the most notable of which has been Mirai), which have been mainly used to perform DDoS attacks. 
What’s this doing on the list you ask? JavaScript is a widely used scripting language and is, in itself, no way malicious. However, in recent times, JavaScript has been by far the most popular file format for malicious email attachments. If you’re lured into opening it, the script will run and download malware to your computer. In some cases, these JavaScript downloaders are easy to spot, since they’ll bear the .JS extension, but attackers will often attempt to obfuscate the file type by adding additional extensions (e.g. invoice.docx.js) or by hiding them in a zipped archive. 
Malware comes in lots of varieties. One of the oldest is the keylogger, which does what it says on the tin, recording every keystroke on an infected computer. It remains one of the most effective ways of stealing information, including user names, passwords, and private communications such as emails and instant messages. Keyloggers are often included as features in financial or spying Trojans and can be doubly effective information stealing tools when combined with screen grabbers, which take snapshots of what’s on the user’s screen. 
Living off the Land describes a cyber attack strategy that eschews traditional tools such as malware and zero-day vulnerabilities in favor of alternatives, such as using operating system features, legitimate tools, and cloud services, to compromise networks. Why would attackers do this? Living off the Land can make attacks more difficult to detect, since it’s harder to spot malicious use of legitimate tools compared to the presence of malware. It’s something we’ve seen more of in recent years as attackers are forced to work harder to stay under the radar. 
A man-in-the-middle attack (MITM for short) is a tactic used to intercept and/or alter supposedly private or secure communications. Generally speaking, the attacker will need to either exploit a vulnerability or use some kind of malware to perform a successful MITM attack. For example, the reason why the FREAK vulnerability, which was discovered in 2015, was so serious was that it facilitated MITM attacks against secure connections. Financial Trojans frequently use a variant of MITM called man-in-the-browser (MITB) to intercept secure banking sessions and alter pages in order to dupe victims into disclosing their banking credentials. 
The single biggest distribution channel for malware, scams, and other kinds of online threats is spam email. The major cyber crime groups generally outsource their distribution to specialist outfits who control vast botnets of thousands of infected computers that power their spamming operations. A small handful of spam botnets dominate malware distribution and, for the past few years, the Necurs botnet has been the daddy of them all. Nothing illustrates this more than the fact that when Necurs briefly went offline for a few months last year, the email malware rate plummeted. In December 2016, the last month of Necurs operations before the pause, one in 98 emails blocked by Symantec contained malware. In January 2017, that rate dropped to one in 772. 
As the number of accounts we have that require authentication grows, the challenge of keeping them secure becomes greater. In some cases, a simple username and password combination is deemed insufficient and extra layers of security are applied, such as two-factor authentication (2FA). One approach to 2FA is the one-time password (OTP), which is only valid for a single login. This reduces the risk around a stolen password being reused. How do they work? OTPs are usually generated on a small device contained in a keyring fob or by a smartphone app. 
Remember there was a huge number of payment card breaches at major retailers a couple of years ago? Most of them were caused by point-of-sale (POS) malware. POS terminals can be infected with malware like any other connected device. When a card is swiped, its details are briefly stored in the terminal’s memory while being transmitted to the payment processor. This provides a brief window for malware on the terminal to copy the card data, which it then transmits back to the attackers. The technique is known as “memory scraping”. The rash of breaches forced retailers and payment providers into upping their security. The introduction of EMV, chip-and-pin type cards to replace traditional magnetic stripe cards has helped to reduce the threat posed by memory scraping POS malware. 
Cryptography is core to cyber security. As computing power continually increases, the strength of encryption needs to increase in parallel. If it doesn’t, then sooner or later, someone is going to build a computer fast enough to perform a brute force attack (i.e. attempting all possible encryption key combinations). 256-bit encryption, which is the strongest standard in common use, is still an unimaginably long time away from being cracked in this fashion. However, the advent of quantum computing does pose a threat to traditional cryptography since it is theoretically possible for it to break public key cryptography at some point. If that comes to pass, the answer could lie in quantum cryptography, where quantum computing is used to create new and more secure forms of cryptography, a quantum solution to a quantum problem, so to speak. 
Ransomware has arguably been the number one cyber security concern over the last few years. A number of factors coalesced to make it a potent threat. First of all, cyber criminals mastered the art of employing strong encryption on infected computers. Secondly, the advent of cryptocurrencies such as Bitcoin meant criminals had an accessible and relatively anonymous payment mechanism. Thirdly, mass-mailing spam botnets such as the aforementioned Necurs meant they could spread their malware far and wide, maximizing the number of potential victims. 
Phishing emails are designed to look like they come from somebody else and are usually designed to trick the recipient into disclosing information. For example, it could be an email pretending to come from a bank asking customers to change their password and providing a link to a bogus website designed to harvest credentials. Spear phishing emails are a variation on that theme, the difference being that they are targeted at specific individuals. Like regular phishing emails, they could be designed to trick the recipient into disclosing credentials. Or they could be used to lure people into installing malware by asking them to open a malicious attachment or follow a malicious link. The tactic is often used in BEC scams (see above) and targeted attacks (see below) 
Most of the activity we see involving malware is indiscriminate. Cyber criminals usually don’t really care who their victim is as long there is a possibility of profiting from them. Targeted attacks are very different. They’re usually the work of organized, state sponsored groups and their main motivation is espionage. By and large, that means intelligence gathering, but targeted attacks can include disruption, subversion and sabotage. Although quite small in number when compared to ordinary cyber crime, targeted attacks nevertheless pose a significant threat. Usually the attackers are more skillful and better resourced than run-of-the mill cyber criminals and are often targeting highly sensitive information. 
Sick and tired of getting constant reminders to update your software to the latest version (which inevitably arrive when you’re busiest)? Avoid clicking “no” or “later” where possible. Software updates play an important role in keeping you secure. Vendors will regularly roll out patches for newly discovered vulnerabilities (see below) in the latest version. If you don’t apply the update, attackers could exploit unpatched vulnerabilities and infect your computer. 
No piece of software is perfect and, even after years of testing, new bugs are often found. Vulnerabilities are a type of bug that permit someone to use the software in a way that wasn’t intended. There are numerous types of vulnerabilities. Some can allow an attacker to crash the software. Others may permit denial of service conditions or allow someone to gain privileges they shouldn’t have. The most serious kinds of vulnerabilities are what’s known as remote code execution vulnerabilities, which allow attackers to run arbitrary code on the vulnerable computer and can be triggered remotely, such as from another computer on the network or over the internet. 
WannaCry was undoubtedly the cyber security news story of 2017. These days it’s quite rare for a single piece of malware to cause overnight panic. WannaCry was similar to most families of ransomware we’ve seen in recent except for one critical difference — it incorporated a Windows exploit known as “EternalBlue” that enabled it to self-propagate, infecting other computers on a network and spreading across the internet. The vulnerability exploited by EternalBlue had been patched by Microsoft two months earlier, but there were enough unpatched computers online for WannaCry to cause chaos. Hundreds of thousands of computers were hit. 
Cross Site Scripting (XSS) is one of the most commonly used methods to attack visitors to a website. XSS works by exploiting vulnerabilities in web applications that permit attackers to insert their own code on to other people’s websites. This could permit attackers to steal other users’ credentials or cookies, allowing them to access their accounts and/or impersonate them. 
As the number of new malware threats multiplies every year, protecting against them presents a constant challenge, requiring innovative new tools and techniques. One tool that is growing in popularity is YARA. An open source initiative, YARA is a tool firms can use to identify and flag malware. While many traditional signature-based detection technologies rely on matching file hashes, YARA additionally works by creating rules that instead flag files based on matching code strings. 
Vulnerability exploits are a key tool for attackers attempting to compromise a computer or web server. Zero-day vulnerabilities are the holy grail for attackers; they are previously unknown vulnerabilities that have yet to be patched by the software vendor. There’s a thriving market for new zero days and attackers are willing to pay big money for critical vulnerabilities in commonly used software. Many software vendors are attempting to fight back by paying “bug bounties” to researchers who find vulnerabilities in their products. 
Check out the Security Response blog and follow Threat Intel on Twitter to keep up-to-date with the latest happenings in the world of threat intelligence and cybersecurity. 
Like this story? Recommend it by hitting the heart button so others on Medium see it, and follow Threat Intel on Medium for more great content. 
Written by 
Written by",Dick O'Brien,2018-01-17T16:23:21.937Z
How To Use Active Learning To Iteratively Improve Your Machine Learning Models | by ___ | Towards AI — Multidisciplinary Science Journal | Medium,"In this article, I will explain how to use active learning to iteratively improve the performance of a machine learning model. This technique is applicable to any model but for the purpose of this article, I will illustrate how it’s done to improve a binary text classifier. All the materials covered in this article are based on the 2018 Strata Data Conference Tutorial titled “Using R and Python for scalable data science, machine learning and AI” from Microsoft. 
I assume the reader is familiar with the concept of active learning in the context of machine learning. If not, then the lead section of this Wikipedia article serves as a good introduction. 
The code to reproduce the results presented in this article is here. 
We will demonstrate the concept of active learning by building a binary text classifier trained on the Wikipedia Detox dataset to detect if a comment constitutes a personal attack or not. Here are a couple of examples to illustrate this problem: 
The training set has 115,374 labeled examples. We will split this training set into three sets, namely, initial training set, unlabeled training set and test set as follows: 
Furthermore, the labels are evenly distributed in the Initial Training Set but on the Test Set, only 13% of the labels are 1. 
We split the training set this way to simulate real-world conditions. This kind of split corresponds to the situation where we have 10,285 high-quality labeled examples and need to decide which of the 105,089 “unlabeled” examples we need to label to get more training data to train our classifier. Since labeling data is expensive, the challenge is to identify examples that will have the biggest contribution to our model’s performance. 
We will see that active learning is a superior sampling strategy relative to random sampling on the unlabeled training set. 
Lastly, the comments are converted to 50-dimensional embeddings using the Glove word embeddings. 
The sampling strategy we will use is a combination of uncertainty sampling and pool-based sampling. Here is how it works: 
The numbers above are chosen to simulate the situation where we are only able to obtain 20 high-quality labels at a time e.g. a radiologist can only process 20 medical images in a day. We do not cluster the entire Unlabeled Training Set because computing entropy requires doing model inference and this may take a long time on large datasets. 
The reason to cluster the samples is to maximize the diversity of the samples that are going to be sent for labeling. For example, if we simply pick the top 20 examples with the highest entropy from that 1,000 samples, then we risk picking very similar examples if these examples form a tight group. In this case, it is better to pick just one example from this group and the rest from another group(s) as diverse examples help the model learn better. 
We will use FastTrees to build the classifier using the comments’ vector embeddings as input. FastTrees is an implementation of FastRank which is a variant of gradient boosting algorithms. This link has more details. 
Since the Test Set is imbalanced, we will use AUC as the primary evaluation metric. 
Here’s a diagram to illustrate the role active learning will play in this experiment: 
To start, we will train our model on the Initial Training Set. Then we will use this model and the sampling strategy described earlier to identify the 20 comments in the Unlabeled Training Set whose classification it is most uncertain i.e. not confident about. These comments will be “sent” to a human for labeling. Now we can expand our initial Training Set to include these new labeled samples from the human and retrain our model (from scratch). This is the active learning part of the experiment. We will repeat the step of expanding our Initial Training Set for 20 iterations and evaluate the model’s performance on the test set at the end of each iteration. 
For comparison, we can iteratively expand our Initial Training Set by just randomly picking any 20 examples from our Unlabeled Training Set. The following figure compares our approach (active) to 3 runs of random sampling (random) using various metrics according to the size of the training set (tss). 
We see that random sampling initially outperforms our active learning approach. However, around the training set size of 300, the active learning approach starts to outperform random sampling in terms of AUC by a wide margin. 
In practice, you would want to continue expanding the Initial Training Set until the ratio of model improvement (e.g. increase in AUC) relative to labeling cost drops below a predetermined threshold. 
To ensure that our results aren’t a fluke, we can simulate the random sampling strategy for 20 iterations 100 times and count the number of times it produces an AUC greater than our active learning approach. The results of my simulation yield only 1 instance where random sampling gave a higher AUC than active learning. This suggests that the results from active learning are statistically significant at the 5% level. Lastly, the average difference in AUC between random sampling and active learning is -0.03. 
In a situation where you have abundant unlabeled data and a limited budget to get these data labeled, adopting an active learning approach to identify which of these unlabeled data to send for human labeling can maximize model performance subject to the given budget constraint. 
Let me know in the comments if you have any questions. 
Towards AI publishes the best of tech, science, and engineering. Subscribe with us to receive our newsletter right on your inbox. For sponsorship opportunities, please email us at pub@towardsai.net Take a look 
Written by 
Written by",___,2020-06-11T17:13:46.980Z
Understanding UX Threat (part 1). Improving your designs by going down… | by Dave Vronay | ringcentral-ux | Medium,"User Experience is enjoying a moment lately. It seems one can’t open a web page without seeing an article about the importance of design, and in most software companies design has moved from being the polish put in at the end to a key part of the development process. 
There is of course still a long way to go before all software manifests excellent design. One of the problems is the lack of objective measures that a design is good. If I am a developer, I can measure things like memory usage, algorithm performance, test coverage, and so forth. I can use techniques like input fuzzing and load testing to see how the code performs under stress. When I do a code review, multiple programmers can review my code and it is likely that they will agree on any issues. 
Design, on the other hand, is often perceived as a matter of opinion. Whether this grid is better than that grid, or whether this background image behind the ListView actually does anything for the user is often taken on faith. Few companies have time to user study every design detail, and often the user studies themselves are subjective rather than performative. Even the main tool of the design discipline — peer critique — often has multiple divergent opinions. 
Some might argue that there is no issue here. They feel that design is by its nature subjective, and consider just “trusting the designer” as a necessary part of a creative process. I strongly disagree with this. This is confusing Design with Art. Art is a personal expression, while Design is the process of doing something on purpose. Design benefits from creativity, of course, but no more so than programming, marketing, human resources, and any other profession. 
In addition, without objective measures of design quality, a good design solution can easily be compromised by other objective factors. Every designer has plenty of stories about how they had to go with a worse design because it was easier to code, or because the product team thought it was safer to just copy a competitor. Without anything objective on your side, the design team more often than not loses these arguments. 
With that intro, I would like to introduce an objective design technique: UX Threat. 
Understanding Threat 
Threat is simply the measure of how risky something is in a given circumstance. There are two components: 
Everything has some level of threat associated with it. In daily life, we make threat assessments all the time. For example, when I eat a burger at a fast food place, I might get food poisoning. If I cross in the middle of the street, I might get hit by a car. 
When we assess threat, it is most natural for us to first think about the consequences. If I pinch my finger in my keyboard, it might hurt for a bit, while being hit by a car could result in being killed or crippled. We naturally try to avoid actions with extreme consequences. 
However, it is equally important to consider the likelihood. Likelihood is more complex, because there are several factors that go into it. 
First, there is context. This is the place, the environment, conditions, etc. For instance, am I crossing a busy street at night in the rain or a rarely traveled country road in the daytime? Is the restaurant I am eating at a big chain in a country with rigorous food safety laws or a street vendor in a third-world country? 
Another aspect is frequency. A single event might be unlikely, but if I do it frequently, the likelihood will increase. Eventually the odds will run out. 
We also consider that actors — the people involved. A child who is small, slow, and moves erratically might be more likely to be hit by a car than an adult. A person with a compromised immune system might be more susceptible to food-borne illness than a healthy person. 
Mitigation 
The next step after evaluating the threat is mitigation. Mitigation is the process of taking proactive steps to decrease threat. 
We can do this by decreasing the likelihood of the event. For our fast food example, a possible mitigation might be to only eat at restaurants with recently published passing grades from the local health inspector. If I am crossing a street, I might not do it at night, or be sure to wear reflective clothing. 
We can also decrease the consequences of the event. For instance, we have seat belts and airbags so that even if an accident happens, we will be less likely to be seriously injured. 
Threat in User Experience 
With that background, we can now see how to apply threat to user experience. This is really just a process of taking a design and thinking off all of the things that can possibly go wrong. We assess the threat of each of these, and that gives us the overall threat of a design. We can then compare that with another alternative design to see if one has less threat than another. 
For example, let’s take a simple banking payment app that sends instant payments, like Zelle. In this app, I can send money to anyone by simply entering their phone number and the amount I want to send. Let’s look at a design: 
In this design, the user first enters a phone number, and then an amount. When they hit the pay button, the money is sent. Note that this design is a rough sketch. Usually a rough sketch is all that is needed to evaluate threat. 
We start the process by thinking “What could go wrong?” This is often hard for designers when they first start thinking about threat, because in design we often focus on the “happy path” — what could go right. Here are just a few possibilities: 
Let’s take the wrong phone number first. 
How likely is this to happen? The answer is “it depends”. If it is a familiar number, it is probably safe. Or if I am cut & pasting it from, say, a text message. But if I am entering a new number, or in a hurry, etc., it is easy to get it wrong. 
And how bad is it if this does happen? Here, the answer is “pretty bad”. These transfers are instant, and there is no way to recall them. So if you send it to the wrong person, the only hope is to call them and ask them nicely to return it. But it is still an inconvenience. 
The other potential problem — wrong amount — is less problematic. Users are familiar with entering numbers using a calculator-style numeric keypad. And if the wrong amount is sent, it is a larger issue if the amount is more than intended. Sending less than intended can be corrected by sending more. 
Between these two adverse events, we might say this UI overall is fairly risky. 
Designing Mitigations 
Once we have a basic threat assessment, we can start designing the mitigations for the threat. Often mitigation is done as an afterthought. The problematic design is not changed, but we add something at the end of the flow in the hope of reducing the risk. For example — I might add a confirmation screen before executing the risky action: 
Unfortunately these solutions are not very effective. For instance, consider the ways someone could have gotten the phone number incorrect. If someone typed the wrong number, merely showing them the same number again won’t do much. People are notoriously bad at recognizing small errors like transcribed digits. In addition, people have a bias towards thinking that their existing input is correct, so they are unlikely to spend time double-checking this number if they already typed it once. 
This brings us to another important aspect of UX threat. Many of the possible sources of threat as well as mitigations can leverage well-known principles from human perception, psychology, ergonomics, and so forth. Once you get into the habit of thinking about threat, there is a great depth of literature you can draw upon to improve your methodology. 
So if merely adding a confirmation screen to a risky design is not great, how can we change the design to be less risky in the first place? 
Let’s start with the source of risk of dialing the wrong number. The first step in reducing any risk is seeing if there is any way of getting rid of it all together. How can we get rid of specifying a phone number? One way might be to show the user a contact list instead. If we have a contact list, the risk becomes that the user will select the wrong contact instead of dialing the wrong number. While there is still a risk there, it is much less likely than dialing a wrong number. This might give us a UI like this: 
This is better, but there is still more we can do if we know more about the task. For instance, perhaps we know that our users tend to send money to the same people over and over again. We could indicate those people (and the typical amount) in the UI to make it not only easier, but less risky: 
We can keep going in this vein, reducing or eliminating sources of threat until you have the safest UI possible. 
Going Deeper down the Unhappy Path 
Years of doing threat analysis have led me to believe that almost anything can go wrong. When analyzing threat, consider: 
It might be tempting to think “hey, I don’t have to worry about this, my user isn’t an idiot”. But that is the wrong perspective. The reality is that your user is probably not an idiot, but at the same time your software is not as intuitive as you think it is. You spend all day, everyday with your designs. You understand them inside and out, and know the reason why every odd thing is there. Your user, meanwhile, is living their own life, doing their own thing, and are trying to spend the minimum time possible interacting with your software to accomplish their task. 
How Safe is Safe Enough? 
A common question I get when talking about UX Threat is “How safe is safe enough?” My answer: it really depends on your application. 
I first developed my approach to UX Threat when designing UI for clinical healthcare software. In this context, safety is paramount. An “adverse event” in clinical UI was literally a matter of life and death. In an attempt to make the safest UI possible, we spent a lot of time studying medical errors that could be traced to bad UI. We were surprised to see that almost anything that could go wrong would go wrong at some times. 
For instance — consider a screen showing lab results. If a lab result is abnormal, a doctor might want to change some medications. As such, most lab viewer software shows the lab results in a spreadsheet format, with a clear indication that a result is abnormal: 
But what if there are a lot of lab results, and the one that is abnormal is offscreen? How likely is it that a doctor would not scroll down to see it? It may seem unlikely, but not only could it happen, but it does happen, and there have been “adverse patient events” as a result. We can mitigate this by showing an obvious indicator when there is an abnormal result that is scrolled off the screen. 
The user could click on this to scroll to the next abnormal result: 
So my advice is make your UI as safe as you can. Once you have a mitigation in your design, you can then have a discussion with your Product and Dev teams on whether the threat is high enough to justify the mitigation. 
Concluding Thoughts 
Threat assessment can be a powerful technique for improving the quality of your UI. It is applicable to many aspects of quality. The examples I gave focused around the threat of errors, but it can be applied to any sort of adverse event in your product. For example: 
UX Threat is also an excellent framework for approaching accessibility. Rather than thinking of accessibility as a checklist of things to satisfy at the end of a design, consider the additional risks that a disabled person might have using your software. You will find that in many cases, what decreases the threat for a disabled user helps the non-disabled user as well. 
That’s it! I’d love to hear your thoughts on UX Threat and any tips you have on assessing or mitigating threat in your own work in the comments. 
Written by 
Written by",Dave Vronay,2020-09-09T16:12:40.639Z
Dasher’s Zero Trust Architecture Approach | by IT Solution Architects | Medium,"January 23,2019 
Catastrophic data breaches now seem like a daily occurance. Ninety-two percent of malware is still delivered by email. Eighty-one percent of network intrusions leverage stolen or weak credentials. It typically takes 191 days — more than 6 months — for organizations to identify data breaches. As we all know, there is no shortage of frightening statistics keeping IT departments up at night. As the cyber threat landscape intensifies, IT leaders are now tasked with creating a targeted, holistic approach to cybersecurity. 
According to Forrester: “In response to increasingly frequent and complex cyberattacks, security pros are devoting resources to ever-more granular aspects of their networks. This is necessary, but it’s also a great way to lose sight of your ultimate goal: protecting customers and empowering your business.” A Zero Trust philosophy helps focus the fight. 
Dasher’s Zero Trust architecture approach covers your most critical cybersecurity needs. Here’s our three-point plan for IT leaders exploring new cyber defense strategies: 
1. Don’t trust anyone 
The concept of “Zero Trust” was popularized by Forrester in 2010, to describe eradicating “the idea of a trusted network inside a defined corporate perimeter.” Previously, companies tended to build cybersecurity programs under the assumption that their corporate networks were secure, which we now know is not the case in reality. 
Start with the premise that you aren’t going to trust anyone, inside or outside the network. Allow only discriminating access and only to needed resources. Everything is always locked down. With nearly every device connected to the internet, Zero Trust ensures that infiltrators from the digital world at large don’t gain lateral access to everything in your internal network. It also mitigates internal threats and serves as a standard to protect and regulate access to sensitive systems and data. Blindly allowing internal users to access everything without identity checks and authorization mechanisms negates the entire security stack. 
2. Know your network 
Zero Trust requires visibility. You have to understand your network to know if it something is amiss. Dasher helps clients achieve true visibility across enterprise networks, detecting every device and keeping watch on traffic patterns. Dasher’s expert engineers deploy monitoring tools that give you visibility into what happens on your network under what circumstances. It’s the smartest way to build an informed cyber-defense strategy. 
Surprisingly, most organizations have no clear concept of typical network behavior, or where there are atypical gaps. For example, sometimes engineering DevOps departments take actions that circumvent IT purview. True network visibility into your entire network shows what is and isn’t happening and what is and isn’t working. This validation provides IT departments with a clear understanding of network operation and helps executives better assess their cybersecurity resources. 
3. Reinforce security best practices 
From startups to Fortune 500 companies, Zero Trust is relevant to businesses of all sizes. But there is overhead involved with implementing a Zero Trust architecture. For this reason, small-to-midsize businesses benefit the most from implementing Zero Trust because the scope of resources involved is still manageable and cost-effective, and best practices can be reinforced with growth. 
Dasher often helps startups that are transitioning into mid-size companies. One Silicon Valley client was recently experiencing incredible growth and moving to a hybrid cloud architecture. While our engineers were addressing its data center and multi-cloud environment, we also implemented our Zero Trust approach. Dasher engineers helped its IT department assess its cybersecurity needs and establish role-based network access control and user rights assignments to ensure network integrity moving forward. 
Dasher deploys easy-to-manage, secure, and cost-effective security solutions. Our cybersecurity programs start with key questions: What are your biggest security worries? What is keeping you awake at night? We make sure your vulnerabilities are addressed with solutions that scale. 
Dasher’s systematic approach to Zero Trust architecture leverages state-of-the-art cybersecurity best practices to secure your network. It also serves as a game plan for healthy IT by providing a clear and strategic policy to champion cybersecurity within your organization. 
Written by 
Written by",IT Solution Architects,2019-01-25T20:12:27.245Z
Navigating the Unsupervised Learning Landscape | by Eugenio Culurciello | Intuition Machine | Medium,"Unsupervised learning is the Holy Grail of Deep Learning. The goal of unsupervised learning is to create general systems that can be trained with little data. Very little data. 
Today Deep Learning models are trained on large supervised datasets. Meaning that for each data, there is a corresponding label. In the case of the popular ImageNet dataset, there are 1M images labeled by humans. 1000 images for each of the 1000 classes. It can take some effort to create such dataset, many months of work. Imagine now creating a dataset with 1M classes. Imagine having to label each frame of a video dataset, with 100M frames. This is not scalable. 
Now, think about how you got trained when you were very little. Yes you got some supervision, but when your parents told you that is a “cat” they would not tell you “cat” every split second you were looking at a cat for the rest of your life! That is what supervised learning is today: I tell you over and over what a “cat” is, maybe 1M times. Then your Deep Learning model gets it. 
Ideally, we would like to have a model that behaves more like our brain. That needs just a few labels here and there to make sense of the multitude of classes of the world. And with classes I mean objects classes, action classes, environment classes, object parts classes, and the list goes on and on. 
As you will see in this review, the most successful models are the ones that predict future representation of a video. One issue that many of these techniques have, and are trying to resolve, is that training for a good overall representation needs to be performed on videos, rather than still images. This is the only way to apply the learned representation to real-life tasks. 
The main goal of unsupervised learning research is to pre-train a model (called “discriminator” or “encoder”) network to be used for other tasks. The encoder features should be general enough to be used in a categorization tasks: for example to train on ImageNet and provide good results, as close as possible as supervised models. 
Up to date, supervised models always perform better than unsupervised pre-trained models. That is because the supervision allows to model to better encode the characteristics of the dataset. But supervision can also be decremental if the model is then applied to other tasks. In this regards, the hope is that unsupervised training can provide more general features for learning to perform any tasks. 
If real-life applications are the targets, as in autonomous driving, action recognition, object detection and recognition in live feeds, then these algorithms need to be trained on video data. 
Originated largely from Bruno Olshausen and David Field in 1996. This paper showed that coding theory can be applied to the receptive field in the visual cortex. They showed that the primary visual vortex (V1) in our brain uses principles of sparsity to create a minimal set of base functions that can be also used to reconstruct the input image. 
A great review is here 
Yann LeCun group also worked a lot in this area. In this page you can see a great animation of how sparse filters V1-like are learned. 
Stacked-auto encoders are also used, by repeating the process of training greedily layer by layer. 
Auto-encoders methods are also called “direct-mapping” methods. 
Advantages: 
Disadvantages: 
One technique uses k-means clustering to learn filters at multiple layers. 
Our group named this technique: Clustering Learning, Clustering Connections and Convolutional Clustering, which very recently achieved very good results on the popular STL-10 unsupervised dataset. 
Our work in this area was developed independently to the work of Adam Coates and Andrew Ng. 
Restricted Boltzmann machines (RBMs), deep Boltzmann machines (DBMs), Deep belief networks (DBNs) have been notoriously hard to train because of the numerical difficulties of solving their partition function. As such they have not been used widely to solve problems. 
Advantages: 
Disadvantages: 
Generative models try to create a categorization (discriminator or encoder) network and a model that generates images (generative model) at the same time. This method originated from the pioneering work of Ian Goodfellow and Yoshua Bengio. Here is a great and recent summary of GAN by Ian. 
The generative adversarial model by Alec Radford, Luke Metz, Soumith Chintala named DCGAN instantiates one such model that got really awesome results. 
A good explanation of this model is here. See this system diagram: 
The DCGAN discriminator is designed to tell if an input image is real, or coming from the dataset, or fake, coming from the generator. The generator takes a random noise vector (say 1024 values) at the input and generates an image. 
In the DCGAN, the generator network is: 
while the discriminator is a standard neural network. See code below for details. 
The key is to train both networks in parallel while not completely overfitting and thus copying the dataset. The learned features need to generalize to unseen examples, so learning the dataset would not be of use. 
Training code of DCGAN in Torch7 is also provided, which allow for great experimentation. 
After both generator and discriminator network are trained, one can use both. The main goal was to train a nice discriminator network to be used for other tasks, for example categorization on other datasets. The generator can be used to generate images out of random vectors. These images have very interesting properties. First of all, they offer smooth transitions from the input space. See an example here, where they show the images produced by moving between 9 random input vectors: 
Also the input vector space offers mathematical properties, showing the learned features are organized by similarity: 
The smooth space learned by the generator suggests that the discriminator also has similar properties, making it a great general feature extractor for encoding images. This should help the typical problem of CNN trained in discontinuous image datasets, where adversarial noise makes them fail. 
A recent update to the GAN training, provided a 21% error rate on CIFAR-10 with only 1000 labeled samples. 
A recent paper on infoGAN was able to produce very sharp images with image features that can be dis-entangled and have more interesting meaning. They, however, did not report the performance of the learned features in a task or dataset for comparison. 
Interesting summary on generative models are also here and here. 
Another very interesting example is given here where the authors use generative adversarial training to learn to produce images out of textual descriptions. See this example: 
What I appreciate the most about this work is that the network is using the textual description as input to the generator, as opposed to random vectors, and can thus control accurately the output of the generator. A picture of the network model is here: 
Advantages: 
Disadvantages: 
These models learn directly from unlabeled data, by devising unsupervised learning tasks that do not require labels, and learning aims at solving the task. 
Unsupervised learning of visual representations by solving jigsaw puzzles is a clever trick. The author break the image into a puzzle and train a deep neural network to solve the puzzle. The resulting network has one of the highest performance of pre-trained networks. 
Unsupervised learning of visual representations from image patches and locality is also a clever trick. Here they take two patches of the same image closely located. These patches are statistically of the same object. A third patch is taken from a random picture and location, statistically not of the same object as the other 2 patches. Then a deep neural network is trained to discriminate between 2 patches of same object or different objects. The resulting network has one of the highest performance of pre-trained networks. 
Unsupervised learning of visual representations from stereo image reconstructions takes a stereo image, say the left frame, and reconstruct the right frame. Albeit this work was not aimed at unsupervised learning, it can be! This method also generates interesting 3D movies form stills. 
Unsupervised Learning of Visual Representations using surrogate categories uses patches of images to create a very large number of surrogate classes. These image patches are then augmented, and then used to train a supervised network based on the augmented surrogate classes. This gives one of the best results in unsupervised feature learning. 
Unsupervised Learning of Visual Representations using Videos uses an encoder-decoder LSTM pair. The encoder LSTM runs through a sequence of video frames to generate an internal representation. This representation is then decoded through another LSTM to produce a target sequence. To make this unsupervised, one way is to predict the same sequence as the input. Another way is to predict future frames. 
Another paper (MIT: Vondrick, Torralba) using videos with very compelling results is here. The great idea behind this work is to predict the representation of future frames from a video input. This is an elegant approach. The model used is here: 
One problem of this technique is that a standard neural network trained on static frames is used to interpret a video. Such networks does not learn the temporal dynamics of video and the smooth transformation of objects moving in space. Thus we argue this network is ill-suited to predict future representations in video. 
To overcome this issue, our group is creating a large video dataset eVDS created to train new network models (recursive and feed-back) directly on video data. 
Predictive deep neural networks are models that are designed to predict future representations of the future. 
PredNet is a network designed to predict future frames in video. See great examples here: https://coxlab.github.io/prednet/ 
PredNet is a very clever neural network model that in our opinion will have a major role in the future of neural networks. PredNet learns a neural representation that extends beyond the single frames of supervised CNN. 
PredNet combine bio-inspired bi-directional [models of the human brain] (https://papers.nips.cc/paper/1083-unsupervised-pixel-prediction.pdf). It uses predictive coding and using [feedback connections in neural models] (http://arxiv.org/abs/1608.03425). This is the PredNet module and example of 2 stacked layer: 
This model also has the following advantages: 
One issue with PredNet is that predicting the future input frame is a relatively easy task some simple motion-based filters at the first layer. In our experiments PredNet this learns to do a great job at re-constructing an input frame, but higher layer do not learn a good representations. In our experiments, higher layer in fact are not able to solve simple tasks like categorization. 
Predicting the future frame is in fact not necessary. What we would like to do is to predict future representations of next frames, just like Carl Vondrick does. 
A very interesting model is the PVM from BrainCorporation. This model aims at capturing bidirectional and recurrent connections in the brain, and also provide local layer-wise training. 
This model is very interesting because it offers connectivity similar to new network models (recursive and feed-back connections). These connections provide temporal information and generative abilities. 
It is also interesting because it can be trained locally, with each PVM unit trying to predict its future output, and adjusting only local weights appropriately. This is quite different from the way deep neural network are trained today: back-propagating errors throughput the entire network. 
The PVM unit is given below. It has inputs from multiple lower layers, to which it also provides feedback, and lateral connections. The feedback is time-delayed to form recurrent loops. 
More details about the PVM system are given here. 
This recent paper (April 2017) trains unsupervised models by looking at motion of objects in videos. Motion is extracted as optical flow, and used as a segmentation mask for moving objects. Even though optical flow signal does not give anywhere close to a good segmentation mask, the averaging effect of training on a large data allow the resulting network to do a good job. Examples below. 
This work is very exciting because it is follows neuroscience theories of how the visual cortex develops by learning to segment moving objects. 
Is your to make. 
Unsupervised training is very much an open topic, where you can make a large contribution by: 
I have almost 20 years of experience in neural networks in both hardware and software (a rare combination). See about me here: Medium, webpage, Scholar, LinkedIn, and more… 
Written by 
Written by",Eugenio Culurciello,2018-12-24T23:07:19.511Z
"Top Cybersecurity Trends in Financial Tech Review | by Christopher | Aug, 2020 | Medium","Cybersecurity has been made an absolute necessity by technologies such as cloud, Internet of Things (IoT), and mobility to protect companies from threats such as ransomware, DDoS, and phishing attacks. In order to protect the sensitive data of their customers, companies must continue to update their approach to cybersecurity as the attacks grow more advanced by rapid technological development. Set at the helm of safety teams that avoid such harmful incidents, experts have come up with ways of identifying potential threats and eliminating them. 
Check This Out: The financial Tech Review 
In order to proactively monitor and thwart threats, cloud-based cyber security tools that cover the end-to-end value chain of one company already round off, reducing stakeholder costs of ownership. Stringent firewalls are kept in check for perpetrators, guided by algorithms of data analysis that regularly test suspicious and iterative designs. There are also comprehensive security endpoint solutions, which allow security administrators to provide and revoke roll-by-roll as well as ad-hoc access to applications, files, and a whole network. 
The new, hedge-based cyber solution, which addresses many issues in cyber risks management including security management, insurance, and crisis management, has been launched by Crystal Financial Institutions (Alliant Insurance Services). 
The Cyber Risk 360° covers new risks powerfully. This includes a cyber insurance policy with a coverage extension available on fee income for unplanned reimbursements. As one of the first solutions offered by this industry, fund managers are now able to recover lost management fee revenue attributed to a cyber violation event. Other Cyber Risk 360° features include cyber extortion, data loss restoration, 24-hour reaction to claims and dedicated loss recovery services. 
Crystal Financial Institutions has been working with industry leaders from the areas of insurance, legal, compliance, cyber defense, IT, cybersecurity, BlueVoyant, the Bohrer PLLC, and Everest Insurance® to develop a new cyber risk 360° solution. 
Check This Out: The financial Tech Review 
No single technology can prevent 100 percent of violations regardless of how comprehensive the endpoint protection solution is. Eventually, it will be a motivated and sophisticated opponent. This makes it possible to prevent silent failure, with strong detection capabilities and enhanced by a team of professionals–both on-the-job and third-party. 
— — 
The fintech services have transformed, automated, and disrupted the industry for the better. But here are still some issues that need to be addressed, like the concern for data security in the implementation of fintech services in the existing banking solutions. 
Here are five hidden risks that are prevailing in the fintech industry. 
Check This Out: The financial Tech Review 
New Encryption Technology 
With the rise of disruptive technologies, the performance of the finance industry has enhanced. But these technologies also create major issues in the industry. For instance, blockchain has opened some security concerns as it can be easily hacked. Transactions in blockchain are based on trust between two or more parties, but the trust is not often kept. 
Data Integrity Risks 
Today, customers can use mobile phones to access their accounts and transfer funds. But if financial firms use mobile devices without a secure encryption algorithm, integrity issues may arise. 
Cloud-based Security Risks 
Cloud-based solutions do not have proper security measures that can corrupt financial information. Companies with inefficient cloud-based solution partners deal with substantial data losses. Hence, it is wise to update and be wary when selecting a cloud-based service provider. 
Online Hacking 
Most banks use the Society for Worldwide Interbank Financial Telecommunication system (SWIFT) to exchange crucial financial information more securely. But the advancement of hackers can be seen by a recent attack on one of the SWIFT infrastructure. As banks and financial firms have loopholes in their processes, hackers take this opportunity and launch malware attacks. 
Application Security Risk 
Banks used fintech apps to access real-time financial details of their customers. Still, if software applications have no comprehensive security modules and efficient codes, they instantly become vulnerable to cybercrimes. Therefore, when creating a fintech software solution, ensure that it includes all the essential features to prevent hackers from stealing customers’ data. 
Written by 
Written by",Christopher,2020-08-25T10:07:37.832Z
Zero Trust — Where should you begin? | by Sitaraman Lakshminarayanan | Medium,"With buzz words around Multi Cloud / Cloud Native, it is reasonable to expect Zero Trust being used heavily in sales & marketing. I saw a quiet a few players both new and old at RSA 2020. The intent of this article is to arm you with enough information so you can ask the right questions before you take the plunge on Zero Trust. 
First couple of reference points. If you are not familiar with Zero Trust beyond basic definition and some high level articles or even if you are curious about all elements of Zero Trust, you should read this book “Zero Trust Networks” by Evan Gilman . I have read this book and I highly recommend it . Next if you can get a copy of report “A Practical Guide To A Zero Trust Implementation” from Forrester Analyst chase cunningham. 
What is Zero Trust? See for yourself what last year RSA attendees described Zero Trust as 
I personally like to stick to the definition by chase cunningham “Trust Nothing, not application or device or network, etc”. But what does that mean for practical implementation of Zero Trust? Where should one start? If you list to the clip, you hear definitions that span across — Identity & Access , Network/Firewall, Vulnerability Management, Stronger authentication, etc. 
So Zero Trust is not a new paradigm but rather shifting the mindset from “Trust ourData Center / firewall and Trust our employees, systems and processes” to more like “We have apps /data across multiple cloud providers, data centers and we can’t just rely on trust anymore and we need a comprehensive solution that has both visibility and stronger security controls”. 
That said, Zero Trust is not a net new investment. You don’t need to rip and replace your existing investments. You might want to start with getting a better understanding of what you have, your infrastructure, cloud services, applications, services, devices, etc. 
Then ask yourself how each logical or physical boundaries are protected — AuthN, AuthZ, Confidentiality, Integrity and what auditing information is available for any early detection or post incident analysis. 
Zero Trust solutions span across multiple domains such as Identity & Access, Network Security/Segmentation/Micro Segmentation, Visibility/Monitoring, Asset & Vulnerability Management, PKI, MFA, etc. So there is no such thing a one Zero Trust solution or not everything needs to be called Zero Trust. 
Zero Trust is actually a Systems thinking approach to Security where every layer /Integration is questioned about its security i.e. who has access and how can this be compromised and if/when how do we detect and respond. 
Zero Trust means ,how can i implement a solution that does not require me to just trust one person /team but rather implement security controls in a more systematic /software defined manner across all layers of Infrastructure/App/database, etc. Implementation details differs based on your set up — data center or cloud? Internal facing ( Corporate/IT apps) or Customer facing Product/Services. 
I plan to write more on this topic starting with Identity & Access Management. Drop me a note if you have any specific question or topic to be covered first. 
Written by 
Written by",Sitaraman Lakshminarayanan,2020-02-26T07:15:22.276Z
Machine Learning — Hidden Markov Model (HMM) | by Jonathan Hui | Medium,"How do you know your spouse is happy or not? Any couple will tell you it can be hard. In machine learning ML, many internal states are hard to determine or observe. An alternative is to determine them from observable external factors. That is what HMM solves. For example, in speech recognition, we listen to a speech (the observable) to deduce its script (the internal state representing the speech). First, let’s look at some commonly-used definitions first. 
A first-order Markov process is a stochastic process in which the future state solely depends on the current state only. The first-order Markov process is often simply called the Markov process. If it is in a discrete space, it is called the Markov chain. 
The assumption of the Markov process may not be true in reality. But even it is not true, we can model extra states in the system to make it closer to the Markov process sometimes. In practice, the Markov process can be an appropriate approximation in solving complex ML and reinforcement learning problems. In addition, the probability of the transition from one state to another can be packed into a transition matrix like the one below: 
This transition matrix is also called the Markov matrix. The element ij is the probability of transiting from state j to state i. Note, some literature may use a transposed notation where each element is the probability of transiting from state i to j instead. 
The columns of a Markov matrix add up to one, i.e. the probability of reaching a state from any possible state is one. Once, it is defined as a matrix, we can use linear algebra and eigenvector to determine its stable state if existed, i.e. if we keep going for a long time, what is the probability of being at a particular state? 
To solve that, let’s have a quick review of eigenvectors first. Eigenvector vᵢ and eigenvalue λᵢ of the matrix A fulfill the following relation. (Note, matrix A can have many eigenvectors.) 
Our state at time k+1 is related to the previous step by the Markov matrix which the stable state is determined with k approaches ∞. 
If uᵢ is an eigenvector of A, 
where λᵢ is the corresponding eigenvalue for uᵢ. 
Consider a vector v₁ in ℝⁿ. We can represent it using the eigenvectors of A. Using the equation above, the state of v at time step k+1 will become (the inner product of two different eigenvectors equals zero) 
If v converges in time, v will have a stable state. uᵢ can be chosen to be unit vectors. In order for v to converge, eigenvalues λᵢ must be smaller or equal to 1. Otherwise, ‖v‖ will continue to grow. 
A Markov matrix always has an eigenvalue 1. All other eigenvalues will have a magnitude smaller or equal to 1. Let’s say, the eigenvector u₁ (say [0.2, 0.5, 0.3]) has an eigenvalue of 1. Then, u₁ will be the stable state, i.e. we have 0.2, 0.5, and 0.3 chance to be in states 1, 2, or 3 respectively as the time approaches infinity. Note, the solution is independent of the initial state. We end up with the same target distribution regardless of where we start. (More details can be found here.) In theory, we can have more than one eigenvectors with eigenvalues equal to one. However, in practice, real problems usually have only one. In fact, if all elements in the matrix are greater than zero, there is exactly one eigenvector with eigenvalue equals to one. 
Random walk 
Calculating an exact solution can be computationally intensive. Alternatively, Markov processes can be solved using random walks. Let’s say we drop off 100 shoppers randomly around the downtown area in San Franciso. We provide a transition matrix to show the probability of where the shoppers may head next in the current position. Eventually, we can spot where most interesting shops are located. 
This strategy allows us to use local information to understand the general structure of the data. In many ML problems, it is much easier to collect. We don’t need to understand the structure of the data. We don’t need to understand how the city plans its shopping districts. Just look around and see what may be more interesting. In addition, the transition matrix is mostly sparse in many problems. This random walk concept is very popular in ranking or making product recommendations. 
As we continue the iterations, our random walk will converge to the stable state that we are interested in. For very large scale problems, this may be easier to execute and to compute. 
In many ML problems, we assume the sampled data is i.i.d. This simplifies the maximum likelihood estimation (MLE) and makes the math much simpler to solve. But for the time sequence model, states are not completely independent. If I am happy now, I will be more likely to stay happy tomorrow. 
In many ML problems, the states of a system may not be observable or fully observable. But we can get insights about this internal state through the observables. For example, if I am happy, there is a 40% chance that I will go to a party. But there is a 10% chance that I will be found at a party when I am sad too. With HMM, we determine the internal state (happy or sad) by making observations — where I was found. 
HMM models a process with a Markov process. 
The complexity of the problem is that the same observations may be originated from different states (happy or not). 
Two major assumptions are made in HMM. The next state and the current observation solely depend on the current state only. 
Given all the observable and the initial state distribution, we can compute a pretty complex equation for the probability for the internal state xt P(xt| y₁, y₂, y₃, … , yt) at time t. For simplicity here, we will not include π in our equation. All equations assume π is a given condition, like P(y) → P(y|π). 
The equation above uses the transition probability and the emission probability to compute the probability of the internal state based on all observations. 
Depending on the situation, we usually ask three different types of questions regarding an HMM problem. 
The remaining section details the solution. Read through it according to your level of interest. 
Likelihood (likelihood of the observation) 
Likelihood is to find the likelihood of observation Y. 
This is computationally intense. But we can do it smartly to avoid summing all possible state sequences one-by-one and to share results for other computations. Otherwise, the complexity will grow exponentially. 
Our strategy will employ a divide-and-conquer. In specifically, if we can express components recursively, we can break down the problem into intermediate steps and share results. 
In HMM, we solve the problem at time t by using the result from time t-1 and/or t+1. A circle below represents an HMM hidden state j at time t. So even the number of state sequence increases exponentially with time, we can solve it linear if we can express the calculation recursively with time. 
This is the idea of dynamic programming that breaks the exponential curse. At time t, the probability of our observations up to time t is: 
Let’s rename the term underlined in red above as αt(j) (forward probability) and check if we can express it recursively. 
Yes, it does. Since the current observation depends on the current state only, α can be expressed as: 
i.e. the likelihood of the observations can be calculated recursively for each time step below. 
Below is an example which we start with the initial state distribution on the left. Then we propagate the value of α to the right for each timestep. Therefore, we break the curse of exponential complexity. 
Decoding (find the internal states — Viterbi algorithm) 
The decoding problem is finding the optimal internal states sequence given a sequence of observations. Again, we want to express our components recursively. Given the state is j at time t, vt(j) is the joint probability of the observation sequence with the best state sequence. If we examine closely, the resulting equation is close to the forward algorithm except the summation is replaced by the max function. 
So not only it can be done, the solution is similar to the forward algorithm except the summation is replaced by the maximum function. Here, instead of summing over all possible state sequences in the forward algorithm, the Viterbi algorithm finds the most likely path. 
As shown below, finding the internal states that maximize the likelihood of observations is similar to the likelihood method. We just replace the summation with the maximum function. 
In this algorithm, we also record the maximum path leading to each node at time t (the red arrow above). e.g. we are transited from a happy state H at t=1 to the happy state H at t=2 above since it is the most optimal (likely) path. 
Let’s study the famous dishonest casino example. To cheat, a dealer switches between a fair dice and a biased die. But, to avoid detection, the dealer does it infrequently. A transition matrix is provided to model the chance of the dealer in switching between the fair dice or biased dice. We also provide a value distribution (the observed dice value distribution) for each dice. For example, for the fair dice, the dice value will be uniformly distributed — this is the emission probability. 
We can use the algorithms described before to make inferences by observing the value of the rolling die even though we don’t know which die is used. 
The internal state is which die is used. The line curve above is the likelihood to be at a particular state at time t. It fluctuates a lot. It gives a narrower view of what may happen at time t. The shaded area is the likely transition between the fair and the biased dice using the Viterbi algorithm. It is much smoother and reflects the transition better. It gives a global view on when states on transited. For the Viterbi algorithm, we find the most likely state sequence that explains the observations. For the likelihood, given different possible sequences, we sum them together accordingly to find the most likely state at time t. 
Learning (Baum–Welch algorithm or Forward-Backward Algorithm — build the model) 
Besides likelihood and decoding, the last algorithm learns the HMM model parameters λ given the observation. Here, we will use the Baum–Welch algorithm to learn the transition and the emission probability. 
This task sounds mission impossible since both probabilities are highly tangled in our calculation. But, if we know the state occupation probability (the state distribution at time t), we can derive the emission probability and the transition probability. If we know these two probabilities, we can derive the state distribution at time t. This is the chicken and egg problem we discussed in the EM algorithm. 
EM algorithm solves the problem in iteration steps. In each step, we optimizing one latent variable while fixing the others. Even for a continuous space, we work with limited provisions, and therefore, there are finite states to explore and improve only. Therefore, if we keep the iterations, the solution will converge. So it is not surprising that the Baum–Welch algorithm is an EM algorithm. 
Let’s get familiar with the following new notations. 
We are already familiar with α (forward probability) in the forward algorithm already. β (backward probability) is its close cousin in the reverse direction (the probability of seeing all the coming observations given a state i at time t). We can express this recursively similar to α but in the reverse direction (a.k.a. backward algorithm). 
To learn the HMM model, we need to know what states we are to explain the observations the best. That will be the occupation probability γ — the probability of state i at time t given all the observations. 
Given the HMM model parameters fixed, we can apply the forward and backward algorithm to calculate α and β from the observations. γ can be calculated by simply multiplying α with β, and then renormalize it. 
ξ is the probability of transiting from state i to j after time t given all the observations. It can be computed by α and β similarly. 
Intuitively, with a fixed HMM model, we refine the state occupation probability (γ) and the transition (ξ) with the given observations. 
Here comes the chicken and egg part. Once the distribution of γ and ξ (θ₂) are refined, we can perform a point estimate on what will be the best transition and emission probability (θ₁: a, b). 
We fix one set of parameters to improve others and continue the iteration until the solution converges. 
The EM algorithm is usually defined as: 
Here, the E-step establishes p(γ, ξ | x, a, b). Then, the M-step finds a, b that roughly maximizes the objective below. 
Here is the recap of the algorithm: 
Written by 
Written by",Jonathan Hui,2020-07-03T21:18:56.264Z
Longformer — The Long-Document Transformer 📝 | by Viktor Karlsson | dair.ai | Medium,"Processing longer forms of text with BERT-like models require us to rethink the attention mechanism in more than one way. How can we reduce the computational cost of the attention calculations, which grow quadratically with sequence length? Do we really need all tokens to attend to every other one in the sequence? These questions, and more, will be answered in this paper summary! 
The fact that Transformer based language models are computationally expensive to both train and use should come as no surprise. This is partly due to an ever-growing number of parameters but also the intrinsic cost of its attention mechanism. 
The attention mechanism allows the models to enrich each token representation with information from anywhere else in the sequence, which is at the core of Transformer based models’ success. Put simply, to process a sequence of n tokens requires n² attention calculations for each attention head during a forward pass. 
BERT addresses this by enforcing a hard limit of 512 tokens, which is more than enough to process the overwhelming majority of sequences in most benchmark datasets. But what if we want to work with longer forms of text, moving away from sentences and into the realm of documents? That would require us to rethink the attention mechanism. 
An initial thought that might seem obvious is the fact that the value a token brings to another in the attention mechanism diminishes the further apart they are. It should, therefore, make sense to limit the attention window each token has access to. This seed of an idea is what has been explored by Beltagy et al. in Longformer: The Long-Document Transformer. Their contribution and experimental findings are summarised below! 
Longformer introduces an attention mechanism that grows linearly with sequence length through introducing a sliding window of size w. This limits each token to only attend a subset of all tokens — the local ones thought to bring the most value. While this attention pattern might seem limited, it still allows a multi-layer transformer network to have a receptive field that covers the entirety of the sequence. 
The authors also introduce a dilated sliding window attention pattern to allow the receptive field to cover an even larger range. Here, w attention positions are separated by d empty spaces. I think you can see how this achieves the intended effect. 
An issue the observant reader will realize is how these attention windows affect special tokens such as [CLS] and [SEP]. The CLS token is supposed to be able to aggregate the entire sequence into a single representation to allow for classification. This becomes a bit weird when it cannot attend all tokens directly, even though it might be able to reach them indirectly through its receptive field. The authors address this by introducing task-specific, global attention on special tokens such as this one. This attention is symmetric in that every token in the sequence can attend to the special token, as it can attend all of them. 
A nicety the authors also mention as a key contribution is the CUDA kernel for these attention patterns. These are different enough to not simply be implemented efficiently by existing libraries due to their banded nature. The image below illustrates the computational speed (left) as well as the above-discussed memory savings (right) the novel attention patterns bring for longer sequences. The comparison is performed between full self-attention, a naive “for loop” implementation of their banded matrix multiplications, and their optimized version. 
How these novel attention patterns are deployed within the Longformer model is a research question in itself. It would make sense for lower layers to learn local features and enable later layers to combine these into higher-level sequences, similarly to how computer vision models learn its representations. The authors share this logic and gradually increase the attention window size for higher layers. Only a couple of the higher layers are configured with dilated sliding window attention. 
These design choices provide a balance between efficiency (smaller attention window allows for faster computation) and performance (larger attention window allows for greater representational power). 
To get a bit ahead of myself the ablation studies in the paper show that this approach achieves higher performance than both consistent window size throughout and configuring lower layers with larger ones and tapering off towards the later ones. 
The authors performed two distinct experiments to evaluate the Longformer, one to evaluate its language modelling capabilities and a second how well suited it is for the common pretraining-finetuning process. It’s the capability of performing both these tasks that set Longformer apart from some of its competition. 
Autoregressive language modelling, also known as left-to-right language modelling, is the task of predicting the next token, either word or character, given the left context. How well a model performs this task is evaluated by a metric called Bits Per Character (BPC), the average log loss measured in base two for the correct token. If you want to really dig deep on this topic I suggest this article as a good starting point. 
Two models are created to allow for valuable comparisons to its competitors; a large 30 layer- and a smaller 12 layer Longformer, both with a hidden dimension of 512. The evaluation was performed by running their model over sequences of 32.256 characters where performance was evaluated on the last 512 characters. This is in line with previous work. The authors achieve state-of-the-art results with their smaller model for both datasets, 1.10 and 1.00 BPC for text8 and enwik8 receptively when comparing to ones of similar size. 
The 30 layer model, when compared to true state-of-the-art achieves comparable performance, even when compared to larger models such as Transformer-XL (102M vs 277M parameters). What is worth noting here is the fact that Longformer can also be used for MLM pre-training tasks which is not possible by all models used in this comparison. 
Pretraining refers to the training scheme where a model initially is trained to perform a base task on a large general dataset, and then fine-tuned on the specific task and dataset. The general task for this NLP model is Masked Language Modelling (MLM) as popularised by BERT. MLM is computationally expensive which is why, to speed up their pretraining process, the authors initialize their model with weights from an already pretrained RoBERTa model (of the same dimensions of course). This highlights an important fact: The attention patterns are simple enough to be dropped into existing model architectures! 
Here, Longformer is evaluated in two distinct scenarios. The first one is to answer if the attention patterns can act as a replacement for standard self-attention patterns. This is achieved through comparing against RoBERTa, which deals with sequences longer than 512 tokens through breaking them up into manageable pieces, processing each separately, and then concatenate the embedded tokens for further processing. 
Compared to RoBERTa-base which was used for weight initialization, Longformer achieves higher performance numbers across all six benchmark tasks. The improvement is most notable on Hyperpartisan (a small dataset of documents with 705 wordpiece tokens on average) 
The second scenario used to evaluate Longformer's capabilities is in comparison to state-of-the-art models on the QA datasets. Some of its competitors employ task-specific architectures and training processes which, while achieving good results, are cumbersome to design and hard to adapt to other datasets or tasks. This would allow us to answer if these methods could be left to the bleeding edge research and allow us to have a simple model that performs well enough in most cases. 
To our benefit, that is essentially what is found! Longformer-large achieves state-of-the-art results on both WikiHOP and TriviaQA by a significant margin (3.6 and 4 points respectively). On HotpotQA, Logformer-large achieves comparable results to both larger and more complex models, which answers the question this evaluation aimed to address. 
This article has summarised the motivations, contributions, and experimental findings of Longformer: The Long-Document Transformer. It becomes clear that the attention patterns introduced by this work are versatile enough to be introduced into already existing Transformer architectures, while at the same time able to outperform its competition for some tasks. Even task-specific architectures and training schemes are surpassed or at least matched with this much simpler approach. 
If you found this summary helpful in understanding the broader picture of this particular research paper, please consider reading my other articles! I’ve already written a bunch and more will definitely be added. I think you might find this one interesting 👋🏼🤖 
Written by 
Written by",Viktor Karlsson,2020-07-11T13:19:14.229Z
Azure – Wortell – Medium,"The launch of Microsoft Surface Duo received a lot of attention! In case you missed the… 
In the past year I build several SOCs for my customers and the organisation I work for. A question I get…",NA,NA
Security – Wortell – Medium,Updates:,NA,NA
Using NLP (BERT) to improve OCR accuracy | by Ravi Ilango | States Title | Medium,"Optical Character Recognition (OCR) is a popular technique used to extract data from scanned documents. As you would expect, the accuracy of an OCR solution is contingent on the quality of images being used as input. One challenge facing practical applications of OCR solutions is the significant drop in word-level accuracy as a function of character-level accuracy. An OCR solution that achieves 98% character-level accuracy will find itself incorrectly extracting words 10–20% of the time, as depicted in the chart below. 
One way to improve the word accuracies is to use NLP (Natural Language Processing) techniques to replace incorrect words with correct ones. In this blog, we will use a spell checker and BERT (pre-trained NLP model) to improve OCR accuracy. 
BERT (Bidirectional Encoder Representations from Transformers) is a Natural Language Processing technique developed by Google. The BERT model has been trained using Wikipedia (2.5B words) + BookCorpus (800M words). BERT models can be used for a variety of NLP tasks, including sentence prediction, sentence classification, and missing word prediction. In this blog, we will use a PyTorch pre-trained BERT model to correct words incorrectly read by OCR. 
Let’s walk through an example with code. I’ll be using python to process a scanned image and create a text document using OCR, and BERT. 
Input scanned image 
Output of OCR with incorrectly parsed words 
Incorrect words are identified by enchant’s SpellChecker function. One thing to be mindful of when using SpellChecker is that it flags uncommon names as misspelled words. I’ll work around this problem by using nltk’s “parts of speech” tagging to exclude person names. To obtain a prediction from BERT, each incorrect word needs to be replaced with a [MASK] token. Finally, we will store replacement word suggestions from SpellChecker in our suggestedwords list. 
Document with incorrect words replaced with [MASK] 
BERT model looks for the [MASK] tokens and then attempts to predict the original value of the masked words, based on the context provided by the other, non-masked, words in the sequence. BERT also accepts segment embeddings, a vector used to distinguish multiple sentences and assist with word prediction. For example the segment vector for “Tom went to store. He bought two gallons of milk.” would be [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]. 
The BERT pre-trained language model is useful for predicting multiple viable replacements for the masked words. With that said, the model is not aware of any characters uncovered by OCR. We can augment this deficiency with our suggested word list from SpellChecker, which incorporates characters from the garbled OCR output. Combining BERT’s context-based suggestions with SpellChecker’s word-based suggestions yielded better predictions than relying solely on BERT. 
Final output with corrected words from BERT and SpellChecker 
The output looks a lot better now! The incorrect words have been accurately replaced thanks to the pre-trained BERT model. 
The BERT language model does a good job of predicting viable replacements for the masked word(s). In the above example, when asked to predict the masked value for “conmer”, the model suggested “tax”, “government”, “business”, and “consumer” as some of the choices. While all these suggestions make sense, “consumer” did not have the highest output probability. Without SpellChecker to augment BERT’s results, we would have misclassified the masked words. This could lead to problems when replacing words where the majority of characters are misidentified by OCR. 
Please note that the methods used in this blog are applicable for words and not numbers. A different approach (something like checksum) needs to be used for checking numbers read by OCR. I also recommend application specific error/suggestion identification as opposed to using SpellChecker alone. 
Written by 
Written by",Ravi Ilango,2019-12-18T18:10:38.728Z
Detecting the Pope Using Machine Learning and TLC Data | by Geoff P | Medium,"I recently made a trip down to Bogotá, Colombia to participate in Bloomberg’s Data For Good Exchange Immersion program — Xavier Gonzalez and I were sent down there to assist Bogotá’s Veeduría Distrital (Office of Anti-corruption and Oversight) in building a data dashboard to allow city leaders to better understand and address citizen complaints. The program was an incredible experience — but what was also incredible was the fact that Pope Francis blessed us (and the dashboard) with a visit to Colombia right in the middle of our immersion. 
This reminded me of the data science work I did a few months ago with Ben Miller and Chris Streich — we were able to build a “Pope Detector” through machine learning and urban open data. More specifically, we built a set of algorithms that predicted a visit from Pope Francis to New York City by looking at NYC Taxi & Limousine Commission data. We used a number of machine learning anomaly detection techniques to pick out outlier days where yellow taxi pickups and drop-offs were way out of whack — we found that one of the biggest outlier days was when Pope Francis visited St. Patrick’s Cathedral on September 24th, 2015. 
To build the “Pope Detector”, we first needed to download TLC data for 2015: 
Next, after a bit of date conversion and data cleaning, we subset the data to a few blocks around St. Patrick’s Cathedral (lat: 40.758477, lon: -73.976223). After the subset, we then aggregated the data by day, summing the number of trips and averaging the total cost for each taxi trip. 
In the plots above, we can see some peaks and troughs that reflect day of the week variations across time. However, you can see some outlier spikes in the data — our next task is to see when exactly those outliers are, how far of an outlier those dates are, and why those dates are outliers. 
We chose to first perform a k-means outlier detection analysis, using only the number of taxi pickups and the average total amount for a taxi trip, aggregated by day, as the feature set. By looking at the data, there seems to be two groups in the data — days with a lot of trips and a higher average cost per trip, and days with fewer taxi trips, and a lower average cost per trip. The silhouette score confirms this. 
In the plot above, you can see the two groups colored in light and dark blue, with the top 10 outlier points colored in red. Labeled are the days I find to be the most interesting. A snowstorm on January 27th ranks as the third highest outlier, as measured by distance from the closest K-means cluster center. September 24th, the day of Pope Francis’s visit to St. Patrick’s, ranks as the second highest outlier. Finally, May 31st, a rainy Sunday, ranks as the most distant outlier. I thought it would be interesting to also include the most normal day, as in, the data point / day closest to a cluster center. That happened to be April 17th, so I included it to give us all a good point of reference to what a typical taxi day around St. Patrick’s Cathedral looks like. 
The maps above (from L to R) show the taxi pickup chart for April 17th (the most “normal” day), September 24th (“Pope Day”), and May 31st — a rainy Sunday. You can right away see the difference between September 24th and the rest — the area around St. Patrick’s Cathedral (between 50th and 51st, 5th ave and Madison) has no pickups, presumably due to a police cordon. 
We also performed an isolation forest anomaly detection algorithm to test the robustness of our model and results. The May 31st and January 27th dates were still tops, with the “Pope Day” of September 24th falling to a not-too-distant fifth place in the list of outliers. 
So what does this all mean? Making “predictions” that happened over two years ago might not seem like the most powerful thing, but the fact that we were able to pick this out programmatically just by looking at aggregated TLC data is, in my opinion, extremely interesting. Much like astronomers scanning the sky for anomalous light sources, scanning the city for anomalous taxi patterns could prove to be very powerful. Taxis and traffic patterns are the lifeblood of New York City, and in addition to event detection, one could use them as a proxy to measure the movement of citizens and livelihood of different areas of the city. 
As always, feedback is more than welcome, and all code is posted on the github repository. 
Written by 
Written by",Geoff P,2017-09-28T14:34:41.971Z
How to Build a Recommender System(RS) | by Gaurav Sharma | Data Driven Investor | Medium,"(Recommender System Concept) 
This blog focuses on how to build a recommender system from scratch. 
It focuses on the technique and in depth conceptual details of building a recommender system from scratch. It gives a clear idea of what are the algorithms used in building an effective recommender system. Here, we have also discussed about the solution of Netflix Prize competition. 
A recommender system is an information filtering system that seeks to predicts the rating given by a user to an item. This predicted rating then used to recommend items to the user. The item for which the predicted rating is high will be recommended to the user. This recommender system is utilized in recommendation of a broad range of items. For instance, it can be used to recommend movies, products, videos, music, books, news, Facebook friends, clothes, Twitter pages, Android/ios apps, hotels, restaurants, routes etc. It is used by almost all of the major companies to enhance their business and to enrich user experience like YouTube for recommending videos, Amazon & Ebay for recommending products, Netflix for recommending Movies, Airbnb for recommending rooms and hotels, Facebook for recommending friends etc. 
In above USER-ITEM matrix, each row represents a user and each column represents an item and each cell represents rating given by a user to an item. There are total ’n’ users and ‘m’ items. Here, Aij is the rating given by a user Ui on item Ij. Aij can range from 1 to 5. Sometimes Aij can also be binary, if a matrix represents whether a user Ui has watched an item Ij or not. Here, Aij would be either 0 or 1. In our case, we are considering Aij as a rating from 1 to 5. 
This USER-ITEM matrix is very sparse matrix, which means that many cells in this matrix are empty. Since, there are many items, and a single user cannot give rating to all of the items. In real-world, a single user does not give ratings to even 1% of the total items. Therefore, around 99% of the cells of this matrix are empty. These empty cells can be represented by “NaN” means not a number. For example, let say ’n’ is 1-Million and ‘m’ is 10k. Now n*m is 10 ^10 which is a very large number. Now, let say an average user gives rating to 5 items. Then total number of ratings given on an average will be 5 * 1-Million = 5 * 10⁶ ratings. Now there is a metric called Sparsity of a matrix. 
Sparsity of Matrix = Number of Empty cells / Total Number of cells. 
Here, Sparsity of matrix = (10¹⁰ — 5*10⁶) / 10¹⁰ = 0.9995 
It means that 99.95% of cells are empty. This is extreme sparsity actually. 
Task of Recommender System(RS): Let say, if there is a user Ui who likes item I1, I5, I7. Then we have to recommend user Ui an item such Ij which he/she will most probably like. 
As depicted in the image above, we will discuss 4 types of Recommender Systems: 
1. Collaborative Filtering 
2. Content Based Filtering 
3. Similarity Based Filtering 
4. Matrix Factorization 
Collaborative filtering RS works with the collaboration of users. If there are many users who liked some item then that item can be recommended to that user who hasn’t seen that item yet. 
Let’s understand this with an example: 
Let say there are four users and four items as depicted in the image above. All four users bought Item-1 and Item-2. USER-1, USER -2 and USER -3 bought Item-3 also but USER-4 hasn’t seen Item-3 yet. So, Item-3 can be recommended to USER -4. Now only USER-3 bought Item-4 so, we cannot recommend Item-4 to USER -4 because only USER -4 bought Item-4 and non of the other users bought this item. THIS IS HOW COLLABORATIVE FILTERING WORKS. 
Core-idea/assumption here is that the users who have agreed in the past tend to also agree in the future. 
Here, all the three users namely USER-1, USER-2 and USER-3 agreed in the past that Item-3 is worth purchasing , hence in the future USER-4 may like Item-3 which is something USER-1, USER-2 and USER-3 agreed to purchase in the past. 
If above assumption does not hold true then collaborative based filtering RS cannot be build. 
Content based filtering is similar in approach with classical machine learning techniques. It needs a way to represent an item Ij and a user Ui. Here, we need to collect information about an item Ij and a user Ui then finally we need to create features of both user Ui and Ij. Then we combine those features and feed them to a Machine Learning model for training. Here, label will be Aij, which is the corresponding rating given by a user Ui on item Ij. 
Let’s take an example to understand this in more detail: 
Let say the item in our data-set is a movie. Now we can create its feature like this: 
· Genre of Movie 
· Year of Release 
· Lead Actor 
· Director 
· Box Office Collection 
· Budget 
Similarly, features for user can also be created: 
· Likes and dislikes of users 
· Gender of a user 
· Age of user 
· Where user lives 
As soon as we have the above mentioned information about items and users, we can create an item vector which shall contain information about the item which is mentioned above. Then, we can similarly create a user vector which shall contain information about the user which is mentioned above. We can generate features for each user Ui and an item Ij. Finally we can combine these features and create a big data-set which can be suitable for feeding to Machine Learning model. 
Here, above I have just explained an approximate way to create features for content based filtering. These features should be carefully designed so that they impact the rating/label directly without being dependent on each other. It is always better to create as independent features as possible and at the same time they should be very much dependent on rating/label means that they should directly affect the rating/label. 
There are broadly two types of similarity based approaches that we can deploy. 
· USER-USER SIMILARITY 
· ITEM-ITEM SIMILARITY 
This is basically very simple. You must have guess it just by watching above image. 
Let’s dive deep into it. 
Step-1: Construct USER-USER Similarity Matrix 
Now, in above USER-ITEM Matrix, each row represents a user which contains ratings given by a user to all the items. For instance, row corresponding to user Ui is a vector of size ‘m’. Hence, each row of the above matrix is nothing but a column vector(Every vector is column vector by default) of size ‘m’. Now, we can construct a USER-USER similarity matrix which will be a square symmetric matrix of size n*n. Here, we can calculate similarity between two users using cosine similarity. 
Here, two users will be similar on the basis of the similar ratings given by both of them. If any two users are similar then it means both of them have given very similar ratings to the items because here the user vector is nothing but the row of USER-ITEM matrix which in turn contains ratings given by user to the items. Now since cosine similarity can range from ‘0’ to ‘1’ and ‘1’ means highest similarity, so therefore all of the diagonal elements will be ‘1’ because the similarity of the user with him/herself is the highest. Here, “Sim12” is a similarity score of user U1 and user U2. Similarly, “Simij” is a similarity score of user Ui and Uj. 
Step-2: Find similar users 
Once, we have a USER-USER similarity matrix now we have to find the similar users. Let say from USER-USER similarity matrix, three most similar users to user U10 →U1, U7, U15. 
Step-3: Pick items liked by similar users 
Now we have to pick all those items which were liked by users U1, U7 and U15 that are not yet seen by user U10. 
Step-4: Recommend items 
Now we have the items liked by users which are most similar to user U10. So, now we can recommend those items to U10. 
This is how USER-USER similarity works. 
But there is one problem with USER-USER similarity. User preferences and taste change over time. If any user liked some item one year ago then it is not necessary that he/she will like the same item even today. 
One solution could be to use recent ratings. Let say, we can use ratings no earlier than 90 days which is equivalent to last 3 months of data. But using only recent data would make USER-ITEM matrix more sparser. 
Another problem with USER-USER similarity is that computing USER-USER similarity would take a lot of time. As some of the popular websites like YouTube, Amazon, Netflix has millions of registered users. So, there could be billions of computations needs to be done in order to build USER-USER similarity matrix because similarity of each user has to be calculated with million other users. 
So, the alternative approach will be item-item similarity. We will discuss it 
This is also very simple and very similar in idea with USER-USER Similarity. You must have guess it just by watching above image. 
Let’s dive deep into it. 
Step-1: Construct ITEM-ITEM Similarity Matrix 
Now, in above USER-ITEM Matrix, each column represents to an item which contains ratings given to this item by all the users. For instance, column corresponding to item Ij is a vector of size ‘n’. Hence, each column of the above matrix is nothing but a item vector of size ‘n’. Now, we can construct ITEM-ITEM similarity matrix which will be a square symmetric matrix of size ‘m*m’. Here also we can calculate the similarity between two items using cosine similarity just like the way we calculated user-user similarity. 
Here, two items will be similar on the basis of the similar ratings given to both the items by all of users. If any two items are similar then it means both of them were given very similar ratings by all the users because here the item vector is nothing but the column of USER-ITEM matrix which in turn contains ratings given by user to the items. Now since cosine similarity can range from ‘0’ to ‘1’ and ‘1’ means highest similarity, so therefore all of the diagonal elements will be ‘1’ because the similarity of an item with the same item is the highest. Here, “Sim12” is a similarity score of user I1 and user I2. Similarly, “Simij” is a similarity score of user Ii and Ij. 
Step-2: Find similar items and then recommend 
Let say a user U10 liked I1, I7 and I15. Now from ITEM-ITEM similarity matrix we found similar items to I1 →{I4, I5, I6}. We also found similar items to I7 →{I4, I8, I9} and I15 →{I10, I11, I12}. Now the I4 is the common item which is similar to both I1 and I7. Hence, we can recommend I4 to user U10. 
This is how ITEM-ITEM similarity works. 
One key advantage of ITEM-ITEM similarity is that the ratings on a given item do not change significantly after initial period. Let’s take example of “Titanic” movie. In the beginning many people gave rating to “Titanic” movie and let say the average rating of titanic was 4 out of 5 stars. After initial period people realized that titanic was a great movie and hence it’s ratings would not change as significantly over time after initial period. 
As a rule of thumb, when we have more users than items and when item ratings do not change much over time after the initial period then ITEM-ITEM similarity based RS is preferable over USER-USER based RS. 
Here, in the above image, Matrix “A” is nothing but a USER-ITEM matrix where each cell in the matrix is a rating given by the user to the item. We have to remember that this USER-ITEM matrix is very sparse matrix. 
Matrix “A” has been decomposed into two matrices “B” & “C”. Now matrices “B” & “C” are nothing but factors of matrix “A”. It is similar in concept when a number is factorized into two smaller numbers like 6 = 2*3. 
Now Aij is a product of Bi_Transpose*Cj. Since, every vector is a column vector by default so ‘Bi’ is nothing but ith row of ‘B’ which is of size ‘d*1’ and “Cj” is nothing jth row of ‘C’ which is also of size ‘d*1’. Now Aij is a rating given by a user ‘i’ to an item ‘j’ which is a scalar number. So, in order to accommodate multiplication we have to take transpose of “Bi” vector. Therefore Aij = Bi_Transpose *Cj. Aij is of size (1*1). 
Now, there are many empty cells in matrix “A” for which we will talk later. As of now, let’s pose this problem as an optimization problem. We already know that any optimization problem can be solved using Stochastic Gradient Descent(SGD) algorithm. 
Our goal is to find matrices ‘B’ & ‘C’. Initially, we initialize matrices ‘B’ & ‘C’ randomly. Then we will solve optimization problem and find ‘B’ & ‘C’. Now we have to find matrices ‘B’ & ‘C’ such that following condition holds as per optimization problem. 
Let’s read the above equation in plain English. 
{I want to find matrices ‘B’ & ‘C’ wherever I have Aij means where Aij is not empty — such that ∑(Aij — Bi_Transpose * Cj)^2 is minimized} 
If you intuitively think, then {Aij — Bi_Transpose* Cj} is nothing but an error which we have to minimize. 
Therefore, this is an optimization problem. This is nothing but a squared-loss which can be minimized by SGD algorithm. This looks like a regression problem. In ideal case (Aij — Bi_Transpose* Cj) will become 0. 
After solving this problem using SGD, we will get matrices ‘B’ & ‘C’. ‘B’ will be of size ‘N*d’ and ‘C’ will be of size ‘M*d’. Once we get matrices ‘B’ & ‘C’ then this problem will become a matrix completion problem. Now for any empty cell Aij in matrix ‘A’, we can calculate {Bi_Transpose * Cj}. Let say A_10,5 is empty. We can now fill A_10,5 as “B_10_Transpose * C_5”. By this we can fill all of the empty cells in matrix ‘A’. These ratings which we got for empty cells are nothing but the predicted ratings. 
Let’s reiterate all the steps: 
Step-1: Solve the optimization problem. Remember here we have used only non empty Aij. 
Step-2: Find matrices ‘B’ & ‘C’. 
Step-3: Complete the matrix ‘A’ by solving “A = B*C_Transpose”. Remember that this new matrix which we got after multiplying B*C_Transpose is no more a sparse matrix. 
When we found matrices ‘B’ & ‘C’, we have to ensure that the non empty cells of the original matrix ‘A’ and the new matrix ‘A’ should be very-very close. 
Now recommending a new item to the user is trivial. It is nothing but just a look-up into our new matrix ‘A’ which is non-empty and non sparse. Let say a user U_50 hasn’t seen an item I_70, then obviously the rating for this item was not present earlier. Now in our new matrix ‘A’ this rating is present which is nothing but a predicted rating. If this predicted rating is high then we recommend item I_70 to user U_50. 
This is how recommender System can be build using Matrix Factorization. 
Ques: Now the question comes, that there were many empty cells in matrix ‘A’ to start with. So, how they are approximated? 
Ans: Here, the fundamental assumption that matrix factorization is making is that if you build you matrices ‘B’ & ‘C’ using the non empty cells of matrix ‘A’ and if you reconstruct the values in empty cells of ‘A’ using matrices ‘B’ & ‘C’ then matrices ‘B’ & ‘C’ can approximate the values which are there in the empty cells also. This is a very fundamental assumption we are making when we build matrix factorization using non empty cells. In real world, this assumption holds true in most of the cases particularly in case of Recommender Systems. Since in optimization problem we are trying to approximate the non-empty cells as closely as possible so it will approximate for empty cells too. 
The Netflix Prize was an open competition for the best collaborative filtering algorithm to predict user ratings for films, based on previous ratings without any other information about the users or films. 
Netflix provided a training data set of 100,480,507 ratings that 480,189 users gave to 17,770 movies. Each training rating is a quadruplet of the form <user, movie, date of grade, grade>. The user and movie fields are integer IDs, while grades are from 1 to 5 (integral) stars. 
Netflix gave the Root Mean Square Error(RMSE) as the key performance metric(KPI). Using this data if any team or individual can build an algorithm which can reduce the RMSE of the actual rating(which Netflix provided) and the predicted rating(which the competitor’s algorithm will predict) by more than 10% of the RMSE — which Netflix already has — will win the competition. 
Let’s move on to the solution part itself: 
Let me introduce few notations before we move forward. 
r_ui = rating given by user ‘u’ to item ‘i’ 
qi = item-vector 
pu = user-vector 
Above equation is similar to our following equation: 
The solution: 
Here, in the above equation, first part is a squared-loss and the second part is L2-regularizer which is applied to avoid over-fitting. Here, ƛ is a hyper-parameter. 
In order to solve this problem there are methods: 
1. Stochastic Gradient Descent(SGD): Here, we can calculate derivative of above equation with respect to qi and with respect to pu. 
2. Alternating Least Squares(ALS): Here, first we fix pu and apply gradient descent on the above equation with respect to qi. Then, we fix qi and apply gradient descent on the above equation with respect to pu. It alternates between pu and qi till the time it converges. 
Method 2 is generally followed because it is faster. 
The above formulation is good but it cannot guaranteed the lowest RMSE. So, the winners of the competition added three new terms: bu, bi, µ. 
Let’s read the complete equation below then I will introduce the meaning of the above three terms: 
Here, bu and bi are bias terms and µ is a mean-term. All three are scalars and not vectors. 
There is one bias term bu for every user. 
There is one bias term bi for every item. 
µ → ‘µ’ is the average rating across all the users and items. It is basically the average of all the ratings that we have in our data. It is a Global Average of all ratings. 
bu, bi → ‘ bu’ is a user bias & ‘bi’ is an item bias. Now there is one bias term for every user and one bias term for every item. You might be wondering what are these bias terms. 
Let’s take an example to understand this. Let say there is a user ‘John’ who is very critical user who gives rating 3.9 to ‘Titanic’ movie which was a very successful movie. 
Here, r_ui = 3.9 (rating given by a user to an item) 
Now since ‘John’ is very critical user so we have to account his bias ‘bu’ into the rating. 
So for ‘John’ bu = -0.3 (since ‘John’ is critical user so his bias will be in negative) 
Now, ‘Titanic’ was very successful movie so we have to account the bias ‘bi’ into the rating. 
So for ‘Titanic’ bi = 0.5 (since ‘Titanic’ was a very successful movie so it’s bias will be positive) 
Now ‘µ’ is nothing but global average. 
Let say here µ = 3.0 
Let say pu_Transpose * qi = 0.7 
Now, r_ui — µ — bu — bi — pu_Transpose * qi = 3.9 -3.0 + 0.3–0.5–0.7 = 0 
In a nutshell, we have to find that bu, bi, p and q which can minimize the above equation. In the end we will have user-bias for every user, item-bias for every item and matrices ‘q’ and ‘p’. 
Equation -1: 
Equation-2 
Now we went from equation-1 to equation-2 in literally one step. We just thought of user-bias and item-bias and incorporated both of them into our formulation. This is the advantage of optimization on matrix factorization. It would not be possible on user-user and item-item similarity. 
In the actual research paper which winners of Netflix prize wrote, they extend this further and commented that “ratings by users and for items are time dependent”. Therefore, they made rating, user-bias and item-bias r_ui(t), bu(t) & bi(t) respectively as a function of time. 
We have coded a full-fledged case-study on “Netflix-Movie-Recommendation-System”. Check out the code here. We haven’t explained this complete code here because we want to focus more on concept of building a recommender system. If you are able to understand the concept which we have explained above then you can easily make a recommender system and off-course our code link will be a reference point for you. 
We have applied all of the techniques which we have explained above in this blog. The data link has also been provided in the code link. You can download the data from provided data source and just follow line-by-line code. 
One important point which I want to mention here about the Netflix-Prize-Solution Implementation. We have implemented all of the concepts which are explained in Netflix-Prize-Solution. For implementation we have used a library called “Surprise” library. We have provided the link in the reference section below. This library has all of the codes implemented which are explained in Netflix-Prize-Solution. 
Download this library as: 
1. https://www.appliedaicourse.com/ 
2. https://datajobs.com/data-science-repo/Recommender-Systems-[Netflix].pdf 
3. http://surpriselib.com/ 
We have tried our best to explain each and every concept of building a recommender system in as easy manner as possible. I hope you like my blog. 
In each issue we share the best stories from the Data-Driven Investor's expert community. Take a look 
Written by 
Written by",Gaurav Sharma,2018-12-06T10:06:11.710Z
Is my data safe in Cloud?. GCP Comics #1: Google Cloud & Privacy… | by Priyanka Vergadia | Google Cloud - Community | Medium,"It is often asked — what happens to my data in cloud? Is it safe and secure? Who has access? 
If you are looking for answers to these questions, then you clicked on the right link 😃 In this first season of GCP Comics we will learn Google Cloud Security concepts and have lots of fun along the way. 
Here you go! Read on and please share your thoughts in the comments below. 
Security requires deep expertise and plentiful dedicated resources to achieve, mainly because it is a multidimensional issue comprising physical (data center) security, platform and network security, proactive threat detection, audits and compliance with industry-specific certifications such as HIPAA and PCI. But the first and most important step in any security conversation is trust. 
First and most important step in any security conversation is trust 
We know that trust is created through transparency. For this reason; Google Cloud has created trust principles which clarify the commitment to protect the privacy of customers data. 
1. Your data belongs to you and no one else 
2. Google Cloud does not sell customer data to third parties. Nor is it used in advertising. 
3. Your data is encrypted in transit and at rest at all times automatically. You do not have to ask or enable it, this happens by default. 
And, if you want, you can apply additional encryption by bringing your own encryption keys. These are the two ways: 
4. Know where your data is stored and rely on it being available when you need it. 
Location of Google data centers is published and they are highly available, resilient and secure. You can rely on your data being available when you request it. You also have control over which locations you would like your data to be stored in depending on the service you use. You can choose to store data closer to your users, apps or both. 
5. There are explicit rules to guard against insider access to your data and no “backdoor” to Google. 
Invalid government requests are rejected, and transparency report is published for those requests. 
6. The privacy practices are audited against international standards. 
This means you can choose to store your data within Google Cloud anywhere in the world without having to worry about standard met for that specific location. 
Google Cloud provides you with the right tools to control access to the data and choose who has access to what parts of your data. 
Privacy team is equally involved in the launch of each product and the documentation to make sure all the privacy requirements and standards are met. 
To learn more about privacy on Google Cloud, check out this link. 
Want more GCP Comics? Visit gcpcomics.com & follow me on Medium, and on Twitter to not miss the next issue! 
Written by 
Written by",Priyanka Vergadia,2020-09-09T07:39:16.118Z
Explore Markov Chains With Examples — Markov Chains With Python | by Sayantini Deb | Edureka | Medium,"Have you ever wondered how Google ranks web pages? If you’ve done your research then you must know that it uses the PageRank Algorithm which is based on the idea of Markov chains. This article on Introduction To Markov Chains will help you understand the basic idea behind Markov chains and how they can be modeled as a solution to real-world problems. 
Here’s a list of topics that will be covered in this blog: 
Andrey Markov first introduced Markov chains in the year 1906. He explained Markov chains as: 
A stochastic process containing random variables, transitioning from one state to another depending on certain assumptions and definite probabilistic rules. 
These random variables transition from one to state to the other, based on an important mathematical property called Markov Property. 
This brings us to the question: 
Discrete Time Markov Property states that the calculated probability of a random process transitioning to the next possible state is only dependent on the current state and time and it is independent of the series of states that preceded it. 
The fact that the next possible action/ state of a random process does not depend on the sequence of prior states, renders Markov chains as a memory-less process that solely depends on the current state/action of a variable. 
Let’s derive this mathematically: 
Let the random process be, {Xm, m=0,1,2,⋯}. 
This process is a Markov chain only if, 
for all m, j, i, i0, i1, ⋯ im−1 
For a finite number of states, S={0, 1, 2, ⋯, r}, this is called a finite Markov chain. 
P(Xm+1 = j|Xm = i) here represents the transition probabilities to transition from one state to the other. Here, we’re assuming that the transition probabilities are independent of time. 
Which means that P(Xm+1 = j|Xm = i) does not depend on the value of ‘m’. Therefore, we can summarise, 
So this equation represents the Markov chain. 
Now let’s understand what exactly Markov chains are with an example. 
Before I give you an example, let’s define what a Markov Model is: 
A Markov Model is a stochastic model that models random variables in such a manner that the variables follow the Markov property. 
Now let’s understand how a Markov Model works with a simple example. 
As mentioned earlier, Markov chains are used in text generation and auto-completion applications. For this example, we’ll take a look at an example (random) sentence and see how it can be modeled by using Markov chains. 
The above sentence is our example, I know it doesn’t make much sense (it doesn’t have to), it’s a sentence containing random words, wherein: 
Moving ahead, we need to understand the frequency of occurrence of these words, the below diagram shows each word along with a number that denotes the frequency of that word. 
So the left column here denotes the keys and the right column denotes the frequencies. 
From the above table, we can conclude that the key ‘edureka’ comes up 4x as much as any other key. It is important to infer such information because it can help us predict what word might occur at a particular point in time. If I were to take a guess about the next word in the example sentence, I would go with ‘edureka’ since it has the highest probability of occurrence. 
Speaking about probability, another measure you must be aware of is weighted distributions. 
In our case, the weighted distribution for ‘edureka’ is 50% (4/8) because its frequency is 4, out of the total 8 tokens. The rest of the keys (one, two, hail, happy) all have a 1/8th chance of occurring (≈ 13%). 
Now that we have an understanding of the weighted distribution and an idea of how specific words occur more frequently than others, we can go ahead with the next part. 
In the above figure, I’ve added two additional words which denote the start and the end of the sentence, you will understand why I did this in the below section. 
Now let’s assign the frequency for these keys as well: 
Now let’s create a Markov model. As mentioned earlier, a Markov model is used to model random variables at a particular state in such a way that the future states of these variables solely depends on their current state and not their past states. 
So basically in a Markov model, in order to predict the next state, we must only consider the current state. 
In the below diagram, you can see how each token in our sentence leads to another one. This shows that the future state (next token) is based on the current state (present token). So this is the most basic rule in the Markov Model. 
The below diagram shows that there are pairs of tokens where each token in the pair leads to the other one in the same pair. 
In the below diagram, I’ve created a structural representation that shows each key with an array of next possible tokens it can pair up with. 
To summarize this example consider a scenario where you will have to form a sentence by using the array of keys and tokens we saw in the above example. Before we run through this example, another important point is that we need to specify two initial measures: 
We’ve defined the weighted distribution at the beginning itself, so we have the probabilities and the initial state, now let’s get on with the example. 
Give yourself a pat on the back because you just build a Markov Model and ran a test case through it. To summarise the above example, we basically used the present state (present word) to determine the next state (next word). And that’s exactly what a Markov process is. 
It is a stochastic process wherein random variables transition from one state to the other in such a way that the future state of a variable only depends on the present state. 
Let’s take it to the next step and draw out the Markov Model for this example. 
The above figure is known as the State Transition Diagram. We’ll talk more about this in the below section, for now just remember that this diagram shows the transitions and probability from one state to another. 
Notice that each oval in the figure represents a key and the arrows are directed toward the possible keys that can follow it. Also, the weights on the arrows denote the probability or the weighted distribution of transitioning from/to the respective states. 
So that was all about how the Markov Model works. Now let’s try to understand some important terminologies in the Markov Process. 
In the above section we discussed the working of a Markov Model with a simple example, now let’s understand the mathematical terminologies in a Markov Process. 
In a Markov Process, we use a matrix to represent the transition probabilities from one state to another. This matrix is called the Transition or probability matrix. It is usually denoted by P. 
Note, pij≥0, and ‘i’ for all values is, 
Let me explain this. Assuming that our current state is ‘i’, the next or upcoming state has to be one of the potential states. Therefore, while taking the summation of all values of k, we must get one. 
A Markov model is represented by a State Transition Diagram. The diagram shows the transitions among the different states in a Markov Chain. Let’s understand the transition matrix and the state transition matrix with an example. 
Consider a Markov chain with three states 1, 2, and 3 and the following probabilities: 
The above diagram represents the state transition diagram for the Markov chain. Here, 1,2 and 3 are the three possible states, and the arrows pointing from one state to the other states represents the transition probabilities pij. When, pij=0, it means that there is no transition between state ‘i’ and state ‘j’. 
Now that we know the math and the logic behind Markov chains, let’s run a simple demo and understand where Markov chains can be used. 
To run this demo, I’ll be using Python. 
Now let’s get started with coding! 
Problem Statement: To apply Markov Property and create a Markov Model that can generate text simulations by studying Donald Trump speech data set. 
Data Set Description: The text file contains a list of speeches given by Donald Trump in 2016. 
Logic: Apply Markov Property to generate Donald’s Trump’s speech by considering each word used in the speech and for each word, create a dictionary of words that are used next. 
Step 1: Import the required packages 
Step 2: Read the data set 
Step 3: Split the data set into individual words 
Next, create a function that generates the different pairs of words in the speeches. To save up space, we’ll use a generator object. 
Step 4: Creating pairs to keys and the follow-up words 
Next, let’s initialize an empty dictionary to store the pairs of words. 
In case the first word in the pair is already a key in the dictionary, just append the next potential word to the list of words that follow the word. But if the word is not a key, then create a new entry in the dictionary and assign the key equal to the first word in the pair. 
Step 5: Appending the dictionary 
Next, we randomly pick a word from the corpus, that will start the Markov chain. 
Step 6: Build the Markov model 
Following the first word, each word in the chain is randomly sampled from the list of words which have followed that specific word in Trump’s live speeches. This is shown in the below code snippet: 
Step 7: Predictions 
Finally, let’s display the stimulated text. 
So this is the generated text I got by considering Trump’s speech. It might not make a lot of sense but it is good enough to make you understand how Markov chains can be used to automatically generate texts. 
Now let’s look at some more applications of Markov chains and how they’re used to solve real-world problems. 
Here’s a list of real-world applications of Markov chains: 
With this, we come to the end of this Introduction To Markov Chains blog. If you wish to check out more articles on the market’s most trending technologies like Artificial Intelligence, DevOps, Ethical Hacking, then you can refer to Edureka’s official site. 
Do look out for other articles in this series which will explain the various other aspects of Deep Learning. 
1. TensorFlow Tutorial 
2. PyTorch Tutorial 
3. Perceptron learning Algorithm 
4. Neural Network Tutorial 
5. What is Backpropagation? 
6. Convolutional Neural Networks 
7. Capsule Neural Networks 
8. Recurrent Neural Networks 
9. Autoencoders Tutorial 
10. Restricted Boltzmann Machine Tutorial 
11. PyTorch vs TensorFlow 
12. Deep Learning With Python 
13. Artificial Intelligence Tutorial 
14. TensorFlow Image Classification 
15. Artificial Intelligence Applications 
16. How to Become an Artificial Intelligence Engineer? 
17. Q Learning 
18. Apriori Algorithm 
19. Object Detection in TensorFlow 
20. Artificial Intelligence Algorithms 
21. Best Laptops for Machine Learning 
22. Top 12 Artificial Intelligence Tools 
23. Artificial Intelligence (AI) Interview Questions 
24. Theano vs TensorFlow 
25. What Is A Neural Network? 
26. Pattern Recognition 
27. Alpha Beta Pruning in Artificial Intelligence 
Originally published at https://www.edureka.co on July 2, 2019. 
Written by 
Written by",Sayantini Deb,2020-05-12T10:52:14.512Z
Education – BerkeleyISchool – Medium,,NA,NA
AI – InfoSec Write-ups – Medium,,NA,NA
Cloud Security – Medium,,NA,NA
We’re Optimizing Ourselves to Death | by Zander Nethercutt | Medium,"/prōˌseləˈrāSH(ə)n/noun 1. The acceleration of acceleration 
—excerpt from The Age of Earthquakes by Shannon Basar, Douglas Coupland, and Hans Ulrich Obrist 
There’s a famous thought experiment in economics known as the “prisoner’s dilemma.” In it, two men have been caught committing a crime. Each of them is placed in a separate interrogation room and effectively has two options: confess or lie. There are three possible outcomes (the payoffs of which are illustrated in the payoff matrix below): 
Outcome 1: Both confess, and both serve eight years in prison (illustrated by payoff “-8, -8” in Figure A). 
Outcome 2: Both men lie, and both serve one year in prison (illustrated by payoff “-1, -1” in Figure A). 
Outcome 3: One man confesses while the other lies. The liar serves the longest possible sentence, 10 years, while the confessor goes free (illustrated by payoff “-10, 0” in Figure A). 
So, if both men lie, they both get off with a lighter sentence. That appears to be the full story—except it isn’t. 
The importance of the prisoner’s dilemma is understanding that in selecting a strategy, each player should account for the effectiveness of that strategy given what the other player might do. 
In a hyperproductive, work-obsessed world, we’ve become acutely aware of any opportunity for optimization. 
Knowing this, consider the game from the perspective of Prisoner 1. If he thinks Prisoner 2 will lie, he should confess, because serving zero years in prison is better than serving one. If he thinks Prisoner 2 will confess, he should also confess, because serving eight years in prison is better than serving 10. In this situation, confessing is both players’ dominant strategy, the strategy they should play regardless of what the other player does. 
This thought experiment illustrates how two self-interested individuals with a clear way to maximize their collective utility fail to do so. It also happens to be a fantastic way to understand our current moment. Millennials—not all of us, but many of us—are burned out, and the prisoner’s dilemma can shed light on why. 
Unfortunately, it also sheds light on a distressing conclusion: Barring some miracle of human coordination, our quest to optimize our lives will never slow, let alone stop. If anything, it will accelerate. 
Imagine a two-player labor market represented by the prisoner’s dilemma matrix. Now imagine both players encountered a service that would help optimize their lives. For a real-world example (and one I use), let’s take the premade meal delivery service Freshly. 
Freshly claims to save people approximately two hours a week in the time they don’t have to spend grocery shopping, meal prepping, and cooking. Now imagine that both players had two choices for how they could spend those hours: either on extra leisure (e.g., sleep, Netflix, a book, etc.) or on productivity (e.g., optimization/work). 
What would each player choose? 
Well, if wealth is considered freedom from busyness, or freedom to spend your time as you wish, the hour would be best spent on leisure. When forming a strategy, however—like with the prisoner’s dilemma—players must consider those strategies in the context of what the other players in the game might do. Consider the adjusted payoff matrix below: 
Outcome 1: Both players use the time afforded by the service’s convenience to optimize/work harder and thus remain in a state of constant acceleration (illustrated by payoff “1, 1” in Figure B). 
Outcome 2: Both players use the time afforded by the service’s convenience to relax (illustrated by payoff “8, 8” in Figure A). 
Outcome 3: Player 1 uses the time afforded by the service’s convenience to optimize/work harder, while Player 2 uses it to relax. Player 1 reaps the benefits of being the only provider of labor in a market and corners it. Player 2 languishes as the world accelerates endlessly and leaves him behind (illustrated by payoff “10, 0” in Figure A). 
Borrowing earlier analysis, it’s clear that given the payoffs, both players have a dominant strategy: work. If Player 2 relaxes, Player 1 should work because a payoff of 10 is better than a payoff of eight. If Player 2 works, Player 1 should also work because a payoff of one is better than a payoff of zero. 
Now, remember, these payoffs—and their explanations—are completely made up. In the modern era, there is no reason to be convinced that torturing yourself with additional employment is associated with any improvement in your lifestyle. And yet this is exactly how most people behave. 
Thus, we arrive at our new Nash equilibrium: Both players use a service—mind you, a service built to supposedly make their lives easier and more relaxing—that ends up making their lives more stressful and complex. Put another way, both players burn out. 
In a recent viral BuzzFeed News article, “How Millennials Became the Burnout Generation,” Anne Helen Petersen notes this seeming paradox of leisure, specifically as it pertains to free time. She writes: 
Attempts [by companies] to discourage working “off the clock” misfire, as millennials read them not as permission to stop working, but a means to further distinguish themselves by being available anyway. 
In other words: Attempts by companies like Google or Freshly to create services that save you time misfire, as millennials see them not as services that will give them more time to relax, but as services that will increase the amount of time they’re available to work. 
As employees in a hyperproductive, work-obsessed world, we’ve become acutely aware of any opportunity for optimization. Our Instagram feeds are filled with every possible combination of meal delivery service and online shopper that exists. Startups emerge daily to automate every mundane activity ever scrawled on and scratched off a legal pad. 
Only the most successful are free enough to spend their time finding better ways to spend their time. 
The escalators I take to work are filled with the same desperate faces and vacant eyes I feel staring through me on the subway, except instead of standing still, they’re bounding up it, subconsciously aware that below their feet is yet another opportunity to optimize on an existing convenience. This, if anything, is a symptom of our current moment: People ignoring the luxury of a moving staircase in favor of whatever they can reach faster by sprinting up it. 
There’s a kind of sick satisfaction derived from optimizing one’s own life, and for good reason: Being able to do so is a status symbol. Only the most successful are free enough to spend their time finding better ways to spend their time. For those at the very top, I imagine these methods of optimization can actually exist in a vacuum; billionaires can optimize for the sake of optimizing, rather than to keep their heads above water. For the rest of the world, optimization is a survival mechanism. To them, the tools that are luxuries to those at the top are good for one thing and one thing only: freeing up time that is only ever used to get more done. 
The one bright side to all this productivity should be that everyone makes more money, but that’s all too often not the case. The popular narrative is that we’re all working harder, though “wages haven’t risen in 40 years” and “purchasing power is lower now than any point in recent memory.” The economist in me has always struggled with this line of thinking. Wages are only truly relevant indicators of wealth in the sense that they allow you increased control over how you spend your time. If you’re earning a wage and a service comes along that saves you the time and effort you’d normally have to expend to access a certain good (read: Freshly for meals), that service effectively increases the value of your existing wage. Thus, even though you’re not earning any more money, you’re now wealthier. 
For consumers, services like Google and Freshly do exactly this. 
The media, though—and a select few politicians—prefer a different narrative. “There’s a finite amount of money in the world,” they effectively claim, “and since we’re making less, and tech companies are making more, it follows that tech companies are to blame for wage stagnation, which is a net bad, always.” 
Reality, though, isn’t that simple. 
It doesn’t feel like our lives are getting easier even when things have never been better. 
Though companies like Google and Amazon do generate healthy—and, yes, quite frankly absurd—returns for their executive teams and shareholders, they’re valuable because people find whatever they offer to be worth more than whatever they’re being asked to pay for it. In the case of Google, that offering is time (via frictionless access to information), and its price is effectively zero. The partial rationalization I make for stagnant wages, then, is that Google and services like it allow people to get more out of the same wage. 
In this world, Google and its contemporaries are to blame for wage stagnation, but only because they’re creating a world where wages are no longer necessarily synonymous with wealth. Ergo, wage stagnation at the hands of tech companies—everyone’s favorite narrative—is a feature, not a bug. 
There’s a problem with this line of thinking, though, and it gets at the root of both the millennial obsession with work and our tendency to burn out. 
Let’s return to the prisoner’s dilemma as it pertains to the millennial “obsession with work.” When presented with time-saving utilities (Google, Freshly, etc.) and effectively given the option to use them to either (a) relax or (b) optimize and work harder, every millennial’s dominant strategy is to optimize and work harder. This explains why it doesn’t feel like our lives are getting easier even as things have never been better. We’re adjusting our behavior in the exact same, suboptimal way to every supposed convenience modernity throws at us. 
This also explains why productivity apps—which includes Amazon and Google because their services save time—proliferate like Hydra’s heads. Each one births a dozen more because, in the modern era, the best way to spend your time is finding better ways to spend your time. 
Take Freshly as an example again. As we’ve noted, Freshly implicitly promises at least several hours a week in saved time by not having to “get in the grocery line,” “watch water boil,” or “plan the week’s meals.” With that hour (or two), it claims, you can “get a good workout in,” “watch the game in real time,” or “plan a movie night.” 
And all of these are fantastic things, but it isn’t hard to imagine three new services arising in response to Freshly’s success. First, a gym productivity app that claims to “shred you in half the time.” Second, an algorithmic highlight tape that captures the best moments of every game and delivers them to you. And third, a service that gives you the same level of content you’d expect from a cinema, but promises you won’t have to leave the couch. The kicker? All of these services (a) already exist and (b) are now to Freshly exactly what Freshly was at some point to some other time-saving utility. Like Freshly, all of these new, hypothetical services also save time that can—and will—be used to find even more ways to save time, ad infinitum. 
Optimization begets optimization and says we’re its beneficiaries — and in many ways, we are. But given our reliable ignorance of what our lives have conditioned us to do with free time (read: optimize and work harder), we’re better characterized as optimization’s subjects, along for the ride as our pace of life accelerates endlessly. 
Yuval Harari may have put it best in Sapiens: 
One of history’s few iron laws is that luxuries tend to become necessities and to spawn new obligations. Once people get used to a certain luxury, they take it for granted. Then they begin to count on it. Finally they reach a point where they can’t live without it. 
This, at its core, is the process that leads to burnout. 
A rather elegant solution to the prisoner’s dilemma was proposed years ago, after the initial thought experiment was conceived. The idea was to have players engage in repeated versions of the same game and have the payoffs of each game carry over into the next round. The rationale was simple: upon realizing the game would continue to be played, people would also realize it was in their best interest to cooperate. This is a pessimistic view of society, but it’s also an accurate one. 
Humanity cooperates because historical precedent (read: repeated games) dictates doing so—with very few exceptions—is everyone’s dominant strategy. 
When you play out the prisoner’s dilemma game in real life with the repeated games wrinkle added, the results are what you’d expect: People begin to cooperate. The problem with this solution is that while it works to inspire cooperation on a small scale, global cooperation is much harder. 
We’re playing a rigged game, and every time we do, our pace of life accelerates, and the world moves faster. 
This gets at why we make suboptimal decisions at a global scale: There isn’t yet a feasible way to facilitate repeated games between seven billion individuals. Even if there were, and we could all agree to only use time-saving utilities to relax for the rest of our lives, all it would take for the entire system to unravel would be one individual cheating on the agreement, optimizing, and working harder. 
Given this impossibility of global coordination, we will continue to behave in our own self-interests. And we’ll continue to make suboptimal decisions. We’re playing a rigged game, and every time we do, our pace of life accelerates, and the world moves faster. 
The acceleration of our collective pace of life is not a result of stupidity or irrationality; rather, it is a symptom of what is perfectly predicted by the prisoner’s dilemma at a global scale: Hyperrational individuals making hyperrational decisions on how to spend their time by launching into an inescapable arms race of productivity. Burnout is inevitable. 
/tīm/ /snak/noun 1. Often annoying moments of pseudo-leisure created by computers when they stop to save a file or to search for software updates or merely to mess with your mind. 
—excerpt from The Age of Earthquakes, by Shannon Basar, Douglas Coupland, and Hans Ulrich Obrist 
The one silver lining here is that millennials, to our credit, seem generally relieved by the knowledge that burnout has a name. Like “depression” or “anxiety,” labeling a condition everyone’s feeling legitimizes it. It also gives those experiencing it the hope that it might be addressed—because it indicates that it needs to be addressed. What is less clear is whether that hope is justified, true as it is that global coordination is impossible, and there will always be someone using the next great convenience to work harder than you. 
This, more than anything, is why we will remain the burnout generation. 
It isn’t because we see intrinsic value in the absurd hours we put in, though to cope, many of us have convinced ourselves we do. It’s because the rules of the game we play dictate that working those hours—and outworking everyone else—is our dominant strategy. When we see long weekends and think “work before play,” when we see Friday nights and think “sleep before clubs,” when we see escalators as accelerators and not opportunities to “just take a second,” we’re nothing more than hyperrational prisoners making a decision that would be inaccurately characterized as a dilemma because the answer is obvious. 
When given the choice, we optimize. 
Then we work. 
Written by 
Written by 
About this Column",Zander Nethercutt,2019-02-21T19:42:35.616Z
How To Use UX Design To Build Trust & Brand Identity | by Inkbot Design | Medium,"How to Use UX Design to Build Trust & a Powerful Brand Identity 
Many consider User Experience (UX) and brand identity to be totally unrelated. 
This couldn’t be further from the truth. 
When it comes to branding, they’re basically two sides of the same coin. Let me explain. 
We’re entering a new era where traditional branding elements, like logos, are important but no longer enough. 
With so many new businesses appearing on the market, customers need a more reliable factor to spot and differentiate good companies. 
Customer experience is quickly emerging as one of them. 
There’s evidence to back this up. More than 73 per cent of customers in 12 countries say that customer experience is the most important factor in their purchasing decisions. 
Credit: PwC Future of Customer Experience Survey 2017/18 
This rise of customer experience as a brand differentiator resulted in some changes in a new way companies design their digital products: design thinking. 
Design thinking represents a connection between UX and brand identity and seeks to make customer interactions with digital products intuitive, simple, and appealing. 
Customer needs and goals are the core of this new philosophy, which is becoming increasingly popular to differentiate a brand’s unique proposition by providing a unique customer experience. 
To be able to use UX design to build a powerful brand identity, you first need to understand what a brand is. 
It’s much more than a logo design or a colour palette. 
A brand is an ever-changing perception of the company based on customer experience. 
Websites, apps, and other digital products provide that brand experience, and taking care of the UX is a way to improve it. 
One way to do that is to make that experience as easy and as relevant as possible. 
This is exactly what TeamBuzz did on their product’s page. 
Instead of making the landing page unnecessarily long by describing all of the many features of the real-time peer feedback, they summarized a big chunk of information efficiently. 
It’s the view of three different features that a user can check out in one place. 
There are three tabs that you can click on — Recognition Feed, Teams, and Employee Profile — to see how the tool works in different scenarios. 
If UX designers decided to describe the three use cases in separate sections, the page would become cluttered. 
This would’ve affected the brand experience negatively. 
Improving a brand experience this way helped with making it easy to understand straight away what TeamBuzz can be used for. 
The best way to create such an experience is to apply brand values and attributes to digital products — simplicity, for example, and foster an appropriate brand perception in its customers’ eyes. 
So, instead of merely presenting data and numbers, businesses should base a digital design in another way: UX that combines brand values and usability. 
Here are some of the best tips on how to do it. 
Brands realize that aesthetically pleasing websites built using the latest design technology serve as a powerful tool to inspire confidence in customers. 
You may also like: 17 Self Branding Tactics That Will Get You Noticed 
Let’s face it, many of us still judge a book by its cover. 
However, this approach to UX has two major flaws: high loading times and poor usability. 
Creating a design-heavy website increases the risk of: 
To minimize this risk, try taking care of usability instead of focusing too much on the appearance. 
This will save you from user frustration resulting from slowly-loading pages and complex navigation. 
What you really want to know is how to create a landing page and find a balance among the aesthetics, brand experience, usability, and marketing needs. 
Video is a perfect way to start. For example, if we visit the home page of Landingi, a landing page builder, we’ll see an introductory video above the fold. 
This helps with two things: reducing clutter and providing the most important information right away. 
The visitor can watch it and be more interested to check out other pages. 
So the main takeaway here is that you should not make your website too heavy for Google but rather focus on usability and visitors’ needs. 
Brand image consistency is a must to facilitate more effective brand memorization, better recognition, and higher customer trust. 
When it comes to digital products, there are two consistencies to pay attention to: visual and brand personality. 
Keep in mind that traditional elements like fonts, colours, graphics, images, and icons are still important for brand recognition and memorization. 
All of these elements should be consistent across all digital products created by a brand. 
Monday is a great example of that. 
The brand leverages the power of UX by using the same visuals and colours in its task management software to build identity and foster recognition. 
By applying similar design principles and colours across all digital products, Monday’s environment becomes easily recognisable and memorable, 
There’s also a consideration of aligning the target audience’s expectations with UX. 
For example, customer perception can suffer if a brand doesn’t keep its reputation and design consistent. 
Finding out about the way customers feel about a business can be tricky but it is something you have to do to check the effectiveness of your design. 
For example, Reputation Management figured that out after some A/B experimenting. 
According to Jonas Sickler, the company’s SEO manager, they struggled with getting high-quality leads despite being successful with bringing traffic to their page. 
“After some analysis, we determined that, although our website copy said that we work with Fortune 1000 companies, our design wasn’t communicating that same premium message, It was only after aligning our design with our target clientele, that we saw a remarkable improvement in lead quality that translated to 7-figure revenue gains.” 
By making visual representation consistent and in line with a brand’s image, you can change your customers’ perception. 
This, in turn, can make a difference in their willingness to do business with your company. 
UX can reinforce the attributes and characteristics associated with a brand. 
For example, website design could be done in a way that makes visitors feel unique and if they were a part of a community. This is what defines your brand’s personality. 
Ever heard of Zest? 
It’s a content discovery tool that finds the best-performing, top-quality content for marketers. 
You may also like: 30 Effective Travel Logos That Take You on an Adventure 
There probably isn’t a platform that mastered the UX design better than they did. 
A cool thing about the site is the Zest New Tab feature which allows Chrome users to turn their browser’s empty tab into a hub for discovering fresh content. 
The app is also very intuitive and easy to use, as you can either browse through the content submitted by others or suggest your own. 
This contributes to the perception of Zest as a credible brand with an easy-to-integrate, useful content solution. 
But that’s not all. 
Going deeper, it is clear that Zest’s branding strategy is extremely thought-through and designed to a T. 
The name “Zest,” the fitting, consistent colour palette, and even the way they relate to and communicate with their community is super consistent and very Zest-like. 
With such a powerful brand identity, it’s hard to mistake them for any of their competitors. 
Take a look at their email’s copy below to understand what I’m talking about. 
Brief, to the point, and friendly — this is exactly what creates that unique brand experience. 
Have a couple of conversations like this and you won’t forget about a brand for a while. Chances are you’ll think of it when you’ll be looking for a similar company. 
A word of caution, though: people will sense if a brand isn’t authentic. Don’t overdo it. 
The same applies to being a bit cheeky in your branding. 
Using conversational language style and emojis is a great way to come across as a friendly (and human!) company that is ready to have an easy conversation with their audience. 
It’s not a secret that we trust people we like (and companies, too!). 
For example, take a look at how Lemlist has approached the idea of supporting their brand identity with emojis in their UX design. 
Not convinced that emojis are right for your business? 
Let me tell you a few more reasons why using natural language and emojis might be worth exploring: 
Typography often gets overlooked as a tool for building a strong brand identity, but it actually can be a very powerful way of creating a consistent and positive UX. 
It’s a medium for conveying the brand’s mission. 
As this MIT research showed, the font has an impact on how long people read information, as well as on their emotional state. 
When they compared the reading times between two pages with different fonts, they found that: 
A page with an easier to read and thoughtful typography had higher reading times while the one with hard-to-read typography caused discomfort and even irritation. 
This shows that the typography can influence both dwell time and bounce rate. 
So how to pick the right font for your branding? 
You may also like: 7 Ways of Developing Your Personal Brand Through Videos 
First, think twice before selecting the most popular fonts. 
They might look appealing and easy to read, but keep in mind that hundreds, if not thousands, of competitors use them, too. 
That’s why going with a non-conventional but still highly readable font is a good idea. 
For example, take a look at this copy from Visme, a free presentation and infographic maker for non-designers. More than anyone else, the really understood the importance of seemingly small and unimportant things, like fonts. And they’ve used it to their advantage. 
It seems like the copy is written in at least a few fonts, but it actually uses only two: Roboto and Montserrat. 
Visme has done a great job of choosing them because these two are concise, untwisted, and not widely used. 
As a result, it would be easier for website visitors to associate these specific fonts with the company if they see it elsewhere. 
A brand’s mission is often the reason why customers become loyal. 
One study found that 40 per cent of customers have stopped buying from a brand because they didn’t like its values and behaviours. 
On the other hand, 49 per cent of them also expressed the willingness to pay more for products from a brand with positive values. 
That’s why your website should clearly convey the answer to the question: “Why are we here?” 
Timberland with its responsibility policy is an excellent example here. 
They explain why they’re so dedicated to the environment and social responsibility in a special website section. 
Of course, there are thousands of people who feel the same way, so they find it easier to connect with Timberland because of that. 
This has become one of the main things that differentiate Timberland from competitors. 
When you communicate your mission, you’re letting people know what you stand for. 
And that’s exactly how you build a brand identity that distinguishes you from the crowd. 
But how to let people know what you stand for in a way that makes you stand out? 
Here’s how Stefan Dubois, SurveyAnyPlace, answers this question. 
“The mission is what you do. e.g.: “We help Consultants to package their expertise in our tool and sell it through a pay-per-use model, enabling them to scale their business faster.” 
Can you relate to that? 
Of course not. 
But with your beliefs, you can go one step further. They are a set of clear, straightforward principles, almost religious beliefs, that go beyond your product. The beliefs explain how you want to achieve your mission.” 
SurveyAnyPlace even included the explanation of their beliefs on the website. 
In almost every industry, there are plenty of companies that share their mission, values, and beliefs with their customers. 
One small comment, though. 
Being honest is a must here. 
Darren Foong, the Growth Manager at CandyBar, explains why. 
“Small companies tend to be shy about being small so they hide anything that might reveal their actual size. It’s as if they believe customers would like them more if they were big. 
Big companies, on the other hand, often hide behind confusing statements with big empty words and cool logo designs but never tell you what they believe in. 
This doesn’t inspire trust. 
We have only 40 employees at CandyBar, and our customers are micro and small businesses too. For that reason, we want to make sure our customers know that we know what it’s like to be a small business.” 
Here’s how the company shares that on the Why CandyBar page: 
“It’s an uphill battle for small businesses against bigger ones. But we are there for them. We’ll help them fight the good fight. And when we receive good ratings, or they share us through word of mouth, we know we’ve done it right…” 
While highlighting their values and beliefs, brands are communicating their mission, also referred to as “brand promise.” This helps with two major goals: 
Make sure to include a similar section on your website to communicate your mission. 
Brand identity and UX are directly related and can really help with making the customer experience more memorable. 
You can — and should — use UX to communicate your brand’s values, mission, and personality and create a unique and positive customer experience. 
Using the tips you just read about should be an excellent start of your journey to delight and engage your customers. 
Good luck! 
Originally published at https://inkbotdesign.com on January 28, 2020. 
Written by 
Written by",Inkbot Design,2020-01-28T22:28:45.286Z
Understanding BERT — The basics. Full credit to Chris Mccormick’s blog… | by Dharti Dhami | Medium,"Full credit to Chris Mccormick’s blog for helping me understand and distill BERT. 
BERT (Bidirectional Encoder Representations from Transformers), released in late 2018 enabled transfer learning models in NLP. BERT is a method of pretraining language representations. 
The idea of pre-training models followed by task-specific fine-tuning is in itself not new — computer vision practitioners regularly use models pre-trained on large datasets like ImageNet, and in NLP we have been doing “shallow” transfer learning for years by reusing word embeddings. But models like BERT, enabled major shift towards deeper knowledge transfer by transferring entire models to new tasks — essentially using large pre-trained language models as reusable language comprehension feature extractors. You can fine-tune these models on a specific task (classification, entity recognition, question answering, etc.) with your own data to produce state of the art predictions. 
Why use BERT rather than train a train a specific deep learning model (a CNN, BiLSTM, etc.) that is well suited for the specific NLP task you need? 
The pre-trained BERT model weights already encode a lot of information about our language. As a result, it takes much less time to train our fine-tuned model — it is as if we have already trained the bottom layers of our network extensively and only need to gently tune them while using their output as features for our task. The authors recommend only 2–4 epochs of training for fine-tuning BERT on a specific NLP task (compared to the hundreds of GPU hours needed to train the original BERT model or a LSTM from scratch!). 
Because of the pre-trained weights this method allows us to fine-tune our task on a much smaller dataset than would be required in a model that is built from scratch. A major drawback of NLP models built from scratch is that we often need a prohibitively large dataset in order to train our network to reasonable accuracy. 
Finally, this simple fine-tuning procedure (typically adding one fully-connected layer on top of BERT and training for a few epochs) was shown to achieve state of the art results with minimal task-specific adjustments for a wide variety of tasks: classification, language inference, semantic similarity, question answering, etc. Rather than implementing custom and sometimes-obscure architectures shown to work well on a specific task, simply fine-tuning BERT is shown to be a better (or at least equal) alternative. 
BERT model has been outperformed on the GLUE benchmarks (RoBERTa, XLNet, …), and new models are coming out every handful of months to one-up the prior state of the art. However, BERT is a landmark model similar to AlexNet for computer vision. Understanding BERT will make it easy to follow the latest developments in the pre-trained NLP models. 
BERT is a departure from the LSTM-based approaches to NLP. So can we instead focus our understanding on Transformer architecture in BERT without going too deep in the recurrence or LSTMs, or even Attention in the context of LSTMs. 
If you would like to look at other resources on deep learning and BERT, these are some of my recommendations. 
Andrew Ng’s deep learning cource on coursera. 
Google AI Blog Open Sourcing BERT: State-of-the-Art Pre-training for Natural Language Processing 
BERT — — The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) 
Transformer — — The Illustrated Transformer 
Attention — — Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention) 
In the next post let’s dive further into the inner-workings of BERT. 
Written by 
Written by",Dharti Dhami,2020-07-12T23:14:09.566Z
Collaborative Filtering — A Type of Recommendation System | by Bindhu Balu | Towards AI — Multidisciplinary Science Journal | Medium,"This is part 3 of my series on recommender systems — 
Part 1: https://medium.com/towards-artificial-intelligence/recommendation-systems-104bdfe3f93f 
Part 2: https://medium.com/towards-artificial-intelligence/content-based-recommender-system-4db1b3de03e7 
The second most common type of filtering used. This is heavily used by Amazon and eBay to recommend products. 
User-Based Collaborative Filtering: 
If people have similar interests in the past, they will have similar interests in the future. Let’s consider 2 young men Vivek and Nikhil — who have similar interests. If Vivek hasn’t watched a movie that Nikhil had liked — them we recommend that movie to Vivek. To understand how this algorithm works — we will answer a few key questions : 
Let’s consider the below Dataset: 
Tasks : 
Correlation between the first 2 rows gives the measure of how similar Vivek is to Nikhil. The correlation coefficient ranges from -1 to 1. If the correlation coefficient is 1 means Vivek and Nikhil have exactly similar tastes, 
-1 indicates exactly opposite tastes and 0 indicates no relation at all. 
Corr(Nikhil,Vivek) = 0.91 
Corr(Nikhil, Abhirami) = 0.70 
Corr(Nikhil, Reetesh) = 0.00 
Corr(Nikhil, Saif) = 0.50 
If there are 1000 users , we will have 999 such similarity measures. 
Now the next step is to predict Nikhil will give to all the movies. 
To do that we need to find the nearest neighbors to Nikhil and what weight we can give them. 
In this example — let’s consider 2 neighbours Vivek and Abhirami. Based on the weighted average of the rating given to movies by them — we can calculate the rating Vivek will give to all the movies. 
You saw that Nikhil is similar to two users Vivek and Abhirami, with similarities of 0.91 and 0.70, respectively. The similarity between two users and b is expressed as sim(a, b). For simplicity, let’s consider only these two nearest neighbors. 
The objective is to predict the rating Nikhil will give to the movies he has not seen but Vivek and Abhirami have seen. For example, let’s say there are 50 such movies in the entire dataset. The system will predict 50 movies and recommend the top 10 or top 5 to Nikhil. 
Among these 50 movies, let’s consider two: Interstellar and Inception. The ratings given by Vivek and Abhirami to these movies are as follows: 
Interstellar Inception Vivek 5 3Abhirami 4 5 
To predict the rating Nikhil will give to Interstellar, one way is to simply take the weighted average, as given below: 
pred(Nikhil, Interstellar) = 
Similarly, you can predict the ratings Nikhil will give to the rest of the 50 movies. 
What will be predicted rating pred(Nikhil, Inception)? 
Ans: 3.87 
We have used the Pearson co-efficient method to predict the ratings. There are other methods too. 
Item-Based Collaborative filtering: 
This is the most successful and widely used recommendation algorithm. It is used by companies like Amazon and other movie recommendation sites. 
Let’s consider the same example. Suppose we need to find the rating Nikhil will give to casino Royale, we assume that it will be similar to the ratings Nikhil had given to other movies of similar genre in the past. 
We will create an item vector for movies. We will not consider ratings of Nikhil while creating item vectors. 
Item vector for ‘Casino Royale’ is (5,1,0,4) 
Item vector for Fast&Furious is (5,2,0,4) 
Item Vector for Pulp Fiction is (4,0,0,3) 
We need a mathematical way to measure the similarities between the movies. 
Similarly — the similarity measure between Casino Royale and fast & Furious comes to 0.98 
sim(Casino Royale, The Fast and The Furious) = 0.98 
To predict the rating Nikhil will give to Casino Royale, we take the weighted average of the nearest movies as follows: 
pred(Nikhil, Casino Royale) = 
Since the ratings all are positive, the value of similarity will lie between o and 1.Values close to 1 indicates strong similarity. This similarity metric is called the Cosine similarity metric and is a well-established measure. 
Similarly, the algorithm will compute the ratings Nikhil is likely to give to all the other movies he has not seen and recommend the list of top-n movies, where the choice of ’n’ is up to the algorithm designer. 
The python implementation of User and item-based collaborative filtering is as follows. 
https://github.com/BindhuVinodh/Recommendation-System 
Towards AI publishes the best of tech, science, and engineering. Subscribe with us to receive our newsletter right on your inbox. For sponsorship opportunities, please email us at pub@towardsai.net Take a look 
Written by 
Written by",Bindhu Balu,2019-10-31T18:08:14.493Z
How Artificial Intelligence (AI) is Adding New Horizons to Cybersecurity Solutions? | by Sanjay Ratnottar | Towards Data Science,"McCarthy and Minsky described Artificial Intelligence as a task performed by a machine, which if, performed by a human instead will require a great deal of intelligence. A collective data of all the behavioral qualities are required to make the precise decision. These behavioral qualities are planning, problem-solving, reasoning and manipulation. 
Toyota invested $100 million in funds for AI; UBS is trying to bring AI to its investment bank’s operations, whereas VCs dream of substitution of all folks with AI to reduce the prices. Many people often feel embarrassed about having never used AI or because of their lack of knowledge of the subject. 
Many cybersecurity vendors use this term in order to increase sales by impressing their customers. In this article, we will be exploring more about cybersecurity and Artificial Intelligence collectively. 
Artificial intelligence can be split broadly into two types. The two broad categorizations are given below: 
• Narrow artificial intelligence 
Narrow Artificial Intelligence is we see all around in computers these days, “Intelligent systems that are taught the way to do specific tasks while not being programmed the way to do so.” Unlike humans, these systems only learn the right way to do a specific task, which is why they are also called “Slim Artificial Intelligence.” 
• General artificial intelligence 
General Artificial Intelligence is the storing of general human abilities into software such that it can find solutions when it encounters a problem. It is sometimes referred to as “Strong Artificial Intelligence” because it can store and process a huge amount of data in a single go and come with solutions. It works just like the human brain, as it makes decisions by processing the information available to it. 
Cybersecurity refers to technologies and practices designed to safeguard networks and information from damage or unauthorized access. Cybersecurity is vital as governments, corporate and military organizations collect, process and store a vast amount of information on computers. 
A considerable part of this data can be sensitive; it can be intellectual property, financial data, personal information, or any other type of data for which unauthorized access or exposure could lead to serious consequences and problems for the people collectively. 
Organizations transmit a vast amount of data globally to different systems. This data, which is stored in systems can be easily attacked and extracted from the systems. This can result in problems beyond our imagination. In recent years, we have had a number of cases of data theft, which took place because of insufficient cybersecurity measures taken by the organization itself. Cybersecurity ensures that data thefts of this magnitude do not take place. It thus helps an individual with privacy. 
Cyber attackers are putting money in automation to launch strikes, while many organizations are still exploring manual efforts to combine internal security findings and contextualizing them with external threat information. With the kind of security systems, we have in place presently, it can take a longer time to sight intrusions and throughout this period, attackers will exploit vulnerabilities to compromise systems and extract knowledge. 
To deal with these challenges, a number of organizations are exploring the employment of AI in their regular cyber risk operations. The role of artificial intelligence in cybersecurity cannot be negated. 
Through AI, the new exploits and weaknesses can quickly be identified and analyzed to help mitigate further attacks. It has the ability to minimize the pressure on humans. They are alerted as soon as an intrusion is detected and when their interference is required. 
• AI can be made Smarter 
A useful analogy is to think about the best professionals working for your organization. If you use this employee to train your machine learning and artificial intelligence programs, the AI will be as smart as your star employees. Now, if you take the time to train your machine learning and artificial intelligence programs with your 100 best employees, the outcome will be a solution which is as smart as the brain of the 100 best employees put together. artificial intelligence vs machine learning was also a trending topic in recent years. 
• AI never takes a day off 
One of the few advantages of Artificial Intelligence in cybersecurity is that AI never takes a day off or gets tired after hours of continuous work. Thus, we can conclude here, that AI helps in doing the job with the maximum efficiency at the maximum possible rate with the best quality products. 
Prevention of cyber-threats and the avoidance of attacks represent the ideal, but it is almost inevitable that stop these incidents from occurring. And once they do, a fast response is crucial, both in minimizing the harm caused by the assault and in recovering from its effects. With a “thinking machine”, a fast response might be written into its system. 
• AI swiftly spots the threats 
Algorithms dedicated to spotting potential threats might be enforced in real time, to provide a flash by moment response to an attack. Existing security software system databases and algorithms have a restricted scope and are typically unable to keep up the pace with the fast development and mutation of new threat vectors. 
Adaptive or machine learning algorithms, designed into an intelligent security system have the potential to spot and answer threats as they occur — even dynamic ones. And these intelligent security devices might have the inherent ability to keep on learning, to check current pools of data and extrapolate from them, to anticipate future threats and acceptable responses. 
• Boosts the scale of resistance 
Artificial Intelligence also has the potential to increase the scale of resistance that a system has to ongoing attacks. If a corporation uses a number of hardware devices such as desktops and mobile phones to communicate and transfer information, the chances of a cyber-assault to extract information from the system are pretty high. 
• Enabled to counter every incoming threat 
To respond to such an assault, machine-driven mechanisms backed by AI can be deployed to counter each incoming threat because it presents itself and takes counter measures in real time. A number of impacts of artificial intelligence on cybersecurity are observed. 
• Plotting an effective strategy against threats 
In the usual security set-up, the real-time response to threats is often hampered by speed and sometimes the changing nature of the attack itself. So a large amount of data is needed to be analyzed in order to formulate a response and to plot out a proper strategy. 
Human security analysts generally can’t handle these tasks alone and need some extent of automation in their cyber-threat response systems. In the present days, AI systems with their machine learning algorithms and real-time counter-measures are first of the stages in what guarantees to be an evolving security landscape. 
There are a number of measures taken by governments worldwide based on Artificial Intelligence. Governments these days are using AI components in their systems to check for any threat and to eliminate it. This is done using Artificial Intelligence. 
Although it sounds simple, it is a very complex process and requires hundreds of people to work on the project. The huge number of people working on the project gives an idea about the importance of AI in cybersecurity. This ensures that the nation’s security is not threatened. 
On October 2016, the White House released a report on “Preparing for the Future of Artificial Intelligence.” This report talked about the strategies we should be employed to manage AI. Events were organized to discuss the technology so that more and more people could be made aware of it and that new technologies could come up. 
These conferences have helped dramatically as many people who were not aware of the topic, now have full-fledged knowledge of it. It has also led to a number of practical applications of AI which is being seen as the future of the world. 
Since technology is relatively new, we are not very sure about its future. Scientists have no idea what problems could come up in the future and how they would be dealing with it. This confusion is in place even after the world has seen so many examples of artificial intelligence in cybersecurity. 
In the future, AI will require some kind of hi-tech monitoring to ensure that it performs the constructive tasks it is meant to perform and not become a tool of destruction. AI should be developed in such a manner that they are prone to cyber attacks. 
A chief scientist once said in an interview that an AV researcher used to see 10,000 viruses in their whole career. However, these days they encounter more than 500,000 each day. He said that his security firm employs Artificial Intelligence to prevent such attacks. Apart from that, AI as a security tool can prove helpful in the present time, with the lack of human resources that the cybersecurity industry is currently facing. More than 40% of the organizations worldwide — claim that they are suffering from a shortage of talented professionals in web security. 
There are a number of issues which we face when dealing with AI and for implementing it every organization must need strong artificial intelligence solutions. There are ethical and legal issues to consider, such as who will bear the responsibility for the actions of an autonomous machine which decides by itself the action it is going to take. 
However, with the vast variety of advantages that it offers, the future of AI in cyber security looks promising. To see how it changes the world in the coming days, we would have to wait for a couple of years, maybe decades. 
Hands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look 
Written by 
Written by",Sanjay Ratnottar,2019-02-14T13:25:59.618Z
Artificial Intelligence and its impact on Cyber Security | by Chiragh Dewan | Medium,"Understanding Artificial Intelligence 
Artificial Intelligence or AI is the intelligence shown by machines. When any machine becomes aware of its surroundings and does something keeping that in mind in order to achieve something. Usually the term Artificial Intelligence is used when a machine behaves like a human in activities such as problem solving or learning, which is also known as Machine Learning. AI is a science and technology based on disciplines such as: 
· Computer Science 
· Biology 
· Psychology 
· Mathematics 
· Engineering 
· Linguistics 
The main goal of Artificial Intelligence is to create a technology that allows computers and machines to function in an intelligent manner. The entire problem has been broken down into the following sub-problems 
· Learning 
· Natural Language Processing 
· Reasoning and problem solving 
· Planning 
· Creativity 
· Social Intelligence 
· General Intelligence 
· Knowledge representation 
· Perception 
· Motion and manipulation 
Are we around Artificial Intelligence right now? 
In 1943 Isaac Asimov coined the term Robotics. In Artificial Intelligence was coined by John McCarthy in 1955. In 1958 John McCarthy invented LISP and by 1997 The Deep Blue Chess Program had beaten the World Chess Champion, Garry Kasparov. There are so many ways we all interact and use Artificial Intelligence without even realizing it. Here are a few: 
· Siri: Apple’s personal assistant on iPhone’s and Mac OS. 
· Netflix: Recommendation engine 
· Nest: Home Automation 
· Alexa: Amazon’s smart hub 
· Gaming: Games like Call of Duty and Far Cry rely heavily on AI 
· News Generation: Companies like Yahoo, AP use AI to write small news stories such as financial summaries, sports recap, etc. 
· Fraud Detection 
· Customer Support: Companies have been using small scale Chat Bots to automate this process. 
· Self-driving cars 
· Speech Recognition 
· Robotics 
Industries that’ll benefit from Artificial Intelligence 
Artificial Intelligence will and is playing a major role in revolutionizing a number of industries. Some reports have suggested that the total global market for Robots and Artificial Intelligence will reach ~$153 billion by 2020. AI not only decreases costs but also saves time, increases accuracy and productivity. The development in the field of AI has been phenomenal. Google estimates that robots will reach levels of human intelligence by 2015, one-third of jobs will be replaced by robots and other smart machines. 
Few industries that’ll greatly benefit with advancements in AI are: 
· Healthcare: Current AI (IBM Watson) is capable of diagnosing a life-threatening diseases and prescribe treatment. We have also seen robotic surgeons, robots that are helping people in wheelchairs walk again, etc. 
· Transportation: With more connected vehicles and self-driving cars, AI is replacing the need of a driver. Studies suggest that the installation rate of AI based systems in new vehicles will increase from 8% in 2015 to 109% in 2025. Furthermore, according to Gartner’s prediction, by 2020, there will be more than 250 million cars interconnected via Wi-Fi that will even allow the cars to communicate among themselves and the roadways. Driverless cars are said to save the U.S. alone $1.3 trillion a year by 2035 to 2050 and a global saving of about $5.6 trillion. 
· Manufacturing: Being one of the first industries to use Artificial Intelligence by using robots for packaging and shipment. Factories have been started to automate their assembly line with little or no human supervision. 
· Customer Service: With the rise of Chat Bots, the customer service industry has taken a turn for good. With decreased costs and high productivity, the customer service industry has been greatly affected by AI. 
· Education: Education is a billion-dollar industry contributing heavily to the various economies. EdTech, as we all know, is a booming industry. AI could be used to grade papers, bots to answer questions, personal assistant systems that act as tutors, virtual reality for hands-on learning, etc. 
· Finance: The financial data has been increasing at a rapid rate making it harder for the financial services companies to keep up. With the help of Artificial Intelligence, predictive systems are being put in place to forecast stock trends and manage finances. Recently, we’ve also seen the trend of “Robo-Advisors” that are automating the industry. 
Future of Artificial Intelligence in Cyber Security 
With AI being introduced in every industry, the cyber security space would be no stranger to it. With advancement, new exploits and vulnerabilities could be easily identified and analyzed to prevent further attacks. Incident response systems could also benefit greatly from AI. When under attack, the system will be able to identify the entry point and stop the attack as well as patch the vulnerability. 
Studies show that it takes, on an average in 2016, 99 days for a company to realize that they have been compromised. Although a long way from 146 days in 2015, yet a very long time for the attackers to gain all the information they were looking for. This time frame is not only enough to steal data but also manipulate it without detection. This can have a great impact on the company as it makes it very difficult for the company to differentiate between the fake and the actual data. 
With the advancements in AI, hopefully all of the above problems would be able to mitigate the problems being faced. 
Current role of Artificial Intelligence in Cyber Security 
A security firm by the name of SparkCognition showcased, what is said to be, the first AI powered cognitive antivirus system called DeepArmour. 
“We are using cognitive algorithms to constantly learn new malware behaviors and recognize how polymorphic file may try to attack in the future” said Keith Moore, senior product manager at SparkCognition. “This keeps every endpoint safe from malware that uses domain-generated algorithms, obfuscation, packing, minor code tweaks and many modern tools” he added. 
Another information security startup, Darktrace is also said to be working on a self-learning security system to enable automatic defense. 
Many more companies across the world are trying to integrate Artificial Intelligence, Machine Learning and Natural Language Processing into new products to make the cyber space more secure. 
Advantages of Artificial Intelligence 
Organizations face millions of threats each day making is impossible for security researcher to analyze and categorize them. This task can be done by using Machine Learning in an efficient way. 
By finding a way to work towards unsupervised and supervised machine learning will enable us to fully utilize our current knowledge of threats and vectors. Once those are combined with the ability to detect new attacks and discover new vulnerabilities, our systems will be able to protect us from threats is a much better and efficient way. 
However, like every Machine Learning algorithm, even these advanced algorithms would require human guidance to learn from as humans are better equipped to look beyond a simple anomaly that a machine could pick up and put it in a different context and decide to ignore it as a security threat. 
Another benefit of using AI based machines is that, in theory, these systems would work in a more calculated approach and in a more accurate way resulting in eliminating human error. Additionally, these systems could work simultaneously on various tasks, monitoring and protecting a vast number of devices and systems. They can therefore mitigate large scale attacks. 
Disadvantages of Artificial Intelligence 
The biggest disadvantage of any AI based system is that we cannot predict what it’ll do. If fallen into the wrong hands, the result could be fatal and a whole different level can could do more damage than good. 
A super-intelligent AI will be really good at completing goals, but, if those goals aren’t aligned with ours, we’ll have a problem. AI in security systems had foregone the utilization of valuable analyst skills and therefore didn’t benefit from human feedback. 
Even though the initial concerns about the development on AI in cyber security may revolve around concerns about eliminating the much needed human expertise, intuition and judgement, the real disadvantage of artificial intelligence is its unpredictability. 
Conclusion 
It is really difficult to predict what the future Artificial Intelligence holds. Some say it’ll help us to better our world while the others lean towards the possibility that it may go rouge. 
Originally published on InfoSec Institute Resources 
Written by 
Written by",Chiragh Dewan,2017-05-11T02:23:46.740Z
Latent Dirichlet Allocation for Beginners: A high level intuition | by Pratik Barhate | Medium,"As the title suggests, unlike other machine learning posts, this one is not going to be a complicated mathematics session revolving around a bunch of text. Now that you are here, I assume you are struggling to work with a large amount of text data and want to learn a bevy of text processing algorithms. Let’s begin with what the algorithm LDA is all about. 
Latent Dirichlet Allocation (LDA) is a generative, probabilistic model for a collection of documents, which are represented as mixtures of latent topics, where each topic is characterized by a distribution over words. 
Now that statement might have been bewildering if you are new to these kind of algorithms. Conceptually, we are just trying to get some ‘topics’ which can represent our collection of documents. It’s a way of automatically discovering topics that are a part of the given documents. You can read more about topic modelling. 
Some use-cases where LDA can be applied are: 
Technically, the model assumes that the topics are specified before any data is generated. LDA represents documents as mixtures of topics that spit out words with certain probabilities. 
For example, let’s say that there is a topic “sports” which is represented by many words related to sports, and the word “football” is the most suitable word to describe the topic (being used 20%). Now let’s go in the opposite direction to that of the algorithm for a while. Imagine that you’re trying to write an article, which has to be 30% about “food” and 70% about “sports”. To generate each word in the document we need to :- 
Using such generative model for a collection of documents, LDA tries to backtrack from the documents to find a set of topics that are likely to have generated the collection. 
Let’s take a small example 
Sample documents are (each line represents a document): 
Suppose we choose k=2 (number of topics are 2) for our model: 
Now some new document can be tagged with the above-given topics using the observations made by the LDA model. 
Here, we can say that sentence 1 is 100% Topic A and sentence 2 is 40% Topic B with 60% Topic A. 
Assumption: We are assuming that all topic assignments except for the current word in question are correct, and update the assignment of the current word using our model on every iteration. 
Suppose we have a collection of documents and we want to learn `K` topics out of them. 
3. X * Y is essentially the probability that topic `t` generated word `w`, so it makes sense that we re-sample the current word’s topic with this probability. 
After repeating these steps for large enough number of times, we will get pretty good topic assignments, such that they generate words describing the documents. 
If you want to dive more into the algorithm, here is the link for a detailed tutorial. You will find Natural Language Processing section with a sub-section of Topic modelling in which you will be able to learn more about LDA. 
Two important parameters of the algorithm are: 
For the symmetric distribution, a high alpha-value means that each document is likely to contain a mixture of most of the topics, and not any single topic specifically. A low alpha value puts less such constraints on documents and means that it is more likely that a document may contain mixture of just a few, or even only one, of the topics. Likewise, a high beta-value means that each topic is likely to contain a mixture of most of the words, and not any word specifically, while a low value means that a topic may contain a mixture of just a few of the words. 
If, on the other hand, the distribution is asymmetric, a high alpha-value means that a specific topic distribution (depending on the base measure) is more likely for each document. Similarly, high beta-values means each topic is more likely to contain a specific word mix defined by the base measure. 
In practice, a high alpha-value will lead to documents being more similar in terms of what topics they contain. A high beta-value will similarly lead to topics being more similar in terms of what words they contain. 
I have used UCI Health news Twitter dataset. The code is written using Apache Spark, an easy to scale in-memory processing engine. The complete code is available on Github. 
2. N-Gram = 3, k = 10, Min document occurrence of key words = 2 
3. N-Gram = 3, k = 16, Min document occurrence of key words = 2 
It is very difficult to analyze the results. The “topics” just spit out some random words. This is one of the drawbacks of the algorithm, that we have to go through many trial and errors. It is tough to draw insights from the outputs. We would need some domain expert for a feedback, to decide whether the results are better suited for a particular use-case or not. We can use Kullback–Leibler divergence score to decide the value of k (Number of topics as the output). 
The algorithm does shorten the large collection of documents to some commonly occurring words or phrases, grouped together according to the usage (these groups are called topics). You can leverage this property of the algorithm towards your use-cases. 
Written by 
Written by",Pratik Barhate,2019-02-05T07:19:49.599Z
"GCP Comics: How can Google Cloud help with security of your apps? | by Priyanka Vergadia | Google Cloud - Community | Jul, 2020 | Medium | Google Cloud - Community","At this point in the evolution of cloud computing it is fair to say that you have at least some apps in the cloud, or are planning to have a few in the near future. So, you may be wondering about the kind of security measures available to you. In this issue of GCP Comics we are covering exactly that! 
We will go over cloud security fundamentals including the three very simple security concepts. 
Here you go! Read on and please share your thoughts in the comments below. 
Google Cloud provides protection from threats through a secure foundation. It offers the core infrastructure that is designed, built and operated to help prevent threats. How is it done? Here are a few of the ways! 
Defense in depth 
Google’s infrastructure doesn’t rely on any single technology to make it secure. Rather, builds security through progressive layers that deliver true defense in depth. 
Other cloud providers may describe a similar stack of capabilities, but the way Google Cloud approaches many of these is unique. Here is how: 
If this is intriguing, here is a white paper on Google infrastructure design that goes into all of these areas in significant details. 
End-to-end provenance & attestation 
Google’s hardware infrastructure is custom-designed by Google “from chip to chiller” to precisely meet their requirements, including security. 
Google’s servers and Operating Systems(OS) are designed for the sole purpose of providing Google services. 
Understanding provenance from the bottom of the hardware stack to the top allows Google Cloud to control the underpinnings of the security posture. Unlike other cloud providers, Google has greatly reduced the “vendor in the middle problem” — if a vulnerability is found, steps can be taken immediately to develop and roll out a fix. This level of control results in greatly reduced exposure. 
Private backbone 
Google operates one of the largest backbone networks in the world. There are more than 130 points of presence across 35 countries — and there is a continuous addition of more zones and regions to meet customers’ preferences and policy requirements. 
Google’s network delivers low latency but also improves security. Once customers’ traffic is on Google’s network it is no longer transiting the public internet, making it less likely to be attacked, intercepted, or manipulated. 
Encryption at rest by default 
We will cover this one in more details in the upcoming comics but in short, all data at rest or in motion is encrypted by default on the Google network. And some services offer the option to supply or manager your own keys. 
Update at scale without disruptions 
Google has the ability to update the cloud infrastructure without disrupting customers using a technology called Live Migration. 
Updates add functionality, but from a security standpoint, they also are required to patch software vulnerabilities. No one writes perfect software, so this is a constant requirement. 
Keeping ahead of threats 
Security landscape rapidly evolves and many organizations struggle to keep pace. Because Google runs on the same infrastructure that is available to the customers, customers can directly benefit from those investments. 
The global footprint across enterprises and consumers gives Google an unprecedented visibility into threats and attacks. As a result, solutions can be developed before many other organizations even see the threats, reducing exposure. 
In the cloud there can be a lot of control options to make sure the app, the data and the services you deploy are secure. The most important thing to understand is that “cloud security requires collaboration” 
Your cloud provider (Google Cloud) is responsible for securing the infrastructure. 
You are responsible for securing your data. 
And.. Google Cloud provides the best practices, templates, products and solutions to help you secure your data and services. 
Keeping this section short because I am planning on doing another comic issue on this topic, there is a lot more to learn here, so stay tuned! 😊 
In order to protect the sensitive data that you store in Google Cloud, it maintains and goes though compliance including complex regulatory, frameworks and guidelines. For example HIPPA, FedRAMP, SOC etc. 
Read about the detailed compliance standards and certifications here. 
To learn more about security fundamentals on Google Cloud, check out this link to the detailed security whitepaper. 
Want more GCP Comics? Visit gcpcomics.com & follow me on Medium, and on Twitter to not miss the next issue! 
Written by 
Written by",Priyanka Vergadia,2020-09-09T07:39:32.707Z
Open Source Anomaly Detection Projects | by Himanshu Mittal | Medium,"Past few weeks I have been spending time to build an anomaly detection service. Got a chance to research on the existing open-source projects. 
In this blog, I would be focussing on well known open source projects that can be used for Anomaly Detection. The intention of this blog is to provide a glossary of existing projects. This is not an exhaustive list, just what I was able to find. 
Descriptions for each project are picked from the official GitHub readme. 
Update: rob_med has a list of tools & datasets for anomaly detection on time-series data.Link: https://github.com/rob-med/awesome-TS-anomaly-detection/ 
In case you know other projects, pls feel free to comment, will add it to the list. Hope this will help you get started !! 
Written by 
Written by",Himanshu Mittal,2019-09-13T10:47:47.991Z
NLP Year in Review — 2019. NLP highlights for the year 2019. | by Elvis | dair.ai | Medium,"2019 was an impressive year for the field of natural language processing (NLP). In this blog post, I want to highlight some of the most important stories related to machine learning and NLP that I came across in 2019. I will mostly focus on NLP but I will also highlight a few interesting stories related to AI in general. The headlines are in no particular order. Stories may include publications, engineering efforts, yearly reports, the release of educational resources, etc. 
Warning! This is a very long article so before you get started I would suggest bookmarking the article if you wish to read it in parts. I have also published the PDF version of this article which you can find at the end of the post. 
Google AI introduces ALBERT which a lite version of BERT for self-supervised learning of contextualized language representations. The main improvements are reducing redundancy and allocating the model’s capacity more efficiently. The method advances state-of-the-art performance on 12 NLP tasks. 
Earlier this year, researchers at NVIDIA published a popular paper (coined StyleGAN) which proposed an alternative generator architecture for GANs, adopted from style transfer. Here is a follow-up work where that focuses on improvements such as redesigning the generator normalization process. 
One of my favorite papers this year was code2seq which is a method for generating natural language sequences from the structured representation of code. Such research can give way to applications such as automated code summarization and documentation. 
Ever wondered if it’s possible to train a biomedical language model for biomedical text mining? The answer is BioBERT which is a contextualized approach for extracting important information from biomedical literature. 
After the release of BERT, Facebook researchers published RoBERTa which introduced new methods for optimization to improve upon BERT and produced state-of-the-art results on a wide variety of NLP benchmarks. 
Researchers from Facebook AI also recently published a method based on an all-attention layer for improving the efficiency of a Transformer language model. More work from this research group includes a method to teach AI systems on how to plan using natural language. 
Explainability continues to be an important topic in machine learning and NLP. This paper provides a comprehensive overview of works addressing explainability, taxonomies, and opportunities for future research. 
Sebastian Ruder published his thesis on Neural Transfer Learning for Natural Language Processing. 
A group of researchers developed a method to perform emotion recognition in the context of conversation which could pave the way to affective dialogue generation. Another related work involves a GNN approach called DialogueGCN to detect emotions in conversations. This research paper also provides code implementation. 
The Google AI Quantum team published a paper in Nature where they claim to have developed a quantum computer that is faster than the world’s largest supercomputer. Read more about their experiments here. 
As mentioned earlier, one of the areas of neural network architectures that require a lot of improvement is explainability. This paper discusses the limitations of attention as a reliable approach for explainability in the context of language modeling. 
Neural Logic Machine is a neural-symbolic network architecture that is able to do well at both inductive learning and logic reasoning. The model does significantly well on tasks such as sorting arrays and finding shortest paths. 
And here is a paper that applies Transformer language models to Extractive and Abstractive Neural document summarization. 
Researchers developed a method that focuses on using comparisons to build and train ML models. Instead of requiring large amounts of feature-label pairs, this technique compares images with previously seen images to decide whether the image should be of a certain label. 
Nelson Liu and others presented a paper discussing the type of linguistic knowledge being captured by pretrained contextualizers such as BERT and ELMo. 
XLNet is a pretraining method for NLP that showed improvements upon BERT on 20 tasks. I wrote a summary of this great work here. 
This work from DeepMind reports the results from an extensive empirical investigation that aims to evaluate language understanding models applied to a variety of tasks. Such extensive analysis is important to better understand what language models capture so as to improve their efficiency. 
VisualBERT is a simple and robust framework for modeling vision-and-language tasks including VQA and Flickr30K, among others. This approach leverages a stack of Transformer layers coupled with self-attention to align elements in a piece of text and the regions of an image. 
This work provides a detailed analysis comparing NLP transfer learning methods along with guidelines for NLP practitioners. 
Alex Wang and Kyunghyun propose an implementation of BERT that is able to produce high-quality, fluent generations. Here is a Colab notebook to try it. 
Facebook researchers published code (PyTorch implementation) for XLM which is a model for cross-lingual model pretraining. 
This works provides a comprehensive analysis of the application of reinforcement learning algorithms for neural machine translation. 
This survey paper published in JAIR provides a comprehensive overview of the training, evaluation, and use of cross-lingual word embedding models. 
The Gradient published an excellent article detailing the current limitations of reinforcement learning and also providing a potential path forward with hierarchical reinforcement learning. And in a timely manner, a couple of folks published an excellent set of tutorials to get started with reinforcement learning. 
This paper provides a light introduction to contextual word representations. 
Machine learning has been applied to solve real-world problems but it has also been applied in interesting and creative ways. ML creativity is as important as any other research area in AI because at the end of the day we wish to build AI systems that will help shape our culture and society. 
Towards the end of this year, Gary Marcus and Yoshua Bengio debated on the topics of deep learning, symbolic AI and the idea of hybrid AI systems. 
The 2019 AI Index Report was finally released and provides a comprehensive analysis of the state of AI which can be used to better understand the progress of AI in general. 
Commonsense reasoning continues to be an important area of research as we aim to build artificial intelligence systems that not are only able to make a prediction on the data provided but also understand and can reason about those decisions. This type of technology can be used in conversational AI where the goal is to enable an intelligent agent to have more natural conversations with people. Check out this interview with Nasrin Mostafazadeh having a discussion on commonsense reasoning and applications such as storytelling and language understanding. You can also check out this recent paper on how to leverage language models for commonsense reasoning. 
Activation Atlases is a technique developed by researchers at Google and Open AI to better understand and visualize the interactions happening between neurons of a neural network. 
Check out the Turing Lecture delivered by Geoffrey Hinton and Yann LeCun who were awarded, together with Yoshua Bengio, the Turing Award this year. 
Tackling climate change with machine learning is discussed in this paper. 
OpenAI published an extensive report discussing the social impacts of language models covering topics like beneficial use and potential misuse of the technology. 
Emotion analysis continues to be used in a diverse range of applications. The Mojifier is a cool project that looks at an image, detects the emotion, and replaces the face with the emojis matching the emotion detected. 
Work on radiology with the use of AI techniques has also been trending this year. Here is a nice summary of trends and perspectives in this area of study. Researchers from NYU also released a Pytorch implementation of a deep neural network that improves radiologists’ performance on breast cancer screening. And here is a major dataset release called MIMIC-CXR which consists of a database of chest Xrays and text radiology reports. 
The New York Times wrote a piece on Karen Spark Jones remembering the seminal contributions she made to NLP and Information Retrieval. 
OpenAI Five became the first AI system to beat a world champion at an esports game. 
The Global AI Talent Report provides a detailed report of the worldwide AI talent pool and the demand for AI talent globally. 
If you haven’t subscribed already, the DeepMind team has an excellent podcast where participants discuss the most pressing topics involving AI. Talking about AI potential, Demis Hassabis did an interview with The Economist where he spoke about futuristic ideas such as using AI as an extension to the human mind to potentially find solutions to important scientific problems. 
This year also witnessed incredible advancement in ML for health applications. For instance, researchers at Massachusetts developed an AI system capable of spotting brain hemorrhages as accurate as humans. 
Janelle Shane summarizes a set of “weird” experiments showing how machine learning can be used in creative ways to conduct fun experimentation. Sometimes this is the type of experiment that’s needed to really understand what an AI system is actually doing and not doing. Some experiments include neural networks generating fake snakes and telling jokes. 
Learn to find planets with machine learning models build on top of TensorFlow. 
OpenAI discusses the implication of releasing (including the potential of malicious use cases) large-scale unsupervised language models. 
This Colab notebook provides a great introduction on how to use Nucleus and TensorFlow for “DNA Sequencing Error Correction”. And here is a great detailed post on the use of deep learning architectures for exploring DNA. 
Alexander Rush is a Harvard NLP researcher who wrote an important article about the issues with tensors and how some current libraries expose them. He also went on to talk about a proposal for tensors with named indices. 
Here I highlight stories related to software and datasets that have assisted in enabling NLP and machine learning research and engineering. 
Hugging Face released a popular Transformer library based on Pytorch names pytorch-transformers. It allows NLP practitioners and researchers to easily use state-of-the-art general-purpose architectures such as BERT, GPT-2, and XLM, among others. If you are interested in how to use pytorch-transformers there are a few places to start but I really liked this detailed tutorial by Roberto Silveira showing how to use the library for machine comprehension. 
TensorFlow 2.0 was released with a bunch of new features. Read more about best practices here. François Chollet also wrote an extensive overview of the new features in this Colab notebook. 
PyTorch 1.3 was released with a ton of new features including named tensors and other front-end improvements. 
The Allen Institute for AI released Iconary which is an AI system that can play Pictionary-style games with a human. This work incorporates visual/language learning systems and commonsense reasoning. They also published a new commonsense reasoning benchmark called Abductive-NLI. 
spaCy releases a new library to incorporate Transformer language models into their own library so as to be able to extract features and used them in spaCy NLP pipelines. This effort is built on top of the popular Transformers library developed by Hugging Face. Maximilien Roberti also wrote a nice article on how to combine fast.ai code with pytorch-transformers. 
The Facebook AI team released PHYRE which is a benchmark for physical reasoning aiming to test the physical reasoning of AI systems through solving various physics puzzles. 
StanfordNLP released StanfordNLP 0.2.0 which is a Python library for natural language analysis. You can perform different types of linguistic analysis such as lemmatization and part of speech recognition on over 70 different languages. 
GQA is a visual question answering dataset for enabling research related to visual reasoning. 
exBERT is a visual interactive tool to explore the embeddings and attention of Transformer language models. You can find the paper here and the demo here. 
Distill published an article on how to visualize memorization in Recurrent Neural Networks (RNNs). 
Mathpix is a tool that lets you take a picture of an equation and then it provides you with the latex version. 
Parl.ai is a platform that hosts many popular datasets for all works involving dialog and conversational AI. 
Uber researchers released Ludwig, an open-source tool that allows users to easily train and test deep learning models with just a few lines of codes. The whole idea is to avoid any coding while training and testing models. 
Google AI researchers release “Natural Questions” which is a large-scale corpus for training and evaluating open-domain question answering systems. 
This year witnessed an explosion of data science writers and enthusiasts. This is great for our field and encourages healthy discussion and learning. Here I list a few interesting and must-see articles and blog posts I came across: 
Christian Perone provides an excellent introduction to maximum likelihood estimation (MLE) and maximum a posteriori (MAP) which are important principles to understand how parameters of a model are estimated. 
Reiichiro Nakano published a blog post discussing neural style transfer with adversarially robust classifiers. A Colab notebook was also provided. 
Saif M. Mohammad started a great series discussing a diachronic analysis of ACL anthology. 
The question is: can a language model learn syntax? Using structural probes, this work aims to show that it is possible to do so using contextualized representations and a method for finding tree structures. 
Andrej Karpathy wrote a blog post summarizing best practices and a recipe on how to effectively train neural networks. 
Google AI researchers and other researchers collaborated to improve the understanding of search using BERT models. Contextualized approaches like BERT are adequate to understand the intent behind search queries. 
Rectified Adam (RAdam) is a new optimization technique based on Adam optimizer that helps to improve AI architectures. There are several efforts in coming up with better and more stable optimizers but the authors claim to focus on other aspects of optimizations that are just as important for delivering improved convergence. 
With a lot of development of machine learning tools recently, there are also many discussions on how to implement ML systems that enable solutions to practical problems. Chip Huyen wrote an interesting chapter discussing machine learning system design emphasizing on topics such as hyperparameter tuning and data pipeline. 
NVIDIA breaks the record for creating the biggest language model trained on billions of parameters. 
Abigail See wrote this excellent blog post about what makes a good conversation in the context of systems developed to perform natural language generation task. 
Google AI published two natural language dialog datasets with the idea to use more complex and natural dialog datasets to improve personalization in conversational applications like digital assistants. 
Deep reinforcement learning continues to be one of the most widely discussed topics in the field of AI and it has even attracted interest in the space of psychology and neuroscience. Read more about some highlights in this paper published in Trends in Cognitive Sciences. 
Samira Abner wrote this excellent blog post summarizing the main building blocks behind Transformers and capsule networks and their connections. Adam Kosiorek also wrote this magnificent piece on stacked capsule-based autoencoders (an unsupervised version of capsule networks) which was used for object detection. 
Researchers published an interactive article on Distill that aims to show a visual exploration of Gaussian Processes. 
Through this Distill publication, Augustus Odena makes a call to researchers to address several important open questions about GANs. 
Here is a PyTorch implementation of graph convolutional networks (GCNs) used for classifying spammers vs. non-spammers. 
At the beginning of the year, VentureBeat released a list of predictions for 2019 made by experts such as Rumman Chowdury, Hilary Mason, Andrew Ng, and Yan LeCun. Check it out to see if their predictions were right. 
Learn how to finetune BERT to perform multi-label text classification. 
Due to the popularity of BERT, in the past few months, many researchers developed methods to “compress” BERT with the idea to build faster, smaller and memory-efficient versions of the original. Mitchell A. Gordon wrote a summary of the types of compressions and methods developed around this objective. 
Superintelligence continued to be a topic of debate among experts. It’s an important topic that needs a proper understanding of frameworks, policies, and careful observations. I found this interesting series of comprehensive essays (in the form of a technical report by K. Eric Drexler) to be useful to understand some issues and considerations around the topic of superintelligence. 
Eric Jang wrote a nice blog post introducing the concept of meta-learning which aims to build and train machine learning models that not only predict well but also learn well. 
A summary of AAAI 2019 highlights by Sebastian Ruder. 
Graph neural networks were heavily discussed this year. David Mack wrote a nice visual article about how they used this technique together with attention to perform shortest path calculations. 
Bayesian approaches remain an interesting subject, in particular how they can be applied to neural networks to avoid common issues like over-fitting. Here is a list of suggested reads by Kumar Shridhar on the topic. 
Perhaps one of the most highly discussed aspects of AI systems this year was ethics which include discussions around bias, fairness, and transparency, among others. In this section, I provide a list of interesting stories and papers around this topic: 
The paper titled “Does mitigating ML’s impact disparity require treatment disparity?” discusses the consequences of applying disparate learning processes through experiments conducted on real-world datasets. 
HuggingFace published an article discussing ethics in the context of open-sourcing NLP technology for conversational AI. 
Being able to quantify the role of ethics in AI research is an important endeavor going forward as we continue to introduce AI-based technologies to society. This paper provides a broad analysis of the measures and “use of ethics-related research in leading AI, machine learning and robotics venues.” 
This work presented at NAACL 2019 discusses how debiasing methods can cover up gender bias in word embeddings. 
Listen to Zachary Lipton presenting his paper “Troubling Trends in ML Scholarship”. I also wrote a summary of this interesting paper which you can find here. 
Gary Marcus and Ernest Davis published their book on “Rebooting AI: Building Artificial Intelligence We Can Trust”. The main theme of the book is to talk about the steps we must take to achieve robust artificial intelligence. On the topic of AI progression, François Chollet also wrote an impressive paper making a case for better ways to measure intelligence. 
Check out this Udacity course created by Andrew Trask on topics such as differential privacy, federated learning, and encrypted AI. On the topic of privacy, Emma Bluemke wrote this great post discussing how one may go about training machine learning models while preserving patient privacy. 
At the beginning of this year, Mariya Yao posted a comprehensive list of research paper summaries involving AI ethics. Although the list of paper reference was from 2018, I believe they are still relevant today. 
Here I will feature a list of educational resources, writers and people doing some amazing work educating others about difficult ML/NLP concepts/topics: 
CMU released materials and syllabus for their “Neural Networks for NLP” course. 
Elvis Saravia and Soujanya Poria released a project called NLP-Overview that is intended to help students and practitioners to get a condensed overview of modern deep learning techniques applied to NLP, including theory, algorithms, applications, and state of the art results — Link 
Microsoft Research Lab published a free ebook on the foundation of data science with topics ranging from Markov Chain Monte Carlo to Random Graphs. 
“Mathematics for Machine Learning” is a free ebook introducing the most important mathematical concepts used in machine learning. It also includes a few Jupyter notebook tutorials describing the machine learning parts. Jean Gallier and Jocelyn Quaintance wrote an extensive free ebook covering mathematical concepts used in machine learning. 
Stanford releases a playlist of videos for its course on “Natural Language Understanding”. 
On the topic of learning, OpenAI put together this great list of suggestions on how to keep learning and improving your machine learning skills. Apparently, their employees use these methods on a daily basis to keep learning and expanding their knowledge. 
Adrian Rosebrock published an 81-page guide on how to do computer vision with Python and OpenCV. 
Emily M. Bender and Alex Lascarides published a book titled “Linguistic Fundamentals for NLP”. The main idea behind the book is to discuss what “meaning” is in the field of NLP by providing a proper foundation on semantics and pragmatics. 
Elad Hazan published his lecture notes on “Optimization for Machine Learning” which aims to present machine training as an optimization problem with beautiful math and notations. Deeplearning.ai also published a great article that discusses parameter optimization in neural networks using a more visual and interactive approach. 
Andreas Mueller published a playlist of videos for a new course in “Applied Machine Learning”. 
Fast.ai releases its new MOOC titled “Deep Learning from the Foundations”. 
MIT published all videos and syllabus for their course on “Introduction to Deep Learning”. 
Chip Huyen tweeted an impressive list of free online courses to get started with machine learning. 
Andrew Trask published his book titled “Grokking Deep Learning”. The book serves as a great starter for understanding the fundamental building blocks of neural network architectures. 
Sebastian Raschka uploaded 80 notebooks about how to implement different deep learning models such as RNNs and CNNs. The great thing is that the models are all implemented in both PyTorch and TensorFlow. 
Here is a great tutorial that goes deep into understanding how TensorFlow works. And here is one by Christian Perone for PyTorch. 
Fast.ai also published a course titled “Intro to NLP” accompanied by a playlist. Topics range from sentiment analysis to topic modeling to the Transformer. 
Learn about Graph Convolutional Neural Networks for Molecular Generation in this talk by Xavier Bresson. Slides can be found here. And here is a paper discussing how to pre-train GNNs. 
On the topic of graph networks, some engineers use them to predict the properties of molecules and crystal. The Google AI team also published an excellent blog post explaining how they use GNNs for odor prediction. If you are interested in getting started with Graph Neural Networks, here is a comprehensive overview of the different GNNs and their applications. 
Here is a playlist of videos on unsupervised learning methods such as PCA by Rene Vidal from Johns Hopkins University. 
If you are ever interested in converting a pretrained TensorFlow model to PyTorch, Thomas Wolf has you covered in this blog post. 
Want to learn about generative deep learning? David Foster wrote a great book that teaches data scientists how to apply GANs and encoder-decoder models for performing tasks such as painting, writing, and composing music. Here is the official repository accompanying the book, it includes TensorFlow code. There is also an effort to convert the code to PyTorch as well. 
A Colab notebook containing code blocks to practice and learn about causal inference concepts such as interventions, counterfactuals, etc. 
Here are the materials for the NAACL 2019 tutorial on “Transfer Learning in Natural Language Processing” delivered by Sebastian Ruder, Matthew Peters, Swabha Swayamdipta and Thomas Wolf. They also provided an accompanying Google Colab notebook to get started. 
Another great blog post from Jay Alammar on the topic of data representation. He also wrote many other interesting illustrated guides that include GPT-2 and BERT. Peter Bloem also published a very detailed blog post explaining all the bits that make up a Transformer. 
Here is a nice overview of trends in NLP at ACL 2019, written by Mihail Eric. Some topics include infusing knowledge into NLP architectures, interpretability, and reducing bias among others. Here are a couple more overviews if you are interested: link 2 and link 3. 
The full syllabus for CS231n 2019 edition was released by Stanford. 
David Abel posted a set of notes for ICLR 2019. He was also nice to provide an impressive summary of NeurIPS 2019. 
This is an excellent book that provides learners with a proper introduction to deep learning with notebooks provided as well. 
An illustrated guide to BERT, ELMo, and co. for transfer learning NLP. 
Fast.ai releases its 2019 edition of the “Practical Deep Learning for Coders” course. 
Learn about deep unsupervised learning in this fantastic course taught by Pieter Abbeel and others. 
Gilbert Strang released a new book related to Linear Algebra and neural networks. 
Caltech provided the entire syllabus, lecture slides, and video playlist for their course on “Foundation of Machine Learning”. 
The “Scipy Lecture Notes” is a series of tutorials that teach you how to master tools such as matplotlib, NumPy, and SciPy. 
Here is an excellent tutorial on understanding Gaussian processes. (Notebooks provided). 
This is a must-read article in which Lilian Weng provides a deep dive into generalized language models such as ULMFit, OpenAI GPT-2, and BERT. 
Papers with Code is a website that shows a curated list of machine learning papers with code and state-of-the-art results. 
Christoph Molnar released the first edition of “Interpretable Machine Learning” which is a book that touches on important techniques used to better interpret machine learning algorithms. 
David Bamman releases the full syllabus and slides to the NLP courses offered at UC Berkley. 
Berkley releases all materials for their “Applied NLP” class. 
Aerin Kim is a senior research engineer at Microsoft and writes about topics related to applied Math and deep learning. Some topics include intuition to conditional independence, gamma distribution, perplexity, etc. 
Tai-Danae Bradley wrote this blog post discussing ways on how to think about matrices and tensors. The article is written with some incredible visuals which help to better understand certain transformations and operations performed on matrices. 
I hope you found the links useful. I wish you a successful and healthy 2020! 
Due to the holidays, I didn’t get much chance to proofread the article so any feedback or corrections are welcomed! 
>> PDF version << 
Written by 
Written by",Elvis,2020-01-05T20:22:09.031Z
5 Things to Know from 12 Months in Cyber Security | by Threat Intel | Threat Intel | Medium,"There is no doubt that the biggest trend in 2017 was the huge growth in cryptocurrency coin mining that occurred in the last few months of the year. 
Driven by various factors, but primarily the huge increase in value of many cryptocurrencies, activity in this space surged by 8,500 percent over the course of 2017. Yes, you are reading that right, eight thousand five hundred percent. Most of this growth was driven by browser-based coinminers: this is where cyber criminals insert a few lines of code onto a web page, and when a user visits this web page this code utilizes their computer’s power to secretly mine cryptocurrencies. The most famous cryptocurrency is Bitcoin, but Bitcoin needs a lot of power to be mined and a regular computer wouldn’t be capable of this. The majority of these coinminers were mining Monero, which is easier to mine and also offers a higher degree of anonymity. 
Coin mining also allows cyber criminals to keep a low profile: many victims may not even realize a coinminer is running on their device, as they could attribute any slowdown (the main impact of coinminers on most users) to other factors. 
However, the reason coin mining was such a trend in 2017 is because it was so profitable for criminals: if the values of cryptocurrencies drop, criminals’ interest in this area is likely to take a similar dive. 
Mobile malware has been around for a long time, so is perhaps not talked about as much as it once was. However, it is still a major threat, and we observed a 54 percent increase in mobile variants in 2017, showing that this is an area that cyber criminals are still taking a lot of interest in. More than 24,000 malicious mobile apps were blocked every single day in 2017. The great majority of mobile malware discovered was found on third-party app stores — underlining the increased risks users can face if they root or jailbreak mobile devices. 
User behavior can be a particularly big factor when it comes to mobile malware, and we previously published a blog with tips on how to improve your smartphone’s security. Another big issue is that only 20 percent of Android users were running the latest major version of the OS, however, that is not simply the fault of the user and often users of certain brands of phone are never offered an option to update to the latest version of Android. 
In the wake of the havoc caused by the Mirai botnet, it was all about Internet of Things (IoT) devices and the dangers posed by them in 2016. But, in 2017, with threats like the above-mentioned coin mining, WannaCry, and Petya/NotPetya grabbing all the attention, IoT fell out of the headlines a little bit. However, it would be wrong to think these devices have fallen off the radar for cyber criminals. Attacks on IoT devices increased by 600 percent in 2017. While coin mining is currently primarily taking place on computers, and also mobile phones, if this area continues to develop it is very possible that cyber criminals will increasingly focus on IoT devices and use these to mine cryptocurrencies. 
With the amount of IoT technology available to us increasing all the time, good security in this area, on the part of both consumers and manufacturers, is more important than ever. 
Apple’s operating systems are often commended for their high levels of security — though they are not immune to threats either. In 2017, we observed an 80 percent increase in new malware on Macs, which is a huge increase. 
However, this increase was primarily driven by coinminers — underlining again the huge impact these threats had in 2017. Browser-based coinminers are able to run on even fully-patched machines, which is one of the things that make them so attractive to cyber criminals, and one of the reasons why they had such an impact on the normally very secure Mac ecosystem. This fact also means they can be difficult for consumers to protect themselves against, and the onus is really on website owners to make sure their websites are secure and well protected, so that criminals cannot inject malicious code onto them. 
WannaCry was almost undoubtedly the biggest cyber security news story of 2017, and is one I’m sure most people are familiar with. This ransomware from the Lazarus group spread across the world at lightning speed — and would have caused even greater damage but for the discovery of a killswitch by a security researcher quite early on in the outbreak. WannaCry was interesting for many reasons: it was ransomware being used by a targeted attack group to make money, which is extremely unusual, though the use of ransomware by targeted attack groups for various different reasons may be something we see more often, as we outline in the report. 
WannaCry was also unusual (for recent times) because it was self-propagating, as it used the EternalBlue exploit to spread. This exploit had been patched some months before WannaCry was unleashed, but there was still a sufficient number of unpatched computers online for WannaCry to cause serious havoc. Petya/NotPetya, a destructive wiper that initially appeared to be ransomware, also used this exploit to spread later in the year, as well as using other SMB spreading techniques that utilized legitimate tools. 
Both these threats exhibited characteristics that we could yet see becoming a bigger trend going forward: the use of ransomware as a decoy and/or a revenue generator by targeted attack groups, and the return of self-propagating threats. 
Will these trends continue as we move further into 2018? Only time will tell. 
Read more about these and many more cyber security threats and trends, including software supply chain attacks and a surge in certain financial Trojans, by downloading ISTR 23 now. 
Check out the Security Response blog and follow Threat Intel on Twitter to keep up-to-date with the latest happenings in the world of threat intelligence and cybersecurity. 
Like this story? Recommend it by hitting the heart button so others on Medium see it, and follow Threat Intel on Medium for more great content. 
Written by 
Written by",Threat Intel,2018-03-28T13:27:01.055Z
Transformer vs RNN and CNN for Translation Task | by Yacine BENAFFANE | Analytics Vidhya | Medium,"For automatic translation with Deep Learning, one uses the sequence to sequence model (Seq2Seq), with architectures such as the RNN and CNN, and by adding the mechanism of the attention. Here are some references to understand the article: 
Google Brain and their collaborators have published an article introducing a new architecture, the Transformer, based only on attention mechanisms (see reference [1]). It surpasses any other NMT models seen before such as Google Neural Machine Translation (GNMT) alias Google Translate. 
The Transformer has been able to reach a new state of the art in translation. In addition to major improvements in the quality of translation, it also allows the realization of many other natural language processing (NLP) tasks. 
RNNs (or LSTMs) have been established as advanced approaches to sequence modeling and transduction problems such as language modeling and machine translation. 
Currently, complex RNN and CNN based on an encoder-decoder scheme dominate the transduction models. Reccurent models don’t allow parallelization during training because of their sequential nature, which poses a problem for learning long-term memory dependencies. The network performs better with more memory, but it ends up limiting the batch processing in case of learning long sequences. That’s why parallelization can’t help. The reduction of this fundamental constraint of the sequential computation has been the goal of many published works such as Bytenet or ConvS2S using convolution. 
Transformer’s new approach is to completely eliminate recurrence and convolution and replace them with personal attention (self attention) to establish the dependencies between inputs and outputs. It’s the first type of architecture to rely entirely on attention to calculate representations of inputs and outputs. In addition, Transformer leaves more room for parallelization. 
Intuitively, this mechanism allows the decoder to “look back” at the entire sentence and selectively extract the information it needs during decoding. 
Specifically, the attention mechanism allow connections between the hidden states (output vector) of each encoder and decoder, so that the target word is predicted based on a combination of vectors, rather than just the hidden state of the decoder, this mechanism gives the decoder access to all the hidden states of the encoder. 
The decoder must make a single prediction for the next word, a complete sequence can not be sent, but some kind of summary vector has to be transmitted instead. What matters is that it asks the decoder (the mechanism) to choose the hidden states to use and those to ignore by weighting the hidden states, the decoder then receives a weighted sum of hidden states to use to predict the next word. Only a small part of the source sentence is relevant for the translation of a target word (see reference [6]). 
The following figure represent a visualisation of attention mechanism. The transparency of the blue link represents how much the decoder give attention to a coded word. 
With RNN, you have to go word by word to access to the cell of the last word. If the network is formed with a long reach, it may take several steps to remember, each masked state (output vector in a word) depends on the previous masked state. This becomes a major problem for GPUs. This sequentiality is an obstacle to the parallelization of the process. In addition, in cases where such sequences are too long, the model tends to forget the contents of the distant positions one after the other or to mix with the contents of the following positions. 
Whenever long-term dependencies are involved, we know that RNN (LSTM / GRU) suffer from an endangered gradient problem (Vanishing Gradient Problem). 
The following visualization shows the progression of GNMT (Google neural machine translation, Google Translate) when translating a Chinese sentence into English. The network encodes the Chinese words in the form of a list of vectors, each vector representing the meaning of all the words read until now “Encoder”. Once it read the whole sentence, the decoder generates the English sentence, by processing word after word (decoding operation). 
To generate the translated word at each step, the decoder gives more attention to a weighted distribution on the most relevant coded Chinese vectors, in order to generate the English word with attention. The transparency of the blue link represents how much the decoder give attention to a coded word. (see reference [3]). 
Autoregressive models (which predict future values from past values) involve sequential calculations, this requires a big processing power. In order to reduce these costs, models such as ByteNet and ConvS2S use CNNs that are easy to parallelize, which isn’t possible with RNN, although they seem to be better suited to sequence modeling. Convolution enables parallelization for graphics processor processing. 
Early efforts were trying to solve the dependency problem with seq2seq convolutions for a solution to the RNN. A long sequence is taken and the convolutions are applied. The disadvantage is that CNN approaches require many layers to capture long-term dependencies in the sequential data structure, without ever succeeding or making the network so large that it would eventually become impractical. 
The ability of a model to learn dependencies between positions decreases rapidly with distance, which makes it critical to find another approach that can parallelize these sequential data, and that’s where Transformer comes in. 
The decoders are generally trained to predict sentences based on all words preceding the current word (same for others architectures like Transformer). So only the encoder can be parallelized. 
See this links for more information. 
and the research paper: https://arxiv.org/abs/1705.03122 
The Transformer presents a new approach, it proposes to encode each position and to apply the mechanism of attention in order to connect two distant words, which can then be parallelized, thus accelerating learning. It applies a mechanism of self-attention. 
To calculate the attention score, the Transformer compares the score to all other words(their score) in the sentence. The result of these comparisons is a score of attention for each word of the sentence. These attention scores determine the contribution of each of the other words to the next representation. 
The decoder predicts the sentences according to all the words preceding the current word. 
The Transformer also allows the resolution of the coreference. 
The idea of ​​the transformer is to extend the mechanism of traditional attention, instead of calculating the attention once, it is calculated several times (multihead attention), which solves the problem of coreference as well as other problems. In the following figure, the attention was well recognized for the 2 sentences for Transformer, but the 2 nd sentence is not well translated with Google Translate. 
for more information, consult this link: 
Learning long-range dependencies is a major challenge in many sequence transductions tasks. A key factor affecting the ability to learn from such dependencies is the length of paths that forward and backward signals must traverse in the network. The shorter these paths are between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies. 
As the length of the sequence increases, RNN establishes long-term dependencies, but its loss of information has become very serious. The attention to the RNN sequence also seems to collect a lot of information, but there is a very large overlap of information (the content in different positions of the sequence has different effects on the output of the final encoder). On the other hand, the information in different positions of the input sequence in the CNN has the same effect on the output of the encoder (the Transformer is similar). To solve this problem, ConvS2S and Transformer put the location information directly into the model entry. 
In Transformer, to the extent that individual attention is applied to each word and word in the set, regardless of their distance, the longest possible path is a path that allows the system to capture the distant dependency relationships. 
There are basically three types of dependencies. The dependencies between (see reference [6]): 
In terms of computational complexity, the self-attention layers are faster than the recursive layers when the length of the sequence n is smaller than the dimensionality of the representation d, which is the case most often used with the representations of sentences used by the most recent models in automatic translations. To improve computational performance for tasks with very long sequences, it’s possible that personal attention is limited to consideration of a neighborhood of size r in the input sequence centered around the respective output position. This would increase the maximum path length to O(n/r). 
Auto-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recursive layer requires O(n) sequential operations. 
The convolutional layers are generally more expensive than the recurrent layers, by a factor k, or the complexity is O(k*n*d²). 
A single convolutive layer of core width k<n does not connect all the pairs of input and output positions. This requires a stack of convolutional layers O(n/ k) in the case of contiguous cores, and O(logk(n)) in the case of dilated convolutions, which increases the length of the longest path between two positions in the network. 
Experiments on two tasks of translation (a large number of sentence pairs are translated (36 million sentences for English-French), and the quality of translation score is calculated.) showed that these models generate better quality translations, while performing parallel calculations and requiring much less training time than the other models. The quality estimate is done using the BLEU score(for more information about BLEU score : https://towardsdatascience.com/evaluating-text-output-in-nlp-bleu-at-your-own-risk-e8609665a213 ). On the English-French translation task of WMT 2014, Transformer sets a new modern BLEU score of 41.8. It took 3.5 days of training with 8 GPUs, while GNMT takes 1 week. 
We saw how powerful the Transformer’s compared to the RNN and CNN for translation tasks. It has defined a new state of the art and provides a solid foundation for the future of many deep learning architectures to use the self-attention mechanism: GPT-2 and XL-NET are two examples of models using it. 
For more information, you can see my repos: https://github.com/Styleoshin/Transformer 
[1] Document « Attention Is All You Need » https://arxiv.org/abs/1706.03762 
[2] https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html 
[3] https://ai.googleblog.com/2016/09/a-neural-network-for-machine.html 
[4] https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/#.XUG73eiiG73 
[5] https://androidkt.com/attention-base-transformer-for-nlp/ 
[6] http://mlexplained.com/2017/12/29/attention-is-all-you-need-explained/ 
-عقبة الأيسر : https://medium.com/@OkbaLeftHanded_18875 
-Amine Horse-man : https://medium.com/@AmineHorseman 
Written by 
Written by",Yacine BENAFFANE,2019-08-29T03:52:38.600Z
Anomaly Detection of Time Series Data | by Jet New | Medium,"Anomaly detection (also outlier detection) is the identification of rare items, events or observations which raise suspicions by differing significantly from the majority of the data. – Wikipedia 
An anomaly is the deviation in a quantity from its expected value, e.g., the difference between a measurement and a mean or a model prediction. – Wikipedia 
Conventional statistical methods are generally more interpretable and sometimes more useful than machine learning-based methods, depending on the specified problem. 
Machine Learning methods can model more complex data and hence able to detect more complex anomalies than conventional statistical methods. 
Holt-Winters is a forecasting technique for seasonal (i.e. cyclical) time series data, based on previous timestamps. 
Holt-Winters models a time series in 3 ways – average, trend and seasonality. An average is a value referenced upon, a trend is a general increase/decrease over time and a seasonality is a cyclical repeating pattern over a period. 
The value forecast at t=x is a factor of the value at t=x, along with a discounted value of the value forecast at t=x-1. (1-a) is recursively multiplied every timestamp back, resulting in an exponential computation. 
ARIMA is a statistical model for time series data, capturing 3 key aspects of the temporal information — Auto-Regression(AR), Integration(I) and Moving Average(MA). 
Auto-Regression — Observations are regressed on its own lagged (i.e., prior) values.Integrated — Data values are replaced by the difference between values.Moving Average — Regression errors are dependent on lagged observations. 
Histogram-Based Outlier Score (HBOS) is a O(n) linear time unsupervised algorithm that is faster than multivariate approaches at the cost of less precision. It can detect global outliers well but performs poorly on local outlier problems. 
Decision Trees are used for anomaly detection to learn rules from data. However, with few labelled data, aside from the class imbalance problem, inferences from rules may not make sense, as leaf nodes may end up with very few observations, e.g. 2 positives and 0 negatives. 
SVMs first maps input vectors into a higher-dimensional feature space, then obtains the optimal separating hyper-plane in the feature space. The decision boundary is determined by support vectors rather than the whole training sample, and thus is extremely robust to outliers. 
LSTM Forecasting is a supervised method that, given a time series sequence as input, predicts the value at the next timestamp. It trains on normal data only, and the prediction error is used as the anomaly score. 
K-Means Clustering is generally not useful in anomaly detection due to its sensitivity to outliers. Centroids cannot be updated if a set of objects close to it is empty. 
Hierarchical Clustering, unlike K-Means, does not require specification of the number of clusters at initialisation. It creates a dendrogram and clusters can be separately selected thereafter. Scipy’s Hierarchical Clustering is recommended over Scikit-Learn’s because of its customisability, ability to select number of clusters after the clustering, and dendrogram plots. 
LSTM Autoencoder is a self-supervised method that, given a time series sequence as input, predicts the same input sequence as its output. With this approach, it learns a representation of normal sequences and the prediction error can be interpreted as the anomaly score. 
Given a only a point, contextual anomalies cannot be detected due to the lack of temporal information. 
A rolling window (representing a point) contains temporal information from a few time steps back, allowing the possibility of detecting contextual anomalies. This is sufficient for LSTM-based models. 
Signal transformations/decompositions such as Fast Fourier Transform (FFT), Continuous Wavelet Transform (CWT) and Singular Spectrum Analysis (SSA), as well as statistical measurements such as Max/Min, Mean, No. of Peaks, can surface temporal information crucial to detecting anomalies. 
Jet New is an incoming Computer Science undergraduate student at the National University of Singapore with working experience in Machine Learning (Anomaly Detection). 
Written by 
Written by",Jet New,2019-06-06T13:14:37.943Z
A Brief Survey of Node Classification with Graph Neural Networks | by ODSC - Open Data Science | Medium,"Shauna is a speaker for ODSC East 2020 this April 13–17 in Boston. Be sure to check out her talk, “Graph Neural Networks and their Applications,” there! 
Graph neural networks have revolutionized the performance of neural networks on graph data. Companies such as Pinterest[1], Google[2], and Uber[3] have implemented graph neural network algorithms to dramatically improve the performance of large-scale data-driven tasks. 
Introduction to Graphs 
A graph is any dataset that contains nodes and edges. Nodes are entities (e.g. people, organizations), and edges represent the connections between nodes. For example, a social network is a graph in which people in the network are considered nodes. Edges exist when two people are connected in some way (e.g. friends, sharing one’s posts). 
In retail applications, customers and products can be viewed as nodes. An edge shows the relationship between the customer and a purchased product. A graph can be used to represent spending habits for each customer. Additionally, nodes can also have features or attributes. People have attributes such as age and height, and products have attributes such as price and size. Pinterest has used graph neural networks in this fashion to improve the performance of its recommendation system by 150%[1]. 
The advent of Graph Neural Networks 
Until the development of graph neural networks, deep learning methods could not be applied to edges to extract knowledge and make predictions, and instead could only operate based on the node features. Applying deep learning to graph data allows us to approach tasks link prediction, community detection, and generating recommendations. 
Deep learning can also be applied to node classification, or predicting the label of an unlabelled node. This takes place in a semi-supervised setting, where the labels of some nodes are known, but others are unknown. A survey of deep learning node classification methods shows a history of advances in state-of-the-art performance while illustrating the range of use cases and applications. 
Methods discussed in this blog post were evaluated on the benchmark CoRA dataset. CoRA consists of journal publications in deep learning. Each publication is a node, and edges exist between nodes if they cite or are cited by another paper in the dataset. The dataset is comprised of 2708 publications with 5429 edges. 
DeepWalk 
DeepWalk[4], released in 2014, was the first significant deep learning-based method to approach node classification. DeepWalk’s approach was similar to that taken in natural language processing (NLP) to derive embeddings. An embedding is a vector-representation of an object such a word in NLP or a node in a graph. To create its embeddings, DeepWalk takes truncated random walks from graph data to learn latent representations of nodes. On the CoRA dataset, DeepWalk achieved 67.2% accuracy on the benchmark node classification experiment[5]. At the time, this was 10% better than competing methods with 60% less training data. 
To demonstrate how the embeddings generated by DeepWalk contain information about the graph structure, below is a figure that shows how DeepWalk works on Zachary’s Karate Network. The Karate Network is a small network of connections of members of a karate club, where edges are made between two members if they interact outside of karate. The node coloring is based on the different sub-communities within the club. The left image is the input graph of the social network, and the right is the two-dimensional output generated by the DeepWalk algorithm operating on the graph data. 
Source: [4]. 
Graph Convolutional Networks 
In 2016, Thomas N. Kipf and Max Welling introduced graph convolutional networks (GCNs)[6], which improved the state-of-the-art CoRA benchmark to 81.5%. A GCN is a network that consists of stacked linear layers with an activation function. Kipf and Welling introduced a new propagation function that operates layer-wise and works directly on graph data. 
Source: [6] The t-SNE visualization of the two-layer GCN trained on the CoRA dataset using 5% of labels. The colors represent document class. 
The number of linear layers in a GCN determines the size of the target node neighborhood to consider when making the classification prediction. For example, one hidden layer would imply that the graph network only examines immediate neighbors when making a classification decision. 
The input to a graph convolutional network is an adjacency matrix, which is the representation of the graph itself. It also takes the feature vectors of each node as input. This can be as simple as a one-hot encoding of each node’s attributes, while more complex versions can be used to represent the complex features of a node. 
Applying GCN to Real-World Data 
Our research team was interested in implementing a GCN on real-world data in order to speed up analyst tasking. We implemented a GCN architecture written in PyTorch to perform node classification on article data. The graph used in our dataset was derived from article data grouped together in “playlists” by a user who determined that they were relevant. The nodes were individual articles and playlists, and edges existed between an article and a playlist if a specific article was included in the playlist. Instead of manually poring through the corpus to determine additional relevant articles, we used the GCN to recommend other potentially relevant documents. After running our corpus of about 100,000 articles and 7 different playlists through a 2-layer GCN, our network performed 5x better than random. 
Graph-BERT 
GCN’s were the leading architecture for years, and many variations of them were subsequently released. Then, in January 2020 Graph-BERT[7] removed the dependency on links and reformatted the way that graph networks are usually represented. This is important for scalability, while also showing improved accuracy and efficiency relative to other types of graph neural networks. We are currently exploring how Graph-BERT will impact the use cases we have been addressing with graph neural networks. 
Conclusion 
Graph neural networks are an evolving field in the study of neural networks. Their ability to use graph data has made difficult problems such as node classification more tractable. 
For a deeper discussion on graph neural networks and the problems that they can help solve, attend my talk at ODSC East, “Graph Neural Networks and their Applications.” 
CITATIONS 
[1] R. Ying, R. He, K. Chen, P. Eksombatchai, W. L. Hamilton, and J. Leskovec, “Graph convolutional neural networks for web-scale recommender systems,” in Proc. of KDD. ACM, 2018, pp. 974–983. 
[2] Sanchez-Lengeling , Benjamin, et al. “Machine Learning for Scent: Learning Generalizable Perceptual Representations of Small Molecules.” ArXiv, no. 1910.10685, 2019. Stat.ML, arxiv.org/abs/1910.10685. 
[3] Uber Engineering. “Food Discovery with Uber Eats: Using Graph Learning to Power Recommendations”, eng.uber.com/uber-eats-graph-learning. 
[4] Perozzi, Bryan, Al-Rfou, Rami and Skiena, Steven. “DeepWalk: Online Learning of Social Representations.” CoRR abs/1403.6652 (2014). 
[5] “Node Classification on CoRA.” paperswithcode.com/sota/node-classification-on-coraom. 
[6] Kipf, Thomas N. and Welling, Max. “Semi-Supervised Classification with Graph Convolutional Networks.” Paper presented at the meeting of the Proceedings of the 5th International Conference on Learning Representations, 2017. 
[7] Zhang, Jiawei, Zhang, Haopeng, Xia, Congying, and Sun, Li. “Graph-Bert: Only Attention is Needed for Learning Graph Representations.” arxiv.org/abs/2001.05140 (2020). 
About the speaker/author: Shauna Revay is an applied machine learning researcher for Novetta’s Machine Learning Center of Excellence. She specializes in rapid prototyping of ML solutions spanning fields such as NLP, audio analysis and ASR, and the implementation of graph neural networks. Shauna earned her PhD in mathematics at George Mason University where she studied harmonic and Fourier analysis. She pursues research in interdisciplinary topics and has published papers in mathematics, biology, and electrical engineering journals and conference proceedings. 
Written by 
Written by",ODSC - Open Data Science,2020-02-26T15:01:01.210Z
A simple way to explain the Recommendation Engine in AI | by Roger Chua | Voice Tech Podcast | Medium,"In my previous blog, it broadly talks about how we could exploit NLP by extracting further values from NLG and NLU. Extending to that, it brings us to the Recommendation Engine as NLP could be a component of a Recommender system. Recommender systems could be based on an NLP module either for feature extraction or for text classification. 
In this blog, we shall touch on Recommendation Engine, why is it essential in the business world context, especially so in the face of the growing amount of information on the internet and with a significant rise in the number of users. Similarly, we will not dive into the technical details as the intent of this blog is to serve the foundation understanding. 
A recommendation engine is a system that suggests products, services, information to users based on analysis of data. Notwithstanding, the recommendation can derive from a variety of factors such as the history of the user and the behaviour of similar users. 
Recommendation systems are quickly becoming the primary way for users to expose to the whole digital world through the lens of their experiences, behaviours, preferences and interests. And in a world of information density and product overload, a recommendation engine provides an efficient way for companies to provide consumers with personalised information and solutions. 
A recommendation engine can significantly boost revenues, Click-Through Rates (CTRs), conversions, and other essential metrics. It can have positive effects on the user experience, thus translating to higher customer satisfaction and retention. 
Let’s take Netflix as an example. Instead of having to browse through thousands of box sets and movie titles, Netflix presents you with a much narrower selection of items that you are likely to enjoy. This capability saves you time and delivers a better user experience. With this function, Netflix achieved lower cancellation rates, saving the company around a billion dollars a year. 
Although recommender systems have been used for almost 20 years by companies like Amazon, it has been proliferated to other industries such as finance and travel during the last few years. 
Recommendation engines need to know you better to be effective with their suggestion. Therefore, the information they collect and integrate is a critical aspect of the process. This can be information relating to explicit interactions, for example, information about your past activity, your ratings, reviews and other information about your profile, such as gender, age, or investment objectives. These can combine with implicit interactions such as the device you use for access, clicks on a link, location, and dates. 
There are three main types of techniques for Recommendation systems; content-based filtering, collaborative filtering, and knowledge-based system. 
Content-based filtering is based on a single user’s interactions and preference. Recommendations are based on the metadata collected from a user’s history and interactions. For example, recommendations will be based on looking at established patterns in a user’s choice or behaviours. Returning information such as products or services will relate to your likes or views. With an approach like this, the more information that the user provides, the higher the accuracy. 
Given the privacy and regulatory issues are important in some industries’ services, personal metadata and individual transactional data can be missing at the outset. These issues are commonly known as ‘cold start’ problems for recommender systems using this approach. Cold start occurs when a recommender system cannot draw inferences for a query due to lack of sufficient information. A particular form of the content-based recommendation system is a case-based recommender. These evaluate items’ similarities and have been extensively deployed in e-commerce. 
A recommendation like ‘products similar to this’, is a typical instance of this type of approach. Overall, these are limited though to the specific domain and the level of categorisation available. 
Collaborative filtering is another commonly used technique. Collaborative filtering casts a much wider net, collecting information from the interactions from many other users to derive suggestions for you. This approach makes recommendations based on other users with similar tastes or situations. For example, by using their opinion and actions to recommend items to you or to identify how one product may go well with another. ‘Next buy’ recommendations is a typical usage. Collaborative filtering method usually has higher accuracy than content-based filtering; however, they can also introduce some increased variability and sometimes less interpretable results. They are especially weak in the absence of previously collected data. Without meaningful information on others, it becomes harder, naturally, to participate in any single person actions. 
Build better voice apps. Get more articles & interviews from voice technology experts at voicetechpodcast.com 
Knowledge-based systems are systems where suggestions are based on an influence about a user’s needs and based on a degree of domain expertise and knowledge. Rules are defined that set context for each recommendation. This, for example, could be criteria that define when a specific financial product, like a trust, would be beneficial to the user. These do not, by default, have to use interaction history of a user in the same way as the content-based approach is, but can include these as well as customer products and service attributes, as well as other expert information. Given the way the system is built up, the recommendations can be easily explained. But building up this type of framework can be expensive. It tends to be better suited to complex domains where items are infrequently purchased or hence, data is lacking. Given this, it doesn’t suffer the same cold-start up problems as others above. 
Providing relevant recommendations is the hallmark of a sound system. Conventional measurement techniques include measures of accuracy or coverage measures. 
Accuracy can be described as the fraction of correct recommendations out of the total possible recommendations; 
Coverage measures the number of items or users that the system is actually able to provide recommendation for. 
For example, accuracy may be high at the same time as coverage is low. This could happen if the recommendation to the eligible subset were valid and accurate. This could happen if the recommendations to the suitable subset were valid and accurate, while many were excluded as few users had rated an item. 
In general, recommendation engines improve with more information. Recommendation engines that display smart, intuitive, visualisation techniques for their results, are much likelier to ensure repeat visits. As such, recommendation engines that continue with you, along with your quest for more and more information and products, will be to gather more and more of the underlying information for usage later. 
Creating a self-sustaining ever-improving environment for the recommendation engine relies on much more than preparing the engine itself. 
Let’s retake the example of Netflix. The recommendation engine is core to Netflix. More than 80% of the TV shows that people watch on the platform are discovered through a recommendation system. What’s unique about the system is that it doesn’t look at broad genres, but rather into nuanced threads within the content. The aim is to help break viewers break preconceived notions and find shows that they might have chosen initially. 
Netflix’s recommendation engine uses ‘three-legged stool’ working concept. The first leg is the history of what Netflix members watched. Tags are done by Netflix employees who understand everything about the content and proprietary machine learning algorithms that take all of the data and put things together. 
Such recommendation engines working concept can serve as an intelligent decision support system that promotes sales activities of products and services for other industries too. These may improve the efficiency of sales representatives or create automatic decision-making processes for the clients themselves. 
Recommendation engines can also be deployed directly for consumers. For example, Credit Karma is a fintech startup from California that provides free access to credit scores and full credit history, making money from a personalised recommendation on credit cards, loans and other products to their users. Its recommendation system relies on millions of data about users’ credit history and current situations, to propose products that not only a user can be interested in, but also has a high probability of being approved for, and therefore has a long-term benefit. 
In conclusion, recommendation systems are increasingly integrating into all walks of human life and decision-making processes. This phenomenal is no different in other industries, especially consumer-facing companies, where information overload, rising client expectations and cost reduction are driving ever more instances of recommendation engines. Ultimately, it serves as both a tool to improve the client experience and maximise the efficiency of advisors. 
From the writer: I welcome your thoughts and reactions and look forward to following this exciting AI landscape together for the coming years. Feel free to give me claps for my blog you liked. It’s a source of great encouragement and energy to continue my blogging! 
Written by 
Written by",Roger Chua,2019-06-29T16:57:39.480Z
Word2Vec Tutorial — The Skip-Gram Model | by Chris McCormick | Nearist.ai | Medium,"This tutorial covers the skip gram neural network architecture for Word2Vec. My intention with this tutorial was to skip over the usual introductory and abstract insights about Word2Vec, and get into more of the details. Specifically here I’m diving into the skip gram neural network model. 
The skip-gram neural network model is actually surprisingly simple in its most basic form; I think it’s the all the little tweaks and enhancements that start to clutter the explanation. 
Let’s start with a high-level insight about where we’re going. Word2Vec uses a trick you may have seen elsewhere in machine learning. We’re going to train a simple neural network with a single hidden layer to perform a certain task, but then we’re not actually going to use that neural network for the task we trained it on! Instead, the goal is actually just to learn the weights of the hidden layer–we’ll see that these weights are actually the “word vectors” that we’re trying to learn. 
Another place you may have seen this trick is in unsupervised feature learning, where you train an auto-encoder to compress an input vector in the hidden layer, and decompress it back to the original in the output layer. After training it, you strip off the output layer (the decompression step) and just use the hidden layer — it’s a trick for learning good image features without having labeled training data. 
So now we need to talk about this “fake” task that we’re going to build the neural network to perform, and then we’ll come back later to how this indirectly gives us those word vectors that we are really after. 
We’re going to train the neural network to do the following. Given a specific word in the middle of a sentence (the input word), look at the words nearby and pick one at random. The network is going to tell us the probability for every word in our vocabulary of being the “nearby word” that we chose. 
When I say “nearby”, there is actually a “window size” parameter to the algorithm. A typical window size might be 5, meaning 5 words behind and 5 words ahead (10 in total). 
The output probabilities are going to relate to how likely it is find each vocabulary word nearby our input word. For example, if you gave the trained network the input word “Soviet”, the output probabilities are going to be much higher for words like “Union” and “Russia” than for unrelated words like “watermelon” and “kangaroo”. 
We’ll train the neural network to do this by feeding it word pairs found in our training documents. The below example shows some of the training samples (word pairs) we would take from the sentence “The quick brown fox jumps over the lazy dog.” I’ve used a small window size of 2 just for the example. The word highlighted in blue is the input word. 
The network is going to learn the statistics from the number of times each pairing shows up. So, for example, the network is probably going to get many more training samples of (“Soviet”, “Union”) than it is of (“Soviet”, “Sasquatch”). When the training is finished, if you give it the word “Soviet” as input, then it will output a much higher probability for “Union” or “Russia” than it will for “Sasquatch”. 
So how is this all represented? 
First of all, you know you can’t feed a word just as a text string to a neural network, so we need a way to represent the words to the network. To do this, we first build a vocabulary of words from our training documents–let’s say we have a vocabulary of 10,000 unique words. 
We’re going to represent an input word like “ants” as a one-hot vector. This vector will have 10,000 components (one for every word in our vocabulary) and we’ll place a “1” in the position corresponding to the word “ants”, and 0s in all of the other positions. 
The output of the network is a single vector (also with 10,000 components) containing, for every word in our vocabulary, the probability that a randomly selected nearby word is that vocabulary word. 
Here’s the architecture of our neural network. 
There is no activation function on the hidden layer neurons, but the output neurons use softmax. We’ll come back to this later. 
When training this network on word pairs, the input is a one-hot vector representing the input word and the training output is also a one-hot vectorrepresenting the output word. But when you evaluate the trained network on an input word, the output vector will actually be a probability distribution (i.e., a bunch of floating point values, not a one-hot vector). 
For our example, we’re going to say that we’re learning word vectors with 300 features. So the hidden layer is going to be represented by a weight matrix with 10,000 rows (one for every word in our vocabulary) and 300 columns (one for every hidden neuron). 
300 features is what Google used in their published model trained on the Google news dataset (you can download it from here). The number of features is a “hyper parameter” that you would just have to tune to your application (that is, try different values and see what yields the best results). 
If you look at the rows of this weight matrix, these are actually what will be our word vectors! 
So the end goal of all of this is really just to learn this hidden layer weight matrix — the output layer we’ll just toss when we’re done! 
Let’s get back, though, to working through the definition of this model that we’re going to train. 
Now, you might be asking yourself–“That one-hot vector is almost all zeros… what’s the effect of that?” If you multiply a 1 x 10,000 one-hot vector by a 10,000 x 300 matrix, it will effectively just select the matrix row corresponding to the “1”. Here’s a small example to give you a visual. 
This means that the hidden layer of this model is really just operating as a lookup table. The output of the hidden layer is just the “word vector” for the input word. 
The 1 x 300 word vector for “ants” then gets fed to the output layer. The output layer is a softmax regression classifier. There’s an in-depth tutorial on Softmax Regression here, but the gist of it is that each output neuron (one per word in our vocabulary!) will produce an output between 0 and 1, and the sum of all these output values will add up to 1. 
Specifically, each output neuron has a weight vector which it multiplies against the word vector from the hidden layer, then it applies the function exp(x) to the result. Finally, in order to get the outputs to sum up to 1, we divide this result by the sum of the results from all 10,000 output nodes. 
Here’s an illustration of calculating the output of the output neuron for the word “car”. 
Note that neural network does not know anything about the offset of the output word relative to the input word. It does not learn a different set of probabilities for the word before the input versus the word after. To understand the implication, let’s say that in our training corpus, every single occurrence of the word ‘York’ is preceded by the word ‘New’. That is, at least according to the training data, there is a 100% probability that ‘New’ will be in the vicinity of ‘York’. However, if we take the 10 words in the vicinity of ‘York’ and randomly pick one of them, the probability of it being ‘New’ is not100%; you may have picked one of the other words in the vicinity. 
Ok, are you ready for an exciting bit of insight into this network? 
If two different words have very similar “contexts” (that is, what words are likely to appear around them), then our model needs to output very similar results for these two words. And one way for the network to output similar context predictions for these two words is if the word vectors are similar. So, if two words have similar contexts, then our network is motivated to learn similar word vectors for these two words! Ta da! 
And what does it mean for two words to have similar contexts? I think you could expect that synonyms like “intelligent” and “smart” would have very similar contexts. Or that words that are related, like “engine” and “transmission”, would probably have similar contexts as well. 
This can also handle stemming for you — the network will likely learn similar word vectors for the words “ant” and “ants” because these should have similar contexts. 
You may have noticed that the skip-gram neural network contains a huge number of weights… For our example with 300 features and a vocab of 10,000 words, that’s 3M weights in the hidden layer and output layer each! Training this on a large dataset would be prohibitive, so the word2vec authors introduced a number of tweaks to make training feasible. These are covered in part 2 of this tutorial. 
I’ve also created a post with links to and descriptions of other word2vec tutorials, papers, and implementations. 
McCormick, C. (2016, April 19). Word2Vec Tutorial — The Skip-Gram Model. Retrieved from http://www.mccormickml.com 
Written by 
Written by",Chris McCormick,2018-07-05T20:04:37.589Z
Cyber-security Framework for Multi-Cloud Environment | by Taslet | Taslet Security | Medium,"Most of the conversations with the customers, three subjects are discussed prominently, 1️⃣ Digital transformation, 2️⃣ Vendor agnostic cloud, and 3️⃣ Cyber-security. Part of digital transformation is providing services faster at lower cost and these services can be accessed from any time, anywhere. This is the reason for many organizations “Cloud First” is becoming a new norm. Cloud hosting provides a multitude of benefits, but it’s also important to remain vigilant and realistic about what data is stored, where, and how it is protected. 
While making the decision to go for multiple clouds, organizations are considering few decision-making factors. These factors are including but not limited to: 
1️⃣ Meet technology requirements of specific workload or application 
2️⃣ Geographic benefits from the point of latency 
3️⃣ Availability of cloud hosting facilities in the region to comply data privacy requirements 
4️⃣ Mitigating vendor locking risks 
5️⃣ Greater flexibility and agility to adapt to high demand from business to getting products services faster to the market and still be competitive 
6️⃣ The Industry specific cloud hosting e.g. Public sector, government or healthcare cloud 
The trend “why” and “how” the organizations are adopting cloud is changed. Organizations have become accustomed to a multi-cloud or hybrid cloud environment. As per the RightScale 2018 State of the Cloud Report™, 81% of organizations are now utilizing multi-cloud environment. And on an average, these organizations are using 5 different public or private clouds. 
Before discussing security concerns of Multi-cloud, Hybrid- Cloud environment, let us understand when the term Multi-Cloud is used and When hybrid cloud. 
As per Radhesh Balakrishnan, general manager, OpenStack, Red Hat 
“Multi-cloud is one wherein you mix and match cloud services from different providers, often to meet specific workload needs, but no connection or orchestration between them”. 
For example, the Multi-Cloud environment can be the combination of IaaS (Infrastructure as a Service) form one CSP (Cloud Service Provider) and SaaS from another vendor (Software as a Service) environment. 
Hybrid cloud environment includes private and public cloud services. Most of the time journey to migrate workloads to cloud starts here. In this case, organizations can maintain high risk or business-critical applications hosted in the data center and migrate low-risk workloads to the cloud to get acquainted with the cloud. 
Though these environments sound different the security concerns of both these environments are similar, not having a single layer of orchestrating, and all the security and regulatory compliance requirements across all the cloud providers has resulted into many other concerns. 
Multi-cloud environments add to the existing cloud security challenges. Following are the challenges security professionals needs to take care of while designing the security for multi-cloud and or Hybrid cloud environment: 
While designing cloud security architecture one should consider an architecture that protects users, safeguards data, and better addresses regulatory compliance requirements. And also consider that all the components that consist of the Multi-cloud environment should have consistent security. The other cloud security considerations like encryption of data, secure communication links, managing the encryption keys and certificates are the must-have controls. 
1️⃣ Legal Involvement — Starting any cloud migration project, it is important to involve a legal team to validate the CSP contract(s). In case of the multi-cloud environment, the output of comparison between different CSP contracts can be used for making a decision which workload to be hosted in which environment. These contracts also provide the important inputs how the Incident will be managed and data breach in multi-tenancy environment will be handled. What, when, how the required data for incident management and forensic investigation will be shared. 
2️⃣ Visibility — One of the most important aspects is the ability to gain even visibility and insight to your own resources and resources of cloud service providers. The security professionals need a single dashboard that can give them insight into what resources are being used and how services are related to each other. 
To avoid the shadow IT problem, the business rules need to be created, these rules can define which users can provision the resources and what types of resources can be provisioned. Role Based Access Control (RBAC) becomes all the more important to provide the right level of access to right users to perform their duties and at the same time maintain need to know access provisioning principle. 
Getting visibility to the attacks those are not directed to your environment and how those will be handled is another important point that needs to be considered. e.g. DDoS attack or the virus infection to the shared infrastructure 
3️⃣ Data Availability — Nowadays most of the cloud service providers provide the portability so you can migrate workloads to another cloud service provider. However, it is very easy to move the virtual machines but when it comes to moving the data sets it is not that easy and hence you need to make sure that data back-up is automated and restoration of the data is tested so that in case of disaster if you need to switch over to other service provider data will be available and can be restored as per your Recovery Time Objective (RTO). 
4️⃣ Centralized Monitoring, Management, and Orchestration — Getting the single dashboard view of security posture is important. The security logs from all environments should be collected and processed at a single location so that uniform process will be applied to analyze and assign priorities to incidents. This will improve the efficiency of the sacred security professionals. The security tools and technologies deployed in a respective environment should be able to manage from the centralized management console this will help to manage the uniform rules and tool-specific policies across the environments. 
All elements of security including tools, technologies, policies, configurations should be administered and managed centrally. This will make sure though the different CSPs may have different vendors still, you will be able to have consistent security policies and security posture. 
5️⃣ Identity and Access Management –Security architect should not create separate identity silos for providing the cloud deployments. Each cloud deployment may have unique accounts within each of those providers and applications. To resolve this issue organizations should look for a centralized Identity and Access Management (IDAM) service that has capability manage all users’ access and authentication before they get access to any applications in the multi-cloud environment. This may include using your organizational directory for employees and utilizing federated identity solution (SAML based) for customers, other third-party users like partners. This will help an organization to easily and securely access the data and applications deployed in multiple CSP via single sign-on. 
The following picture depicts the high-level Multi-Cloud Cyber-security framework. This depicts the secure connectivity between the clouds, end users — may it be employees or third-party users from wherever they are assessing the applications, either form the enterprise environment or from anywhere. This can be accomplished by availing for Network as a Service (NaaS). 
Irrespective of where the workload/application is deployed, security policies will be maintained as per the policy definitions and this can be achieved by utilizing the CASB or other centralized policy management solution which can span across the multiple clouds and data centers/hosted facilities. This solution should have the capability to enforce, monitor and manage the policies uniformly. 
Logs from the different CSP environment involved in providing services should be gathered at a central location for centralized security monitoring, event analysis, and incident management. This will give the single view of the emerging threats across the organizations data assets. This Artificial Intelligence (AI) / Machine Learning (ML) based solution with capability of User and Entity Behavior Analysis (UEBA) will help to reduce the efforts and anomaly detection time which in turn will reduce the incident contentment time and cost of data breach. 
All the users who want to access the application will be going through the centralized identity access management solution either as IDaaS as a service or the IDAM solution that is capable of accommodating the federated identity and managing different types of users, provide single sign-on and integrate or have the multi-factor authentication feature. 
Having a centralized data backup solution will make sure that data from all the different environment will be available when it is most needed during the disaster. 
The centralized automation, analytics and monitoring solution that needs to be utilized for managing the security infrastructure across the different environment. This can be the tool used for IT infra orchestration which can encompass to the security tools and technologies. 
The encryption keys and the certificates should be managed from the central key vault or HSM solution. 
Last but not the list, the Security Operation Center (SOC) should be empowered with the automation tools that help to automate playbooks and workflows e.g. getting the workflow automated for change request approval. 
I would like to summarize this article by quoting Vivek Kundra, former federal CIO of United States 
“Cloud computing is often far more secure than traditional computing because companies like Google and Amazon can attract and retain cyber-security personnel of a higher quality than many governmental agencies.” 
This is not only applicable to the government agencies but looking at the cyber-security skill set shortage it is applicable to most of the organizations. 
Skills are one part of the security equation, we need to have tools to empower these skilled resources. The need of the time is to have security tools that are powered by Artificial Intelligence and Machine Learning (AI-ML), tools that can encompass across the cloud vendors, automate the repetitive processes and can execute remediation irrespective of which cloud service providers environment needs mitigation. Tools should also able to proactively detect anomalies across the cloud providers by performing user, entity and network analysis, correlate it to provide the risk score. This will help not only getting a centralized view of security risks and improve the security posture but also identify the security attacks at early stages of cyber kill chain to reduce time and cost of security incident management, faster recovery from the incidents. 
Written by 
Written by",Taslet,2018-09-18T13:21:21.929Z
DeepMind et al Paper Trumpets Graph Networks | by Synced | SyncedReview | Medium,"The paper Relational inductive biases, deep learning, and graph networks, published last week on arXiv by researchers from DeepMind, Google Brain, MIT and University of Edinburgh, has stimulated discussion in the artificial intelligence community. The paper introduces a new machine learning framework called Graph Networks, which some believe promises huge potential for approaching the holy grail of artificial general intelligence. 
Due to the development of big data and increasingly powerful computational resources over the past few years, modern AI technology — primarily deep learning — has show its prowess and even outsmarted humans in tasks such as image recognition and speech detection. However, AI remains challenged by tasks that involve complicated learning and reasoning with limited experience and knowledge, which is exactly what humans are good at. Although “a word to the wise is sufficient,” machines require much more. 
The paper argues that Graph Networks can effectively support two critical human-like capabilities: relational reasoning — ie drawing logical conclusions of how different objects and things relate to one another; and combinatorial generalization — ie constructing new inferences, predictions, and behaviors from known building blocks. 
Graph Networks can generalize and extend different types of neural networks that perform calculations on graphs, and implement relational inductive bias, a capacity for reasoning about inter-object relations. 
The GN framework is based on Graph Network blocks, also referred to as “graph-to-graph” modules. Each graph’s features are represented in three forms: nodes, edgesas relations, and global attributes as system-level properties. 
The Graph Network block will take a graph as an input, perform calculations from the edge, to the node, and to the global attributes, and then come up with a new graph as an output. 
The 38-page paper has been met favorably by many AI researchers, who praised the authors’ efforts. Founder of AI chip unicorn Graphcore Christopher Gray tweeted that “this paper…will kickstart what seems to be a far more fruitful basis for AI than DL alone.” Oriol Vinyals, a renowned research scientist at DeepMind, praised the paper as “a pretty comprehensive review.” 
Meanwhile, some questioned how well GNs will live up to the hype. As this is a review paper, it does not offer any convincing experiment results. Graph Networks are thus far an early-stage research theory that still requires more proof. 
The Graph Network concept was spawned with ideas not only from AI research, but also from computer and cognitive sciences. The paper emphasizes that “just as biology does not choose between nature versus nurture — it uses nature and nurture jointly, to build wholes which are greater than the sums of their parts — we, too, reject the notion that structure and flexibility are somehow at odds or incompatible, and embrace both with the aim of reaping their complementary strengths.” 
* * * 
Journalist: Tony Peng | Editor: Michael Sarazen 
* * * 
Follow us on Twitter @Synced_Global for more AI updates! 
* * * 
Subscribe here to get insightful tech news, reviews and analysis! 
* * * 
Synced and TalkingData will be jointly holding DTalk Episode One: Deploying AI in Mobile-First Customer-facing Financial Products: A Tale of Two Cycles. Jike Chong will share his ideas on employing AI techniques in FinTech business model. Scan the QR code to register! See you on June 21st in Silicon Valley. 
Written by 
Written by",Synced,2018-06-15T18:19:46.873Z
Introduction to Word Embedding and Word2Vec | by Dhruvil Karani | Towards Data Science,"Word embedding is one of the most popular representation of document vocabulary. It is capable of capturing context of a word in a document, semantic and syntactic similarity, relation with other words, etc. 
What are word embeddings exactly? Loosely speaking, they are vector representations of a particular word. Having said this, what follows is how do we generate them? More importantly, how do they capture the context? 
Word2Vec is one of the most popular technique to learn word embeddings using shallow neural network. It was developed by Tomas Mikolov in 2013 at Google. 
Let’s tackle this part by part. 
Why do we need them? 
Consider the following similar sentences: Have a good day and Have a great day. They hardly have different meaning. If we construct an exhaustive vocabulary (let’s call it V), it would have V = {Have, a, good, great, day}. 
Now, let us create a one-hot encoded vector for each of these words in V. Length of our one-hot encoded vector would be equal to the size of V (=5). We would have a vector of zeros except for the element at the index representing the corresponding word in the vocabulary. That particular element would be one. The encodings below would explain this better. 
Have = [1,0,0,0,0]`; a=[0,1,0,0,0]` ; good=[0,0,1,0,0]` ; great=[0,0,0,1,0]` ; day=[0,0,0,0,1]` (` represents transpose) 
If we try to visualize these encodings, we can think of a 5 dimensional space, where each word occupies one of the dimensions and has nothing to do with the rest (no projection along the other dimensions). This means ‘good’ and ‘great’ are as different as ‘day’ and ‘have’, which is not true. 
Our objective is to have words with similar context occupy close spatial positions. Mathematically, the cosine of the angle between such vectors should be close to 1, i.e. angle close to 0. 
Here comes the idea of generating distributed representations. Intuitively, we introduce some dependence of one word on the other words. The words in context of this word would get a greater share of this dependence. In one hot encoding representations, all the words are independent of each other, as mentioned earlier. 
How does Word2Vec work? 
Word2Vec is a method to construct such an embedding. It can be obtained using two methods (both involving Neural Networks): Skip Gram and Common Bag Of Words (CBOW) 
CBOW Model: This method takes the context of each word as the input and tries to predict the word corresponding to the context. Consider our example: Have a great day. 
Let the input to the Neural Network be the word, great. Notice that here we are trying to predict a target word (day) using a single context input word great. More specifically, we use the one hot encoding of the input word and measure the output error compared to one hot encoding of the target word (day). In the process of predicting the target word, we learn the vector representation of the target word. 
Let us look deeper into the actual architecture. 
The input or the context word is a one hot encoded vector of size V. The hidden layer contains N neurons and the output is again a V length vector with the elements being the softmax values. 
Let’s get the terms in the picture right:- Wvn is the weight matrix that maps the input x to the hidden layer (V*N dimensional matrix)-W`nv is the weight matrix that maps the hidden layer outputs to the final output layer (N*V dimensional matrix) 
I won’t get into the mathematics. We’ll just get an idea of what’s going on. 
The hidden layer neurons just copy the weighted sum of inputs to the next layer. There is no activation like sigmoid, tanh or ReLU. The only non-linearity is the softmax calculations in the output layer. 
But, the above model used a single context word to predict the target. We can use multiple context words to do the same. 
The above model takes C context words. When Wvn is used to calculate hidden layer inputs, we take an average over all these C context word inputs. 
So, we have seen how word representations are generated using the context words. But there’s one more way we can do the same. We can use the target word (whose representation we want to generate) to predict the context and in the process, we produce the representations. Another variant, called Skip Gram model does this. 
Skip-Gram model: 
This looks like multiple-context CBOW model just got flipped. To some extent that is true. 
We input the target word into the network. The model outputs C probability distributions. What does this mean? 
For each context position, we get C probability distributions of V probabilities, one for each word. 
In both the cases, the network uses back-propagation to learn. Detailed math can be found here 
Who wins? 
Both have their own advantages and disadvantages. According to Mikolov, Skip Gram works well with small amount of data and is found to represent rare words well. 
On the other hand, CBOW is faster and has better representations for more frequent words. 
What’s ahead? 
The above explanation is a very basic one. It just gives you a high-level idea of what word embeddings are and how Word2Vec works. 
There’s a lot more to it. For example, to make the algorithm computationally more efficient, tricks like Hierarchical Softmax and Skip-Gram Negative Sampling are used. All of it can be found here. 
Thanks for reading! I have started my personal blog and I don’t intend to write more amazing articles on Medium. Support my blog by subscribing to thenlp.space 
Hands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look 
Written by 
Written by",Dhruvil Karani,2020-09-02T07:44:46.961Z
Similarity and Distance Metrics for Data Science and Machine Learning | by Gonzalo Ferreiro Volpi | DataSeries | Medium,"In a previous article introducing Recommendation Systems, we mentioned several times the concept of ‘similarity measures’. Why? Because in Recommendation Systems, both Content-Based filtering and Collaborative filtering algorithms, use some specific similarity measure to find how equal two vectors of users or items are in between them. So in the end, a similarity measure is not more than the distance between vectors. 
Note: remember that all my work, including the specific repository with the application of all this content and more about Recommendation Systems, is available in my GitHub profile. 
In any kind of algorithm, the most common similarity measure is finding the cosine of the angle between vectors, i.e. cosine similarity. Suppose A is user’s A list of movies rated and B is user’s B list of movies rated, then the similarity between them can be calculated as: 
Mathematically, the cosine similarity measures the cosine of the angle between two vectors projected in a multi-dimensional space. When plotted on a multi-dimensional space, the cosine similarity captures the orientation (the angle) of each vector and not the magnitude. If you want the magnitude, compute the Euclidean distance instead. 
The cosine similarity is advantageous because even if the two similar documents are far apart by the Euclidean distance because of the size (like one word appearing a lot of times in a document or a user seeing a lot of times one movie) they could still have a smaller angle between them. Smaller the angle, the higher the similarity. 
Take the following example from www.machinelearningplus.com: 
The above image is counting the number of appearances of the word ‘sachin’, ‘dhoni’ and ‘cricket’ in the three documents shown. According to that, we could plot these three vectors to easily see the difference in between measuring the cosine and Euclidean distance for these documents: 
Now, Regular Cosine Similarity by the definition reflects differences in direction, but not the location. Therefore, using the cosine similarity metric does not consider for example the difference in ratings of users. Adjusted cosine similarity offsets this drawback by subtracting respective user’s average rating from each co-rated pair, and is defined as below: 
Let’s take the following example from Stackoverflow to better explain the difference between cosine and adjusted cosine similarity: 
Assume a user give scores in 0~5 to two movies. 
Intuitively we would say user b and c have similar tastes, and a is quite different from them. But the regular cosine similarity tells us a wrong story. IN cases like this, calculating the adjusted cosine similarity would give us a better understanding of the resemblance between users. 
By the way, in our previous article about Recommendation Systems, we presented the following function to find the adjusted cosine similarity: 
And you can use this function in a very easy way, just feeding: 
Take the following example as a reference: 
Finally, let’s briefly review some other methods that can be used to calculate the similarity for recommendation systems, but also for any other distance-based algorithm in Machine Learning: 
And then: 
Where |𝐼𝑢𝑣| is just the number of items rated by both users 𝑢 and 𝑣. 
Let’s briefly remember how Collaborative filtering works using an example from our previous introductory article about Recommendation Systems: suppose I like the following books: ‘The blind assassin’ and ‘A Gentleman in Moscow’. And my friend Matias likes ‘The blind assassin’ and ‘A Gentleman in Moscow’ as well, but also ‘Where the crawdads sing’. It seems that Matias and I have both the same interests. So you could probably affirm I would like ‘Where the crawdads sing’ too, even though I didn’t read it. And this is exactly the logic behind collaborative filtering, with the only exception that you can compare users in between them, as well as compare items. 
Let’s visualize the difference between computing use-user and item-item similarities for a recommendation system: 
User-user similarity 
Item-item similarity 
Now, understanding this, let’ illustrate some of the measures we presented taking the following examples from our friend from Analytics Vidhya, which I found particularly clear for both, user-user and item-item similarity: 
Image and example are taken from Analytics Vidhya 
Here we have a user movie rating matrix. To understand this in a more practical manner, let’s find the similarity between users (A, C) and (B, C) in the above table. Common movies rated by A and C are movies x2 and x4 and by B and C are movies x2, x4 and x5. Knowing this, let’s find the Pearson’s correlation or correlation similarity: 
The correlation between user A and C is more than the correlation between B and C. Hence users A and C have more similarity and the movies liked by user A will be recommended to user C and vice versa. 
Here the mean item rating is the average of all the ratings given to a particular item (compare it with the table we saw in user-user filtering). Instead of finding the user-user similarity, we find the item-item similarity. To do this, first we need to find such users who have rated those items and based on the ratings, the similarity between the items is calculated. Let us find the similarity between movies (x1, x4) and (x1, x5). Common users who have rated movies x1 and x4 are A and B while the users who have rated movies x1 and x5 are also A and B. 
The similarity between movie x1 and x4 is more than the similarity between movie x1 and x5. So based on these similarity values, if any user searches for movie x1, they will be recommended movie x4 and vice versa. 
Well, this is all for now about Recommendation Systems. However, remember that similarity measures and distance metrics appear throughout machine learning as a very fundamental concept. So I hope you’ve found this content useful not only to improve the performance of your Recommender ;) 
If you enjoyed this post, don’t forget to check out some of my last articles, like 10 tips to improve your plotting skills, 6 amateur mistakes I’ve made working with train-test splits or Web scraping in 5 minutes. All of them and more available in my Medium profile. 
Get in touch also by… 
See in you in the next post! 
Cheers. 
Written by 
Written by",Gonzalo Ferreiro Volpi,2019-12-04T10:25:48.112Z
How to Perform Fraud Detection with Personalized Page Rank | by Antoine Moreau | Sicara's blog | Medium,"Read the full article on Sicara’s blog here. 
This article shows how to perform fraud detection with Graph Analysis. Thanks to Personalized Page Rank algorithm and Networkx python package. 
Fraud detection is a major field of interest for data science. As fraud is a rare event, the main challenge is to find a way to bring to light abnormal behavior. That is why graph analysis is a useful approach to perform fraud detection. Many algorithms exist to extract information from graphs. In this article, we will study one of them: the Personalized Page Rank algorithm. To manipulate our graphs and compute this algorithm we will use the python package Networkx. 
Page Rank is a well-known algorithm developed by Larry Page and Sergey Brin in 1996. They were studying at Standford University and it was part of a research project to develop a new kind of search engine. They then successfully founded Google Inc. 
This algorithm assigns a numerical weighting to every node of a connected network. This measure represents the relative importance of a node within the graph (its rank). 
To compute Page Rank a random walk is performed. This random walk is defined as follow : 
The PageRank theory holds that the imaginary walker who is randomly walking on links will eventually stop. The probability, at any step, that the person will continue is called the damping factor α. 
As an example, for Google, the network is composed of websites that point to each other through links. The page rank measure of each web page is then the probability that a person randomly surfing on the internet would finally arrive on this specific page. 
In mathematical terms, The Page Rank of a node is the stationary measure of the Markov Chain described by the random walk. 
On the animation below you can visualize a Random Walk performed on a connected graph with a damping factor set to 0.85. 
On the above example, one would predict that the node ‘c’ is the one with the higher rank. This is the most central node. On the contrary, the node ‘h’ is more likely to have a low rank. 
The python package Networkx gives the possibility to perform graph analysis. A lot of algorithms are implemented in this package (community detection, clustering…), pagerank is one of them. 
With the python script below, thanks to Networkx, we will first generate a random graph and then apply pagerank function. 
We have seen that the Page Rank is a representation of the importance of nodes within a network. Personalized Page Rank gives the possibility to bring out nodes in a graph that are central from the perspective of a set of specific nodes. 
… 
Read the full article on Sicara’s blog here. 
Written by 
Written by",Antoine Moreau,2020-01-30T13:53:45.733Z
Interpreting Deep Learning Models for Computer Vision | by Dipanjan (DJ) Sarkar | Google Developers Experts | Medium,"Artificial Intelligence (AI) is no longer a field restricted only to research papers and academia. Businesses and organizations across diverse domains in the industry are building large-scale applications powered by AI. The questions to think about here would be, “Do we trust decisions made by AI models?” and “How does a machine learning or deep learning model make its decisions?”. Interpreting machine learning or deep learning models has always been a task often overlooked in the entire data science lifecycle since data scientists or machine learning engineers would be more involved with actually pushing things out to production or getting a model up and running. 
However, unless we are building a machine learning model for fun, accuracy is not the only thing which counts! Business stakeholders and consumers will often ask the tough questions of fairness, accountability and transparency of any machine learning model being used to solve real-world problems! 
There is a whole branch of research for explainable artificial intelligence (XAI) now! While the scope of this article is not to cover XAI, if you are interested you can always read my series on XAI. 
In this article, we will look at concepts, techniques and tools to interpret deep learning models used in computer vision, to be more specific — convolutional neural networks (CNNs). We will take a hands-on approach and implement our deep learning models using Keras and TensorFlow 2.0 and leverage open-source tools to interpret decisions made by these models! In short, the purpose of the article is to find out — what do deep learning models really see? 
The most popular deep learning models leveraged for computer vision problems are convolutional neural networks (CNNs)! 
CNNs typically consist of multiple convolution and pooling layers which help the deep learning model in automatically extracting relevant features from visual data like images. Due to this multi-layered architecture, CNNs learn a robust hierarchy of features, which are spatial, rotation, and translation invariant. 
The key operations in a CNN model are depicted in the figure above. Any image can be represented as a tensor of pixel values. The convolution layers help in extracting features from this image (forms feature maps). Shallower layers (closer to the input data) in the network learn very generic features like edges, corners and so on. Deeper layers in the network (closer to the output layer) learn very specific features pertaining to the input image. The following graphic helps summarize the key aspects of any CNN model. 
Since we are only concerned with understanding how CNN models perceive images, we won’t be training any CNN models from scratch here. Rather, we will be leveraging the power of transfer learning and pre-trained CNN models in our examples. 
A pre-trained model like VGG-16 has already been pre-trained on a huge dataset (ImageNet) with a lot of diverse image categories. Considering this fact, the model should have already learned a robust hierarchy of features. Hence, the model, having learned a good representation of features for over a million images belonging to 1,000 different categories, can act as a good feature extractor for new images suitable for computer vision problems. 
Here’s the interesting part, can we really unbox the opacity presented to us by a seemingly black-box CNN model and try and understand what’s really going on under the hood and what does the model really see when it looks at an image? There are a wide variety of techniques and tools for interpreting decisions made by vision-based deep learning models. Some of the major techniques covered in this article are depicted as follows. 
Let’s look at each of these techniques and interpret some deep learning CNN-based models built with Keras and TensorFlow. 
This technique tries to combine a multitude of ideas from Integrated Gradients, SHapley Additive exPlanations (SHAP) and SmoothGrad. This technique tries to explain model decisions using expected gradients (an extension of integrated gradients). This is a feature attribution method designed for differentiable models based on an extension of Shapley values to infinite player games. We will use the shap framework here for this technique. 
Integrated gradients values are a bit different from SHAP values, and require a single reference value to integrate from. However in SHAP Gradient Explainer, expected gradients reformulates the integral as an expectation and combines that expectation with sampling reference values from the background dataset. Thus this technique uses an entire dataset as the background distribution versus just a single reference value. Let’s try and implement this on some sample images. To get started, we load up some basic dependencies and model visualization function utilities. 
The next step is to load a pre-trained VGG-16 model, which was trained previously on the Imagenet dataset. We can do that easily with the following code. 
Once our CNN model is loaded, we will now load a small dataset of images which can be used as a background distribution and we will use four sample images for model interpretation. 
We have four different types of images including a picture of one of my cats! Let’s first look at our model’s prediction for each of these images. 
Let’s start by trying to visualize what the model sees in the 7th layer of the neural network (typically one of the shallower layers in the model). 
This gives us some good perspective into the top two predictions made by the model for each image and why did it take such decisions. Let’s take a look at one of the deeper layers in the VGG-16 model and visualize the 14th layer’s decisions. 
Now you can see the model gets stronger and more confident with the prediction decision based on the shap value intensities and also aspects like why the model predicts a screen vs. a desktop_computer where it also looks at the keyboard. Predicting my cat as a tabby because of specific features like the nose, whiskers, facial patterns and so on! 
For the remaining four techniques, we will leverage a pre-trained model using TensorFlow 2.0 and use the popular open-source framework tf-explain. The idea here is to look at different model intepretation techniques for CNNs. 
Let’s load one of the most complex pre-trained CNN models out there, the Xception model which claims to be slightly better than the Inception V3 model. Let’s start by loading the necessary dependencies and our pre-trained model. 
You can see from the model architecture snapshot above that this model has a total of 14 block with multiple layers in each block. Definitely one of the deeper CNN models! 
We will reuse the sample image of my cat and make the top-5 predictions with our Xception model. Let’s load our image first before making predictions. 
Let’s now making the top-5 predictions on this image using our Xception model. We will pre-process the image before inference. 
Interesting predictions, at least the top 3 here definitely are relevant! 
This technique is typically used to visualize how a given input comes out of specific activation layers. The key idea is to explore which feature maps are getting activated in the model and visualize them. Usually this is done by looking at each specific layer. The following code showcases activation layer visualizations for one of the layers in Block 2 of the CNN model. 
This kind of gives us an idea of which feature maps are getting activated and what parts of the image they typically focus on. 
The idea of interpretation using occlusion sensitivity is quite intuitive. We basically try to visualize how parts of the image affects our neural network model’s confidence by occluding (hiding) parts iteratively. This is done by systematically occluding different portions of the input image with a grey square, and monitoring the output of the classifier. 
Ideally specific patches of the image should be highlighted in red\yellow like a heatmap but for my cat image it kind of highlighted the overall image in a red hue, the reason for this could be because of the zoomed image of a cat. However the left side of the image has a higher intensity focusing more on the shape of the cat to an extent rather than the texture of the image. 
This is perhaps one of the most popular and effective methods for interpreting CNN models. Using GradCAM, we try and visualize how parts of the image affects neural network’s output by looking into the class activation maps (CAM). Class activation maps are a simple technique to get the discriminative image regions used by a CNN to identify a specific class in the image. In other words, a class activation map (CAM) lets us see which regions in the image were relevant to this class. 
Given an image and a class of interest (e.g., ‘tiger cat’ or any other type of differentiable output) as input, we forward propagate the image through the CNN part of the model and then through task-specific computations to obtain a raw score for the category. The gradients are set to zero for all classes except the desired class (tiger cat), which is set to 1. This signal is then backpropagated to the rectified convolutional feature maps of interest, which we combine to compute the coarse Grad-CAM localization (blue heatmap) which represents where the model has to look to make the particular decision. 
Let’s look at GradCAM visualizations for specific blocks in our CNN model. We start by visualizing one of the layers from block 1 (shallower layer). 
Like we expected, this being one of the shallow layers, we see higher level features like edges and corners being activated in the network. Let’s now visualize GradCAM outputs from one of the deeper layers in the network in Block 6. 
Things definitely start to get more interesting, we can clearly see that when the model predicts the cat as tabby, it is focusing on both the textures and also the overall shape and structure of the cat versus when it predicts the cat as an Egyptian_cat. Finally, let’s take a look at one of the deepest layers in the model from Block 14. 
Very interesting to observe that for the tabby cat label prediction, the model is also looking at the region surrounding the cat which is basically focusing on the overall shape \ structure of the cat and also some aspects of the cat’s facial structure! 
This technique helps us visualize stabilized gradients on the inputs towards the decision. The key objective is to identify pixels that strongly influence the final decision. A starting point for this strategy is the gradient of the class score function with respect to the input image. This gradient can be interpreted as a sensitivity map, and there are several techniques that elaborate on this basic idea. 
SmoothGrad is a simple method that can help visually sharpen gradient-based sensitivity maps. The core idea is to take an image of interest, sample similar images by adding noise to the image, then take the average of the resulting sensitivity maps for each sampled image. 
For the tabby cat focus is definitely on key points on the face including patches and stripes which are very distinguishing characteristics. 
This should give you a good idea of how you can not only leverage pre-trained complex CNN models to predict on new images but to even try and make an attempt to visualize what the neural network models are really seeing! The list of techniques here are not exhaustive but definitely cover some of the most popular and widely used methods to interpret CNN models. I recommend you to try these out with your own data and models! 
All the code used in this article is available on my GitHub in this repository as Jupyter notebooks. 
I am a Google Developer Expert in Machine Learning and I look forward to sharing more interesting aspects of machine learning and deep learning over time. Feel free to check out my Medium and LinkedIn for updates on interesting content! 
Written by 
Written by",Dipanjan (DJ) Sarkar,2019-08-15T11:45:46.936Z
The Google ‘vs’ Trick. How ego graphs can help you learn about… | by David Foster | Applied Data Science | Medium,"Have you ever found yourself Googling something followed by ‘ vs ’ to see the autocomplete suggestions that are a bit like the thing you wanted to find out about? 
Yeah, me too. 
Turns out it’s a thing. 
There are 3 reasons why this trick works really well if you want to understand a technology, product or concept better: 
This got me thinking — if you passed the autocompleted terms from a Google ‘vs’ search back into another ‘vs’ search and kept going, you’d end up with a pretty graph network of terms that are linked. 
Something a bit like this… 
This is really useful technique for creating a mental ‘map’ of technologies, products or ideas and how they relate to each other. 
Here’s how to build your own… 
There’s a URL you can use to return the autocomplete suggestions in XML format. It doesn’t look very official so probably not a good idea to spam it with tons of requests. 
Here, output=toolbar returns the response as XML, gl=us gives the country, hl=en provides the language and q=<search_term> is what you want to autocomplete. 
Standard two letter country codes and language codes can be used as values for gl and hl respectively. 
So let’s pick a starting term — say, tensorflow . 
The first step is to hit the URL with q=tensorflow%20vs%20 . 
Now we need to apply a series of criteria against each autocomplete suggestion in turn, to judge if it is kept or is discarded: 
The criteria I use are: 
The term shouldn’t contain the search term (i.e. tensorflow) 
The term shouldn’t contain any previously accepted terms (e.g. pytorch) 
The term shouldn’t contain multiple ‘vs’ 
Once you’ve found 5 suitable terms, discard the rest 
This is just one way of ‘cleaning’ the returned list of suggestions. I also sometimes find it useful to only include terms with exactly one word, but it depends on the use case. 
So using this set of criteria we obtain the following five weighted connections: 
We then feed these five terms into the autosuggestion API followed by ‘ vs ’ and again store the top 5 filtered connections. 
And so on and so on — expanding the words in the target column that haven’t yet been explored. 
Do this enough times and you end up with a big table of weighted edges — perfect for visualising in a graph… 
The graph network at the top of this article is what’s known as an ego graph for tensorflow. An ego graph is the graph of all nodes that are less than a certain distance from the tensorflow node. 
But wait — how do we define distance? 
Let’s first see the graph again: 
The weight of the edge from term A and term B we already know — it’s the ranking from 1 to 5 from the autosuggestion list. To make the graph undirected, we can just sum the weights in both directions (i.e. A➡B and B➡A, if it exists), to get an edge weight between 1 and 10. 
The distance of each edge is then simply 11-weight. We choose 11 because the maximum weight of an edge is 10 (if both terms appear at the top of each others autosuggestion list), so this definition gives a minimum distance of 1 between terms. 
The size and colour of each node is determined by the count of edges where it is the target (that is, the number of times it appears as the autosuggestion). So, the larger the node, the more important the concept. 
The ego graph above has a radius of 22. That means that you can reach each of the terms within a distance of 22, starting from the tensorflow node. Let’s see what happens if we increase the radius of the graph to 50: 
Pretty cool — this graph contains most of the established technologies that an AI engineer would need to know about, clustered in a logical way. 
All from one keyword. 
I used a neat online tool called Flourish. 
It lets you build network graphs and other charts really easily in a simple-to-use GUI — definitely worth checking out. 
You use the networkxPython package, which has a handy function called ego_graph . You specify the radius as as input parameter to the function. 
I also use a second function — k_edge_subgraphs to remove some of the terms that go off on a tangent. 
For example, storm is an open source, distributed realtime computation system. storm is also a character in the Marvel universe — if you type ‘storm vs ’ into Google, guess which one dominates? 
The k_edge_subgraphs function finds groups of nodes that cannot be separated by less than or equal to k cuts — k=2 or k=3tends to work well. Only keeping the subgraph thattensorflow belongs to ensures that we stay close to home and don’t literally wander off into the realms of fantasy. 
Let’s move on from the Tensorflow example and take a look at another ego-chart — this time, for my other love … the Ruy Lopez chess opening. 
It’s pretty neat that this trick can very quickly get you a graph of most of the most common opening ideas, for you to organise you research. 
OK, now for some fun stuff. 
Mmmm kale. Everyone loves kale. 
But what if you fancy a change from delicious, delicious kale? Kale ego graph to the rescue! 
So many dogs, so little time. Need one. But which one? 
Dog / kale didn’t do the trick? Need a partner instead? There’s a small, very self-contained ego graph for that. 
Watch a series with a tub of kale (or newly discovered rucola) flavoured ice cream instead. If you like The Office (UK obviously), you might also like… 
That’s the end of my adventures into the world of ego graphs and the Google ‘vs’ trick for now. 
I hope it helps you in some small way to find love, a labrador and lettuce. 
If you’ve enjoyed this post, please do leave some claps 👏👏👏👏 — I’d really appreciate it :) 
Also, if you have any ideas for how this could be extended, I’d love to hear about it — leave your thoughts in the comments below! 
Applied Data Science Partners is a London based consultancy that implements end-to-end data science solutions for businesses, delivering measurable value. If you’re looking to do more with your data, please get in touch via our website. Follow us on LinkedIn for more AI and data science stories! 
Written by 
Written by",David Foster,2020-06-22T15:30:56.295Z
Cyber SC – Medium,"Cyber Security Advice (www.cyber.sc) 
WHAT IS CLOUD STORAGE? 
At it’s root, cloud storage is online data storage supplied by a third-party provider. Cloud storage lets you store your files, pictures, videos etc. on the server of your provider of choice. The… 
THE BENEFITSThe open source software model provides organizations with great benefits: it can be less expensive and provide more flexibility than a typical commercial off-the shelf application. From an economical point of view, there is a cost benefit: for cash-strapped… 
The typical Chief Information Security Officer has a unique set of skills and experiences that allow them to adapt to almost any corporate culture. In order to get the most out of your relationship with your CISO, it will be helpful for you to… 
The cyber security talent pool is becoming increasingly shallow. Not only is it difficult to attract quality cyber security professionals, it is just as hard to keep them. With cyber attacks and regulatory requirements on the rise, we are entering… 
Every organization needs to understand the importance of building their internal and external cyber security teams, functionality and capabilities. We often see that the cyber security function is performed by someone in the IT department under the direction of a…",NA,NA
Cybersecurity – InfoSec Write-ups – Medium,"Hashcat is the world’s fastest and most advanced password recovery utility, supporting five unique modes of… 
Let’s see how to harden your WordPress site",NA,NA
Latent Dirichlet Allocation(LDA): A guide to probabilistic modelling approach for topic discovery | by Awan-Ur-Rahman | Towards Data Science,"Latent Dirichlet Allocation(LDA) is one of the most common algorithms in topic modelling. LDA was proposed by J. K. Pritchard, M. Stephens and P. Donnelly in 2000 and rediscovered by David M. Blei, Andrew Y. Ng and Michael I. Jordan in 2003. In this article, I will try to give you an idea of what topic modelling is. We will learn how LDA works and finally, we will try to implement our LDA model. 
Topic Modelling is one of the most interesting fields in Machine Learning and Natural Language Processing. Topic Modelling means the extraction of abstract “topics” from the collection of documents. One of the primary application of natural language processing is to know what people are talking about in a large number of text documents. And it is really hard to read through all of these documents and extract or compile topics. In these cases, topic modelling is used to extract documents information. To understand the concept of topic modelling let’s see an example. 
Suppose, you are reading some articles on a newspaper and in those articles, the word “climate” appears most than any other words. So, in a normal sense, you can say that these articles will more probably about something related to climate. Topic modelling does the same thing in a statistical way. It produces topics by clustering similar words. Here come two terms: one is “Topic Modelling” and the other is “Topic Classification”. Though they look similar, they are totally different processes. The first is an unsupervised machine learning technique and the second one is the supervised technique.Let’s elaborate on the concept. 
Topic classification often involves mutually-exclusive classes. That means each document is labelled with a specific class. On the other hand, Topic modelling is not mutually exclusive. The same document may involve with many topics. As Topic modelling works on the basis of the probability distribution, the same document may have a probability-distribution spread across many topics. 
For topic modelling, there are several existing algorithms that you can use. Non-Negative Matrix Factorization(NMF), Latent Semantic Analysis or Latent Semantic Indexing(LSA or LSI) and Latent Dirichlet Allocation(LDA) are some of these algorithms. Here in this article, we will talk about Latent Dirichlet Allocation, one of the most common algorithms for topic modelling. 
“ The latent Dirichlet allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. For example, if observations are words collected into documents, it posits that each document is a mixture of a small number of topics and that each word’s presence is attributable to one of the document’s topics.” — Wikipedia 
Okay, Let’s try to understand this definition. 
The basic idea of Latent Dirichlet allocation (LDA) is that documents are considered as random mixtures of various topics and topics are considered a mixture of different words. Now, suppose you need some articles which are related to animals and you have thousands of articles in front of you and you really don’t know what these articles are about. Reading all these articles are really cumbersome to find out the articles related to animals. Let’s see an example of it. 
As an example, let’s consider we have four articles. Article no. 1 related to animal, article no. 2 related to genetic type, article no. 3 related to computer types and article no. 4 is a combination of animal and genetic type. As a human, you can easily differentiate these topics according to the words it contains. But what will you do if there are thousands of articles and each article has thousands of lines? The answer will be like -“If we can do this with the help of a computer then we should do so”. Yes, Computer can do so with the help of Latent Dirichlet allocation. Now we will try to understand how LDA works. First, we will see the graphical representation of LDA and then we will see the probability calculation formula. 
The above figure is a graphical representation of LDA. In the above figure, we can see that there are six parameters- 
α(alpha) and η(eta) — represents Dirichlet distribution. The high alpha value indicates that each document contains most of the topics and on the contrary, a lower alpha value indicates that the documents are likely to contain a fewer number of topic. Same as alpha, a Higher value of η indicates that the topics are likely to cover most of the words and on the contrary, lower eta value indicates that the topics are likely to contain a fewer number of words. 
β(beta) and θ(theta) — represents multinomial distribution. 
z — represents a bunch of topics 
w — represents a bunch of words 
The left side of the formula indicates the probability of the document. In the right of the formula, there are four terms. The 1st and 3rd term of the formula will help us to find topics. The 2nd and 4th will help us to find the words in articles. The first two terms of the right side of the formula indicate Dirichlet distribution and the rest portion of the right side is multinomial distribution. 
Let’s assume, in the above figure, in the left triangle, the blue circles indicate different articles. Now if we distribute the articles over different topics, it will be distributed as shown in the right triangle. The blue circles will move to the corners of the triangle which depends on the percentage of its being that topic. This process is done by the first term of the right side of the formula. Now we use multinomial distribution to generate topics based on the percentage get from the first term. 
Now after getting the topics we will find which words are more relatable to these topics. This is done by another Dirichlet distribution. Topics are distributed based on the words as shown below. 
Now we will use another multinomial distribution to find the words which are more related to those topics and generate words with probability using that Dirichlet distribution. This process is done multiple times. 
Thus we will find the words which are more relatable to the topics and can distribute the articles based on those topics. 
You can find the code in GitHub. For implementing LDA you can use either gensim or sklearn. Here, we will use gensim. 
For implementing purpose, I have used the Kaggle dataset. This dataset consists of 2150 datasets information in 15 columns: 
For processing the data, first, we select the columns that are meaningful for this process. Then remove the rows containing any missing values. 
We will then calculate the number of unique tag in Tags columns as we will consider this as the number of topics for our model. 
Removing punctuations and transforming the whole text in the lower casing makes the training task easier and increase the efficiency of the model. 
We need to tokenize our dataset and perform stemming operation. 
By using WordCloud, we can verify whether our preprocessing is correctly done or not. A word cloud is an image made of words that together resemble a cloudy shape. It shows us how often a word appeared in a text — its frequency. 
For the LDA model, we first need to build a dictionary of words where each word is given a unique id. Then need to create a corpus which contains word id mapping with word_frequency — ->(word_id, word_frequency). 
Finally, train the model. 
Coherence measures the relative distance between words within a topic. 
Coherence value: 0.4 
The topic at the top got the highest probability and it is related to something like economic. 
You can find all the code on GITHUB. 
Hands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look 
Written by 
Written by",Awan-Ur-Rahman,2020-04-13T03:01:19.189Z
Genetic Algorithm: Part 1 -Intuition | by Satvik Tiwari | Koderunners | Medium,"Suppose, we are solving a regression problem in which we have to fit a line across a set of data points having a convex error function. For such problems techniques like Normal Equation and Gradient Descent can easily be used. But what if our function is non-convex? 
In the above figure, if we use the Gradient Descent then we might only be limited to a certain search space as we will be stuck to a local optima. We can randomly go to a particular search space and exploit the information available to reach peak but it may or may not be global maxima. To reach the global maxima we need to explore all the search spaces. 
In such cases, Evolutionary Algorithms (EAs) come in handy. 
Genetic Algorithm (GA) given by John Holland in 1970 is one of the most popular EAs. It is based on Darwin’s theory of “Survival of the Fittest”. It is basically used to optimize our problems. In GA we create random changes to the current solutions through Selection, Crossover and Mutation to create new solutions until we reach the best solution. 
It starts with defining an initial population which contains a certain number of solutions. Each solution is called an individual. Each individual solution is encoded as a chromosome which in turn is represented by a set of genes. There are various ways of chromosome encoding which we will discuss later. The figure below gives an idea of how one generation of population looks like. 
After chromosome encoding, fitness for every solution is calculated. Higher the fitness, the better is a solution. Fitness is given with the help of a fitness function which is problem specific. 
Now, we select the best individuals from this newly created population as parents for the new generation. The higher the fitness value of a solution the more chances of it are there to be selected as a parent rendering bad solutions to be left out. These selected individuals go through crossover to create new offspring. By mating the high quality individuals, we can expect to get offsprings better than their parents. By the way, do you know any other algorithm which uses sex so elegantly??? Comment and let us know. 
After mating, all the offsprings that we get will contain the same bad characteristics as their parents. To overcome that problem they go through mutation by applying small random changes to their genes thus, creating a diversity in population. These new individuals become the new population or generation. Their fitness is calculated and they go through selection, crossover and mutation. This process goes on and on until we reach the best solution or we complete certain number of generations. 
The diagram below gives the whole workflow of GA. 
The chromosomes are encoded in mainly 3 ways: 
We select best individuals from previous population for crossover. It is done in many ways. The most common of them are: 
We combine the genetic material of the selected parent chromosome to produce offspring. Its not necessary that a selected pair of chromosomes will undergo crossover. We define a probabilistic factor called crossover rate which acts as a threshold. This factor decides whether a pair of chromosomes will undergo crossover or not. There are various ways in which we can perform crossover. For example : 
Uniform Crossover -In this two individuals create one offspring. We randomly take an individual from the two for every gene of offspring to contribute for the same. 
One point Crossover - In this two individuals create two offsprings. We decide a common point for both individuals and their offsprings about which for the first offspring we take all left genes of first parent and all right genes of second parent and vice-versa for second offspring. The figure below gives the idea of one point crossover. 
The problem about which we discussed in the beginning gets solved here. Because of mutation, we don’t get stuck to a local optima as it introduces genetic diversity in the population. In mutation, we have a mutation rate similar to crossover rate which governs whether an individual will undergo mutation or not. If an individual is selected for mutation then we randomly/uniformly change one/more(as required) genes. For example if our chromosome is binary encoded then we flip the gene value(if it is 0 then we change it to 1 or vice-versa). 
Thank you for reading this. Stay tuned for more Machine Learning stuff :) 
Written by 
Written by",Satvik Tiwari,2019-04-28T10:20:38.176Z
What is this Microsoft Intelligent Security Graph everybody is talking about? | by Maarten Goet | Medium,"Ever since Microsoft announced the Intelligent Security Graph earlier this year, as a preview at the //Build/ conference, later as being generally available at #MSIgnite, there has been a lot of talk about it. 
Microsoft describes it as a way to ‘build solutions that correlate alerts, get context for investigation, and automate security operations in a unified manner.” 
But what is the Intelligent Security Graph exactly? And how do I use it for my own security operations? And didn’t Microsoft already have a Graph, for Office? Does it work with my other security solutions? 
What is a Graph? 
The dictionary defines a graph as: “a diagram representing a system of connections or interrelations among two or more things by a number of distinctive dots, lines, bars, etc.”. In the context of security, John Lambert describes it in better details as: 
The graph in your network is the set of security dependencies that create equivalence classes among your assets. 
“The design of your network, the management of your network, the software and services used on your network, and the behavior of users on your network all influence this graph. Take a domain controller for example. Bob admins the DC from a workstation. If that workstation is not protected as much as the domain controller, the DC can be compromised. 
Any other account that is an admin on Bob’s workstation can compromise Bob and the DC. Every one of those admins logs on to one or more other machines in the natural course of business. If attackers compromise any of them, they have a path to compromise the DC.” 
A great example of a Graph that unveils what the ‘shortest path to Domain Admins’ is in Active Directory, is project Bloodhound. A free open-source project created by the specialists of SpecterOps. 
Project Oslo 
Almost a decade ago Microsoft started working on what was codenamed Project “Oslo”. The core focus was to deliver on a social and collaborative working application for the Office products to transform the way people work. To power “Oslo”, Microsoft was developing API’s for Office that would expose the required data programmatically. In early 2014, at its SharePoint conference, Microsoft announced “Oslo” as Office Delve, and the API’s as Office Graph. 
The Office Graph has been extensively used by Office 365 and other Microsoft properties, but has also built a large developer community. Many companies are using the API’s nowadays as the primary integration point for their app development. 
Why does this matter to me? 
John Lambert clearly describes the need for a graph-based defender mindset: 
“A lot of network defense goes wrong before any contact with an adversary, starting with how defenders conceive of the battlefield. Most defenders focus on protecting their assets, prioritizing them, and sorting them by workload and business function. Defenders are awash in lists of assets — in system management services, in asset inventory databases, in BCDR spreadsheets. 
There’s one problem with all of this. Defenders don’t have a list of assets — they have a graph. Assets are connected to each other by security relationships. Attackers breach a network by landing somewhere in the graph using a technique such as spear phishing and they hack, finding vulnerable systems by navigating the graph.” 
In September of 2018 Microsoft organized its annual premium security conference called Bluehat. There was a full track of Graph talks and workshops. The slides can be found here: https://github.com/JohnLaTwC/Bluehat2018GraphWorkshop 
“Defenders think in lists. Attackers think in graphs. As long as this is true, attackers win.” 
Security Graph API 
Early 2018, during Microsoft’s developer conference //Build/, program manager Sarah Fender announced a preview of what Microsoft would be calling the Intelligent Security Graph. 
This is how Microsoft describes the Intelligent Security Graph: “the Graph Security API can be defined as an intermediary service (or broker) that provides a single programmatic interface to connect multiple security providers. Requests to the graph are federated to all applicable providers. The results are aggregated and returned to the requesting application in a common schema.” 
This new security-focused API will live alongside the Office Graph. 
Brad Anderson, corporate vice president at Microsoft, responsible for their enterprise mobility offerings, recorded a brief video about the Intelligent Security Graph, giving you an overview of the what and the why. 
Later, during Microsoft’s IT Pro focused conference Ignite, the team announced that the Intelligent Security Graph was generally available, and that you could easily access alerts from the following security solutions: 
The API now also allows you to update the alerts, they can be tagged with additional context or threat intelligence to inform response and remediation, comments and feedback can be captured for visibility to other workflows, and alert status and assignments can be kept in sync. 
Integration with Office 365 ATP and Azure ATP is coming soon. 
In the June 2018 update of the Microsoft Cybersecurity Reference Architecture, the Intelligent Security Graph was also included. 
Windows 10 & Security Graph work in tandem 
Since the Windows 10 “1709 release” Microsoft introduced a new feature to the newly-renamed Windows Defender Application Control (WDAC): the ability to allow any applications to run that have obtained positive application reputation in Microsoft’s Intelligent Security Graph (MISG) cloud service. WDAC now comprises most, but not all, of the functionality that used to fall under the label “Device Guard” pre-1709. 
WDAC, when integrated with MISG, could hold the potential to make adoption of application whitelisting much less painful for organizations and individuals by allowing commonly used Windows programs from reputable publishers to run without it being necessary to have specific, per-application or per-publisher rules for them specifically set out in a whitelisting policy. 
Here are the steps to configure the WDAC + MISG integration: 
There is a guide on GitHub that you can use to get further instructions, background on the process and some key pointers for troubleshooting. 
Tanmay Ganachary, General Manager at Microsoft for Windows Defender Security Research, also points to an article on the Microsoft Docs website which has further information on the integration. 
PowerShell 
Because the Security Graph API allows for making HTTPS REST API requests, it’s easy to work with the API with PowerShell. Microsoft published a sample at this GitHub repository. 
To enable Azure PowerShell to query Azure Resource Graph, the module must be added. This module can be used with locally installed Windows PowerShell and PowerShell Core, or with the Azure PowerShell Docker image. 
First step, install the module and authenticate: 
Next step, query the API: 
You can also work with the Graph API directly by using the graph.microsoft.com endpoint. Note, there is a Beta endpoint that surfaces even more information about your environment: 
PRO TIP: Use the new Az module for Azure PowerShell. This new module is written from the ground up in .NET Standard. Using .NET Standard allows Azure PowerShell to run under PowerShell 5.x on Windows or PowerShell 6 on any platform, for instance Linux. The Az module is now the intended way to interact with Azure through PowerShell. AzureRM will continue to get bug fixes, but no longer receive new features. 
READING TIP: My good friend Ronny de Jong, an Enterprise Mobility MVP from The Netherlands, has written a blog about another example: keeping your Microsoft Intune tenant clean and tidy using #Azure Automation and the Security Graph API. 
Matt Graeber also posted a link to the documentation of the Schema, and how to use it for Bulk IOC updating. 
Sample web UI 
Now as an added bonus, Microsoft has open sourced a web UI which interacts with all the data from Security Graph API and shows it in a neat UI. You can find it on GitHub: https://github.com/microsoftgraph/aspnet-security-api-sample 
Third Parties 
Many security companies have begun integrating their solutions with the Microsoft Intelligent Security Graph: 
Hackathon 
Last but not least, Microsoft is currently hosting a Security Graph API hackathon until March 1st 2019. Build something cool with the API and win prizes up to $15K and a chance to speak at //Build/ 2019! More info here: https://graphsecurityhack.devpost.com. 
Summary 
Microsoft’s Intelligent Security Graph is aggregating all their security properties for ‘signal sharing’ to build a bigger context around the events happening in your environment. 
Together with their machine learning backend, Microsoft is upping their game over other threat detection vendors to detect and protect you against malicious intent. 
— Maarten Goet, MVP & RD 
Written by 
Written by",Maarten Goet,2019-01-07T07:07:35.400Z
A Practitioner's Guide to Natural Language Processing (Part I) — Processing & Understanding Text | by Dipanjan (DJ) Sarkar | Towards Data Science,"Unstructured data, especially text, images and videos contain a wealth of information. However, due to the inherent complexity in processing and analyzing this data, people often refrain from spending extra time and effort in venturing out from structured datasets to analyze these unstructured sources of data, which can be a potential gold mine. 
Natural Language Processing (NLP) is all about leveraging tools, techniques and algorithms to process and understand natural language-based data, which is usually unstructured like text, speech and so on. In this series of articles, we will be looking at tried and tested strategies, techniques and workflows which can be leveraged by practitioners and data scientists to extract useful insights from text data. We will also cover some useful and interesting use-cases for NLP. This article will be all about processing and understanding text data with tutorials and hands-on examples. 
The nature of this series will be a mix of theoretical concepts but with a focus on hands-on techniques and strategies covering a wide variety of NLP problems. Some of the major areas that we will be covering in this series of articles include the following. 
Feel free to suggest more ideas as this series progresses, and I will be glad to cover something I might have missed out on. A lot of these articles will showcase tips and strategies which have worked well in real-world scenarios. 
This article will be covering the following aspects of NLP in detail with hands-on examples. 
This should give you a good idea of how to get started with analyzing syntax and semantics in text corpora. 
Formally, NLP is a specialized field of computer science and artificial intelligence with roots in computational linguistics. It is primarily concerned with designing and building applications and systems that enable interaction between machines and natural languages that have been evolved for use by humans. Hence, often it is perceived as a niche area to work on. And people usually tend to focus more on machine learning or statistical learning. 
When I started delving into the world of data science, even I was overwhelmed by the challenges in analyzing and modeling on text data. However, after working as a Data Scientist on several challenging problems around NLP over the years, I’ve noticed certain interesting aspects, including techniques, strategies and workflows which can be leveraged to solve a wide variety of problems. I have covered several topics around NLP in my books “Text Analytics with Python” (I’m writing a revised version of this soon) and “Practical Machine Learning with Python”. 
However, based on all the excellent feedback I’ve received from all my readers (yes all you amazing people out there!), the main objective and motivation in creating this series of articles is to share my learnings with more people, who can’t always find time to sit and read through a book and can even refer to these articles on the go! Thus, there is no pre-requisite to buy any of these books to learn NLP. 
When building the content and examples for this article, I was thinking if I should focus on a toy dataset to explain things better, or focus on an existing dataset from one of the main sources for data science datasets. Then I thought, why not build an end-to-end tutorial, where we scrape the web to get some text data and showcase examples based on that! 
The source data which we will be working on will be news articles, which we have retrieved from inshorts, a website that gives us short, 60-word news articles on a wide variety of topics, and they even have an app for it! 
In this article, we will be working with text data from news articles on technology, sports and world news. I will be covering some basics on how to scrape and retrieve these news articles from their website in the next section. 
I am assuming you are aware of the CRISP-DM model, which is typically an industry standard for executing any data science project. Typically, any NLP-based problem can be solved by a methodical workflow that has a sequence of steps. The major steps are depicted in the following figure. 
We usually start with a corpus of text documents and follow standard processes of text wrangling and pre-processing, parsing and basic exploratory data analysis. Based on the initial insights, we usually represent the text using relevant feature engineering techniques. Depending on the problem at hand, we either focus on building predictive supervised models or unsupervised models, which usually focus more on pattern mining and grouping. Finally, we evaluate the model and the overall success criteria with relevant stakeholders or customers, and deploy the final model for future usage. 
We will be scraping inshorts, the website, by leveraging python to retrieve news articles. We will be focusing on articles on technology, sports and world affairs. We will retrieve one page’s worth of articles for each category. A typical news category landing page is depicted in the following figure, which also highlights the HTML section for the textual content of each article. 
Thus, we can see the specific HTML tags which contain the textual content of each news article in the landing page mentioned above. We will be using this information to extract news articles by leveraging the BeautifulSoup and requests libraries. Let’s first load up the following dependencies. 
We will now build a function which will leverage requests to access and get the HTML content from the landing pages of each of the three news categories. Then, we will use BeautifulSoup to parse and extract the news headline and article textual content for all the news articles in each category. We find the content by accessing the specific HTML tags and classes, where they are present (a sample of which I depicted in the previous figure). 
It is pretty clear that we extract the news headline, article text and category and build out a data frame, where each row corresponds to a specific news article. We will now invoke this function and build our dataset. 
We, now, have a neatly formatted dataset of news articles and you can quickly check the total number of news articles with the following code. 
There are usually multiple steps involved in cleaning and pre-processing textual data. I have covered text pre-processing in detail in Chapter 3 of ‘Text Analytics with Python’ (code is open-sourced). However, in this section, I will highlight some of the most important steps which are used heavily in Natural Language Processing (NLP) pipelines and I frequently use them in my NLP projects. We will be leveraging a fair bit of nltk and spacy, both state-of-the-art libraries in NLP. Typically a pip install <library> or a conda install <library> should suffice. However, in case you face issues with loading up spacy’s language models, feel free to follow the steps highlighted below to resolve this issue (I had faced this issue in one of my systems). 
Let’s now load up the necessary dependencies for text pre-processing. We will remove negation words from stop words, since we would want to keep them as they might be useful, especially during sentiment analysis. 
❗ IMPORTANT NOTE: A lot of you have messaged me about not being able to load the contractions module. It’s not a standard python module. We leverage a standard set of contractions available in the contractions.py file in my repository.Please add it in the same directory you run your code from, else it will not work. 
Often, unstructured text contains a lot of noise, especially if you use techniques like web or screen scraping. HTML tags are typically one of these components which don’t add much value towards understanding and analyzing text. 
It is quite evident from the above output that we can remove unnecessary HTML tags and retain the useful textual information from any document. 
Usually in any text corpus, you might be dealing with accented characters/letters, especially if you only want to analyze the English language. Hence, we need to make sure that these characters are converted and standardized into ASCII characters. A simple example — converting é to e. 
The preceding function shows us how we can easily convert accented characters to normal English characters, which helps standardize the words in our corpus. 
Contractions are shortened version of words or syllables. They often exist in either written or spoken forms in the English language. These shortened versions or contractions of words are created by removing specific letters and sounds. In case of English contractions, they are often created by removing one of the vowels from the word. Examples would be, do not to don’t and I would to I’d. Converting each contraction to its expanded, original form helps with text standardization. 
We leverage a standard set of contractions available in the contractions.py file in my repository. 
We can see how our function helps expand the contractions from the preceding output. Are there better ways of doing this? Definitely! If we have enough examples, we can even train a deep learning model for better performance. 
Special characters and symbols are usually non-alphanumeric characters or even occasionally numeric characters (depending on the problem), which add to the extra noise in unstructured text. Usually, simple regular expressions (regexes) can be used to remove them. 
I’ve kept removing digits as optional, because often we might need to keep them in the pre-processed text. 
To understand stemming, you need to gain some perspective on what word stems represent. Word stems are also known as the base form of a word, and we can create new words by attaching affixes to them in a process known as inflection. Consider the word JUMP. You can add affixes to it and form new words like JUMPS, JUMPED, and JUMPING. In this case, the base word JUMP is the word stem. 
The figure shows how the word stem is present in all its inflections, since it forms the base on which each inflection is built upon using affixes. The reverse process of obtaining the base form of a word from its inflected form is known as stemming. Stemming helps us in standardizing words to their base or root stem, irrespective of their inflections, which helps many applications like classifying or clustering text, and even in information retrieval. Let’s see the popular Porter stemmer in action now! 
The Porter stemmer is based on the algorithm developed by its inventor, Dr. Martin Porter. Originally, the algorithm is said to have had a total of five different phases for reduction of inflections to their stems, where each phase has its own set of rules. 
Do note that usually stemming has a fixed set of rules, hence, the root stems may not be lexicographically correct. Which means, the stemmed words may not be semantically correct, and might have a chance of not being present in the dictionary (as evident from the preceding output). 
Lemmatization is very similar to stemming, where we remove word affixes to get to the base form of a word. However, the base form in this case is known as the root word, but not the root stem. The difference being that the root word is always a lexicographically correct word (present in the dictionary), but the root stem may not be so. Thus, root word, also known as the lemma, will always be present in the dictionary. Both nltk and spacy have excellent lemmatizers. We will be using spacy here. 
You can see that the semantics of the words are not affected by this, yet our text is still standardized. 
Do note that the lemmatization process is considerably slower than stemming, because an additional step is involved where the root form or lemma is formed by removing the affix from the word if and only if the lemma is present in the dictionary. 
Words which have little or no significance, especially when constructing meaningful features from text, are known as stopwords or stop words. These are usually words that end up having the maximum frequency if you do a simple term or word frequency in a corpus. Typically, these can be articles, conjunctions, prepositions and so on. Some examples of stopwords are a, an, the, and the like. 
There is no universal stopword list, but we use a standard English language stopwords list from nltk. You can also add your own domain-specific stopwords as needed. 
While we can definitely keep going with more techniques like correcting spelling, grammar and so on, let’s now bring everything we learnt together and chain these operations to build a text normalizer to pre-process text data. 
Let’s now put this function in action! We will first combine the news headline and the news article text together to form a document for each piece of news. Then, we will pre-process them. 
Thus, you can see how our text pre-processor helps in pre-processing our news articles! After this, you can save this dataset to disk if needed, so that you can always load it up later for future analysis. 
For any language, syntax and structure usually go hand in hand, where a set of specific rules, conventions, and principles govern the way words are combined into phrases; phrases get combines into clauses; and clauses get combined into sentences. We will be talking specifically about the English language syntax and structure in this section. In English, words usually combine together to form other constituent units. These constituents include words, phrases, clauses, and sentences. Considering a sentence, “The brown fox is quick and he is jumping over the lazy dog”, it is made of a bunch of words and just looking at the words by themselves don’t tell us much. 
Knowledge about the structure and syntax of language is helpful in many areas like text processing, annotation, and parsing for further operations such as text classification or summarization. Typical parsing techniques for understanding text syntax are mentioned below. 
We will be looking at all of these techniques in subsequent sections. Considering our previous example sentence “The brown fox is quick and he is jumping over the lazy dog”, if we were to annotate it using basic POS tags, it would look like the following figure. 
Thus, a sentence typically follows a hierarchical structure consisting the following components, 
sentence → clauses → phrases → words 
Parts of speech (POS) are specific lexical categories to which words are assigned, based on their syntactic context and role. Usually, words can fall into one of the following major categories. 
Besides these four major categories of parts of speech , there are other categories that occur frequently in the English language. These include pronouns, prepositions, interjections, conjunctions, determiners, and many others. Furthermore, each POS tag like the noun (N) can be further subdivided into categories like singular nouns (NN), singular proper nouns (NNP), and plural nouns (NNS). 
The process of classifying and labeling POS tags for words called parts of speech tagging or POS tagging . POS tags are used to annotate words and depict their POS, which is really helpful to perform specific analysis, such as narrowing down upon nouns and seeing which ones are the most prominent, word sense disambiguation, and grammar analysis. We will be leveraging both nltk and spacy which usually use the Penn Treebank notation for POS tagging. 
We can see that each of these libraries treat tokens in their own way and assign specific tags for them. Based on what we see, spacy seems to be doing slightly better than nltk. 
Based on the hierarchy we depicted earlier, groups of words make up phrases. There are five major categories of phrases: 
Shallow parsing, also known as light parsing or chunking , is a popular natural language processing technique of analyzing the structure of a sentence to break it down into its smallest constituents (which are tokens such as words) and group them together into higher-level phrases. This includes POS tags as well as phrases from a sentence. 
We will leverage the conll2000 corpus for training our shallow parser model. This corpus is available in nltk with chunk annotations and we will be using around 10K records for training our model. A sample annotated sentence is depicted as follows. 
From the preceding output, you can see that our data points are sentences that are already annotated with phrases and POS tags metadata that will be useful in training our shallow parser model. We will leverage two chunking utility functions, tree2conlltags , to get triples of word, tag, and chunk tags for each token, and conlltags2tree to generate a parse tree from these token triples. We will be using these functions to train our parser. A sample is depicted below. 
The chunk tags use the IOB format. This notation represents Inside, Outside, and Beginning. The B- prefix before a tag indicates it is the beginning of a chunk, and I- prefix indicates that it is inside a chunk. The O tag indicates that the token does not belong to any chunk. The B- tag is always used when there are subsequent tags of the same type following it without the presence of O tags between them. 
We will now define a function conll_tag_ chunks() to extract POS and chunk tags from sentences with chunked annotations and a function called combined_taggers() to train multiple taggers with backoff taggers (e.g. unigram and bigram taggers) 
We will now define a class NGramTagChunker that will take in tagged sentences as training input, get their (word, POS tag, Chunk tag) WTC triples, and train a BigramTagger with a UnigramTagger as the backoff tagger. We will also define a parse() function to perform shallow parsing on new sentences 
The UnigramTagger , BigramTagger , and TrigramTagger are classes that inherit from the base class NGramTagger , which itself inherits from the ContextTagger class , which inherits from the SequentialBackoffTagger class . 
We will use this class to train on the conll2000 chunked train_data and evaluate the model performance on the test_data 
Our chunking model gets an accuracy of around 90% which is quite good! Let’s now leverage this model to shallow parse and chunk our sample news article headline which we used earlier, “US unveils world’s most powerful supercomputer, beats China”. 
Thus you can see it has identified two noun phrases (NP) and one verb phrase (VP) in the news article. Each word’s POS tags are also visible. We can also visualize this in the form of a tree as follows. You might need to install ghostscript in case nltk throws an error. 
The preceding output gives a good sense of structure after shallow parsing the news headline. 
Constituent-based grammars are used to analyze and determine the constituents of a sentence. These grammars can be used to model or represent the internal structure of sentences in terms of a hierarchically ordered structure of their constituents. Each and every word usually belongs to a specific lexical category in the case and forms the head word of different phrases. These phrases are formed based on rules called phrase structure rules. 
Phrase structure rules form the core of constituency grammars, because they talk about syntax and rules that govern the hierarchy and ordering of the various constituents in the sentences. These rules cater to two things primarily. 
The generic representation of a phrase structure rule is S → AB , which depicts that the structure S consists of constituents A and B , and the ordering is A followed by B . While there are several rules (refer to Chapter 1, Page 19: Text Analytics with Python, if you want to dive deeper), the most important rule describes how to divide a sentence or a clause. The phrase structure rule denotes a binary division for a sentence or a clause as S → NP VP where S is the sentence or clause, and it is divided into the subject, denoted by the noun phrase (NP) and the predicate, denoted by the verb phrase (VP). 
A constituency parser can be built based on such grammars/rules, which are usually collectively available as context-free grammar (CFG) or phrase-structured grammar. The parser will process input sentences according to these rules, and help in building a parse tree. 
We will be using nltk and the StanfordParser here to generate parse trees. 
Prerequisites: Download the official Stanford Parser from here, which seems to work quite well. You can try out a later version by going to this website and checking the Release History section. After downloading, unzip it to a known location in your filesystem. Once done, you are now ready to use the parser from nltk , which we will be exploring soon. 
The Stanford parser generally uses a PCFG (probabilistic context-free grammar) parser. A PCFG is a context-free grammar that associates a probability with each of its production rules. The probability of a parse tree generated from a PCFG is simply the production of the individual probabilities of the productions used to generate it. 
We can see the constituency parse tree for our news headline. Let’s visualize it to understand the structure better. 
We can see the nested hierarchical structure of the constituents in the preceding output as compared to the flat structure in shallow parsing. In case you are wondering what SINV means, it represents an Inverted declarative sentence, i.e. one in which the subject follows the tensed verb or modal. Refer to the Penn Treebank reference as needed to lookup other tags. 
In dependency parsing, we try to use dependency-based grammars to analyze and infer both structure and semantic dependencies and relationships between tokens in a sentence. The basic principle behind a dependency grammar is that in any sentence in the language, all words except one, have some relationship or dependency on other words in the sentence. The word that has no dependency is called the root of the sentence. The verb is taken as the root of the sentence in most cases. All the other words are directly or indirectly linked to the root verb using links , which are the dependencies. 
Considering our sentence “The brown fox is quick and he is jumping over the lazy dog” , if we wanted to draw the dependency syntax tree for this, we would have the structure 
These dependency relationships each have their own meaning and are a part of a list of universal dependency types. This is discussed in an original paper, Universal Stanford Dependencies: A Cross-Linguistic Typology by de Marneffe et al, 2014). You can check out the exhaustive list of dependency types and their meanings here. 
If we observe some of these dependencies, it is not too hard to understand them. 
Spacy had two types of English dependency parsers based on what language models you use, you can find more details here. Based on language models, you can use the Universal Dependencies Scheme or the CLEAR Style Dependency Scheme also available in NLP4J now. We will now leverage spacy and print out the dependencies for each token in our news headline. 
It is evident that the verb beats is the ROOT since it doesn’t have any other dependencies as compared to the other tokens. For knowing more about each annotation you can always refer to the CLEAR dependency scheme. We can also visualize the above dependencies in a better way. 
You can also leverage nltk and the StanfordDependencyParser to visualize and build out the dependency tree. We showcase the dependency tree both in its raw and annotated form as follows. 
You can notice the similarities with the tree we had obtained earlier. The annotations help with understanding the type of dependency among the different tokens. 
In any text document, there are particular terms that represent specific entities that are more informative and have a unique context. These entities are known as named entities , which more specifically refer to terms that represent real-world objects like people, places, organizations, and so on, which are often denoted by proper names. A naive approach could be to find these by looking at the noun phrases in text documents. Named entity recognition (NER) , also known as entity chunking/extraction , is a popular technique used in information extraction to identify and segment the named entities and classify or categorize them under various predefined classes. 
SpaCy has some excellent capabilities for named entity recognition. Let’s try and use it on one of our sample news articles. 
We can clearly see that the major named entities have been identified by spacy. To understand more in detail about what each named entity means, you can refer to the documentation or check out the following table for convenience. 
Let’s now find out the most frequent named entities in our news corpus! For this, we will build out a data frame of all the named entities and their types using the following code. 
We can now transform and aggregate this data frame to find the top occuring entities and types. 
Do you notice anything interesting? (Hint: Maybe the supposed summit between Trump and Kim Jong!). We also see that it has correctly identified ‘Messenger’ as a product (from Facebook). 
We can also group by the entity types to get a sense of what types of entites occur most in our news corpus. 
We can see that people, places and organizations are the most mentioned entities though interestingly we also have many other entities. 
Another nice NER tagger is the StanfordNERTagger available from the nltk interface. For this, you need to have Java installed and then download the Stanford NER resources. Unzip them to a location of your choice (I used E:/stanford in my system). 
Stanford’s Named Entity Recognizer is based on an implementation of linear chain Conditional Random Field (CRF) sequence models. Unfortunately this model is only trained on instances of PERSON, ORGANIZATION and LOCATION types. Following code can be used as a standard workflow which helps us extract the named entities using this tagger and show the top named entities and their types (extraction differs slightly from spacy). 
We notice quite similar results though restricted to only three types of named entities. Interestingly, we see a number of mentioned of several people in various sports. 
Sentiment analysis is perhaps one of the most popular applications of NLP, with a vast number of tutorials, courses, and applications that focus on analyzing sentiments of diverse datasets ranging from corporate surveys to movie reviews. The key aspect of sentiment analysis is to analyze a body of text for understanding the opinion expressed by it. Typically, we quantify this sentiment with a positive or negative value, called polarity. The overall sentiment is often inferred as positive, neutral or negative from the sign of the polarity score. 
Usually, sentiment analysis works best on text that has a subjective context than on text with only an objective context. Objective text usually depicts some normal statements or facts without expressing any emotion, feelings, or mood. Subjective text contains text that is usually expressed by a human having typical moods, emotions, and feelings. Sentiment analysis is widely used, especially as a part of social media analysis for any domain, be it a business, a recent movie, or a product launch, to understand its reception by the people and what they think of it based on their opinions or, you guessed it, sentiment! 
Typically, sentiment analysis for text data can be computed on several levels, including on an individual sentence level, paragraph level, or the entire document as a whole. Often, sentiment is computed on the document as a whole or some aggregations are done after computing the sentiment for individual sentences. There are two major approaches to sentiment analysis. 
For the first approach we typically need pre-labeled data. Hence, we will be focusing on the second approach. For a comprehensive coverage of sentiment analysis, refer to Chapter 7: Analyzing Movie Reviews Sentiment, Practical Machine Learning with Python, Springer\Apress, 2018. In this scenario, we do not have the convenience of a well-labeled training dataset. Hence, we will need to use unsupervised techniques for predicting the sentiment by using knowledgebases, ontologies, databases, and lexicons that have detailed information, specially curated and prepared just for sentiment analysis. A lexicon is a dictionary, vocabulary, or a book of words. In our case, lexicons are special dictionaries or vocabularies that have been created for analyzing sentiments. Most of these lexicons have a list of positive and negative polar words with some score associated with them, and using various techniques like the position of words, surrounding words, context, parts of speech, phrases, and so on, scores are assigned to the text documents for which we want to compute the sentiment. After aggregating these scores, we get the final sentiment. 
Various popular lexicons are used for sentiment analysis, including the following. 
This is not an exhaustive list of lexicons that can be leveraged for sentiment analysis, and there are several other lexicons which can be easily obtained from the Internet. Feel free to check out each of these links and explore them. We will be covering two techniques in this section. 
The AFINN lexicon is perhaps one of the simplest and most popular lexicons that can be used extensively for sentiment analysis. Developed and curated by Finn Årup Nielsen, you can find more details on this lexicon in the paper, “A new ANEW: evaluation of a word list for sentiment analysis in microblogs”, proceedings of the ESWC 2011 Workshop. The current version of the lexicon is AFINN-en-165. txt and it contains over 3,300+ words with a polarity score associated with each word. You can find this lexicon at the author’s official GitHub repository along with previous versions of it, including AFINN-111. The author has also created a nice wrapper library on top of this in Python called afinn, which we will be using for our analysis. 
The following code computes sentiment for all our news articles and shows summary statistics of general sentiment per news category. 
We can get a good idea of general sentiment statistics across different news categories. Looks like the average sentiment is very positive in sports and reasonably negative in technology! Let’s look at some visualizations now. 
We can see that the spread of sentiment polarity is much higher in sports and world as compared to technology where a lot of the articles seem to be having a negative polarity. We can also visualize the frequency of sentiment labels. 
No surprises here that technology has the most number of negative articles and world the most number of positive articles. Sports might have more neutral articles due to the presence of articles which are more objective in nature (talking about sporting events without the presence of any emotion or feelings). Let’s dive deeper into the most positive and negative sentiment news articles for technology news. 
Looks like the most negative article is all about a recent smartphone scam in India and the most positive article is about a contest to get married in a self-driving shuttle. Interesting! Let’s do a similar analysis for world news. 
Interestingly Trump features in both the most positive and the most negative world news articles. Do read the articles to get some more perspective into why the model selected one of them as the most negative and the other one as the most positive (no surprises here!). 
TextBlob is another excellent open-source library for performing NLP tasks with ease, including sentiment analysis. It also an a sentiment lexicon (in the form of an XML file) which it leverages to give both polarity and subjectivity scores. Typically, the scores have a normalized scale as compare to Afinn. The polarity score is a float within the range [-1.0, 1.0]. The subjectivity is a float within the range [0.0, 1.0] where 0.0 is very objective and 1.0 is very subjective. Let’s use this now to get the sentiment polarity and labels for each news article and aggregate the summary statistics per news category. 
Looks like the average sentiment is the most positive in world and least positive in technology! However, these metrics might be indicating that the model is predicting more articles as positive. Let’s look at the sentiment frequency distribution per news category. 
There definitely seems to be more positive articles across the news categories here as compared to our previous model. However, still looks like technology has the most negative articles and world, the most positive articles similar to our previous analysis. Let’s now do a comparative analysis and see if we still get similar articles in the most positive and negative categories for world news. 
Well, looks like the most negative world news article here is even more depressing than what we saw the last time! The most positive article is still the same as what we had obtained in our last model. 
Finally, we can even evaluate and compare between these two models as to how many predictions are matching and how many are not (by leveraging a confusion matrix which is often used in classification). We leverage our nifty model_evaluation_utils module for this. 
In the preceding table, the ‘Actual’ labels are predictions from the Afinn sentiment analyzer and the ‘Predicted’ labels are predictions from TextBlob. Looks like our previous assumption was correct. TextBlob definitely predicts several neutral and negative articles as positive. Overall most of the sentiment predictions seem to match, which is good! 
This was definitely one of my longer articles! If you are reading this, I really commend your efforts for staying with me till the end of this article. These examples should give you a good idea about how to start working with a corpus of text documents and popular strategies for text retrieval, pre-processing, parsing, understanding structure, entities and sentiment. We will be covering feature engineering and representation techniques with hands-on examples in the next article of this series. Stay tuned! 
All the code and datasets used in this article can be accessed from my GitHub 
The code is also available as a Jupyter notebook 
I often mentor and help students at Springboard to learn essential skills around Data Science. Thanks to them for helping me develop this content. Do check out Springboard’s DSC bootcamp if you are interested in a career-focused structured path towards learning Data Science. 
A lot of this code comes from the research and work that I had done during writing my book “Text Analytics with Python”. The code is open-sourced on GitHub. (Python 3.x edition coming by end of this year!) 
“Practical Machine Learning with Python”, my other book also covers text classification and sentiment analysis in detail. The code is open-sourced on GitHub for your convenience. 
If you have any feedback, comments or interesting insights to share about my article or data science in general, feel free to reach out to me on my LinkedIn social media channel. 
Thanks to Durba for editing this article. 
Hands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look 
Written by 
Written by",Dipanjan (DJ) Sarkar,2018-12-04T17:36:44.537Z
How To Zero Trust: Roll out Zero Trust BeyondCorp security for your Enterprise by extending your Single Sign-On solution | by Tarun Desikan | Banyan Security | Medium,"This is the first in a series of blogs on “How To Zero Trust”, geared towards enterprise IT and security professionals. Our objective is to highlight real-world scenarios and practical solutions to improve the security posture of your organization toward Zero Trust access control. In this first post, we’ll address a common question we hear from many enterprise security professionals: How should an enterprise extend its Cloud Single Sign-On (SSO) to get to a Zero Trust BeyondCorp-style security posture? 
Consider this scenario — your organization is rapidly migrating apps to the cloud to cut costs and improve business agility, and has invested in a cloud-based identity provider such as Okta or AzureAD. However, your employees are still stuck using their traditional VPN. You realize it is time to move past an on-premises VPN-based security model to a modern cloud-integrated Zero Trust model, and you can clearly see how a framework like Google’s BeyondCorp could benefit your organization, but it all just seems overly complicated and intimidating. Device inventory, trust inference, authorization policies, certificate management, access control engine… How does one even start? Who will manage all this? Is it even worth it? To make matters worse, your teams just don’t have enough time or resources. So, just how do you roll out Zero Trust? 
Let’s start by boiling down Zero Trust to its essence: a Zero Trust security model is one that does away with the idea of a privileged network. Instead, access depends solely on device and user credentials, regardless of network location or application hosting model. This means all access to enterprise resources must be fully authenticated, authorized, and encrypted based upon device state and user credentials. 
A Zero Trust security model is one that does away with the idea of a privileged network. 
Thus, instead of relying on your network perimeter to safeguard your enterprise, with Zero Trust you rely on: 
And, that’s all there is to it! 
As you can see, your investment in a cloud-based identity provider and migration to cloud-based SSO is a great starting point. Most SSO vendors also provide Multi-Factor Authentication (MFA), which you should enable where appropriate, so you can ensure secure user authentication via primary and second factor credentials. With your SSO strategy in place, you have checked off the first requirement for Zero Trust — securely identifying users accessing corporate resources. 
The next step towards Zero Trust is to create an inventory of all your devices. This is a crucial step because a Zero Trust model requires that network location be immaterial; instead, access to corporate applications is only granted to trusted devices. 
If your organization has mature desktop and mobile device management (MDM) and private public key infrastructure (Private PKI) capabilities, this step is fairly straightforward as well. You can use your device management solution to deploy a device certificate to all your endpoints so you can uniquely identify the device and attest to its status. If your SSO vendor also offers device identification, you can enable it to enhance this process. In addition to simply identifying the device, it is important to validate the device posture, or how much risk this device is introducing into the request process. Any anti-virus, MDM, EDR, SIEM and other such solutions can help with this validation, and they should be part of the Zero Trust integration to establish the overall trust of the request. This will help in the next step to determine what resources are safe to access from this device. 
The device inventory step can get more complicated if your organization uses disparate device management tools, allows Bring Your Own Device (BYOD) programs or needs to manage access for multiple third-parties. In such cases, you’ll need to create a meta-inventory database to amalgamate and normalize device information from multiple sources, and enable a lightweight process for third-parties to register their devices. You should leverage your existing tooling where possible, instead of replacing or duplicating their capabilities. 
Once you have the ability to securely identify your users and devices, you can focus on your applications. Enterprises today run their corporate applications and services across diverse environments — on-premises in datacenters, in cloud IaaS environments such as AWS and Azure, and in cloud SaaS environments such as Salesforce and Dropbox. 
One of the core tenets of modern Zero Trust implementations is to avoid blanket network access technologies such as VPNs, VLANs, etc. 
You should start by creating a general policy framework for your organization, that applies regardless of where an application is hosted. Start with a set of “Global Rules”, that are coarse-grained policies that affect all corporate resources. For example, you could create a classification system that creates tiers of applications based on the sensitivity of the data handled. Then, you could specify that applications in the “high-sensitivity” tier can only be accessed by users authenticated using SSO on trusted devices registered with your device manager. As you create Global Rules, you can also start developing Service-level Rules that apply to individual applications and typically involve assertions about the user, as well as Resource-specific Rules that take policies down to the API-level. If your organization is investing in DevOps and cloud-native development, be sure to include applications and services deployed in those environments — SSH servers, IaaS consoles, Kubernetes APIs, CI/CD tools, etc. — in your framework. 
One of the core tenets of modern Zero Trust implementations is to avoid blanket network access technologies such as VPNs, VLANs, etc. Instead enforce access control policies using application-layer techniques, such as an Access Proxy, that provide the uniform coverage and granular controls needed for today’s enterprise environments. With the right enforcement mechanisms in place, you can now apply application controls to match your organization’s policy framework. 
As you can see, the conceptual pieces needed to implement Zero Trust BeyondCorp security are straightforward. If your enterprise runs entirely on Google Cloud using Google products (such as, GSuite Enterprise, Google Identity SSO & MFA, Google Endpoint Management, Google Cloud Platform, etc), then read no further. Simply hop over to Google’s BeyondCorp Remote Access offering, work through their checklist and you’ll have a Zero Trust posture rolled out in no time. For those of you who are not operating in a 100% Google environment, you will need to go beyond BeyondCorp — read on. 
Now, if your organization does not run entirely on Google or have the engineering resources to dedicate to a homegrown Zero Trust project, Banyan can help. Once you have your SSO in place, Banyan can accelerate your Zero Trust journey. 
For Device Trust, Banyan offers multiple deployment options that you can tailor to your organization’s needs. Banyan integrates with your Device Manager for secure device identification and other tools such as AV, EDR, SIEM, NAC, and others for posture validation on managed corporate devices. For BYOD, third-parties and other scenarios, Banyan also provides Desktop and Mobile Apps that can perform device attestation and posture reporting. Exceptions can be granted to unknown devices in order to support incremental roll outs. In either case, Banyan supports continuous validation to reject access at any time, as necessary. 
For policy-based access to applications, we provide an access proxy and policy engine that uses seamless integrations to enable click-button access controls across your diverse environments — on-premises, Cloud IaaS, Cloud SaaS — with comprehensive policy definition and enforcement. 
Banyan is designed to make it easy for your IT teams to roll out Zero Trust. And best of all, it is designed for incremental roll-out in complex environments. You decide the best approach for your organization — such as which apps to tackle first, piloting across key users first, departmental roll-outs, etc — and you can get started with Zero Trust in just a few minutes. 
In summary, once you have SSO in place, Banyan can help you configure the rest of your enterprise environment for a Zero Trust security posture. Get ready to Zero Trust! 
Written by 
Written by",Tarun Desikan,2020-06-24T16:14:54.103Z
"How a Single Medium Article Received 100,000 Views | by Casey Botticello | Blogging Guide | Medium","Many Medium writers are looking for exposure. While making money from the Medium Partner Program is a nice bonus, they primarily want their content to be seen and accessible. 
The good news is — if you are a writer, as described above — it is actually very simple to generate massive traffic for your Medium story. 
Medium has extremely high Domain Authority (95/100). 
Domain authority is a score that hints at the “strength” and relevance of a website for a specific subject area or industry. It’s a logarithmic scale of points, typically ranging from zero to 100, which predicts how well a website will rank on search engine result pages (SERPs). The higher the number of points, the higher a website’s Domain Authority is. 
This means that any content you publish from Medium is already given preferential treatment in search engines. This establishes the potential for large amounts of external traffic to your Medium article. 
The article that I wrote which received over 100,000 views is How to Bypass Virtually Every News Paywall: 
It’s not surprising that this article generated a decent amount of views because it was a timely answer to a popular online question. But it is surprising that it generated so many views. 
How did this Medium article accomplish this? 
It leveraged Medium’s high domain authority and created content that was attractive to search engines. 
A staggering 97% of the total views received by this article were from external traffic sources. The vast majority of these external sources are search engines. Google alone, generated well over 80,000 views. Even less common search engine like DuckDuckGo generated over 1,000 views. 
Did this happen overnight? No. Even the best content will take weeks, if not months to achieve its optimal rank in most search engines. 
As you can see from the screenshots of my Medium article analytics, below, the article initially saw a surge of traffic a few days in. 
Traffic then remained flat for over a month before it jumped from being completely overlooked (Google Search Results Page 10+) to being somewhat accessible (Google Search Results Page 3–5). Also worth noting, the article was almost immediately indexed in Google after publication due to: (1) Medium being a publishing platform with high domain authority; (2) The initial surge of traffic ensured it would be indexed quickly. 
As the article oscillates in page rank in Google’s search algorithm, daily traffic continues to grow at a steady rate: 
Now, in May and June, several months after the initial article was published, traffic is still consistently growing with approximately 1,000 views per day! 
What caused this? Primarily, views continued to grow at a fairly steady pace as the article climbed page rank in Google’s search engine. It gradually reached the bottom of the first page, then the number one spot on page 1, and finally it started becoming the featured snippet (position 0) in Google’s Search results. 
Google’s algorithm evaluates a number of “on-page” factors to determine what a page is about. These on-page ranking factors include the following: 
Medium provides writers with the ability to include a target keyword in each of these fields. If you want to generate huge amounts of external views, it is essential to optimize each of these fields. 
A title tag is an HTML element that specifies the title of a web page. Title tags are displayed on search engine results pages (SERPs) as the clickable headline for a given result, and are important for usability, SEO, and social sharing. The title tag of a web page is meant to be an accurate and concise description of a page’s content. 
Brand Name | Major Product Category — Minor Product Category — Name of Product 
A subheading, or subhead, are mini-headlines and play a huge role in capturing and holding the scanners attention. It also keeps them moving down the page from one subhead to the next. 
The subheading would ideally be: 
The meta description is an HTML attribute that provides a brief summary of a web page. Search engines such as Google often display the meta description in search results where they can highly influence user click-through rates. 
A URL (Uniform Resource Locator), more commonly known as a “web address”, specifies the location of a resource (such as a web page) on the internet. The URL also specifies how to retrieve that resource, also known as the “protocol”, such as HTTP, HTTPS, FTP, etc. 
On the Internet, content sends signals to visitors and search engines about the quality and purpose of a site. Good writing, images, and other forms of content help visitors engage with a site and can build trust. Meanwhile, duplicate content and keyword-stuffed copywriting can indicate that a site is low-quality or even spammy. Content, especially when created according to a defined content strategy, is a cornerstone of effective digital marketing. 
Up until recently, Medium did not allow proper image tagging. However, a few weeks ago, Medium added alt-text functionality, which allows writers to properly tag their images. 
If you’re creating content on a topic that requires the support of visuals, consider how your audience might prefer to find answers to their questions on that topic. In many cases, Google searchers don’t want the classic blue, hyperlinked search result — they want the image itself, embedded inside your webpage. 
One of the most important things image alt text can do for you is turn your images into hyperlinked search results — giving your website yet another way to receive organic visitors. 
— Casey Botticello 
Thanks for reading this article! Leave a comment below if you have any questions. Be sure to sign up for the Blogging Guide newsletter, to get the latest tips, tricks, and news about writing on Medium and to join our Facebook group, Medium Writing, to share your latest Medium posts and connect with other writers. 
Casey Botticello is a partner at Black Edge Consulting. Black Edge Consulting is a strategic communications firm, specializing in online reputation management, digital marketing, and crisis management. Prior to founding Black Edge Consulting, he worked for BGR Group, a bipartisan lobbying and strategic communications firm. 
Casey is the founder of the Cryptocurrency Alliance, an independent expenditure-only committee (Super PAC) dedicated to cryptocurrency and blockchain advocacy. He is also the editor of several Medium publications, including Medium Blogging Guide, Investigation, Strategic Communications, K Street, and Escaping the 9 to 5. He is a graduate of The University of Pennsylvania, where he received his B.A. in Urban Studies. 
Medium Blogging Guide is the premier publication dedicated to helping writers achieve success on Medium! Take a look 
Written by 
Written by",Casey Botticello,2020-04-04T00:57:03.881Z
Understanding Principal Component Analysis | by Rishav Kumar | Medium,"The purpose of this post is to give the reader detailed understanding of Principal Component Analysis with the necessary mathematical proofs. 
Should be read on big screen. Phone screen doesn’t render the png images properly, and images are important. :) 
I have used text and formulas in images because medium doesn’t support latex. So, I had to export latex to png to display it here.You can skip the proofs if you don’t want to indulge in Linear Algebra.It’s my first medium post and I expect the readers to really try to understand whatI’m showing here. 
In real world data analysis tasks we analyze complex data i.e. multi dimensional data. We plot the data and find various patterns in it or use it to train some machine learning models. One way to think about dimensions is that suppose you have an data point x , if we consider this data point as a physical object then dimensions are merely a basis of view, like where is the data located when it is observed from horizontal axis or vertical axis. 
As the dimensions of data increases, the difficulty to visualize it and perform computations on it also increases. So, how to reduce the dimensions of a data-* Remove the redundant dimensions* Only keep the most important dimensions 
break1 
First try to understand some terms - 
Variance : It is a measure of the variability or it simply measures how spread the data set is. Mathematically, it is the average squared deviation from the mean score. We use the following formula to compute variance var(x). 
Covariance : It is a measure of the extent to which corresponding elements from two sets of ordered data move in the same direction. Formula is shown above denoted by cov(x,y) as the covariance of x and y.Here, xi is the value of x in ith dimension.x bar and y bar denote the corresponding mean values.One way to observe the covariance is how interrelated two data sets are. 
Positive covariance means X and Y are positively related i.e. as X increases Y also increases. Negative covariance depicts the exact opposite relation. However zero covariance means X and Y are not related. 
Continue break1 
Now lets think about the requirement of data analysis.Since we try to find the patterns among the data sets so we want the data to be spread out across each dimension. Also, we want the dimensions to be independent. Such that if data has high covariance when represented in some n number of dimensions then we replace those dimensions with linear combination of those n dimensions. Now that data will only be dependent on linear combination of those related n dimensions. (related = have high covariance) 
So, what does Principal Component Analysis (PCA) do? 
PCA finds a new set of dimensions (or a set of basis of views) such that all the dimensions are orthogonal (and hence linearly independent) and ranked according to the variance of data along them. It means more important principleaxis occurs first. (more important = more variance/more spread out data) 
How does PCA work - 
To understand the detail working of PCA , you should have knowledge of eigen vectors and eigen values. Please refer to this visual explanation of eigen vectors and values. 
Make sure you understand the eigenvectors and eigen values before proceeding further. 
Assuming we have the knowledge of variance and covariance, Lets look into what a Covariance matrix is. 
A covariance matrix of some data set in 4 dimensions a,b,c,d. Va : variance along dimension aCa,b : Covariance along dimension a and b 
If we have a matrix X of m*n dimension such that it holds n data points of m dimensions, then covariance matrix can be calculated as 
It is important to note that the covariance matrix contains -* variance of dimensions as the main diagonal elements.* covariance of dimensions as the off diagonal elements. 
Also, covariance matrix is symmetric. (observe from the image above)As, we discussed earlier we want the data to be spread out i.e. it should have high variance along dimensions. Also we want to remove correlated dimensions i.e. covariance among the dimensions should be zero (they should be linearly independent). Therefore, our covariance matrix should have -* large numbers as the main diagonal elements.* zero values as the off diagonal elements.We call it a diagonal matrix. 
A cat, just to get your attention. :) 
So, we have to transform the original data points such that their covariance is a diagonal matrix. The process of transforming a matrix to diagonal matrix is called diagonalization. 
Always normalize your data before doing PCA because if we use data(features here) of different scales, we get misleading components. We can also simply use correlation matrix instead of using covariance matrix if features are of different scales. For the simplicity of the post, I’m assuming we have already normalized data. 
This defines the goal of PCA - 
Lets try to understand what I mean by projection error. Suppose we have to transform a 2 dimensional representation of data points to a one dimensional representation. So we will basically try to find a straight line and project data points on them. (A straight line is one dimensional). There are many possibilities to select the straight line. Lets see two such possibilities - 
Say magenta line will be our new dimension.If you see the red lines (connecting the projection of blue points on magenta line) i.e. the perpendicular distance of each data point from the straight line is the projection error. Sum of the error of all data points will be the total projection error.Our new data points will be the projections (red points) of those original blue data points. As we can see we have transformed 2 dimensional data points to one dimensional data points by projection them on 1 dimensional space i.e. a straight line. That magenta straight line is called principal axis. Since we are projecting to a single dimension, we have only one principal axis. 
Clearly, Second choice of straight line is better because -* The projection error is less than that in the first case.* Newly projected red points are more widely spread out than the first case. i.e. more variance. 
The above mentioned two points are related i.e. if we minimize the reconstruction error, the variance will increase.How ?Proof : (Optional) https://stats.stackexchange.com/questions/32174/pca-objective-function-what-is-the-connection-between-maximizing-variance-and-m/136072#136072 
Steps we have performed so far -* We have calculated the covariance matrix of original data set matrix X. 
Now we want to transform the original data points such that the covariance matrix of transformed data points is a diagonal matrix. How to do that ? 
Here’s the trick- If we find the matrix of eigen vectors of Cx and use that as P (P is used for transforming X to Y, see the image above) , then Cy (covariance of transformed points) will be a diagonal matrix. Hence Y will be the set of new/transformed data points.Now, if we want to transform points to k dimensions then we will select first k eigen vectors of the matrix Cx (sorted decreasingly according to eigen values) and form a matrix with them and use them as P. 
So, if we have m dimensional original n data points thenX : m*nP : k*m Y = PX : (k*m)(m*n) = (k*n)Hence, our new transformed matrix has n data points having k dimensions. 
But why does this trick work ?Proof: 
First lets look at some theorems - 
Proof : 
Proof : 
Having these theorems, we can say that 
A symmetric matrix is diagonalized by a matrix of its orthonormal eigenvectors. Orthonormal vectors are just normalized orthogonal vectors. (what normalization is? google ;) ) 
It is evident that the choice of P diagonalizes Cy. This was the goal for PCA. We can summarize the results of PCA in the matrices P and Cy. 
Conclusion - 
In the next post we will be implementing PCA in python and using it for color data augmentation. 
Note: PCA is an analysis approach. You can do PCA using SVD, or you can do PCA doing the eigen-decomposition (like we did here), or you can do PCA using many other methods. SVD is just another numerical method. So, don’t confuse the terms PCA and SVD. However, there are some performance factors of sometimes choosing SVD over eigen-decomposition or the other way around(not that you should care much about). We will explore SVD in our upcoming posts. 
Let’s discuss in comments if you find anything wrong in the post or if you have anything to add. and give me claps :PThanks. 
Written by 
Written by",Rishav Kumar,2018-04-10T15:05:18.716Z
A simple Word2vec tutorial. In this tutorial we are going to… | by Zafar Ali | Medium,"In this tutorial we are going to explain, one of the emerging and prominent word embedding technique called Word2Vec proposed by Mikolov et al. in 2013. We have collected these content from different tutorials and sources to facilitate readers in a single place. I hope it helps. 
Word2vec is a combination of models used to represent distributed representations of words in a corpus C. Word2Vec (W2V) is an algorithm that accepts text corpus as an input and outputs a vector representation for each word, as shown in the diagram below: 
There are two flavors of this algorithm namely: CBOW and Skip-Gram. Given a set of sentences (also called corpus) the model loops on the words of each sentence and either tries to use the current word w in order to predict its neighbors (i.e., its context), this approach is called “Skip-Gram”, or it uses each of these contexts to predict the current word w, in that case the method is called “Continuous Bag Of Words” (CBOW). To limit the number of words in each context, a parameter called “window size” is used. 
The vectors we use to represent words are called neural word embeddings, and representations are strange. One thing describes another, even though those two things are radically different. As Elvis Costello said: “Writing about music is like dancing about architecture.” Word2vec “vectorizes” about words, and by doing so it makes natural language computer-readable — we can start to perform powerful mathematical operations on words to detect their similarities. 
So, a neural word embedding represents a word with numbers. It’s a simple, yet unlikely, translation. Word2vec is similar to an autoencoder, encoding each word in a vector, but rather than training against the input words through reconstruction, as a restricted Boltzmann machine does, word2vec trains words against other words that neighbor them in the input corpus. 
It does so in one of two ways, either using context to predict a target word (a method known as continuous bag of words, or CBOW), or using a word to predict a target context, which is called skip-gram. We use the latter method because it produces more accurate results on large datasets. 
When the feature vector assigned to a word cannot be used to accurately predict that word’s context, the components of the vector are adjusted. Each word’s context in the corpus is the teacher sending error signals back to adjust the feature vector. The vectors of words judged similar by their context are nudged closer together by adjusting the numbers in the vector. In this tutorial, we are going to focus on Skip-Gram model which in contrast to CBOW consider center word as input as depicted in figure above and predict context words. 
We understood that we have to feed some strange neural network with some pairs of words but we can’t just do that using as inputs the actual characters, we have to find some way to represent these words mathematically so that the network can process them. One way to do this is to create a vocabulary of all the words in our text and then to encode our word as a vector of the same dimensions of our vocabulary. Each dimension can be thought as a word in our vocabulary. So we will have a vector with all zeros and a 1 which represents the corresponding word in the vocabulary. This encoding technique is called one-hot encoding. Considering our example, if we have a vocabulary made out of the words “the”, “quick”, “brown”, “fox”, “jumps”, “over”, “the” “lazy”, “dog”, the word “brown” is represented by this vector: [ 0, 0, 1, 0, 0, 0 ,0 ,0 ,0 ]. 
The Skip-gram model takes in a corpus of text and creates a hot-vector for each word. A hot vector is a vector representation of a word where the vector is the size of the vocabulary (total unique words). All dimensions are set to 0 except the dimension representing the word that is used as an input at that point in time. Here is an example of a hot vector: 
The above input is given to a neural network with a single hidden layer. 
We’re going to represent an input word like “ants” as a one-hot vector. This vector will have 10,000 components (one for every word in our vocabulary) and we’ll place a “1” in the position corresponding to the word “ants”, and 0s in all of the other positions. The output of the network is a single vector (also with 10,000 components) containing, for every word in our vocabulary, the probability that a randomly selected nearby word is that vocabulary word. 
In word2vec, a distributed representation of a word is used. Take a vector with several hundred dimensions (say 1000). Each word is represented by a distribution of weights across those elements. So instead of a one-to-one mapping between an element in the vector and a word, the representation of a word is spread across all the elements in the vector, and each element in the vector contributes to the definition of many words. 
If I label the dimensions in a hypothetical word vector (there are no such pre-assigned labels in the algorithm of course), it might look a bit like this: 
Such a vector comes to represent in some abstract way the ‘meaning’ of a word. And as we’ll see next, simply by examining a large corpus it’s possible to learn word vectors that are able to capture the relationships between words in a surprisingly expressive way. We can also use the vectors as inputs to a neural network. Since our input vectors are one-hot, multiplying an input vector by the weight matrix W1 amounts to simply selecting a row from W1. 
From the hidden layer to the output layer, the second weight matrix W2 can be used to compute a score for each word in the vocabulary, and softmax can be used to obtain the posterior distribution of words. The skip-gram model is the opposite of the CBOW model. It is constructed with the focus word as the single input vector, and the target context words are now at the output layer. The activation function for the hidden layer simply amounts to copying the corresponding row from the weights matrix W1 (linear) as we saw before. At the output layer, we now output C multinomial distributions instead of just one. The training objective is to mimimize the summed prediction error across all context words in the output layer. In our example, the input would be “learning”, and we hope to see (“an”, “efficient”, “method”, “for”, “high”, “quality”, “distributed”, “vector”) at the output layer. 
Here’s the architecture of our neural network. 
For our example, we’re going to say that we’re learning word vectors with 300 features. So the hidden layer is going to be represented by a weight matrix with 10,000 rows (one for every word in our vocabulary) and 300 columns (one for every hidden neuron). 300 features is what Google used in their published model trained on the Google news dataset (you can download it from here). The number of features is a “hyper parameter” that you would just have to tune to your application (that is, try different values and see what yields the best results). 
If you look at the rows of this weight matrix, these are what will be our word vectors! 
So the end goal of all of this is really just to learn this hidden layer weight matrix — the output layer we’ll just toss when we’re done! The 1 x 300 word vector for “ants” then gets fed to the output layer. The output layer is a softmax regression classifier. Specifically, each output neuron has a weight vector which it multiplies against the word vector from the hidden layer, then it applies the function exp(x) to the result. Finally, in order to get the outputs to sum up to 1, we divide this result by the sum of the results from all 10,000 output nodes. Here’s an illustration of calculating the output of the output neuron for the word “car”. 
If two different words have very similar “contexts” (that is, what words are likely to appear around them), then our model needs to output very similar results for these two words. And one way for the network to output similar context predictions for these two words is if the word vectors are similar. So, if two words have similar contexts, then our network is motivated to learn similar word vectors for these two words! Ta da! 
Each dimension of the input passes through each node of the hidden layer. The dimension is multiplied by the weight leading it to the hidden layer. Because the input is a hot vector, only one of the input nodes will have a non-zero value (namely the value of 1). This means that for a word only the weights associated with the input node with value 1 will be activated, as shown in the image above. 
As the input in this case is a hot vector, only one of the input nodes will have a non-zero value. This means that only the weights connected to that input node will be activated in the hidden nodes. An example of the weights that will be considered is depicted below for the second word in the vocabulary: 
The vector representation of the second word in the vocabulary (shown in the neural network above) will look as follows, once activated in the hidden layer: 
Those weights start off as random values. The network is then trained in order to adjust the weights to represent the input words. This is where the output layer becomes important. Now that we are in the hidden layer with a vector representation of the word we need a way to determine how well we have predicted that a word will fit in a particular context. The context of the word is a set of words within a window around it, as shown below: 
The above image shows that the context for Friday includes words like “cat” and “is”. The aim of the neural network is to predict that “Friday” falls within this context. 
We activate the output layer by multiplying the vector that we passed through the hidden layer (which was the input hot vector * weights entering hidden node) with a vector representation of the context word (which is the hot vector for the context word * weights entering the output node). The state of the output layer for the first context word can be visualised below: 
The above multiplication is done for each word to context word pair. We then calculate the probability that a word belongs with a set of context words using the values resulting from the hidden and output layers. Lastly, we apply stochastic gradient descent to change the values of the weights in order to get a more desirable value for the probability calculated. 
In gradient descent we need to calculate the gradient of the function being optimised at the point representing the weight that we are changing. The gradient is then used to choose the direction in which to make a step to move towards the local optimum, as shown in the minimisation example below. 
The weight will be changed by making a step in the direction of the optimal point (in the above example, the lowest point in the graph). The new value is calculated by subtracting from the current weight value the derived function at the point of the weight scaled by the learning rate. The next step is using Backpropagation, to adjust the weights between multiple layers. The error that is computed at the end of the output layer is passed back from the output layer to the hidden layer by applying the Chain Rule. Gradient descent is used to update the weights between these two layers. The error is then adjusted at each layer and sent back further. Here is a diagram to represent backpropagation: 
Word2vec uses a single hidden layer, fully connected neural network as shown below. The neurons in the hidden layer are all linear neurons. The input layer is set to have as many neurons as there are words in the vocabulary for training. The hidden layer size is set to the dimensionality of the resulting word vectors. The size of the output layer is same as the input layer. Thus, if the vocabulary for learning word vectors consists of V words and N to be the dimension of word vectors, the input to hidden layer connections can be represented by matrix WI of size VxN with each row representing a vocabulary word. In same way, the connections from hidden layer to output layer can be described by matrix WO of size NxV. In this case, each column of WO matrix represents a word from the given vocabulary. 
The input to the network is encoded using “1-out of -V” representation meaning that only one input line is set to one and rest of the input lines are set to zero. 
Let suppose we have a training corpus having the following sentences: 
“the dog saw a cat”, “the dog chased the cat”, “the cat climbed a tree” 
The corpus vocabulary has eight words. Once ordered alphabetically, each word can be referenced by its index. For this example, our neural network will have eight input neurons and eight output neurons. Let us assume that we decide to use three neurons in the hidden layer. This means that WI and WO will be 8×3 and 3×8 matrices, respectively. Before training begins, these matrices are initialized to small random values as is usual in neural network training. Just for the illustration sake, let us assume WI and WO to be initialized to the following values: 
Suppose we want the network to learn relationship between the words “cat” and “climbed”. That is, the network should show a high probability for “climbed” when “cat” is inputted to the network. In word embedding terminology, the word “cat” is referred as the context word and the word “climbed” is referred as the target word. In this case, the input vector X will be [0 1 0 0 0 0 0 0]t. Notice that only the second component of the vector is 1. This is because the input word is “cat” which is holding number two position in sorted list of corpus words. Given that the target word is “climbed”, the target vector will look like [0 0 0 1 0 0 0 0 ]t. With the input vector representing “cat”, the output at the hidden layer neurons can be computed as: 
Ht = XtWI = [-0.490796 -0.229903 0.065460] 
It should not surprise us that the vector H of hidden neuron outputs mimics the weights of the second row of WI matrix because of 1-out-of-Vrepresentation. So the function of the input to hidden layer connections is basically to copy the input word vector to hidden layer. Carrying out similar manipulations for hidden to output layer, the activation vector for output layer neurons can be written as 
HtWO = [0.100934 -0.309331 -0.122361 -0.151399 0.143463 -0.051262 -0.079686 0.112928] 
Since, the goal is produce probabilities for words in the output layer, Pr(wordk|wordcontext) for k = 1, V, to reflect their next word relationship with the context word at input, we need the sum of neuron outputs in the output layer to add to one. Word2vec achieves this by converting activation values of output layer neurons to probabilities using the softmax function. Thus, the output of the k-th neuron is computed by the following expression where activation(n) represents the activation value of the n-th output layer neuron: 
Thus, the probabilities for eight words in the corpus are: 
0.143073 0.094925 0.114441 0.111166 0.149289 0.122874 0.119431 0.144800 
The probability in bold is for the chosen target word “climbed”. Given the target vector [0 0 0 1 0 0 0 0 ]t, the error vector for the output layer is easily computed by subtracting the probability vector from the target vector. Once the error is known, the weights in the matrices WO and WI can be updated using backpropagation. Thus, the training can proceed by presenting different context-target words pair from the corpus. This is how Word2vec learns relationships between words and in the process develops vector representations for words in the corpus. 
The idea behind word2vec is to represent words by a vector of real numbers of dimension d. Therefore the second matrix is the representation of those words. The i-th line of this matrix is the vector representation of the i-th word. Let’s say that in your example you have 5 words : [“Lion”, “Cat”, “Dog”, “Horse”, “Mouse”], then the first vector [0,0,0,1,0] means you’re considering the word “Horse” and so the representation of “Horse” is [10, 12, 19]. Similarly, [17, 24, 1] is the representation of the word “Lion”. 
To my knowledge, there are no “human meaning” specifically to each of the numbers in these representations. One number is not representing if the word is a verb or not, an adjective or not… It’s just the weights that you change to solve your optimization problem to learn the representation of your words. 
A visual diagram best elaborating word2vec matrix multiplication process is depicted in following figure: 
The first matrix represents the input vector in one hot format. The second matrix represents the synaptic weights from the input layer neurons to the hidden layer neurons. Especially notice the left top corner where the Input Layer matrix is multiplied with the Weight matrix. Now look at the top right. This matrix multiplication InputLayer dot-producted with Weights Transpose is just a handy way to represent the neural network at the top right. 
The first part, [0 0 0 1 0 … 0] represents the input word as a one hot vector and the other matrix represents the weight for the connection of each of the input layer neurons to the hidden layer neurons. As Word2Vec trains, it backpropagates (using gradient descent) into these weights and changes them to give better representations of words as vectors. Once training is accomplished, you use only this weight matrix, take [0 0 1 0 0 … 0] for say ‘dog’ and multiply it with the improved weight matrix to get the vector representation of ‘dog’ in a dimension = no of hidden layer neurons. In the diagram, the number of hidden layer neurons is 3. 
In a nutshell, Skip-gram model reverses the use of target and context words. In this case, the target word is fed at the input, the hidden layer remains the same, and the output layer of the neural network is replicated multiple times to accommodate the chosen number of context words. Taking the example of “cat” and “tree” as context words and “climbed” as the target word, the input vector in the skim-gram model would be [0 0 0 1 0 0 0 0 ]t, while the two output layers would have [0 1 0 0 0 0 0 0] t and [0 0 0 0 0 0 0 1 ]t as target vectors respectively. In place of producing one vector of probabilities, two such vectors would be produced for the current example. The error vector for each output layer is produced in the manner as discussed above. However, the error vectors from all output layers are summed up to adjust the weights via backpropagation. This ensures that weight matrix WO for each output layer remains identical all through training. 
We need few additional modifications to the basic skip-gram model which are important for making it feasible to train. Running gradient descent on a neural network that large is going to be slow. And to make matters worse, you need a huge amount of training data to tune that many weights and avoid over-fitting. millions of weights times billions of training samples means that training this model is going to be a beast. For that, authors have proposed two techniques called subsampling and negative sampling in which insignificant words are removed and only a specific sample of weights are updated. 
Mikolov et al. also use a simple subsampling approach to counter the imbalance between rare and frequent words in the training set (for example, “in”, “the”, and “a” provide less information value than rare words). Each word in the training set is discarded with probability P(wi) where 
f(wi) is the frequency of word wi and t is a chosen threshold, typically around 10–5. 
Word2vec has been implemented in various languages but here we will focus especially on Java i.e., DeepLearning4j [6], darks-learning [10] and python [7][8][9]. Various neural net algorithms have been implemented in DL4j, code is available on GitHub. 
To implement it in DL4j, we will go through few steps given as following: 
Create a new project in IntelliJ using Maven. Then specify properties and dependencies in the POM.xml file in your project’s root directory. 
Now create and name a new class in Java. After that, you’ll take the raw sentences in your .txt file, traverse them with your iterator, and subject them to some sort of pre-processing, such as converting all words to lowercase. 
String filePath = new ClassPathResource(“raw_sentences.txt”).getFile().getAbsolutePath(); 
log.info(“Load & Vectorize Sentences….”); 
// Strip white space before and after for each line 
SentenceIterator iter = new BasicLineIterator(filePath); 
If you want to load a text file besides the sentences provided in our example, you’d do this: 
Word2vec needs to be fed words rather than whole sentences, so the next step is to tokenize the data. To tokenize a text is to break it up into its atomic units, creating a new token each time you hit a white space, for example. 
Now that the data is ready, you can configure the Word2vec neural net and feed in the tokens. 
This configuration accepts several hyperparameters. A few require some explanation: 
The next step is to evaluate the quality of your feature vectors. 
The line vec.similarity(""word1"",""word2"") will return the cosine similarity of the two words you enter. The closer it is to 1, the more similar the net perceives those words to be (see the Sweden-Norway example above). For example: 
With vec.wordsNearest(""word1"", numWordsNearest), the words printed to the screen allow you to eyeball whether the net has clustered semantically similar words. You can set the number of nearest words you want with the second parameter of wordsNearest. For example: 
1) http://mccormickml.com/2016/04/27/word2vec-resources/ 
2) https://towardsdatascience.com/word2vec-skip-gram-model-part-1-intuition-78614e4d6e0b 
3) https://deeplearning4j.org/docs/latest/deeplearning4j-nlp-word2vec 
4) https://intothedepthsofdataengineering.wordpress.com/2017/06/26/an-overview-of-word2vec/ 
5) https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/ 
6) Word2vec in Java in http://deeplearning4j.org/word2vec.html 
7) Word2Vec and Doc2Vec in Python in genism http://radimrehurek.com/2013/09/deep-learning-with-word2vec-and-gensim/ 
8) http://rare-technologies.com/word2vec-tutorial/ 
9) https://www.tensorflow.org/versions/r0.8/tutorials/word2vec/index.html 
10) https://www.programcreek.com/java-api-examples/index.php?source_dir=darks-learning-master/src/main/java/darks/learning/word2vec/Word2Vec.java 
Written by 
Written by",Zafar Ali,2019-01-07T06:43:38.319Z
A Beginner’s Guide To Natural Language Processing | by Manish Shivanandhan | Manish Shivanandhan’s Blog | Medium,"Hey Siri, set an alarm for 6 AM tomorrow. 
Done — your alarm is set for 7 AM tomorrow. 
Ever wondered how devices like Siri and Alexa understand and interpret your voice? Have you been slightly annoyed when they couldn’t pick up certain terms? The answer is Natural Language Processing (NLP). 
NLP is a branch of artificial intelligence that uses both computer science and linguistics to aid computers in understanding “human language.” The purpose of NLP is to bridge the gap between the human language and the command line interface of a computer. 
Humans have hundreds of languages like English, Spanish, Hindi, Chinese, or French. Computers, on the other hand, have only one native language, which is called the machine language. All of the processes in your computers and smart devices communicate via millions of zeros and ones to perform a specific function. The machine code is unintelligible to humans, which makes NLP a critical part of human-computer interactions. 
Let us look at some of the core use cases and a few real-world applications of NLP. 
Nowadays, people express their feedback through surveys, customer feedback, and social media platforms. As a result, organizations have to rely on software that can understand human emotions expressed via text to understand their customer's feedback. These analyses are used to adapt products and services to meet customer expectations. Sentiment analysis is a crucial tool to help achieve this goal. 
Sentiment Analysis is the process of identifying opinions expressed in text and understand whether the author’s attitude towards the discussed product or service is positive, neutral, or negative. 
Real-Time Sentiment analysis helps in identifying critical issues in real-time. For example, whenever a crisis or a scandal is about to affect an organization due to escalating protests on social media, businesses can rely on sentiment analysis models to quickly recognize the issues and get in front of the customer and address it before it blows out of proportion. 
Here is an article by MonkeyLearn that explains Sentiment Analysis in depth. 
NLP is used extensively in spam filtering as well. The function of a spam filter is to spot unwanted e-mails and send them in a separate folder instead of your regular inbox. 
Simple spam filtering can be achieved using classification models in machine learning. However, NLP provides better capabilities to distinguish between useful emails and real-spam. NLP techniques such as n-gram modelling are applied to emails to classify them as spam or ham with higher accuracy than traditional classification models. 
A chatbot is an artificial intelligence (AI) software that can simulate a conversation with a user in natural language. A chatbot is an advanced implementation of natural language processing, taking us closer to communicating with computers in a way similar to human-to-human conversations. 
Chatbots use a combination of Natural Language Processing, Natural Language Understanding, and Natural Language Generation in order to achieve Conversational User Interface. 
Chatbots are a great use case for customer support, saving businesses time and money. Since the majority of questions raised by customers are asked frequently, they can be handled by chatbots. This helps customer service agents prioritize important customer queries, thereby ensuring overall customer satisfaction. 
There are numerous products that we use on a daily basis without realizing the fact that they are powered by NLP. Here are some of the most popular ones. 
Every leading tech giant has developed its own virtual assistant to provide a wholesome experience to the users. Alexa, Cortana, Siri, and Google Assistant are used extensively. These programs are not only able to control your smartphone, but also a vast number of compatible smart devices Air conditioners, Smart TVs, etc. 
NLP is an essential part of these virtual assistants. It is used to detect, interpret, and understand the text or voice commands to perform the requested function. All of these assistants are continually evolving through AI and machine learning to expand the accuracy of their responses. 
Grammarly has become one of, if not the most popular writing tool used by people all around the world. It is a fascinating tool that has the capability of suggesting different kinds of changes in your writing. Other than the spellings and grammar check, Grammarly can check the usage of Active and Passive voice, the tone of the document and suggest changes according to your writing goals. 
It is quite understandable if you have wondered about the working mechanism of Grammarly. Contrary to popular belief that some kinds of writing robots are checking your work, AI and NLP are the primary driving forces behind Grammarly’s functions. It offers different types of features and options for checking one’s writing. Hence, Grammarly’s AI system is composed of a wide range of NLP algorithms that can deal with different writing styles and tones. 
The entire machine learning system of Grammarly is quite remarkable. It is continuously being updated to become the best writing assistant available on the internet. NLP allows Grammarly to process English writing and perform various tasks on them to produce a thorough report. These tasks include writing improvement, readability scoring, sentiment analysis, and suggestions to use alternate words, phrases, and sentence structure. 
Language translation is an important application of Natural Language Processing. It has saved organizations billions of dollars in terms of the effort and man-power required in order to translate documents & audio from one language to the other. 
Google Translate is perhaps the most popular and efficient translator available. It is an incredibly efficient system that makes use of AI, machine learning, and NLP to translate text from one language to another. As of now, Google Translate supports 101 languages. 
It is important to note that translation is a very tricky process because the software has to understand each word, phrase, and sentence structure for accurate translation. Hence, Google Translate is continuously updated to improve the quality and accuracy of the language-translation. 
Spell-check is an underrated tool that is invaluable in our everyday lives. Not everyone can produce a perfect sentence without any spellings or grammar errors. In such a case, spell checks play a huge role in improving writing. Moreover, the autocorrection feature is prevalent in smartphones because it allows you to write down your thoughts immediately without worrying about the spellings. It eradicates the spelling errors from your messages and improves communication. 
Autocomplete and Autocomplete is another useful application of NLP that is being used by almost every web / mobile application, including search engines like Google. 
Autocomplete helps us to quickly find what we are looking for, using previous searches performed by other customers. Ultimately, autocomplete searches leads to increased customer satisfaction as they will be able to find the required product, service, or any other information quickly and more accurately. 
Similarly, Autosuggest helps in quick e-mailing and messaging. You can select the suitable word or even a phrase with just a single click, all thanks to the advancements in NLP. We use these features on a daily basis without realizing that they are the applications of Natural Language Processing. 
There are hundreds of languages in the world that make communication a complex phenomenon. It varies from culture to culture. Moreover, there is a lot of diversity in the languages in terms of the writing style, syntax, and grammar rules. Similarly, varying accents and dialects also make a major difference in communication. 
Hence, NLP is a developing concept that still requires a lot of research and innovation to cater to all kinds of use cases. Along with deep learning, syntactic and semantic learning are also becoming essential parts of the NLP to remove language ambiguities and enhance the quality of NLP-based products and services. 
Loved this article? Join my Newsletter and get a summary of my articles and videos every Monday. 
Written by 
Written by",Manish Shivanandhan,2020-09-05T11:10:51.474Z
Machine Learning on AWS. “AWS is our ML platform of choice… | by IP Specialist | Medium,"“AWS is our ML platform of choice, unlocking new ways to deliver on our promise of being the world’s travel platform” — Matthew Fryer, Chief Data Science Officer, Expedia Group 
Most of the businesses look at machine learning as if it is something unachievable because it is expensive and demands talent. Well, for some cases, it can be quite demanding but, with the trend of making everything-as-a-service, machine learning solutions are becoming easier to develop. Any business or even an individual can jump-start a machine learning initiative without much investment. With the use of machine learning cloud services, you can start building your machine learning models. 
Machine learning as a Service (MLaaS) is an umbrella definition that covers most of the issues such as data reprocessing, model training, and model evaluation, with further prediction. Prediction results can be bridged with your internal IT infrastructure through REST APIs. 
Let’s have a look at the most widely used ML cloud platform, Amazon Web Services (AWS), and the services that it provides. AWS is the most dominant public cloud provider. It offers the broadest range of machine learning services and the deepest set of Artificial Intelligence tools for businesses and individuals. These services are used in creating machine learning solutions with a faster pace. More than ten thousand customers, from the largest enterprises to the hottest startups, have chosen AWS Machine Learning services. This number of ML customers is way bigger than any other cloud services provider. 
Amazon Machine Learning Services 
Amazon Machine Learning services are available on two levels: predictive analytics with Amazon ML and the SageMaker tool for developers and data scientists. Since Amazon Machine Learning is no longer available for new customers, we will skip it and talk about Amazon SageMaker. 
Amazon SageMaker 
Amazon SageMaker provides tools for quick model building, training, and deployment. If you want to simplify data exploration and analysis without any server management, then Jupyter (an authoring notebook) is the best option provided by Amazon SageMaker. It covers the entire workflow of machine learning; labeling and preparing data, selecting an algorithm, making predictions, and finally taking action. SageMaker is an easy and low cost service for machine learning. Amazon also provides built-in algorithms such as Linear Learner, Factorization Machines, XGBoost, Image Classification etc. 
Flexible Frameworks 
You can use the built-in algorithms as well as you can create your own. You can also integrate SageMaker with other machine learning libraries like TensorFlow, PyTorch, and Apache MXNet. 
Machine Learning APIs 
By using Machine Learning APIs, users can easily implement machine learning whether adept with machine learning or not. Amazon also offers high-level APIs beside full development platform. Amazon Lex, Amazon Transcribe, Amazon Polly, Amazon Comprehend, and Amazon Translate are speech and text processing APIs. Amazon also provides APIs for image and video analysis known as Amazon Rekognition. 
Compute Options in Amazon Machine Learning 
You can use GPUs for compute-intensive deep learning, high-memory instance for running inference, and FPGAs for specialized hardware acceleration. AWS offers numbers of EC2 instances in accordance with the different machine learning requirements. 
Other Important Aspects 
When you perform machine learning, you also want security, data store, and analytics services. Amazon Web Services is a platform that provides all the requirements to support your machine learning workloads. It provides Amazon S3 and Amazon S3 Glacier for storage purpose, Amazon Redshift for analytics, as well as a secured service platform. 
Want to start Machine Learning? All you have to do is register yourself in AWS and use Amazon Machine Learning services to avail the benefits. 
Written by 
Written by",IP Specialist,2019-08-29T10:15:01.741Z
Effective Approaches for Time Series Anomaly Detection | by Aditya Bhattacharya | Towards Data Science,"In the current situation of Covid19, the whole world is experiencing unprecedented scenarios everywhere, which often everyone is terming as the “new normal”. But before becoming the “new normal”, these abnormal or anomalous outcomes can result in positive or negative impact for any organization and are important to keep track of, for formulating a long term business strategy. So, anomaly detection in every domain will be an important topic of discussion and the knowledge about the effective ways to perform anomaly detection will be an important skill to have for any Data Scientist and Data Analyst. 
Before we even deep dive, we must clarify, what exactly is an anomaly? The definition of Anomaly, can vary from one domain to another. In the cover picture that we see above, the lioness is an anomaly among the herd of zebras. So, technically in a generalized way, we can say that an anomaly is an outlier data point, which does not follow the collective common pattern of the majority of the data points and hence can be easily separated or distinguished from the rest of the data. 
Now, coming to the topic in scope for today, Time Series Anomaly Detection. We will talk about the what, the why and the how part of time series anomaly detection in this article. For a detailed coding walk through, please visit my website. 
In time series data, an anomaly or outlier can be termed as a data point which is not following the common collective trend or seasonal or cyclic pattern of the entire data and is significantly distinct from rest of the data. By significant, most data scientists mean statistical significance, which in order words, signify that the statistical properties of the data point is not in alignment with the rest of the series. 
From the above time series plot, we can see that, 5 data points which are significantly different from the overall series is highlighted in red circle. So these 5 anomaly data points does not follow the overall sinusoidal nature of the time series and hence can be termed as time series anomaly. 
As mentioned before, in order to estimate the “new normal” and regroup and restructure business strategies and decision making process, it is very important to keep track of anomalies in every sector and there is an urgent and continuous need to study these anomalies in details. The role of data scientists becomes not only critical in these tough times, but it becomes a natural expectation for Data Scientists to come up with an approach to track, study and analyze the anomaly data points and derive meaningful information for the business. From Sales and Marketing, to Supply Chain and Manufacturing, every stage of the business requires sufficient information, especially about these anomalies to shape their process and maximize productivity and outcome. Hence, wherever we have a common pattern in data, especially for time series data, it is very important to segregate the outliers and spare time and attention to study these. 
Now, coming to the most important part of this article, about the how part of doing time series anomaly detection. From a very high level and in a very generic way, time series anomaly detection can be done by three main ways: 
In this section, we will be only focusing on techniques and in the next article or post we will experiencing the exact code part of the algorithms and effective ways of how to program these approaches in Python. 
One way of doing anomaly detection with time series data is by building a predictive model using the historical data to estimate and get a sense of the overall common trend, seasonal or cyclic pattern of the time series data. Using the predictive model to forecast future values and based on the error rates (which can be calculated using MAPE — Mean Absolute Percentage Error), we can come up with a confidence interval, or a confidence band for the predicted values and any actual data point which is falling beyond this confidence band is an anomaly. For building the predictive model, popular time series modelling algorithms like ARIMA, SARIMA, GARCH, VAR or any Regression or Machine Learning and Deep Learning based algorithm like LSTM can also be used effectively. The main advantage of this approach is finding local outlier but the main disadvantage is, this approach highly depends on the efficiency of the predictive model. Any loop hole in the predictive model can give false positives and false negatives. 
This approach is probably the most favorite approach for statisticians and mathematicians and has been in use effectively in the field of economics and finance sectors. Generating a statistical model or profile of the given data can be the fastest and the most useful approach, as this method can provide a more controlled and explainable outcome. This can be done by calculating statistical values like mean or median moving average of the historical data and using a standard deviation to come up with a band of statistical values which can define the uppermost bound and the lower most bound and anything falling beyond these ranges can be an anomaly. As I said, this approach is very handy and can always be the baseline approach, instead of going with any sophisticated and complex methods which require a lot of fine tuning and may not be explainable. This is very effective for highly volatile time series as well, as most of the time series predictive model algorithms fail when the data is highly volatile. But the main drawback of this approach is detecting the local outliers. As we see in the previous figure, out of five obvious anomaly points only 2 most significant anomaly points got detected. 
Unsupervised approaches are extremely useful for anomaly detection as it does not require any labelled data, mentioning that a particular data point is an anomaly. So, clustering algorithms can be very handy for time series anomaly detection. Now, one common pitfall or bottleneck for clustering algorithms for anomaly detection is defining the number of clusters, which is required by most clustering algorithm as an input. Although there are many techniques of estimating the number of clusters, but with time series data, it is not feasible to dynamically estimate the number of clusters for each series. That is when Density Based Spatial Clustering of Applications with Noise (DBSCAN) becomes the natural choice. DBSCAN does not require any predefined number of clusters and has only two parameters (minimum number of points in a cluster and epsilon, distance between clusters), so it is very easy to tune and very fast in performance. DBSCAN becomes the most obvious choice for doing anomaly detection because of these benefits and it does not group all data points to a cluster like conventional hard clustering techniques like K-Means. DBSCAN does not group anomaly or outlier data point to any cluster and thus it becomes very easy to apply. Also, DBSCAN helps to map the “new normal” which most of the other approaches may fail. But there are some drawbacks with DBSCAN. Some anomaly data points if it repeats many times in a sparse interval, these might not be mapped as an anomaly according to DBSCAN. So, in such a case, going with a rolling window based DBSCAN helps to map these local anomalies more effectively. 
Thus, this brings us to the end of this article. In my personal website, I have written a detailed article discussing and providing the exact python codes of how to apply these techniques and explain the common advantages and disadvantages in more depth. Till then, please clap and motivate me to discuss more and share my findings with the community. Hope I was able to help! Keep following: https://medium.com/@adib0073 and my website: https://www.aditya-bhattacharya.net/ 
Hands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look 
Written by 
Written by",Aditya Bhattacharya,2020-07-22T03:56:43.274Z
Word2vec – Towards Data Science,,NA,NA
Design Systems Will Change UX/UI Jobs | by Debbie Levitt | Delta CX | Medium,"And I can’t wait. 
Right now, many jobs that want someone with talent, skills, and expertise in CX or UX also expect them to be strong with visual design and UI. The misconceptions that “everybody’s a visual designer” or that “every designer is great at UX” will melt away as more companies use visually-designed component libraries and design systems. 
When you’re not using a component library or a design system, someone has to visually design every state of every screen. Think of this as The Waterfall or Assembly Line of UX: your CX or UX Designer did sketches or wireframes, possibly without final visual design. The same person or a separate specialized visual designer then applied the look, feel, brand, colors, typography, mood, and other aesthetics. 
Rather than going into the history of why companies combined UX and UI, how much money trade schools and bootcamps make off trying to quickly make you a “UX/UI Designer,” or the many downsides of giving one person two jobs (especially where they might not have talent, skill, expertise, or good process in both), let’s just look at the future. 
A design system has our reusable components, elements, and style guides. It’s the language of our brand and/or product including how it looks, feels, and sounds (brand voice). If you’re doing it right, everybody working for that brand or product line is using this design system, which has been socialized around the entire company or division. Bonus points for going Atomic with it. No bonus points for it being fully accessible because you must do that anyway. 
Let’s imagine that your company has a design system. Perhaps it’s “co-owned” by CX/UX and Marketing so that it covers every aspect of our voice and visual language across digital and non-digital channels and experiences. This means No More UX Waterfall: after I do a medium-fidelity prototype, I don’t need someone else to then visually design my tested and finalized interaction designs. It won’t matter if I’m great at visual designer or not because I only need to drop the right elements in the right places. I don’t need to pass my work to the next step on the assembly line and have someone else do the aesthetics on the entire screens or pages. 
Quick note: as you socialize the design system, you might have to explain that you can’t replace CX/UX specialists by dropping components on a screen. I’ve seen some companies think that once they have a design system, they don’t need CX, UX, or interaction designers because anybody can mix and match components and make a great interface or experience. 
Once you have a design system and jobs have started shifting, companies will actually have to assess the CX/UX skills of applicants. You won’t be able to look at a visual design portfolio or pretty wireframes and assume they can “do the UX.” Because with a design system, anybody can make a wireframe pretty using our pre-defined components. You’ll actually have to assess CX/UX talent, skill, knowledge, abilities, approaches, processes, depth, and quality. 
Then, we won’t need a “UX/UI” job. Or the “UI” part will mostly mean “can you drop things from our component library or design system into the right place?” But we will still need visual designers… so what will everybody do? 
In my book, Delta CX, I suggest that we look at changing the roles that are in a CX/UX department. Also note I’m using the term “CX” and phasing out “UX.” There are too many reasons to go into that here, but mostly, we care about and are involved in the entire brand experience — products and services — not just an isolated digital experience. Plus UX is so badly misunderstood and often disrespected that it benefits us to press the reset button and shift away from it. 
Here are the short versions of who we would hire and what they would do. 
CX Architect. Previously your UX Designer or Information Architect. Shifting away from “designer” for people who are not doing art or visual design will help better explain what we are really about. Because not all “designers” are “designers.” We are planning the building; we are architects. This role encompasses mostly information architecture, interaction design, and the realistic prototyping we need for our ideation and better testing. 
CX Researcher. Previously your UX Researcher but also sometimes combined into other “designer” jobs. Our research work is so important yet so often skimped on. This role includes all areas of research as well as testing. 
CX Data Scientist/Analyst. Ongoing collection, interpretation, and reporting on CX data including app ratings, customer support tickets, and metrics. Inventing new ways to measure the customer experience including new behavior models. Measuring successes and failures related to KPIs and other project or initiative goals. 
CX Visual Designer. Previously your Visual Designer, UI Designer, or combined into various UX jobs. This person owns/co-owns/updates the design system and also fills in additional design as needed such as creating or selecting illustrations, photos, and logos. They should be experts in accessibility and understand foundational UCD and human factors concepts so they can bring those into their work. 
CX Writer. Previously UX Writer or sometimes just Copywriter. Excellent at the economy of words while zeroing in on the main pieces of information a potential or current customer needs to know at this moment. 
These roles are designed to be collaborators with each other. Just because we are specialists doesn’t mean we silo ourselves within our own department. Another quick note from the Delta CX model: we suggest that CX Architects work in pairs for a few reasons (short versions): 
Design systems help us move back towards specialties in CX and UX, and back towards prioritizing quality over speed. With most of the visual design having been “pre-done” and standardized, each role can focus on what they do best. Jobs can stop turning down great CX Researchers because they didn’t have a visual design portfolio (yes, this happens). Jobs can stop having great visual designers who are poor at UX try to do UX work. 
Authors and false gurus who believe the future is about hybridizing jobs, being unicorns, and being Jacks-Of-All-Trades tend to forget that most of these workers are fantastic at one role but often mediocre (or even poor) at their other “trades.” Our products and services are too important, and our work is too mission critical to be left to people who are poor to mediocre at anything. 
While UX practitioners are too often seen as order takers who just sketch pretty screens, it’s easy to dismiss what we do. If we are measuring the ROI of CX/UX and holding entire teams accountable when customers are unhappy, companies will start to shift back towards wanting specialists to do this important work. 
Let’s create change. Thank you, design systems, for helping to usher in a next wave of change. 
Follow me on LinkedIn and subscribe to our YouTube channel to keep up with all of the livestream podcasts I do on various CX and UX topics. 
Delta CX is a refreshing model bringing CX and UX together in task and in name with the key goal of improving the products, services, and experiences (PSE) that we offer our potential and current customers. Rather than following trends or drinking the snake oil, Delta CX presents a time-tested, thorough approach that helps you establish values, vision, strategies, and goals. 
Great PSE require the right teams and strategies in place to proactively predict and mitigate the risk of delivering wrong or flawed PSE. Adopting Delta CX means we all finally speak the same language, from tasks and deliverables to job titles and required skills to where CX fits into Agile organizations to processes and teams. Calculate the ROI of investing more time and resources into building the right PSE the first time. 
Written by 
Written by",Debbie Levitt,2020-01-31T09:42:36.303Z
4 Types of Distance Metrics in Machine Learning | by Pulkit Sharma | Analytics Vidhya | Medium,"Distance metrics are a key part of several machine learning algorithms. These distance metrics are used in both supervised and unsupervised learning, generally to calculate the similarity between data points. 
An effective distance metric improves the performance of our machine learning model, whether that’s for classification tasks or clustering. 
Let’s say we want to create clusters using the K-Means Clustering or k-Nearest Neighbour algorithm to solve a classification or regression problem. How will you define the similarity between different observations here? How can we say that two points are similar to each other? 
This will happen if their features are similar, right? When we plot these points, they will be closer to each other in distance. 
Hence, we can calculate the distance between points and then define the similarity between them. Here’s the million-dollar question — how do we calculate this distance and what are the different distance metrics in machine learning? 
That’s what we aim to answer in this article. We will walk through 4 types of distance metrics in machine learning and understand how they work in Python. 
Let’s start with the most commonly used distance metric — Euclidean Distance. 
Euclidean Distance represents the shortest distance between two points. 
Most machine learning algorithms including K-Means use this distance metric to measure the similarity between observations. Let’s say we have two points as shown below: 
So, the Euclidean Distance between these two points A and B will be: 
Here’s the formula for Euclidean Distance: 
We use this formula when we are dealing with 2 dimensions. We can generalize this for an n-dimensional space as: 
Where, 
Let’s code Euclidean Distance in Python. This will give you a better understanding of how this distance metric works. 
We will first import the required libraries. I will be using the SciPy library that contains pre-written codes for most of the distance functions used in Python: 
These are the two sample points which we will be using to calculate the different distance functions. Let’s now calculate the Euclidean Distance between these two points: 
This is how we can calculate the Euclidean Distance between two points in Python. Let’s now understand the second distance metric, Manhattan Distance. 
Manhattan Distance is the sum of absolute differences between points across all the dimensions. 
We can represent Manhattan Distance as: 
Since the above representation is 2 dimensional, to calculate Manhattan Distance, we will take the sum of absolute distances in both the x and y directions. So, the Manhattan distance in a 2-dimensional space is given as: 
And the generalized formula for an n-dimensional space is given as: 
Where, 
Now, we will calculate the Manhattan Distance between the two points: 
Note that Manhattan Distance is also known as city block distance. SciPy has a function called cityblock that returns the Manhattan Distance between two points. 
Let’s now look at the next distance metric — Minkowski Distance. 
Minkowski Distance is the generalized form of Euclidean and Manhattan Distance. 
The formula for Minkowski Distance is given as: 
Here, p represents the order of the norm. Let’s calculate the Minkowski Distance of the order 3: 
The p parameter of the Minkowski Distance metric of SciPy represents the order of the norm. When the order(p) is 1, it will represent Manhattan Distance and when the order in the above formula is 2, it will represent Euclidean Distance. 
Let’s verify that in Python: 
Here, you can see that when the order is 1, both Minkowski and Manhattan Distance are the same. Let’s verify the Euclidean Distance as well: 
When the order is 2, we can see that Minkowski and Euclidean distances are the same. 
So far, we have covered the distance metrics that are used when we are dealing with continuous or numerical variables. But what if we have categorical variables? How can we decide the similarity between categorical variables? This is where we can make use of another distance metric called Hamming Distance. 
Hamming Distance measures the similarity between two strings of the same length. The Hamming Distance between two strings of the same length is the number of positions at which the corresponding characters are different. 
Let’s understand the concept using an example. Let’s say we have two strings: 
“euclidean” and “manhattan” 
Since the length of these strings is equal, we can calculate the Hamming Distance. We will go character by character and match the strings. The first character of both the strings (e and m respectively) is different. Similarly, the second character of both the strings (u and a) is different. and so on. 
Look carefully — seven characters are different whereas two characters (the last two characters) are similar: 
Hence, the Hamming Distance here will be 7. Note that larger the Hamming Distance between two strings, more dissimilar will be those strings (and vice versa). 
Let’s see how we can compute the Hamming Distance of two strings in Python. First, we’ll define two strings that we will be using: 
These are the two strings “euclidean” and “manhattan” which we have seen in the example as well. Let’s now calculate the Hamming distance between these two strings: 
As we saw in the example above, the Hamming Distance between “euclidean” and “manhattan” is 7. We also saw that Hamming Distance only works when we have strings of the same length. 
Let’s see what happens when we have strings of different lengths: 
You can see that the lengths of both the strings are different. Let’s see what will happen when we try to calculate the Hamming Distance between these two strings: 
This throws an error saying that the lengths of the arrays must be the same. Hence, Hamming distance only works when we have strings or arrays of the same length. 
These are some of the similarity measures or the distance matrices that are generally used in Machine Learning. 
Originally published at https://www.analyticsvidhya.com on February 25, 2020. 
Written by 
Written by",Pulkit Sharma,2020-02-26T08:51:47.598Z
An Intro to the Page Rank Algorithm | by Zachary Marszal | Medium,"An Intro to the Page Rank Algorithm 
The Google Search Engine has become an integral part of today as we’ve grown into the age of instant access to information. From searches to help with your academics to just simple searches for random facts, Google has become the popular way of gaining that information. So much so that “Googling” information has become synonymous with searching information. But what are the ideas behind this powerful search engine? 
First, it is necessary to have a bit of a background of Graph’s. Not the typical graph from your high school Algebra class, but Computer Science Graphs. A graph is a structure that contains nodes and edges. Edges link nodes together and form a relationship between two nodes. There are many different types of Graphs, but for the purpose of this article we only need directed graphs. An edge is directed if it unidirectional — you can get from one node to another, but not back to the starting node with that same edge. 
Now that we have a little background, we can move onto the Page Rank Algorithm. When you type a search into a search bar, a collection of web pages is gathered. We set out to rank the pages by importance using how much the page is referenced by other web pages. We measure this with the quality links on each page. So the basic structure is each web page is a node, and a link to another page is a directed edge from the current web page to the web page referenced by the link. This algorithm ranks the pages by which page is referenced the most, or is considered “most important”. 
It’s best to see the implementation through example. Say we have 5 webpages in our search with the following connections: 
Looking at Web Page E, E references A and D and is referenced by B and A. So starting the algorithm, each node starts with a value of 1/n, which in this case, n=5 for the number of nodes. For each iteration, each node sends out all of its value to each of the nodes it references(value/outgoing edges to each referenced node). So Web Page E sends out a value of 1/10 to node A and a value of 1/10 to node D. We do this for each node, and then add up how much each node received from other nodes for its new value. So after the first iteration the new values for the graph look like this: {A: 8/30, B: 8/30, C: 5/30, D: 3/30, E: 6/30}. Note that after each iteration, the sum of all the nodes values is, and will always equal 1. Running this same process through many iterations will cause the node values to eventually stabilize around a number. Iterating through this example 75 times gives us a sufficient amount of iterations to see the node values stabilize at the values: {A: 0.263, B: 0.212, C: 0.171, D: 0.118, E: 0.237}. With these values, we can see that the display order will be: A, E, B, C, D. 
Now, I will go over one way to implement this example into code. I started by creating an array to hold the relations between the nodes, and another array to hold the value, # of nodes the current node references, and the amount of value received from other nodes in each iteration. For our example above it looks like this: 
Next, we need to iterate through each node and its relations in the adjacencyGraph. If a relation exists(1), then we need to send a value to the node that the current node has a relation with. This is done by looking at the current node in the valueGraph and sending its (value/adjNodes) to the adjacent nodes nextVal. Next, we need to go through each node and change its nextVal into its value, and that completes the iteration. Lastly, we can choose how many times we would like to iterate through the algorithm. Implementing this looks like: 
Running the Page Rank Algorithm through 75 times gives us the resulting valueGraph array: 
Google has many different search algorithms, but the Page Rank algorithm is the most widely known. While this is a simplified version of the actual algorithm Google uses, it gives you a good idea of what goes into every search we make into the Google Search Engine. 
Written by 
Written by",Zachary Marszal,2019-03-29T19:46:39.039Z
Genetic Algorithm and its Wide Spectrum | by Shaashwat Agrawal | The Startup | Medium,"Genetic algorithms are a part of evolutionary algorithms used for searching and optimization problems. They are an algorithm inspired by the evolution happening naturally and work on the motto “Survival of the Fittest”. This article will cover some basic implementation of this algorithm but more importantly a wider view of its potential and applications. 
John Holland introduced this algorithm in 1960 and it has been famous ever since. The algorithm itself works in a way similar to the natural order of evolution, from parent to child. 
Each individual can be distinguished by their physical characteristics which are defined by certain genes that vary for each person. These genes are passed on from the parent to their child and hence the child itself closely resembles their parent(s). Each gene in an individual, an animal, or plant has a certain role. For example, a certain gene may define red hair whereas the other of the same type, brown hair. Also, the number of genes that define every living being is different. Humans are said to have about 20,000–25,000 genes. 
If you understand the above paragraph then be assured the algorithm itself is not very difficult. We will go through all details step by step’. 
Genetic algorithms are a small part of the group of evolutionary algorithms. These algorithms all get their inspiration from nature or the biodiversity around us. Some are inspired by groups/swarms of birds and insects and help solve mathematical problems relating to such issues, PSO(Particle Swarm Optimization) algorithm being a good example. Another good example is the ABC(Artificial Bee Colony Algorithm) which was inspired by the colony of honey bees and helps solve problems like the XOR problem. These types of algorithms are mainly categorized under Swarm Intelligence. 
Some algorithms like the Selfish Gene algorithm are very similar to the genetic algorithm but vary in methodology. GA focuses on an individual solving the problem but selfish gene as the name suggests concentrates more on the genes. Unlike GA it does not have a real population and only constitutes of genes. 
Cuckoo Search is yet another such example. It is an optimization algorithm that was found in 2009, inspired by the awkward behavior of the cuckoo to lay eggs in other bird’s nest. Theoretically, the best nest carries forward the egg to produce a better generation of the cuckoo, and the process continues. 
The genetic algorithm starts with an initial population. A population is a group of individuals where each individual tries to solve a certain problem and overcome it. Every individual is distinguished by the genes it contains. Every gene has a particular role in completing the problem. 
A population at a certain point in time is called a generation. It consists of individuals with similar problem-solving capabilities yet variable characteristics. If a particular generation cannot solve the problem, a new one is created from the experiences(genes) of the past ones through mating. Mating or Cross-over is the process of forming a new individual using two parents from the past generation. 
Some individuals of the old generation who are exceptional at solving the problem are directly promoted to the next, these individuals are called Elites and the process, Elitism. After forming the new generation, each individual is mutated. The mutation is the process of randomizing some genes in order to adapt better in the future. 
After every generation tries to solve the problem, some excel, some don't. The ones who do, are selected to mate and form the new generation. These parents are chosen since they have good fitness or a good ability to solve the problem. 
This ends the basic implementation of the code in python of genetic algorithm. I hope you understood some of the key terminologies required to work on these algorithms. 
The two main applications of genetic algorithms can be categorized as optimization problems and searching problems. It may not seem like much but these two domains cover so many events around us. Take an example of the school/college timetable that you follow. It is no more manually generated but done using algorithms that can easily be optimized by genetic algorithms. Now we will go over some concepts and examples of where genetic algorithms are used. 
Optimization is a very common and general issue that is required in every field today. Imagine you build an algorithm that takes an hour to perform the job or performs it incompletely. Optimization can be plainly stated as the search for certain parameters and flows to make work smoother and faster. 
Multimodal functions are functions which have various local optima. A local optimum is a solution local to that are. These functions have more than one local as well as global solutions. This property makes it harder to find good solutions to problems but let's look at it with an example. 
A college staff committee needs to build a time table for their new session. Manual methods are no longer used but algorithms that do the same task but much better and faster. There can be many time tables which are suitable and manually a team can produce a good 1 but running a genetic algorithm the staff could obtain many suitable ones within no time. 
Mathematical optimization is a field of applied mathematics that mostly deals with real-life problems such as optimization of industrial processes, scheduling processes, manufacturing processes, and also in revenue optimization. Many operators throughout the globe use genetic algorithms to optimize the generation and delivery of power. 
Route Optimization is the process of finding the most cost-effective route from a source to destination with certain parameters. The path with the least traffic, least pollution could be some of the factors affecting the cost of the road. 
The Travelling Salesman Problem using a genetic algorithm is the best example of such optimization. This problem asks a simple optimization question “Given a list of cities and the distances between each pair of cities, what is the shortest possible route that visits each city and returns to the origin city?”. This problem has been optimally solved by genetic algorithm and problems like these can also be done in a similar manner. 
Neuroevolution is probably the most used filed off genetic algorithms. It is a major field of AI that combines neural networks with genetic algorithms. The role of the neural networks is to act as the brain of the application while the GA acts as the optimizer, optimizing the training process. 
Have you heard of NEAT? NeuroEvolution of Augmenting Topologies is a famous algorithm that uses Artificial Neural networks with genetic algorithms to perform several AI-related projects today. Flappy birds or the google T-rex game have been reproduced using this NEAT by many developers. 
In computational complexity, searching is a computational problem that requires finding a path from start to the end, generally in a directed graph. It may sound similar to route optimization but the aim of both is completely different and hence the approach too. 
Genetic Algorithms have been seen as search procedures that can quickly locate high-performance regions of vast and complex search spaces, but the output may not be very accurate. In many pieces of research today it has been used in local search approaches and is called Local Genetic Algorithms. 
Evolutionary Heuristic A* Search is a great example of genetic algorithms in heuristic searches. Search algorithms like A* need a good heuristic function to perform optimally but these functions tend to get very complex to get accurate results and give huge time delays. Genetic Algorithms come in very handy in these cases as they can automatically be optimized to provide good accuracy as well as low computation. 
Written by 
Written by",Shaashwat Agrawal,2020-07-10T21:10:45.519Z
Understanding Random Forest. How the Algorithm Works and Why it Is… | by Tony Yiu | Towards Data Science,"A big part of machine learning is classification — we want to know what class (a.k.a. group) an observation belongs to. The ability to precisely classify observations is extremely valuable for various business applications like predicting whether a particular user will buy a product or forecasting whether a given loan will default or not. 
Data science provides a plethora of classification algorithms such as logistic regression, support vector machine, naive Bayes classifier, and decision trees. But near the top of the classifier hierarchy is the random forest classifier (there is also the random forest regressor but that is a topic for another day). 
In this post, we will examine how basic decision trees work, how individual decisions trees are combined to make a random forest, and ultimately discover why random forests are so good at what they do. 
Let’s quickly go over decision trees as they are the building blocks of the random forest model. Fortunately, they are pretty intuitive. I’d be willing to bet that most people have used a decision tree, knowingly or not, at some point in their lives. 
It’s probably much easier to understand how a decision tree works through an example. 
Imagine that our dataset consists of the numbers at the top of the figure to the left. We have two 1s and five 0s (1s and 0s are our classes) and desire to separate the classes using their features. The features are color (red vs. blue) and whether the observation is underlined or not. So how can we do this? 
Color seems like a pretty obvious feature to split by as all but one of the 0s are blue. So we can use the question, “Is it red?” to split our first node. You can think of a node in a tree as the point where the path splits into two — observations that meet the criteria go down the Yes branch and ones that don’t go down the No branch. 
The No branch (the blues) is all 0s now so we are done there, but our Yes branch can still be split further. Now we can use the second feature and ask, “Is it underlined?” to make a second split. 
The two 1s that are underlined go down the Yes subbranch and the 0 that is not underlined goes down the right subbranch and we are all done. Our decision tree was able to use the two features to split up the data perfectly. Victory! 
Obviously in real life our data will not be this clean but the logic that a decision tree employs remains the same. At each node, it will ask — 
What feature will allow me to split the observations at hand in a way that the resulting groups are as different from each other as possible (and the members of each resulting subgroup are as similar to each other as possible)? 
Random forest, like its name implies, consists of a large number of individual decision trees that operate as an ensemble. Each individual tree in the random forest spits out a class prediction and the class with the most votes becomes our model’s prediction (see figure below). 
The fundamental concept behind random forest is a simple but powerful one — the wisdom of crowds. In data science speak, the reason that the random forest model works so well is: 
A large number of relatively uncorrelated models (trees) operating as a committee will outperform any of the individual constituent models. 
The low correlation between models is the key. Just like how investments with low correlations (like stocks and bonds) come together to form a portfolio that is greater than the sum of its parts, uncorrelated models can produce ensemble predictions that are more accurate than any of the individual predictions. The reason for this wonderful effect is that the trees protect each other from their individual errors (as long as they don’t constantly all err in the same direction). While some trees may be wrong, many other trees will be right, so as a group the trees are able to move in the correct direction. So the prerequisites for random forest to perform well are: 
The wonderful effects of having many uncorrelated models is such a critical concept that I want to show you an example to help it really sink in. Imagine that we are playing the following game: 
Which would you pick? The expected value of each game is the same: 
Expected Value Game 1 = (0.60*1 + 0.40*-1)*100 = 20 
Expected Value Game 2= (0.60*10 + 0.40*-10)*10 = 20 
Expected Value Game 3= 0.60*100 + 0.40*-100 = 20 
What about the distributions? Let’s visualize the results with a Monte Carlo simulation (we will run 10,000 simulations of each game type; for example, we will simulate 10,000 times the 100 plays of Game 1). Take a look at the chart on the left — now which game would you pick? Even though the expected values are the same, the outcome distributions are vastly different going from positive and narrow (blue) to binary (pink). 
Game 1 (where we play 100 times) offers up the best chance of making some money — out of the 10,000 simulations that I ran, you make money in 97% of them! For Game 2 (where we play 10 times) you make money in 63% of the simulations, a drastic decline (and a drastic increase in your probability of losing money). And Game 3 that we only play once, you make money in 60% of the simulations, as expected. 
So even though the games share the same expected value, their outcome distributions are completely different. The more we split up our $100 bet into different plays, the more confident we can be that we will make money. As mentioned previously, this works because each play is independent of the other ones. 
Random forest is the same — each tree is like one play in our game earlier. We just saw how our chances of making money increased the more times we played. Similarly, with a random forest model, our chances of making correct predictions increase with the number of uncorrelated trees in our model. 
If you would like to run the code for simulating the game yourself you can find it on my GitHub here. 
So how does random forest ensure that the behavior of each individual tree is not too correlated with the behavior of any of the other trees in the model? It uses the following two methods: 
Bagging (Bootstrap Aggregation) — Decisions trees are very sensitive to the data they are trained on — small changes to the training set can result in significantly different tree structures. Random forest takes advantage of this by allowing each individual tree to randomly sample from the dataset with replacement, resulting in different trees. This process is known as bagging. 
Notice that with bagging we are not subsetting the training data into smaller chunks and training each tree on a different chunk. Rather, if we have a sample of size N, we are still feeding each tree a training set of size N (unless specified otherwise). But instead of the original training data, we take a random sample of size N with replacement. For example, if our training data was [1, 2, 3, 4, 5, 6] then we might give one of our trees the following list [1, 2, 2, 3, 6, 6]. Notice that both lists are of length six and that “2” and “6” are both repeated in the randomly selected training data we give to our tree (because we sample with replacement). 
Feature Randomness — In a normal decision tree, when it is time to split a node, we consider every possible feature and pick the one that produces the most separation between the observations in the left node vs. those in the right node. In contrast, each tree in a random forest can pick only from a random subset of features. This forces even more variation amongst the trees in the model and ultimately results in lower correlation across trees and more diversification. 
Let’s go through a visual example — in the picture above, the traditional decision tree (in blue) can select from all four features when deciding how to split the node. It decides to go with Feature 1 (black and underlined) as it splits the data into groups that are as separated as possible. 
Now let’s take a look at our random forest. We will just examine two of the forest’s trees in this example. When we check out random forest Tree 1, we find that it it can only consider Features 2 and 3 (selected randomly) for its node splitting decision. We know from our traditional decision tree (in blue) that Feature 1 is the best feature for splitting, but Tree 1 cannot see Feature 1 so it is forced to go with Feature 2 (black and underlined). Tree 2, on the other hand, can only see Features 1 and 3 so it is able to pick Feature 1. 
So in our random forest, we end up with trees that are not only trained on different sets of data (thanks to bagging) but also use different features to make decisions. 
And that, my dear reader, creates uncorrelated trees that buffer and protect each other from their errors. 
Random forests are a personal favorite of mine. Coming from the world of finance and investments, the holy grail was always to build a bunch of uncorrelated models, each with a positive expected return, and then put them together in a portfolio to earn massive alpha (alpha = market beating returns). Much easier said than done! 
Random forest is the data science equivalent of that. Let’s review one last time. What’s a random forest classifier? 
The random forest is a classification algorithm consisting of many decisions trees. It uses bagging and feature randomness when building each individual tree to try to create an uncorrelated forest of trees whose prediction by committee is more accurate than that of any individual tree. 
What do we need in order for our random forest to make accurate class predictions? 
Thanks for reading. I hope you learned as much from reading this as I did from writing it. Cheers! 
More by me on data science and other machine learning algorithms: 
How Much Are Data Scientists Paid? 
Understanding PCA (Principal Components Analysis) 
Understanding Neural Networks 
Logistic Regression 
A/B Testing 
The Binomial Distribution 
Are Data Scientists at Risk of Being Automated? 
Hands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look 
Written by 
Written by",Tony Yiu,2019-08-14T04:06:49.957Z
"Time Series Anomaly Detection With LSTM Autoencoders | by Sarit Maitra | The Startup | Sep, 2020 | Medium","Anomaly provides evidence that actual results differ from predicted results based on ML models. We are talking about price prediction and how ML model is behaving compared to actual price data. Here anomaly is defined as a point in time where the behavior of the system is unusual and significantly different from past behavior. So, going by this definition, an anomaly does not necessarily imply a problem. An important use case is the ability to detect anomalies by analyzing and learning the time series. That means AI can be used to detect anomalous data points in the time series by understanding the trends and changes seen from historical data. 
Much of the worlds data is streaming, time-series data, where anomalies give significant information in critical situations. However, detecting anomalies in streaming data is challenging, requiring to process data in real-time, and learn while simultaneously making predictions. The underlying system is often non-stationary, and detectors must continuously learn and adapt to changing statistics while simultaneously making predictions. 
Here we will look at neural network (LSTM) implementations for use cases using time series data as examples. We will develop an anomaly detection model for Time Series data. 
Let us load Henry Hub Spot Price data from EIA. 
The common characteristic of different types of market manipulation for data scientists would be the unexpected pattern/behavior in data. 
Here, the goal is identifying an anomalous subsequence within a given long time series (sequence). 
We’ll use 95% of the data and train our model on it: 
Next, we’ll re-scale the data using the training data and apply the same transformation to the test data: 
Finally, we’ll split the data into sub-sequences with the help of a helper function. 
Our Autoencoder should take a sequence as input and outputs a sequence of the same shape.We have a total of 5219 data points in the sequence and our goal is to find anomalies. We are trying to find out when data points are abnormal. If we can predict a data point at time t based on the historical data until t-1, then we have a way of looking at an expected value compared to an actual value to see if we are within the expected range of values for time t. 
We can compare y_pred with the actual value (y_test). The difference between y_pred and y_test gives the error, and when we get the errors of all the points in the sequence, we end up with a distribution of just errors. To accomplish this, we will use a sequential model using Keras. The model consists of a LSTM layer and a dense layer. The LSTM layer takes as input the time series data and learns how to learn the values with respect to time. The next layer is the dense layer (fully connected layer). The dense layer takes as input the output from the LSTM layer, and transforms it into a fully connected manner. Then, we apply a sigmoid activation on the dense layer so that the final output is between 0 and 1. 
We also use the adam optimizer and the mean squared error as the loss function. 
This is challenging because ML algorithms, and neural networks are designed to work with fixed length inputs. Another challenge with sequence data is that the temporal ordering of the observations can make it challenging to extract features suitable for use as input to supervised learning models 
Once the model is trained, we can predict using test data set and compute the error (mae). Let’s start with calculating the Mean Absolute Error (MAE) on the training data. 
RMSE is 0.099, which is low, and this is also evident from the low loss from the training phase after 20 epochs: loss: 0.0749— val_loss: 0.0382. 
Objective is that, anomaly will be detected when the error is larger than selected threshold value. 
Looks like we’re thresholding extreme values quite well. Let’s create a DataFrame using only those: 
Finally, let’s look at the anomalies found in the testing data: 
The red dots are the anomalies here and are covering most of the points with abrupt changes to the existing spot price. The threshold values can be changed as per the parameters we choose, especially the cutoff value. If we play around with some of the parameters we used, such as number of time steps, threshold cutoffs, epochs of the neural network, batch size, hidden layer etc., we can expect a different set of results. 
Here we have show a brief overview of finding anomalies in time series with respect to stock trading. 
The main challenge related to anomaly detection is unknown nature of the anomaly. Therefore, it is impossible to use classical machine learning techniques to train the model, as we don’t have labels of time series with anomaly. One of the best machine learning methods is autoencoder based anomaly detection. 
Anomalies are crucial because they represent serious but exceptional events, and they can stimulate severe actions to be taken in a broad range of application regions. Though the stock market is highly efficient, it is impossible to prevent historical and long term anomalies. Therefore, the investors may use them to their advantage. Exploiting the anomalies to earn superior returns is a risk since the anomalies may or may not persist in the future. 
Connect me here. 
Note: The programs described here are experimental and should be used with caution for any commercial purpose. All such use at your own risk….by Author… 
Written by 
Written by",Sarit Maitra,2020-09-16T07:02:39.451Z
Microsoft security integration: Graph Security API to integrate a variety of security solutions | by Antonio Formato | Medium,"This article is the first in my Microsoft security integrations series. I’d like to share insights about how leverage existing integrations to build a fully integrated IT Security ecosystem. 
Most organizations have dozens of security solutions deployed in their environments, dealing with noisy security alerts, often with low or no context information. 
I should like to start with a quote: “Defenders think in lists. Attackers think in graphs. As long as this is true, attackers win.” — John Lambert 
Microsoft has built its graph — Intelligence Security Graph — which combines massive amount of security signals and threat intelligence feeds from Microsoft products and partners. 
At Microsoft, our security products are all powered by Intelligence Security Graph. 
Last year at RSA Conferece 2018 Microsoft announced a preview of Graph Security API (GA oct 2018), unified rest API that basically allows partners and customers to connect and integrate existing solutions to build an integrated security ecosystem. 
A more formal definition could be: “the Graph Security API can be defined as an intermediary service (or broker) that provides a single programmatic interface to connect multiple security providers. Requests to the graph are federated to all applicable providers. The results are aggregated and returned to the requesting application in a common schema.” 
Ever since Microsoft Graph Security API has been released, a lot of work has been done by vendor partners, customers and individual contributors. I will be pleased to summarize and share some of interesting use cases about leveraging integrations to better protect against threats and malicious actors. 
ISG now supports: 
Vendor partner already integrated are: 
The Microsoft Graph can be accessed through a single endpoint https://graph.microsoft.com and adopts a standard schema for authentication, based on OpenID Connect and OAuth 2.0, and the use of Web REST API with standard JSON response formats. 
Just by way of example, you can get alerts from supported security providers using Graph Explorer and following query: 
In addition, you can connect to Microsoft Graph Security with PowerShell Module, instructions here. 
Tons of other examples are available, see references below. 
Recently Microsoft has organized Graph Security Hackathon, we’ve got several submissions that covered real world security use cases. I suggest to read blog post announcing winners and appreciate the value of Security API and integrations. 
In the next posts I’ll be focus on real security integration use cases with particular emphasis on SecOps. 
Stay tuned :) 
References: 
https://github.com/JohnLaTwC/Shared/blob/master/Defenders%20think%20in%20lists.%20Attackers%20think%20in%20graphs.%20As%20long%20as%20this%20is%20true%2C%20attackers%20win.md#defenders-think-in-lists-attackers-think-in-graphs-as-long-as-this-is-true-attackers-win 
https://www.microsoft.com/security/blog/2018/04/17/connect-to-the-intelligent-security-graph-using-a-new-api/ 
https://www.microsoft.com/microsoft-365/partners/news/article/microsoft-announcements-from-rsa-conference-2018 
https://medium.com/@maarten.goet/what-is-this-microsoft-intelligent-security-graph-everybody-is-talking-about-d18d0072ea1b 
https://techcommunity.microsoft.com/t5/Security-Privacy-and-Compliance/The-Microsoft-Graph-Security-API-is-now-generally-available/ba-p/254128 
https://www.microsoft.com/security/blog/2018/07/18/jumpstart-your-microsoft-graph-security-api-integration-with-the-new-javascript-sample-app/ 
https://github.com/microsoftgraph 
https://docs.microsoft.com/en-us/graph/api/resources/security-api-overview?view=graph-rest-1.0 
https://techcommunity.microsoft.com/t5/Security-Privacy-and-Compliance/IT-Pros-can-now-easily-connect-to-Microsoft-Graph-Security-with/ba-p/399308 
Disclaimer: Opinions expressed are solely my own and do not express the views or opinions of my employer. 
Written by 
Written by",Antonio Formato,2019-04-12T13:12:09.044Z
Hidden Markov Model. Hidden Markov Model (HMM) is a… | by Eugine Kang | Medium,"Hidden Markov Model (HMM) is a statistical Markov model in which the system being modeled is assumed to be a Markov process with unobserved (i.e. hidden) states. 
Hidden Markov models are especially known for their application in reinforcement learning and temporal pattern recognition such as speech, handwriting, gesture recognition, part-of-speech tagging, musical score following, partial discharges and bioinformatics. 
Terminology in HMM 
The term hidden refers to the first order Markov process behind the observation. Observation refers to the data we know and can observe. Markov process is shown by the interaction between “Rainy” and “Sunny” in the below diagram and each of these are HIDDEN STATES. 
OBSERVATIONS are known data and refers to “Walk”, “Shop”, and “Clean” in the above diagram. In machine learning sense, observation is our training data, and the number of hidden states is our hyper parameter for our model. Evaluation of the model will be discussed later. 
T = don’t have any observation yet, N = 2, M = 3, Q = {“Rainy”, “Sunny”}, V = {“Walk”, “Shop”, “Clean”} 
State transition probabilities are the arrows pointing to each hidden state. Observation probability matrix are the blue and red arrows pointing to each observations from each hidden state. The matrix are row stochastic meaning the rows add up to 1. 
The matrix explains what the probability is from going to one state to another, or going from one state to an observation. 
Initial state distribution gets the model going by starting at a hidden state. 
Full model with known state transition probabilities, observation probability matrix, and initial state distribution is marked as, 
How can we build the above model in Python? 
In the above case, emissions are discrete {“Walk”, “Shop”, “Clean”}. MultinomialHMM from the hmmlearn library is used for the above model. GaussianHMM and GMMHMM are other models in the library. 
Now with the HMM what are some key problems to solve? 
Problem 1 in Python 
The probability of the first observation being “Walk” equals to the multiplication of the initial state distribution and emission probability matrix. 0.6 x 0.1 + 0.4 x 0.6 = 0.30 (30%). The log likelihood is provided from calling .score. 
Problem 2 in Python 
Given the known model and the observation {“Shop”, “Clean”, “Walk”}, the weather was most likely {“Rainy”, “Rainy”, “Sunny”} with ~1.5% probability. 
Given the known model and the observation {“Clean”, “Clean”, “Clean”}, the weather was most likely {“Rainy”, “Rainy”, “Rainy”} with ~3.6% probability. 
Intuitively, when “Walk” occurs the weather will most likely not be “Rainy”. 
Problem 3 in Python 
Speech recognition with Audio File: Predict these words 
[‘apple’, ‘banana’, ‘kiwi’, ‘lime’, ‘orange’, ‘peach’, ‘pineapple’] 
Amplitude can be used as the OBSERVATION for HMM, but feature engineering will give us more performance. 
Function stft and peakfind generates feature for audio signal. 
The example above was taken from here. Kyle Kastner built HMM class that takes in 3d arrays, I’m using hmmlearn which only allows 2d arrays. This is why I’m reducing the features generated by Kyle Kastner as X_test.mean(axis=2). 
Going through this modeling took a lot of time to understand. I had the impression that the target variable needs to be the observation. This is true for time-series. Classification is done by building HMM for each class and compare the output by calculating the logprob for your input. 
Mathematical Solution to Problem 1: Forward Algorithm 
Alpha pass is the probability of OBSERVATION and STATE sequence given model. 
Alpha pass at time (t) = 0, initial state distribution to i and from there to first observation O0. 
Alpha pass at time (t) = t, sum of last alpha pass to each hidden state multiplied by emission to Ot. 
Mathematical Solution to Problem 2: Backward Algorithm 
Given model and observation, probability of being at state qi at time t. 
Mathematical Solution to Problem 3: Forward-Backward Algorithm 
Probability of from state qi to qj at time t with given model and observation 
Sum of all transition probability from i to j. 
Transition and emission probability matrix are estimated with di-gamma. 
Iterate if probability for P(O|model) increases 
Written by 
Written by",Eugine Kang,2017-08-31T21:59:26.919Z
What Is Optimisation?. Understanding The Science Of… | by Farhad Malik | FinTechExplained | Medium,"The machine and deep learning models revolve around the concept of optimisation. On top, the portfolio theory of Nobel prize winner, Harry Markowitz can be implemented by using optimisation routines. We can use optimisation to generate the portfolios with the maximum return for the minimum risk. Hence, it is pretty important to understand how optimisation works! 
This article will document the following three key points: 
Optimisation techniques help us find a solution of a function faster without gathering a large amounts of data and without using analytical techniques. 
Let’s consider that you are at the top of a hill on a foggy day. Your friends are at the bottom of the hill. They are asking you to come down. 
However, unable to see clearly due to fog, you are finding it hard to figure out the quickest path to take to reach them. 
What can you do? 
This is a form of optimisation as it will potentially take you fewer steps (iterations) to reach your friends (your objective)! 
So far so good! 
I will use the analogy explained above to explain mathematical optimisation. 
Let’s consider we want to estimate the future price of a company share. We can prepare a mathematical model (expression) that we think can model its future price. Let’s refer to the model as f(x), where x is an independent variable. 
Let’s consider that we want to find the value of x that evaluates f(x) to 0. 
What can we do? 
We can use the Newton-Raphson methodology as an optimised method to find the value of x. 
The whole idea is in iterations of four steps: 
Therefore, the formula of Newton-Raphson methodology is: 
Therefore, let’s assume our initial guess value for x = 2 and we want to find the next value of x. 
If f(x) = x³ and x = 2 then f(2) = 2³ = 8. 
And as f’(x) = 3x², f’(2) = 3(2²) = 12. As a result, the next value of x = 2 — (8/12) = 1.33 
Now we can apply the same routine where x =1.33 to get the next value of x. 
The trick is to compute the derivative of the function and solve the equation when f’(x) =0. 
The derivative of a function measures how sensitive the output of a function is to the input of the function. 
Derivative helps us understand how a function changes when we make a minimal change to the value of inputs. 
To compute the derivative of the function, we can apply the power rule. 
If a is an integer and the function is y = f(x^a) then the derivative of the function is: 
Therefore, we performed two steps: 
This is known as the power rule. 
To read more about differentiation in detail, please read this article: 
Let’s use the knowledge we have gained above about Newton-Raphson methodology to solve the f(x)=0 problem! 
Let’s assume our stock price predictive model is: 
I am going to perform the optimisation manually so that we can understand every single step. 
We need to find the value of x where f(x) = 0 by using the following 4 steps: 
Using the power rule, the derivative of our target function is: 
4. Update the value of x using this formula: 
For each value of f(x), we will determine how close we are to our target value. Our target value is this instance is 0. 
The new value of x = 6.53125 
2. Second iteration: x = 6.53125 
Within an iteration, we have dropped the value of f(x) from 1110 to 331.262 and as a result, we are now getting closer to 0. 
3. After just 18 iterations, we have reached the goal where f(x)=0. It is when x=-2.544511528 
Using optimisation methodologies, we do not need to visualise the chart at all; or gather a large set of values of x or even use an analytical or algebric approach to find the solution. 
2.2 Constrained Optimisation 
Constraints can be imposed on the optimisation. We can also use the optimisation techniques to find the minimum and maximum values of x. 
The way it works is that the optimiser attempts to find the value of our target variable x in such a way that minimises the differences between the actual and observed values by staying within the specified boundaries. 
Therefore, if our constraint is such that the value of x needs to be within -1 to 5 then we will not reach the answer of “x=-2.544511528” which we reached above. 
Think of constraints as placing boundaries on your target variable x. The constraints are essentially equations. As an instance, if the objective function is x²y then we can place a constraint that x²+y²=a² 
There are a number of libraries available in Python that can perform optimisation routines on a function such as CVXPY and SciPy. 
In this section, I will provide a quick overview of the SciPy package which is one of the key Python packages to get familiar with. 
The scipy.optimize module, within the SciPy package, offers a variety of optimisation algorithms. The module includes un/constrained algorithms, global optimisation routines, least-squares minimisation, scalar and multivariate minimisers etc. 
It offers Newton-Krylov, Newton Conjugate Gradient, SLSQP, curve fit and dual annealing algorithms amongst others. 
Let’s understand how SciPy works! 
There is a minimize function within the scipy.optimize module that performs the minimisation of a scalar function of one or more variables. The minimize function makes it easier for us to execute the required algorithm on an objective function. 
The signature of the method is: 
The key arguments are: 
To perform constrained minimisation for multivariate scalar functions, we can use the minimize function using the SLSQP algorithm. 
SLSQP stands for Sequential Least SQuares Programming. The idea is to minimise a function of x subject to functions of constraints. The constraints can be linear and non-linear. They are defined as dictionaries. 
The keys of the dictionary include: 
As an instance: 
2. If a constraint is A+ B ≥C then it’s an equality constraint and therefore represent it as A + B — C≥0 
This article documented the following three key points: 
Optimisation techniques help us find a solution of a function faster without gathering a large amounts of data and without using analytical techniques. 
I hope it helps. 
Written by 
Written by",Farhad Malik,2019-08-24T08:49:02.014Z
What similarity metric should you use for your recommendation system? | by Brett Vintch | bag of words | Medium,"There isn’t a single right answer. It depends on many things, including the model you choose and the type and distribution of your data. However, understanding the mechanics of each similarity metric from a geometric perspective can help make the process of choosing a metric more transparent. 
This article focuses on similarity metrics in Euclidean space, which are commonly used for models like Collaborative Filtering with explicit data, Approximate k-Nearest Neighbors (e.g. random projections or LSH), and Matrix Factorization. Specifically, we will look at inner product similarity, cosine similarity, Pearson correlation, and Euclidean proximity. The first three are particularly interesting because they are all variations of inner products. 
User ratings can be represented as vectors in Euclidean space. Take for example a music service where users rate artists with between 1 and 5 stars. Joe doesn’t much care for Bach, is indifferent to Taylor Swift, and loves Drake; thus, for the item set [Bach, Taylor Swift, Drake] Joe’s ratings vector is [1, 3, 5]. Two other users, Nancy and Kai, have vectors [2, 4, 3] and [5, 3 1], respectively. These user vectors are depicted in the figure above, and in this toy scenario the average rating for each user is the same: 3 stars. 
Note that in this particular example, all three users have rated all three artists; there are no null values. While this is not typical of most real-life settings where item catalogs are large, this geometric interpretation is still valid for both Collaborative Filtering and Matrix Factorization. For Collaborative Filtering algorithms, this is because we only compare users over the set of items for which each user has provided a rating (i.e. the intersection of items). Therefore, the specific subspace of items is different for every two pairs of users and there are no null values. For Matrix Factorization, this is because most existing models learn dense embeddings for every user and every item. 
The first three similarity metrics that we will consider are all variations on the inner product between user rating vectors. These metrics are commonly used in Collaborative Filtering because matrix operations are fast, and in Matrix Factorization because the interaction between users and items is usually explicitly modeled as an inner product. 
From the figure above, it should be apparent that each of the similarity metrics is some version of a normalized inner product. Cosine similarity is identical to an inner product if both vectors are unit vectors (i.e. the norm of a and b are 1). This also means that cosine similarity can be calculated by first projecting every vector to the unit sphere, and then taking the inner product between the resulting vectors. Furthermore, Pearson correlation is the same thing as cosine similarity if the vectors are centered first by removing the vector mean (denoted with a subscripted c). Then, the same practice of projecting to the unit sphere and then taking the inner product applies. 
If we view these metrics as operations on pairs of user ratings vectors, then we can summarize them as follows: 
The other obvious metric in Euclidean space is Euclidean proximity (that is, the inverse of Euclidean distance), although it has not historically been as popular as the others for Collaborative Filtering or Matrix Factorization. This may be because the operation is less efficient than inner product-based metrics, but it may also be because odd things can happen for short vectors near the origin. The nearest neighbor of a vector with a small magnitude could be a vector 180 degrees away, reflected across the origin. In cases where the vector direction doesn’t have a lot of meaning this may not be very important, but this is usually not the case for recommendation systems. 
There’s no simple rule of thumb. The best way to choose a metric is to optimize predictions offline with cross-validation, or optimize them online with an A/B test. The right metric for one data set may not be the right metric for another because of the distribution of feedback values over users, items, and interactions. However, combining the geometric interpretation above with a bit of experience allows us to make a few observations: 
If we are seeking the nearest items for a user with vectors obtained from Matrix Factorization, there are few other things to note: 
Your mileage may vary. The most important things to remember are: 1) the performance of each metric is dependent upon the distribution of your data, and 2) your specific business objectives may help clarify the choice (for example, you may be willing to accept a slight dip in accuracy if the results are more diverse). It’s worth noting that there are many other ways to measure distance and we didn’t cover metrics well suited to implicit feedback, such as Jaccard distance, or its nearest neighbor approximation MinHash. Even then, a geometric approach can yield a new perspective and aid in the decision making process. 
Written by 
Written by",Brett Vintch,2020-05-11T21:28:26.391Z
Cyber Risk – Emergynt Thinking – Medium,"How digitization, evolving threats, and passion combine to create a unique set of risks for… 
The first and only data-driven, AI-powered scenario-analysis platform for… 
Want in on the Secret? Subscribers to Emergent News just got a sneak preview of something… 
We are seeing a new wave in cyber risk, and it’s not a technical one. 
Remarks as delivered to the 16th Annual International Conference on Policy Challenges for the Financial Sector at the World Bank Group and the International Monetary Fund 
June 2, 2016 — Washington, DC 
Thank you distinguished guests and panelists for taking the time out of your busy schedules to be…",NA,NA
Making sense of topic models. Topic models can produce clusters of… | by Patrick van Kessel | Pew Research Center: Decoded | Medium,"(Related posts: An intro to topic models for text analysis, Overcoming the limitations of topic models with a semi-supervised approach and Interpreting and validating topic models) 
In my first post about topic models, I discussed what topic models are, how they work and what their output looks like. The example I used trained a topic model on open-ended responses to a survey question about what makes life feel fulfilling and examined three topics in particular: 
By looking at the top words for each topic, we can clearly see that Topic 4 is related in some way to hobbies; Topic 5 seems to pertain to charity and giving back to society; and Topic 88 has something to do with spending time pursuing and contributing to more serious things, like work. However, we also notice some quirks, which raise an important question: How do we figure out what these topics are, exactly? As it turns out, that can be a very difficult question to answer. 
Let’s say we find Topic 5 present in enough documents that it’s clearly capturing something unique about a subset of our corpus — something we’d like to analyze. Again, the most common words in Topic 5 are “giving,” “woman,” “grateful,” “charitable,” “giving community,” “nonprofit,” and “philosophy.” What exactly could such an analysis tell us? What is Topic 5? If we’re interpreting this topic as being about charity and giving, does the word “woman” really belong in it? What about “philosophy”? When working with topic models, it’s important to remember that these algorithms cannot guarantee that the words in each topic will be related to one another conceptually — only that they frequently occur together in your data for some reason. 
Text data naturally involve the variety, complexity, and context of language. So, if you think about it, it’s completely reasonable that topic modeling algorithms may pick up on noise or quirky, idiosyncratic correlations of the words in our documents. However, if we wish to use our topic model to make classifications and measure different concepts in our documents, then it’s also reasonable to be rather concerned about false positives arising from words in our topics that don’t align with how we want to interpret them. Since they don’t align with the general concept that we think the model is picking up — the concept that we want to use the model to measure — we might consider words like these to be conceptually spurious. For one reason or another, they are associated in our data with the other words in the topic, but these associations are driven by something other than semantics. That can be a result of where the data came from, perhaps, or how the data were collected. 
When analyzing survey responses, for example, conceptually spurious words can appear not because they’re related to other words in a topic semantically, but because they’re demographically correlated. That is, certain types of respondents may mention some themes more than others, and these themes can sometimes coincide together frequently in their responses even though they’re not conceptually related. 
Another topic in our model highlights this problem particularly well, consisting of the following words: “grandchild”, “grand”, “child grandchild”, “grand child”, “child”, “florida”, “child grand”, “grandchild great”, and “great grandchild”. Clearly, one of these words is not like the others. Having trained our topic model on open-ended responses from a nationally representative survey, in this analysis we’re interested in using our topics to measure what respondents talk about so that we can then characterize the population in terms of who talks about what. Of course, that means that we need to make sure that our measures don’t conflate the who and the what. So while it’s insightful to find that certain respondents commonly mention both grandchildren and Florida, we probably don’t want both of these concepts present together in a single topic. 
Conceptually spurious words may seem like a minor problem, but they can profoundly impact the results of your analysis. Even just a few extra words can have surprisingly dramatic consequences. Assuming that we want to use Topic 5 to measure whether or not a respondent mentions charity or giving, we probably want to remove several out-of-place terms: “woman,” “grateful,” and “philosophy.” Unfortunately, with most common topic modeling algorithms, this is not an easy task. The words and their distributions across each topic are difficult to modify without a fair amount of programming and statistical knowledge. For the sake of simplicity, though, let’s assume that we can roughly approximate whether a document has a topic or not simply by checking to see if it contains any of the top words in the topic. 
If we do this for the top words in Topic 5 (giving, woman, grateful, charitable, giving community, nonprofit, philosophy), we find that there are 163 responses in our corpus that match to at least one of these keywords. But if we remove the three words that seem unrelated to the concept of giving or charity (woman, grateful, philosophy), our findings change dramatically. Just 60 documents now match — less than half the number that we got from the words in the raw topic model output. 
This highlights a major limitation of topic modeling: the “topics” they produce are often not necessarily topics in the traditional sense of the term, and even one out-of-place word can make the difference between a model that accurately measures whether documents mention a coherent concept and one that vastly overestimates how often that concept actually appears in the corpus. 
Conceptually spurious words are not the only potential problem with topic models. Another issue arises from the simple fact that researchers must specify a predetermined number of topics without knowing what that number should be. Regardless of the number you pick, many of your topics may come out looking like this: 
Topic 106 above is an example of what some of us at Pew Research Center have come to colloquially refer to as an “undercooked” topic. Rather than capturing one coherent theme, we can see a variety of themes embedded in this topic, all mixed together. How might we interpret this group of words as a single theme? Is this topic about security, finance, success, health, or relationships? Here, our model appears to be grouping multiple distinct topics into a single topic. Accordingly, we may look at this topic and decide to re-run the model with a greater number of topics so it has the space to break these topics apart. 
However, in the very same model, we also have Topic 15, an example of an “overcooked” topic. Rather than being too broad or confounded to be interpretable, this topic is too granular. We might be inclined to give it a label like “mentioning your job or career,” but in that case, we would probably want to include other words like “career,” “work,” and “profession.” In fact, the word “career” actually exists elsewhere in our model in Topic 80 — another potentially “undercooked” topic — and if we want to bring those words together into a single topic, we might actually need to reduce the total number of topics to encourage the model to merge them. 
Even then, the topics may not condense the way we want them to. In fact, “job” and “career” may simply not co-occur frequently enough in our documents for the very reason we want them to be in the same topic together — they’re more or less synonyms, and likely to be substituted for one another. Depending on our data, the use of both words in a single document could be redundant and therefore uncommon, making it impossible for our model to connect the dots between the two words. Indeed, based on the output of this model, it seems that both words occur frequently alongside a wide variety of other terms (“family” and “work” in particular), but not alongside each other enough for the model to pick up on it. 
No matter which particular algorithm and parameters we’ve used, or how many topics we’ve asked a model to produce, we’ve found that undercooked and overcooked topics are largely unavoidable. Metrics like perplexity and coherence can help guide you towards a number of topics that minimizes this problem, but no “perfect” number of topics exists that can eliminate it entirely. This is because while topic modeling algorithms are great at identifying clusters of words that frequently co-occur, they do not actually understand the context in which those words occur. 
As researchers, we’re interested in measuring the distribution of certain themes in our documents, but some of those themes may be general, while others might be nuanced, detailed and specific. Topic models can find useful exploratory patterns, but they’re unable to reliably capture context or nuance. They cannot assess how topics conceptually relate to one another; there is no magic number of topics; and they can’t say how the topics should be interpreted. Of course, supervised classification algorithms can make use of more sophisticated linguistic models to overcome some of these limitations, but this requires training data that can be time-consuming and costly to collect. And the very promise of unsupervised topic modeling is that it’s fast, easy, and avoids the need for manual coding. 
In future posts, I’ll explore potential ways to overcome the limitations of topic models and assess the extent to which we can actually use them to reliably measure concepts in a corpus. 
Patrick van Kessel is a senior data scientist at Pew Research Center. 
Written by 
Written by",Patrick van Kessel,2019-11-21T22:50:47.696Z
How to Apply Distance Metric Learning to Street-to-Shop Problem | by Aleksandr Movchan | ML Review | Medium,"Let’s start with a definition of street-to-shop problem — identifying a fashion item in a user image and finding it in an online shop. Have you ever seen somebody in the street and thought “Wow, this is a nice dress, I wonder where I can buy it?” I haven’t. But for me, it was the cool task to try distance metric learning techniques. I hope that you will find it interesting too. 
Firstly, we need a dataset for it. Actually, I came to this idea after I found out that there are tons of images taken by users on Aliexpress. And I thought “Wow, I can make a search by image using this data, just for fun of course”. I have decided to focus on women’s top clothing for simplicity. 
Below is the list of the categories I used for scrapping: 
I used requests and BeautifulSoup for scrapping. Seller images can be obtained from the main page of the item, but for user’s images, we need to go through feedback pages. There is a thing called “colors” on item’s page. Color can be just item of another color or even completely other items. So we will consider different colors as different items. 
You can find the code that I have used to get all information about one item (it scraps even more than we need for our task) by link https://github.com/movchan74/street_to_shop_experiments/blob/master/get_item_info.py. 
All we need is to go through search pages by each category, take URLs of all items and use the function above to get the info about each item. 
Finally, we will have two sets of images for each item: images from a seller ( field url for each element initem['colors']) and images from users (field imgs for each element initem['feedbacks']). 
For each color, we have only one image from a seller, but it can be more than one image for each color from users (sometimes there are no images for color at all). 
Great! We got data. However, the collected dataset is noisy: 
To mitigate this problem I have labeled 5000 images into two categories: good images and noise images. In the beginning, my plan was to train a classifier for two categories and use it to clean dataset. But later I decided to leave this idea for future work and just added cleaned images to the test and validation sets. 
One of the most popular distance metric learning method is the triplet loss: 
where max(x, 0) is the hinge function, d(x, y) is the distance function between x and y, F(x) is deep neural network, M is the margin, a is the anchor, p is the positive point, n is the negative point. 
F(a), F(p), F(n) are points in high dimensional space (embeddings) produced by a deep neural network. It is worth mentioning that the embeddings often needs to be normalized to have unit length, i.e., ||x|| = 1, in order to be robust to illumination and contrast changes and for training stability. The anchor and the positive samples belong to the same class, the negative sample is the instance of another class. 
So the main idea of the triplet loss is to separate embeddings of the positive pair (anchor and positive) from embeddings of the negative pair (anchor and negative) by a distance margin M. 
But how to select the triplet (a, p, n)? We can just randomly select samples as a triplet but it causes following problems. Firstly, there are N³ possible triplets. It means that we need a lot of time to go through all possible triplets. But actually, we don’t need to do it, because after few iterations of training there will be many triplets which don’t violate the triplet constraint (give zero loss). It means that these triplets are useless for a training. 
One of the most common way of triplet selection is hard negative mining: 
Selecting the hardest negatives can in practice lead to bad local minima early on in training. Specifically, it can result in a collapsed model (i.e. F(x) = 0). In order to mitigate this we can use semi-hard negative mining. 
Semi-hard negative samples are further away from the anchor than the positive sample but they are still hard (violate triplet constraint) because they lie inside the margin M. 
There are two way to generate semi-hard (and hard) negative samples: online and offline. 
Good! We already can start train the model with the triplet loss and offline semi-hard negative mining. But! There is always a “but” in this imperfect world. We need one more trick to successfully solve street-to-shop problem. Our task is to find seller’s image most similar to user’s image. However, usually seller’s images have much better quality (in terms on lighting, camera, position) than user’s images so we have two domains: seller’s images and user’s images. In order to get efficient model we need to reduce a gap between these two domains. This problem is called domain adaptation. 
I propose a really simple technique to reduce domain gap: let’s select anchors from seller’s images, positive and negative samples from user’s images. That’s all! Simple yet effective. 
To implement my ideas and to do fast experimenting I have used Keras library with Tensorflow backend. 
I chose Inception V3 model as base CNN for my model. As usual, I initialized CNN with ImageNet weights. I have added two fully connected layers after global pooling with L2-normalization at the end of the network. The size of embedding is 128. 
We also need to implement the triple loss function. We pass the anchor, the positive/negative samples as single mini-batch and divide it into 3 tensors inside the loss function. The distance function is squared euclidean distance. 
And compile model: 
Performance is measured in terms of recall at K (R@K). 
Let’s take a look how to calculate R@K. Each user’s image from validation set was used as a query and we need to find the corresponding seller’s image. We take one query image, calculate embedding vector and search nearest neighbors of this vector among vectors of all seller’s images. We use not only seller’s images from the validation set but images from the train set too because it allows to increase the number of distractors and makes our task more challenging. 
So we have a query image and a list of the most similar seller’s images. If there is a corresponding seller image in the K most similar images then we return 1 for this query else return 0. Now we need to make it for each user’s image in the validation set and find an average of scores from each query. It will be R@K. 
As I said before I have cleaned the small amount of user’s images from noisy images. So I have measured a quality of the model on two validation datasets: full validation set and a subset of only clean images. 
Results are far from ideal, there are many things to do: 
I have made a demo of the model. You can check it out here: http://vps389544.ovh.net:5555/. You can upload your own image for search or use random image from validation set. 
Code and trained model: https://github.com/movchan74/street_to_shop_experiments 
Thanks for reading. If you’re enjoying the article, please let me know by clapping. If you want more information, you can connect with me on LinkedIn. 
Written by 
Written by",Aleksandr Movchan,2018-01-18T17:00:53.331Z
"Yes, all event messages are captured in Kafka from the applications and also using debezium… | by Piyush Kumar | Medium","Yes, all event messages are captured in Kafka from the applications and also using debezium (OLTP/MySQL : CDC Change data capture) is also captured into kafka as well for select schemas. Hence both the event stream from applications + stream changes from database are captured in Kafka! 
Written by 
Written by",Piyush Kumar,2018-01-03T04:49:34.490Z
A Comprehensive Guide to Genetic Algorithms (and how to code them) | by Rishabh Anand | Sigmoid | Medium,"Original article by Rishabh Anand 
In the mid 19th century, Charles Darwin postulated the theory of evolution and how it played a key role in enabling organisms to adapt to their environments through natural selection – a process where the fittest in a given population survive and live long enough to pass on their traits and characteristics to future generations to ensure their survival. 
Presently, Machine Learning (ML) has kicked off a new era of smarter machines capable of making better decisions compared to their rule-based counterparts from the late 90’s and early 2000’s. 
Harnessing the sheer amount of computational power we now possess, ML algorithms, specifically deep neural networks, have leveraged our large pools of data, both structured and unstructured, to deliver insights, leads, predictions and much more with a high degree of accuracy. 
State-of-the-art ML models can now classify, categorise, and generate data from scratch with a few hours/days of training. Deep Neural Networks have now proliferated into multiple domains, now being able to work with different data formats ranging from images to audio, many of such networks having surpassed human level capabilities in said areas. Below is an instance of an agent playing Atari games: 
Recently, research organisations like OpenAI and DeepMind have been dabbling with a field of Machine Learning called Reinforcement Learning. It’s a system where an agent learns and improves over time whilst interacting with an environment by making mistakes and collecting the respective rewards (either positive or negative) for the respective states. 
In this ecosystem of smart agents trying to navigate environments, genetic algorithms form a small subset of the field where semi brute-force methods are applied to create a “fit” agent that is able to “survive” (remain on top). Why do I say semi brute-force? It’s because the parameters for the agent are randomly generated so there is no pre-defined set of test values that are used for the agent. 
Genetic Algorithms (GA) work on the basic principles of evolution as it is a meta heuristic to natural selection and the various subprocesses that occur spontaneously. This involves incorporating the 4 fundamental steps of evolution: 
Together, these 4 processes, when performed on a population of agents yield the fittest agent for the task being performed. 
From this point onwards, when I mention the word “survive” or any variant of the word, I mean to say that the agent remains part of the top few agents that are viable enough to move on to the next generation. 
This refers to how strong the agent is in completing the task at hand. It quantifies how capable an agent is and this increases the probability that it will crossover with another agent in the population to possibly create stronger offspring models with traits of both the parent models. 
For different tasks, the fitness function undergoes slight modifications. However, in essence, its primary function is to play the role of a differentiator in the population that separates the stronger learners from the weaker learners. 
This process is self-explanatory in that the top few models are allowed to remain in the population while the remaining weaker ones are discarded of as they serve no purpose. 
This leaves us with empty slots in the population giving us space for the new stronger offspring on the remaining parent agents. 
This process enables two strong parents to create offspring that have the traits of both parents increasing the chances of survival. It’s a best-of-both-worlds scenario. 
During crossover, all the properties of one of the agents are split up or broken down into more fundamental units and half of them are exchanged with those of the other parent agent. Biologically speaking, a part of the genes from one parent are connected to another part of the genes from the other to create an offspring that has two sets of gene segments increasing its chances of survival in terms of fitness. 
Similar to the evolutionary process, mutation plays a key role in introducing some randomness and stochasticity into the population. By doing so, the agents that survive better (higher fitness score) with this mutation are able to pass on this survival mutation trait to future generations to ensure their survival as well. 
Similar to the Fitness function, the mutation function also needs to be finalised beforehand. This mutation can range from a point mutation that occurs in one specific location in the gene sequence (DNA) or in multiple locations. 
Either way, mutation is an integral component of the evolutionary process as our model cannot survive for long without it. 
What happens when an agent achieves the goal or objective at hand? To cater to such a situation, a stopping criteria is used. This function checks if any one of the agents has crossed a certain threshold value (hyper-parameter) or criteria in general that pertains to completing the task. 
In most cases it’s a single minimum threshold value that an agent needs to cross in order to complete the activity or task. In others, there can be a range of criteria or a group of thresholds that the model needs to cross every generation to be called the fittest/one of the fittest in the population. 
All together, these 4 processes enable the production of new, stronger, more viable offspring that are capable of surviving in an environment. 
Now that we’ve completed the refresher on genetic algorithms, a coding project is the best way to apply whatever you have learnt and strengthen your understanding of the concepts used by GAs. 
While studying about GA, I came across loads of interesting projects ranging from the famous traveling salesman problem to the knapsack problem. I even found myself discovering schedule-building agents that create the optimal schedule for events. 
While all this did seem interesting, I wanted to take things to the next level. I created a network optimisation algorithm that takes an existing Neural Network architectures hyper-parameters and enhances it to perform better with higher accuracies on the testing set. You’d probably say this is discount NASNet, but this endeavour was slightly challenging, thereby making the final outcome more wholesome and fulfilling 🎉. 
As always, the code for this article can be found here on my GitHub. 
Note: I’m experimenting with the Polacode VS Code plugin that allows users to take aesthetic polaroid pictures of the selected code in the editor. If the code in the images is too small or jarring, please do tell me in the comments below so I can go back to using GitHub Gists. 
For visibility purposes, I suggest reading the following part of the article on a large display such as a monitor or laptop. 
As mentioned above, the primary objective is to take an existing Neural Network architecture and randomly evolves it over a certain number of generations to improve it. Here, I loosely use the word ‘evolves’ as in reality, genetic algorithms are basically performing a completely random action and pray that it improves something. I have used the Keras Machine Learning library for the project. I have set the accuracy threshold (the stopping criteria) to 99.5% (really ambitious on my part) and have taken 1000 instances from the 60K training and testing set of MNIST (so predictable). 
Now, for the bigger picture, we have a Network agent that comes up with a bunch of random hyper-parameters. There is a built-in function in Keras, model.predict() , that calculates the average fitness score for each Network agent over the testing set and assigns that score as the accuracy for the secondary agent. 
Over time, the agent that has the better hyper-parameters comes out on top and is identified as the one with the best set of model hyper-parameters. 
Firstly, we begin with the Network agent. Its main function is to randomly generate a set of hyper-parameters, which will later be plugged into the model design and tested on the subset of MNIST. Here the hyper-parameters I am choosing to randomise are really basic and not too complex (the learning rate, for example), but keep in mind that it can be done! 
Moving on to the main event loop, we have the necessary functions such as fitness(), selection(), crossover(), mutate() as discussed above. The fitness() calculates the accuracy of the model with the random hyper-parameters. Some of the model architectures do not work and break the code due to dimensionality issues or other errors, which is why we place the training and evaluation functions in a try-except block.. The selection() function is pretty self-explanatory and simple to understand — the top 20% of the models are allowed to move on to the next generation. 
The important functions to look out for are the crossover() and mutation() functions. The quality of the child agents depend on the the quality of these functions ie. it creates offspring that are almost completely different from the parents and adds randomness and novelty to the gene pool and population. 
For the crossover() function, the hyper-parameters of the parents are split between the two child agents created. A proportion of the hidden units are assigned to the child agents. The function isn’t that fancy (kinda ironic based on what I mentioned earlier) but this is just a project to explain a concept and I wanted to keep things simple (also, I couldn’t think of ‘interesting’ crossover and mutation functions). 
For the Mutation function, I’ve summed the original hidden units with a random integer between 0 and 100 (as I said, I couldn’t think of an interesting function). The chances of a mutation occurring should be less that 0.1. This controls the randomness to a small but certain extent. 
Down below, the main() function spawns agents into the environment. Every generation, the above-mentioned functions are run one after the other. At the end, the model that crosses the accuracy threshold of 99.5% is considered the best 
The code snippets above are only parts of the entire thing. So I urge you to visit the repo to get the source code for the project (Stars are appreciated 😁). All together, the different components work together in unison to generate the fittest model possible to achieve the task at hand (in this case, achieve an accuracy of 99.5% and above — again really ambitious). 
It took me some time to wrap my head around the concepts mentioned above. Doing a coding project on the topic made things much more clearer as application of theory is certainly reinforcement to the learning process (pun intended). 
Feel free to clone the project repo and experiment with different architectures, crossover functions, and mutation functions! Any interesting results? Drop them down in the comments below! 
Note: If there are any errors in the code, please do highlight them here. Much appreciated! 
I have decided to (probably) make this a series on Genetic Algorithms and their applications in the real world with code examples to accompany them (if time permits). If you have any suggestions on what you want to read or want to chat in general, don’t hesitate to drop a line! You can contact me on LinkedIn! I love to talk about technology and its application in the real world! 
Shoutout to Sarvasv Kulpati for his feedback on the draft-work. Go check out his profile for articles on science, tech and philosophy! 
Here are some of the other articles I’ve written on Machine Learning and things related to up-and-coming technology: 
Till then, I’ll see you in the next one! 
Written by 
Written by",Rishabh Anand,2018-08-28T01:19:41.489Z
How I got AWS Machine Learning Certified | by Paulthi Victor | WomeninAI | Medium,"First let’s go through the information made available by AWS regarding the ML Specialty exam. I think it has a pretty good summary of what they are looking for in candidates. You can find basic details such as the duration of the exam and exam fees. There is also the link for scheduling the exam. Under the section titled “Exam Resources” you will find an Exam Guide and Sample Questions. 
What to expect on the exam: 
The Sample Questions contain 10 questions following the same pattern and topic distribution as the ML exam. I eagerly tried to answer the questions and verified my answers with the answers provided at the end of the document. By doing so I was able to identify the areas I needed to work on, and I focused my preparation time accordingly. I liked that along with the answers, a short explanation and additional links are provided. Studying these links is not required for the exam. 
The exam is intended to test candidates on using AWS platform for Machine Learning, so I broke up the requirements into two sections: AWS & ML. 
For the ML part of the exam you need to know: 
For the AWS part of the exam you need to know the following: 
Since the important aspect is the intuition behind the algorithms, you don’t have go over university level courses. If you are completely new to ML, I would suggest starting with Google developer’s ML crash course. The most popular course on ML is of course Andrew Ng’s Coursera course, although it is not necessary to complete this in order to pass the AWS ML exam, my suggestion is to listen only to the parts of the lectures where he explains the working of the algorithms and the intuition behind the algorithms. If you need deeper understanding of this domain, then the best resource is the book Pattern Recognition and Machine Learning by Christopher M. Bishop (I always have this book opened and nearby :) ). 
On the Exam Guide provided by AWS there are some links to resources. Some of it are topics from AWS service called ML(which is being replaced by Sagemaker but somehow still included in the exam), and some reInvent videos. Sagemaker is going to be one of the main focuses of the exam, I strongly suggest that you go over the Sagemaker deveoper guide. You will find questions in the exam on some nitty-gritty details found in the folds of the developer guide. Make sure you know all the built-in algorithms provided by Sagemaker, the differences between them, when exactly to choose them, and try them out! Other topics include the security aspects building a ML solution in AWS. Using the high level APIs provided by Sagemaker. Using services such as Kinesis and Glue for data ingestion and transformation. And all related services. For this, I found it best to follow a course from online learning platforms such as A Cloud Guru or Linuxacademy. 
A Cloud Guru’s course for clearing this exam is well organised with quizzes and labs. Just listening to this course will not be enough. You will have to go through all the additional resources provided during each lecture. Experience with using AWS is a definite requirement, if you don’t have that then you can obtain the familiarity of using AWS by doing these labs(and any other labs you can find, because nothing can beat hands-on experience). 
Linux Academy is in the process of releasing their course, so I was able to use only the early access portion of the course, which I found to be a good way to do a fast revision the day before the exam. 
Make sure you take the practice exam provided by AWS. The exam will be very similar, but I found the questions were much more difficult in the final exam than the practice exam. The AWS Machine Learning exam is designed to fish out beginners, so make sure you are through with the details of the AWS services and the intuition behind ML algorithms. No point in memorizing. 
Required to pass the exam: 
Not required for the exam but useful: 
One final word.. During the exam read the questions and answers carefully, pay attention to clues in the question and obvious trap answers. Try answering using techniques like method of elimination. This exam is pretty difficult but not impossible, think of it like a challenging puzzle to solve at the end of which you get a pretty badge like this 
Written by 
Written by",Paulthi Victor,2019-07-24T09:19:29.012Z
“Different types of Distances used in Machine Learning” | by Chandrima Sarkar | Medium,"The term “distance metrics” has got a wide variety of definitions among the mathematics,statistics and machine learning practitioners. As a result, those terms, concepts and their usage went a far way beyond. Distance metrics are very important in machine learning to make a good data based decision. Choosing a good distance metric is important to recognize the similarity between the contents.This blog of mine deals with the different types of distances and their uses in machine learning. 
To define Minkowski Distance,we need to learn some mathematical terms.They include the followings: 
Minkowski distance is defined as the similarity metric between two points in the normed vector space.It is represented by the formula, 
It represents also a generalized metric that includes Euclidean and Manhattan distance.We can manipulate the value of p and calculate the distance in three different ways which is also known as Lp form. 
Where it is used? 
Minkowski distance is frequently used when the variables of interest are measured on ratio scales with an absolute zero value. 
When we talk about distances, we mostly think about it as a more or less a straight line.If we think of flying from one city to another, we think about how many kilometers we have to travel by flight.These examples of distances that we can think of are examples of Euclidean distance. Basically, it measures the length of a segment that connects two points. 
Remember the Pythagorus Theorem from your mathematics classes? 
The Pythagorus Theorem gives this distance between two points. We can get the equation for Euclidean distance by substituting p=2 in the formula of Minkowski Distance. It is also called the L2 norm. 
There are some situations where Euclidean distance fails to give us the proper metric. In those cases, we need to make use of the distance functions mentioned below. 
Let’s say that we want to calculate the distance between two blocks in a city. This time, we will calculate the distance in a grid-like path which represents the various blocks in a city. 
Suppose, we want to travel from block A to block B in a city. The distance travelled from block A to block B is called the Manhattan distance. 
We can get the equation for Manhattan distance if we substitute p=1 in the formula of Minkowski Distance. It also called the L1 norm. 
Manhattan distance is also known as Taxicab Geometry, City Block Distance etc. 
Where Euclidean and Manhattan distance is used? 
Manhattan and Euclidean distances are both used in regression and classification problems. But Euclidean distance does not perform well for high dimensional data. This occurs due to something that is known as the ‘curse of dimensionality.” The curse of dimensionality refers to various phenomena that arise when analyzing and organizing data in high-dimensional spaces (often with hundreds or thousands of dimensions) that do not occur in low-dimensional settings such as the three-dimensional physical space of everyday experience. In high dimensional data Manhattan distance is preferred. Also, if you are calculating errors, Manhattan Distance is useful when you want to emphasis on outliers due to its linear nature. 
Hamming distance is a metric for comparing two binary data strings. While comparing two binary strings of equal length, Hamming distance is the number of bit positions in which the two bits are different.The Hamming distance between two strings, a and b is denoted as d(a,b). 
Calculation of Hamming Distance: 
In order to calculate the Hamming distance between two strings, we perform their XOR operation, (a⊕ b), and then count the total number of 1s in the resultant string. 
Suppose there are two strings 1101 1001 and 1001 1101. 
11011001 ⊕ 10011101 = 01000100. Since, this contains two 1s, the Hamming distance, d(11011001, 10011101) = 2. 
Use of Hamming Distance: 
Hamming distance is used in error correction of Nearest Neighbors. Nearest neighbor error correction involves first defining code words, typically denoted as C, that is known to both the source and sink. Upon identifying an incorrect code word, nearest neighbor calculates the Hamming distance between it and every code word contained in C. The code word with the smallest Hamming distance has the highest probability of being correct. 
To determine how similar two documents or any type of entity, we use cosine similarity.In order to calculate it, we need to measure the cosine of the angle between two vectors. Cosine distance and Cosine similarity are inversely proportional to each other. If the cosine distance increases cosine similarity decreases and vice versa. Therefore, two points close to each other have similar properties than the points that are far apart. 
Cosine similarity =cosθ 
Cosine distance =1-cosθ. 
In the above figure,the angle made by the two lines A and B is 45°.Therefore cosine similarity is cos 45° which is 0.53 approximately which means the points are 53% similar.The cosine distance will be (1–0.53) which equals 0.47.The value of cosine similarity must lie between -1 to +1. 
Where cosine distance and cosine similarity is used? 
Cosine distance and cosine similarity is used in Recommender Systems. 
Let us consider a movie recommender system.Taking genre “action” in x-axis and genre “comedy” in y-axis,let us take two movies like “Avengers” which falls under the genre “action” and the other one, say “Despicable Me” which falls under the genre “comedy.”So we can plot the points considering the name of the movies as (0,1) and (1,0) respectively . The point (0,1) means that the movie is not much of action but more of a comedy.Thus, the two points have 0 similarity which means cos 90°. Therefore a movie recommender system won’t recommend someone who loves movies of genre “comedy”, with a movie of genre “action”. 
Similarly,if we take movies like “Despicable Me” and “Toy Story” on y-axis having coordinates(0,0.9) and (0,1), the angle between them is 0° which means the cosine similarity will be 1(cos 0°).Therefore the system would recommend a user to watch “Despicable Me” if he/she has already watched “Toy Story”. 
At the end of this blog,we got to know about the various distance metrics and their uses in machine learning. Hope this will be helpful for people who are into Machine Learning/Data Science. 
Thanks for reading:) 
. 
Written by 
Written by",Chandrima Sarkar,2019-11-11T16:37:38.513Z
Piyush Kumar – Medium,"Building Data Products @MakeMyTrip ! Techie, open source enthusiast, hacker, web ops, infosec, data engg , startups, mHealth! nD a basketball player :) 
Data Team is one of the key horizontal function at MakeMyTrip and essentially works on developing data services/products for Personalization, context aware mobile experiences, Customer Segmentation…",NA,NA
Infosec – Data Driven Investor – Medium,The video conference company Zoom has skyrocketed to new heights and plummeted to new lows in…,NA,NA
Teri Radichel – Medium,"Cloud Security Training and Penetration Testing | GSE, GSEC, GCIH, GCIA, GCPM, GCCC, GREM, GPEN, GXPN | AWS Hero | Infragard | IANS Faculty | 2ndSightLab.com",NA,NA
"Introduction to recommendation systems and How to design Recommendation system,that resembling the Amazon | by Madasamy M | Medium","It`s all starts with a simple question: 
Why do we need recommendation systems? 
Traditionally, we would like to buy a product that friends or colleagues has suggested. 
So let`s consider another example of a book store, 
On above image book store, They had drawn special attention to New Collection books, popular books etc.… 
So the buyer can quickly choose a book, 
In a digital world using these kind of strategies as recommendation systems, The product owner can recommend items that customers might also liked and required 
A recommendation system is an extensive class of web applications that involves predicting the user responses to the options. 
A recommendation system has been a hot topic for a long time. A recommendation system 
Often termed as Recommender Systems, they are simple algorithms which aim to provide the most relevant and accurate items to the user by filtering useful stuff from of a huge pool of information base. Recommendation engines discovers data patterns in the data set by learning consumers choices and produces the outcomes that co-relates to their needs and interests. 
In Real time examples are like Amazon, they have been using a recommendation engine for suggesting the goods or products that customers might also like. 
You could have seen below image example for amazon recommendation system, 
Netflix, using for suggesting recommendation engine might also like, eventually the goal is same for all giants to accomplish the recommendation for their items to customers 
How to design a recommendation system? 
Although machine learning (ML) is commonly used in building recommendation systems, it doesn’t mean it’s the only solution. There are many ways to build a recommendation system? simpler approaches, for example, we may have very few data, or we may want to build a minimal solution fast etc.. 
Assume that, for simpler video recommendation,In such that case, based on videos a user has watched, we can simply suggest same authors videos or same publications videos. 
i.Nearest neighbor 
ii.Matrix factorization 
I will explain each method as short manner in order you to understand over all idea about designing recommendation systems 
Easiest way to build a recommendation system is popularity based, simply over all the products that are popular, So how to identify popular products, which could be identified by which are all the products that are bought most, 
Example, In shopping store we can suggest popular dresses by purchase count. 
2.Classification based 
Second way to build a recommendation system is classification model , In that use feature of both users as well as products in order to predict whether this product liked or not by the user. 
When new users come, our classifier will give a binary value of that product liked by this user or not, In such a way that we can recommend a product to the user . 
In above example using user features like Age, gender and product features like cost, quality and product history, based on this input our classifier will give a binary value user may like or not , based on that boolean we could recommend product to a customer 
Collaborative filtering: 
collaborative filtering models which are based on assumption that people like things similar to other things they like, and things that are liked by other people with similar taste. 
collaborative filtering models are two types, 
I.Nearest neighbor 
II.Matrix factorization 
let me explain each method of collaborative filtering in a nutshell, 
Nearest neighbor collaborative filtering: 
In these type of recommendation systems are recommending based on nearest neighbors, nearest neighbor approach used to find out either similar users or similar products, 
It can be looked at two ways, 
i.User based filtering 
ii.Item based filtering 
Above image source from https://medium.com/@cfpinela/recommender-systems-user-based-and-item-based-collaborative-filtering-5d5f375a127f 
User-based collaborative filtering: 
Find the users who have similar taste of products as the current user , similarity is based on purchasing behavior of the user, so based on the neighbor purchasing behavior we can recommend items to the current user. 
Item-based collaborative filtering : 
Recommend Items that are similar to the item user bought,similarity is based on co-occurrences of purchases 
Item A and B were purchased by both users X and Y then both are similar. 
Matrix factorization: 
It is basically model based collaborative filtering and matrix factorization is the important technique in recommendation system. 
let me give an abstractive explanation for matrix factorization, 
When a user gives feed back to a certain movie they saw (say they can rate from one to five), this collection of feedback can be represented in a form of a matrix. Where each row represents each users, while each column represents different movies. Obviously the matrix will be sparse since not everyone is going to watch every movies, (we all have different taste when it comes to movies). 
further more information on matrix factorization kindly refer http://www.quuxlabs.com/blog/2010/09/matrix-factorization-a-simple-tutorial-and-implementation-in-python/ 
Hybrid Recommendation systems: 
Hybrid Recommendation systems are combining collaborative and content-based recommendation can be more effective. Hybrid approaches can be implemented by making content-based and collaborative-based predictions separately and then combining them. 
source code : https://www.kaggle.com/rounakbanik/movie-recommender-systems 
prebuilt reccomendation services also availlable like, Recombee https://www.recombee.com/ 
Thanks for Reading!. 
References: 
https://www.youtube.com/watch?v=39vJRxIPSxw. 
https://www.youtube.com/watch?v=ZspR5PZemcs 
Written by 
Written by",Madasamy M,2019-02-16T07:59:00.289Z
Graph Convolutional Neural Networks- A Talk at IISc | by Sukriti Paul | ACM-W Manipal | Medium,"“ What if we could input a computer network as a graph, directly, and use neural networks to predict..maybe the top few computers that are prone to attacks by hackers?”- Dr. Tijmen Tieleman 
Being an intern at IISc has its own perks, one of them being the talks given by researchers in trending technological domains. I was fortunate to attend a talk recently, organised by the IEEE Signal Processing Society, Bangalore Chapter and the Electrical Engineering department at IISc. Current CTO of mind.ai and a former graduate from Geoff Hinton’s lab (University of Toronto), Dr. Tijmen Tieleman introduced us to the concept of graph convolutional neural networks within an hour! I’ve tried to pen down the remnants of the session, from my perspective. This one’s for the AI & Deep Learning enthusiasts. 
We are accustomed to focussing on grid-based data, in the Euclidian domain, with defined neighbours. For example, in case of a traditional CNN, we know the neighbourhood of an image pixel, especially while applying filters in the convolutional or pooling layers (I’ll elaborate on this a little later). Imagine a scenario where we wish to input data in the form of a graph, with all the neighbourhood connections maintained. Many scenarios require data to be analysed in the non-Euclidean space, as graphs. Computer networks, social media networks, studies pertaining to genetics and chemical bonds- all of these contain data which are in the form of graphs. 
“Graph CNNs fall under bleeding edge research, when it comes to deep learning.” - Dr. Tijmen Tieleman 
The traditional grid CNNs have two characteristics which pose to be challenges in case of graph CNNs. It’s difficult to assign weights to nodes when the relative location of the nodes are not in the grid format. How does one know which node to assign w1, w2 or w3 to, without a 3x3 or NxN spatial structure? The following represent the challenges faced while designing a graph CNN. 
Hence, researchers started looking at localisation techniques. There is an inherent tension between semantics and location in semantic segmentation: local information resolves where while global information seeks to resolve what. 1x1 CNN or Fully Convolutional Networks combine shallow, appearance information with deep, coarse semantic information. Hence, although using a 1X1 filters may seem a little absurd owing to the fact that the neighbouring pixels are not considered for convolution, converting fully connected layers to convolutional layers with this concept proves to be effective in terms of localised information. The method proposed by Jonathan Long et al. learns end-to-end. Addressing the two issues earlier, the following architecture was proposed: 
Using concepts of graph theory can be useful in several cases. Most of the current research work focuses on the spatial domain, however, results in the spectral domain also prove to be efficient and comparable to state-of -the art results. For instance, average pooling can be substituted by graph clustering! 
A modification of this would be to have the same weights for immediate neighbours and a different weight for a loop, as shown. Furthermore, a graph network within a graph network, can store the local node information per node. This is inspired by the Network in Network architecture where each node is another smaller network, instead of simply computing a convolution followed by a non-linear activation at once! Following this, the audience had many doubts; a plethora of them are open-questions. 
2. Can graph CNN concepts be used for Grid CNN concepts?Yes. Essentially, grid representations can be viewed as a simple form of graph representations with known and structured neighbours. A good suggestion to this was that a graph representation can be viewed as a stacked or flattened grid representation where each layer can represent a central node with a weight of w for the spatial locations where other neighbouring or connected nodes are present (as shown). 
3. What if 2 graph nodes are not directly connected?ConvNets always follow a hierarchy, when it comes to passing information. So incase two nodes are not directly connected, their information will be combined, with corresponding weights, over deeper layers. 
4. What if we have to consider weighted graphs?Currently, they are working on techniques where the link weights are represented as nodes. 
5. What if 2 graphs have identical relative degrees?The corresponding weights will vary because two things are taken into consideration for such an ideology- connectedness (which will be the same in this case) and individual node identity(which will vary, based on the information). 
6. If we use the concept of same weights to grid layers, will it be effective?No. Having lesser weights reduces synapses and learning capacity. Graph CNNs use it as a novel approach, but ideally, different weights would prove to be a better approach. 
Note: This is a summary of the hour long session and none of the methodologies are proposed by me. Feel free to correct me or provide your suggestions in the comments’ section. Also, excuse my drawings :P 
Written by 
Written by",Sukriti Paul,2018-02-21T17:10:49.167Z
Main Cloud Security Threats. Cloud computing continues to transform… | by IDM | Medium,"Cloud computing continues to transform the way companies use, store, and share data, applications, and workloads. It has also introduced a range of new security threats and challenges. With so much data going into the cloud — and into public cloud services in particular — these resources become natural targets for hackers. Here are the top issues in cloud security: 
A data breach might be the primary objective of a targeted attack or simply the result of human error, application vulnerabilities, or poor security practices. It might involve any kind of information that was not intended for public release, including personal health information, financial information, personally identifiable information, trade secrets, and intellectual property. A company’s cloud-based data may have value to different parties for different reasons. The risk of data breach is not unique to cloud computing, but it consistently ranks as a top concern for cloud customers. 
Bots masquerading as legitimate users, operators, or developers can read, modify, and delete data; issue control plane and management functions; snoop in data in transit or release malicious software that appears to originate from a legitimate source. A consequence of insufficient identity credentials and poor password management can result in unauthorized access to data, and potentially cause catastrophic damage to companies or end users. 
Cloud providers expose a set of software user interfaces (UIs) or APIs that customers use to manage and interact with cloud services. Provisioning, management, and monitoring are all performed with these interfaces, and the security and availability of general cloud services depend on the security of API. They need to be designed to protect against accidental and malicious attempts to circumvent policy. 
System vulnerabilities are exploitable bugs in programs that attackers can use to infiltrate a system to steal data, taking control of the system or disrupting service operations. Vulnerabilities within the components of the operating system put the security of all services and data at significant risk. With the advent of multi-tenancy in the cloud, systems from various companies are placed close to each other and given access to shared memory and resources, creating a new attack surface. 
Account or service hijacking is not new, but cloud services add a new threat to the landscape. If attackers gain access to a user’s credentials, they can eavesdrop on activities and transactions, manipulate data, return falsified information and redirect clients to illegitimate sites. Account or service instances might become a new base for attackers. With stolen credentials, attackers can often access critical areas of cloud computing services, allowing them to compromise the confidentiality, integrity, and availability of those services. 
APTs are a parasitical form of cyber attack that infiltrates systems to establish a foothold in the IT infrastructure of target companies, from which they steal data. APTs pursue their goals stealthily over extended periods of time, often adapting to the security measures intended to defend against them. Once in place, APTs can move laterally through data centre networks and blend in with normal network traffic to achieve their objectives. 
Data stored in the cloud can be lost for reasons other than malicious attacks, CSA says. An accidental deletion by the cloud service provider, or a physical catastrophe such as a fire or earthquake, can lead to the permanent loss of customer data unless the provider or cloud consumer takes adequate measures to back up data, following best practices in business continuity and disaster recovery.Don’t want to lose your data? Choose IDM and forget about problems listed above! 
Written by 
Written by",IDM,2018-07-30T14:23:47.528Z
How Machine Learning Can Enable Anomaly Detection | by Countants | Data Driven Investor | Medium,"As human, our brains are always tuned to spotting something out of the “normal” or the “usual stuff.” In short, some anomaly that does not fit with the usual pattern. With the abundant growth of data, data science tools are also looking for anomalies that do not subscribe to the normal data flow. For example, an “unusually high” number of login attempts may point to a potential cyberattack, or a major hike in credit card transactions in a short period could potentially be a credit card fraud. 
At the same time, detecting anomalies in the face of a continuous stream of unstructured data from various sources has its own challenges. An example of a challenge is to assume that a majority of credit card transactions are legitimate and proper while looking for major deviations in a few transactions that fall outside the “normal” range. 
Thanks to the growth of various deep learning technologies, anomaly detection using machine learning (or ML) is a practical solution today. Machine learning algorithms can be deployed to define data patterns that are normal and using ML models to find deviations or anomalies. 
So, as a data analyst, how can you implement anomaly detection using machine learning? And what are the methods and benefits of anomaly detection using deep learning technologies? Let’s answer each of these questions and more in the following sections. 
Also referred to as outlier detection, anomaly detection is simply the mode of detecting and identifying anomalous data in any data-based event or observation that differs majorly from the rest of the data. Anomalous data can be critical in detecting a rare data pattern or potential problem in the form of financial frauds, medical conditions, or even malfunctioning equipment. 
How do you go about detecting an anomaly in data? Let’s examine this with the aid of an anomaly detection use case using 2 variables (X & Y). Consider the following visualized data that plots the X and Y variables. 
Consider the data patterns of the 2 variables based on the plotted graphs to the right. Based on these data points, it’s not possible to detect any anomaly (or outlier). However, when the 2 variables are plotted against each other (as shown in the left figure), we can clearly detect the anomaly. 
Does this bring us to the question as to why machine learning is required in anomaly detection? Detecting anomalies can be very challenging if you are plotting not two but hundreds of such variables in real-life scenarios. 
How does machine learning help in outlier analysis? Let’s discuss this in the next section. 
An outlier is identified as any data object or point that significantly deviates from the remaining data points. In data mining, outliers are commonly discarded as an exception or simply noise. However, the same cannot be done in anomaly detection, hence the emphasis on outlier analysis. 
An example of performing anomaly detection using machine learning is the K-means clustering method. This method is used to detect the outlier based on their plotted distance from the closest cluster. 
K-means clustering method involves the formation of multiple clusters of data points each with a mean value. Objects within a cluster have the closest mean value. Any object with a threshold value greater than the nearest cluster mean value is identified as an outlier. Here is the step-by-step method used in K-means clustering: 
Next, let’s look at some of the other methods of executing anomaly detection using machine learning. 
Based on different machine learning algorithms, anomaly detection methods are primarily classified under the following two headings 
As the name suggests, this anomaly detection method requires the existence of a labeled dataset that contains both normal and anomalous data points. Examples of supervised methods include anomaly detection using neural networks, Bayesian networks, and the K-nearest neighbors (or k-NN) method. 
Supervised methods provide a better rate of anomaly detection thanks to their ability to encode any interdependency between variables and including previous data in any predictive model. 
Unsupervised methods of anomaly detection do not depend on any training data with manual labeling. These methods are based on the statistical assumption that most of the inflowing data are normal and only a minor percentage would be anomalous data. These methods also estimate that any malicious data would be different statistically from normal data. Some of the unsupervised methods include the K-means method, autoencoders, and hypothesis-based analysis. 
In the next sections, we shall look at some of the business benefits of anomaly detection using machine learning. 
Using the capability of machine learning, anomaly detection has practical applications and benefits in different areas of business operations. Some of the benefits of the anomaly detection medium include: 
Any nefarious activity that can damage an information system can be broadly classified as an intrusion. Anomaly detection can be effective in both detecting and solving intrusions of any kind. Common data-centric intrusions include cyberattacks, data breaches, or even data defects. 
Another benefit of anomaly detection using machine learning is in the domain of gathering and analyzing mobile sensor data. The growing adoption of IoT devices and the reduced costs of data capture through mobile sensors is definitely driving this trend. 
For instance, a particular industry case study is that of the IBM Data Science Experience that developed a tool for anomaly detection using Jupyter Notebook for capturing sensor data from mobile phones and connected IoT devices. 
Be it a mobile app or a network failure, a sudden degradation in performance can affect any business. Want to detect a sudden rise in the number of failed server requests? Anomaly detection code in Python programming can be used to detect any failing server on your network. 
Additionally, anomaly detection can provide you with any supportive data that can identify the root cause of the problem. 
Statistical Process Control (or SPC) is a quality standard that is common in the manufacturing process. Quality-related data on product or process measure is retrieved during the manufacturing run-time process and is plotted on a graph to monitor if the data is within the configured control limits. 
Anomaly detection is deployed to check if any data falls outside the control limits and to determine the root cause. In short, anomaly detection in SPC can be used to detect any product variation or any issue occurring in the manufacturing process that needs to be immediately resolved. 
Future advancement in machine learning and deep learning technologies will only add to the scope of anomaly detection techniques and its value to business data. The increasing volume and complexity of data translate to major opportunities in harnessing this information for business success. 
Since its inception, Countants has mastered deep learning solutions in artificial intelligence and machine learning for its global customers. If you have invested in machine learning tools like Python or Jupyter Notebook, then we can help you build business leverage from anomaly detection methods. Visit us at our website or call us now with your data-related queries. 
In each issue we share the best stories from the Data-Driven Investor's expert community. Take a look 
Written by 
Written by",Countants,2020-01-14T05:21:11.390Z
Cyber Risk Register – Medium,Every company is (or should) be spending on cybersecurity to protect its information…,NA,NA
Machine Learning in cyber security Secure Us | by RATNAKAR KUMAR | Medium,"INTRODUCTION 
There is no doubt that artificial intelligence (AI) and machine learning (ML) offer major advantages for modern cybersecurity applications compared to older, automated versions. The ability for applications to learn based on experience and use the knowledge to inform their behavior when confronted with similar issues in the future delivers a significant benefit compared to more traditional passive applications. 
ML is not a panacea for cybersecurity, but it does introduce intelligence to an organization’s first level of defense against cyberthreats. And it enables organizations to deploy that intelligence across all the major categories of security tasks: prediction, prevention, detection, response and monitoring. 
ML is not the silver bullet that will defend against all cybersecurity threats. The biggest reason for this is both sides of the cybersecurity landscape, good and bad, are adopting the technology. So while it’s true that organizations can gain much from using ML, they also face a greater threat from hackers and criminals using the technology against them. 
But for all its advantages, ML is still heavily reliant on the human element to be successful. Human monitoring and continuous input are required if ML software is to successfully learn and adapt. Without it, there is no way to guarantee ML is using the correct data to arrive at the right conclusions. Human monitoring makes it possible to detect whether the data sets used by ML are becoming corrupted, to test whether the conclusions produced by ML are correct and to help guarantee compliance.And ML is only as good as the humans who program the software to ask the right questions and ensure it is presented with the right data to learn. 
Machine Learning in Cyber Security Domain; In recent years, attackers have been developing more sophisticated ways to attack systems. Thus, recognizing these attacks is getting more complicated in time. Most of the time, network administrators were not capable to recognize these attacks effectively or response quickly. 
Therefore, there is a lot of software has been developed to support human in order to be able to manage and protect their systems effectively. Initially, these software has been developed to handle some operations like mathematical calculations which seem very complex for human being. And then we need more. Next step was extending the ability of software using artificial intelligence and machine learning techniques. As technology advances, huge amount of data is being produced to be processed every day and every hour. Finally, the concept of “Big Data” was born and people began to need more intelligent system for processing and getting make sense of these data. 
Written by 
Written by",RATNAKAR KUMAR,2019-07-30T05:19:45.653Z
"#explainCovid19 challenge. For some time, I’ve been interested in… | by Przemyslaw Biecek | Medium","For some time, I’ve been interested in the verification of software libraries for eXplainable Artificial Intelligence (XAI). Not only in terms of the number of implemented algorithms but also actual usability for the end user. I found that it is challenging to gather useful feedback from end-users because it is not easy to find a predictive model that wide group of users really cares about. One positive example is the FICO challenge. The task was to build and explain a predictive model for risk scoring. 
Maybe it is time for a more important challenge? 
Outbreak of COVID19 disease cased by SARS-CoV-2 is severe. Various data related to this outbreak is shared publicly. Selected individual data for infected persons (country, age, gender, date of infection, possible recovery or death) can be downloaded from this spreadsheet or this Kaggle data or for selected countries from other databases. This data makes possible training a predictive model for survival and also trying different XAI methods that can explain model predictions.Using the DALEX I built a simple baseline solution. I trained a simple gradient boosting model that estimates odds of recovery based on gender, country and age (with forced monotonicity constraints). Then the model is explained with modelStudio interactive dashboard. Take a look and play with the model yourself https://pbiecek.github.io/explainCOVID19/ 
Usually XAI tools highlight any imperfections in the model or in the training data. This is the case here. Building a more complex model was difficult because of the incompleteness of the data on individual level. But even a simple model with three variables can be an interesting tool for a fresh look at the problem of model explainability. 
If you have a better data, better model or better explanations please let me know. #explainCovid19 
Written by 
Written by",Przemyslaw Biecek,2020-03-13T10:38:27.120Z
Principal Component Analysis(PCA) | by Aniket Patil | Analytics Vidhya | Medium,"Principal component analysis (PCA) is a dimension reduction process that allows reducing number of variables from a given dataset to a smaller set of variables that can be used in data analysis. PCA can be defined formally as a statistical procedure used to map a set of interrelated variables into a smaller set of linearly uncorrelated variables while retaining as much variance as possible in the original dataset. 
The reduced set of variables try to retain the maximum variance present in the original dataset. This is very useful in machine learning where the amount of data required for training is related to number of variables used in modelling. The removed variables are the ones which are either similar in nature or can be deduced from other unremoved variables, so they add no new value to dataset. 
In a simple example, given three dimensions of measurements, volume of the cup, volume of tea in the cup, and volume that can be filled with tea, PCA can reduce them to one variable that includes change in information with change in tea levels. 
Suppose we are asked to build a predictive model of a stock performance. For accurate modelling, we would consider all the dimensions of measurements related to stock performance. 
These variables can span from different categories of financial ratios, labour market, housing variables, sentiment measures, GDP measurements, inflation, and unemployment along with closing price, opening price, intraday high, intraday low, alpha, beta, and fundamentals of individual stock. 
A quick analysis of variables provided by a major stock brokerage firm for selecting shares provides more than 30 variables for analysis. 
Some of the variables may be interrelated, and some of them may not even add any additional information for the analysis. It may not be obvious even to financially literate person to intuitively evaluate the correlations among all the variables. 
PCA can be applied to these large set of variables and extract a reduced set of alternative variables that represent close to entire variance in the original data. The smaller set can then be used to evaluate different models for stock performance. 
Performance models using the reduced principal components can then be used to plan for capacity requirements, performance bottleneck detection, and performance prediction. 
Image compression is a common application of PCA using dimension reduction Technique. 
Datasets with large number of variables usually have a smaller subset of variables that capture most of the variance represented by the variables. 
One way to understand the application of PCA is to tease out the subset of variables that represent the data in a different subspace while preserving most of the variance of the original dataset. 
A simple solution to derive principal components using linear algebra is based on covariance matrix. A generic version of PCA can be derived using singular value decomposition (SVD) method. 
Computing relative relationship between variables can be done by using covariance computation. If there is no relationship between variables, then the covariance will be 0, and if there is high correlation between variables, then the covariance value will be close to 1 with positive correlation or -1 for negative correlation. 
A matrix is a diagonal matrix if all the elements of matrix are 0 expect for the 
diagonal elements. diagonal matrices scale the data along the different coordinate axes. A diagonal matrix of eigenvalues gives a measure of the data variance (their scale) along the different principal component axes. 
Eigenvector is a vector that when applied to a square matrix M will only change the scalar part of the matrix and leaves the direction unchanged. 
A symmetric matrix has the property that the transpose is equal to the original Matrix. 
In linear algebra, the singular value decomposition (SVD) is a factorization of a real or complex matrix that generalizes the eigen decomposition of a square normal matrix to any mxn matrix via an extension of the polar decomposition. 
Mathematical applications of the SVD include computing the pseudoinverse, matrix approximation, and determining the rank, range, and null space of a matrix. The SVD is also extremely useful in all areas of science, engineering, and statistics, such as signal processing, least squares fitting of data, and process control. 
Principal components can be expressed as a linear equation of observed metrics. There is a research on dealing with nonlinearity of the variable relationship called kernel PCA. We are focusing on linear PCA. 
Principal components derived using PCA account for overall variance of the original dataset in the variance of computed PCs with smaller number of variables. 
There is an implicit assumption that mean and variance describe the entire distribution of variables under observation. These zero mean distributions that can be described by variance is true only in Gaussian or normal distribution. 
Large variance in observed variable will contribute most to the overall variance of computed principal component. If the observed values of variables have very different ranges, then the data needs to be normalized/scaled. Across the variables when applying correlation analysis for principal component computations. 
Consider the example of two variables measured being distance and time. If the distance is measured in centimetres and time in hours, then the resultant principal component is skewed toward the axis of distance. 
However, if the distance is measured in kilometres and the time in seconds, then the principal component computed will skew toward the axis of time instead of distance. 
To overcome the problem of skew due to scale of variable value, Feature Scaling through standardization is performed as pre-processing step on the data. Standardization, also called z-mean normalization, converts the data to mean of 0 and standard deviation of 1. 
Principal components are computed from computing a covariance matrix and performing SVD on the resultant matrix. 
Typically, you would like to have the error in variance captured to be less than 0.01 /1%, and the components are selected to have the principal component capture 99% of the original variance in dataset. 
The error can easily be computed by simply computing the proportion of the total variance computed. 
Independent variables become less interpretable: 
After implementing PCA on the dataset, your original features will turn into Principal Components. Principal Components are the linear combination of your original features. Principal Components are not as readable and interpretable as original features. 
Data standardization is must before PCA: 
You must standardize your data before implementing PCA, otherwise PCA will not be able to find the optimal Principal Components.   For instance, if a feature set has data expressed in units of Kilograms, Light years, or Millions, the variance scale is huge in the training set. If PCA is applied on such a feature set, the resultant loadings for features with high variance will also be large. Hence, principal components will be biased towards features with high variance, leading to false results.  Also, for standardization, all the categorical features are required to be converted into numerical features before PCA can be applied.  PCA is affected by scale, so you need to scale the features in your data before applying PCA. Use StandardScaler from Scikit Learn to standardize the dataset features onto unit scale (mean = 0 and standard deviation = 1) which is a requirement for the optimal performance of many Machine Learning algorithms. 
Information Loss: 
Although Principal Components try to cover maximum variance among the features in a dataset, if we don’t select the number of Principal Components with care, it may miss some information as compared to the original list of features. 
Overfitting: 
Model construction overfitting occurs when the number of features is large and sufficient data is not there. To reduce number of features, sometimes PCA is used under the assumption that the reduced number of features will avoid overfitting. 
Using PCA to avoid overfitting can lead to bad results. The problem of overfitting is better addressed using regularization techniques and better datasets. 
Model Generation: 
PCA is used to generate Principal components (PCs) as reduced dimension set. The reduced dimension data is computed and then used to train the model using Logistic Regression to model the system. 
The use of PCs to construct model can cause large errors in predicting data for new inputs. 
One alternative is to extract original factors from the data. Common factor analysis is a method that identifies combination of original variables that can maximize variance of observed variables. 
This is different from PCA which maximizes total variance of original variables. These are also called latent factors or hidden variables that encapsulate combined variance of observed variables. 
This method of computing factors is useful when the correlation matrix has high values not only in the diagonal but also other parts of the matrix. The latent factors tend to perform better in modelling than PCs. 
Modelling based on original variables should be preferred, and only if issues arise with scale of dimensions should you consider model generation based on PCs. 
Model Interpretation: 
Principal component loadings represent the correlation factor for the variables. We can infer that the variable is positively associated or negatively associated based on the size and sign of the loading. 
However, we must not overinterpret the loading size and direction as a variable with low loading value in principal component 1 may have a higher values in other principal components. 
Using common factor (CF) analysis will provide a better interpretation when trying to interpret the model. 
Written by 
Written by",Aniket Patil,2020-05-03T16:34:18.450Z
Spotting Defects! — Deep Metric Learning Solution For MVTec Anomaly Detection Dataset | by daisukelab | Analytics Vidhya | Medium,"Everyday we are checking if there’s anything wrong visually, and it happens naturally in our life. For example, when you are taking a food out of fridge, you would be doing unconsciously to have a glimpse of it to see if it is OK or not. 
In business, visual inspection is widely done in the final process of production. And this would be the major application of machine learning anomaly detection. 
https://www.mvtec.com/company/research/datasets/mvtec-ad/ 
A German company MVTec Software GmbH recently released novel MVTec Anomaly Detection dataset[1], it has realistic data from 15 categories. 
Categories from industrial to agricultural, defects from each different domains, with various alignments in the images, and even with segmentation data of defect areas in the annotations — it’s a great dataset. 
“To the best of our knowledge, no comparable dataset exists for the task of unsupervised anomaly detection.” 
MVTec AD is introduced to play a role of MNIST, CIFAR10, or ImageNet for unsupervised anomaly detection (and segmentation) research area. 
This dataset comes with a paper[1] which not only introduces the dataset but also evaluates baseline methods such as GAN, auto encoder, or other traditional methods. 
This article made some experiments to apply deep metric learning to solve anomaly detection task with this dataset. Major deep metric learning such as ArcFace[3]/ CosFace[4] are popular in face verification/recognition tasks, and these methods can measure distance between data. The distance is then used to determine if the faces in two photos has the same identities or not, for example. 
Big benefit of these deep metric learning methods is its simpleness. 
In all the experiments, following methods are tested: 
3 different classification problem setups are experimented. One failed, and the next failed, then final one succeeded with a technique newly invented. 
As MVTec AD paper claims, many prior works use major image dataset for evaluation such as MNIST, CIFAR10 for example. And the problem settings is: 
So the similar setting is applied here: 
Training finishes successfully, it’s easy problem to classify images that looks very different each other. 
But unlike usual CNN classification, this actually trains metric-learning-enabled CNN to learn metric that measures distance between samples. 
In the test phase, all the test samples are measured their distances from normal class by following steps: 
Training model to learn metric have failed (less than 0.5; worse than random-answer), measured distances are almost incorrect. This is natural that: 
So the traditional problem setting of metric learning evaluation doesn’t work in the realistic scenario… 
We need to motivate model to learn to measure small differences.It’s not difference between cat and car, even not between black cat and gray cat.It’s between a clean screw and a screw with a tiny scratch! 
Models are supposed to find tiny difference from normal samples. To do that, 
But it also resulted in failure, about 0.5 to be close to random-answer. 
Basically anomaly sample size is too small; normal sample size is about 200+, while defect anomaly sample size is about 10, very imbalanced. Followings are made to mitigate this problem, but didn’t make it work. 
Journey to train models so that they discriminate tiny “something wrong” continues… 
This is based on a Kaggle-ish simple idea. 
Now what we need is anomaly samples that have tiny difference from normal ones. Then, we can just simply generate anomaly samples from normal samples. 
Once anomaly samples are ready, we can train models so that they distinguish tiny difference with the original normal samples. This training problem is binary classification. We use normal samples only, then this is also a self-supervised training. 
This experiment invented a new dataset class (actually it’s an ImageList class for fast.ai library[7]) that: 
The results make sense. 
Let’s check samples with grad-CAM activation heatmap. Successful cases shows that the heatmap captures defect part on the image: 
Failure cases below shows that model is not looking at the defect. Model didn’t learn correctly to find these type of defects. There are some more examples like this, showing that it’s still leaving some more spaces to improve. 
As a final result, followings are the grad-CAM heatmaps, after tuning model to achieve AUC 90%. It was fairly easy to tune, we can tweak how to create anomaly twin samples so that it simulates defect modes. 
But what does this mean? This is done by using the knowledge of defect modes which happens in the test samples. It’s like cheating. 
All the results on the original MVTec AD paper[1] are based on segmentation output. Then basically it is difficult to compare. 
Experiments in this article showed that: 
Find example code here: https://github.com/daisukelab/metric_learning/tree/master/MVTecAD 
Many thanks to fast.ai library[7] for minimizing time to develop experiments. 
Written by 
Written by",daisukelab,2019-09-20T12:04:37.327Z
Different Types of Distance Metrics used in Machine Learning | by Kunal Gohrani | Medium,"Many Supervised and Unsupervised machine learning models such as K-NN and K-Means depend upon the distance between two data points to predict the output. Therefore, the metric we use to compute distances plays an important role in these models. 
In this blog post, we are going to learn about some distance metrics used in machine learning models. They are:- 
According to Wikipedia, “A Normed vector space is a vector space on which a norm is defined.” Suppose A is a vector space then a norm on A is a real-valued function ||A||which satisfies below conditions - 
The distance can be calculated using the below formula:- 
Minkowski distance is a generalized distance metric. We can manipulate the above formula by substituting ‘p’ to calculate the distance between two data points in different ways. Thus, Minkowski Distance is also known as Lp norm distance. 
Some common values of ‘p’ are:- 
We will discuss these distance metrics below in detail. 
We use Manhattan distance, also known as city block distance, or taxicab geometry if we need to calculate the distance between two data points in a grid-like path. Manhattan distance metric can be understood with the help of a simple example. 
In the above picture, imagine each cell to be a building, and the grid lines to be roads. Now if I want to travel from Point A to Point B marked in the image and follow the red or the yellow path. We see that the path is not straight and there are turns. In this case, we use the Manhattan distance metric to calculate the distance walked. 
We can get the equation for Manhattan distance by substituting p = 1 in the Minkowski distance formula. The formula is:- 
When is Manhattan distance metric preferred in ML? 
Quoting from the paper, “On the Surprising Behavior of Distance Metrics in High Dimensional Space”, by Charu C. Aggarwal, Alexander Hinneburg, and Daniel A. Kiem. “ for a given problem with a fixed (high) value of the dimensionality d, it may be preferable to use lower values of p. This means that the L1 distance metric (Manhattan Distance metric) is the most preferable for high dimensional applications.” 
Thus, Manhattan Distance is preferred over the Euclidean distance metric as the dimension of the data increases. This occurs due to something known as the ‘curse of dimensionality’. For further details, please visit this link. 
Euclidean distance is the straight line distance between 2 data points in a plane. 
It is calculated using the Minkowski Distance formula by setting ‘p’ value to 2, thus, also known as the L2 norm distance metric. The formula is:- 
This formula is similar to the Pythagorean theorem formula, Thus it is also known as the Pythagorean Theorem. 
Hamming distance is a metric for comparing two binary data strings. While comparing two binary strings of equal length, Hamming distance is the number of bit positions in which the two bits are different. 
The Hamming distance between two strings, a and b is denoted as d(a,b). 
In order to calculate the Hamming distance between two strings, and, we perform their XOR operation, (a⊕ b), and then count the total number of 1s in the resultant string. 
Suppose there are two strings 11011001 and 10011101. 
11011001 ⊕ 10011101 = 01000100. Since, this contains two 1s, the Hamming distance, d(11011001, 10011101) = 2. 
Cosine distance & Cosine Similarity metric is mainly used to find similarities between two data points. As the cosine distance between the data points increases, the cosine similarity, or the amount of similarity decreases, and vice versa. Thus, Points closer to each other are more similar than points that are far away from each other. Cosine similarity is given by Cos θ, and cosine distance is 1- Cos θ. Example:- 
In the above image, there are two data points shown in blue, the angle between these points is 90 degrees, and Cos 90 = 0. Therefore, the shown two points are not similar, and their cosine distance is 1 — Cos 90 = 1. 
Now if the angle between the two points is 0 degrees in the above figure, then the cosine similarity, Cos 0 = 1 and Cosine distance is 1- Cos 0 = 0. Then we can interpret that the two points are 100% similar to each other. 
In the above figure, imagine the value of θ to be 60 degrees, then by cosine similarity formula, Cos 60 =0.5 and Cosine distance is 1- 0.5 = 0.5. Therefore the points are 50% similar to each other. 
Cosine metric is mainly used in Collaborative Filtering based recommendation systems to offer future recommendations to users. 
Taking the example of a movie recommendation system, Suppose one user (User #1) has watched movies like The Fault in our Stars, and The Notebook, which are of romantic genres, and another user (User #2) has watched movies like The Proposal, and Notting Hill, which are also of romantic genres. So the recommendation system will use this data to recommend User #1 to see The Proposal, and Notting Hill as User #1 and User #2 both prefer the romantic genre and its likely that User #1 will like to watch another romantic genre movie and not a horror one. 
Similarly, Suppose User #1 loves to watch movies based on horror, and User #2 loves the romance genre. In this case, User #2 won’t be suggested to watch a horror movie as there is no similarity between the romantic genre and the horror genre. 
In this blog post, we read about the various distance metrics used in Machine Learning models. We studied about Minkowski, Euclidean, Manhattan, Hamming, and Cosine distance metrics and their use cases. 
Manhattan distance is usually preferred over the more common Euclidean distance when there is high dimensionality in the data. Hamming distance is used to measure the distance between categorical variables, and the Cosine distance metric is mainly used to find the amount of similarity between two data points. 
Written by 
Written by",Kunal Gohrani,2019-11-10T10:02:23.597Z
Recommender Systems — User-Based and Item-Based Collaborative Filtering | by Carlos Pinela | Medium,"This is part 2 of my series on Recommender Systems. The last post was an introduction to RecSys. Today I’ll explain in more detail three types of Collaborative Filtering: User-Based Collaborative Filtering (UB-CF) and Item-Based Collaborative Filtering (IB-CF). 
Let’s begin. 
Imagine that we want to recommend a movie to our friend Stanley. We could assume that similar people will have similar taste. Suppose that me and Stanley have seen the same movies, and we rated them all almost identically. But Stanley hasn’t seen ‘The Godfather: Part II’ and I did. If I love that movie, it sounds logical to think that he will too. With that, we have created an artificial rating based on our similarity. 
Well, UB-CF uses that logic and recommends items by finding similar users to the active user (to whom we are trying to recommend a movie). A specific application of this is the user-based Nearest Neighbor algorithm. This algorithm needs two tasks: 
1.Find the K-nearest neighbors (KNN) to the user a, using a similarity function w to measure the distance between each pair of users: 
2.Predict the rating that user a will give to all items the k neighbors have consumed but a has not. We Look for the item j with the best predicted rating. 
In other words, we are creating a User-Item Matrix, predicting the ratings on items the active user has not see, based on the other similar users. This technique is memory-based. 
Back to Stanley. Instead of focusing on his friends, we could focus on what items from all the options are more similar to what we know he enjoys. This new focus is known as Item-Based Collaborative Filtering (IB-CF). 
We could divide IB-CF in two sub tasks: 
1.Calculate similarity among the items: 
2.Calculation of Prediction: 
The difference between UB-CF and this method is that, in this case, we directly pre-calculate the similarity between the co-rated items, skipping K-neighborhood search. 
Slope One is part of the Item-Based Collaborative Filtering family, introduced in a 2005 paper by Daniel Lemire and Anna Maclachlan called Slope One Predictors for Online Rating-Based Collaborative Filtering. 
The main idea behind this model is the following: 
Suppose we have two different users: A and B. Also, we have item I and item J. User A rated item I with 1 star and the item J with 1.5. If the User B rated Item I with a 2. We can make the assumption that the difference between both items will be the same as User A. With this in mind, User B would rate Item J as: 2+ (1,5–1) = 2,5 
The authors focus on 5 objectives:1. Easy to implement and maintain.2. Updatable online: new ratings should change predictions quickly.3. Efficient at the time of consultation: storage is the main cost.4. It works with little user feedback.5. Reasonably accurate, within certain ranges in which a small gain in accuracy does not mean a great sacrifice of simplicity and scalability. 
We saw User-Based and Item-Based Collaborative Filtering. The first has a focus on filling an user-item matrix and recommending based on the users more similar to the active user. On the other hand, IB-CF fills a Item-Item matrix, and recommends based on similar items. 
It is hard to explain all these subjects briefly, but understanding them is the first step to getting deeper into RecSys. 
Written by 
Written by",Carlos Pinela,2017-11-06T02:26:03.383Z
A Short Introduction to Active Learning | by Raul Incze | Cognifeed | Medium,"In the previous blog post we laid down the basics of supervised learning while highlighting a few of its shortcomings. This time around, we’re going to address some of these issues through the concept of active learning. 
Similar to active learning in humans, Machine Active Learning seeks to engage the learner into the teaching/training process. When it comes to machines, this is achieved by allowing the algorithm to ask a human expert (often called an “oracle”) questions. This often results in the algorithm converging to a solution faster. Alright, the whole “asking questions” part is kind of click bait worthy due to unnecessary anthropomorphisation. What’s actually going on in an active learning scenario as opposed to the supervised one? 
Active learning is most useful when you have data but no labels, although you need them. This usually happens at the beginning of any ML project or when acquiring labels for data entries is difficult (expensive, takes a lot of time, it requires a highly knowledgeable oracle, etc). So we have our unlabeled dataset U. At the beginning of the process we randomly pick a few entries and we forward them to the human oracle to be labeled (askOracle). We now have a very small set of labeled data X with their labels y. Here’s the algorithm: 
At each step, we train our model on all the labeled samples up to that point. Then, in generateQuery we use the model to predict labels for all of the samples that don’t have one yet. We use these predictions to determine n most confusing entries for the model. These samples will be picked and returned as X’ and their predicted label as y’. 
Remember that we said earlier that the algorithm asks questions? Well, here is the question it asks at every point in the learning process: “Are y’ the correct labels for the entries in X’ ?”. The oracle answers this question by correcting any mistakes, helping the model improve for the next iteration. The corrected labels l are added to the set of known labels y and the samples X’ (the most informative) are added to X and removed from U. The process continues until the model performs well enough or we end up without any unlabeled entries. 
Alright, this is quite cool and pretty simple. But is it helpful? What happens in generateQuery? How does the model know which samples are confusing? And is this really better than just choosing samples at random to be labeled each step? 
Let’s consider the example from our previous article, where we approximated the sign function in an supervised learning context. Here’s how it would work in an active learning scenario. 
Notice how rather than randomly picking the next sample, we always choose the closest unlabeled sample to our decision boundary (the orange line in the animation). Using such a querying strategy improves the sample efficiency of the learner from O(1/𝜀) to O(ln(1/𝜀)), in other words we’re dealing with an exponential improvement in sample efficiency (Agnostic Active Learning, Balcan et al.). For our simple example comparing the passive supervised version of the learning process to the active one is analogue to comparing random search with binary search. 
Unfortunately this drastic improvement doesn’t hold when dealing with complex multi-dimensional and noisy data. In practice, improvements of up to 5x in sample efficiency have been observed, but the impact of this particular querying strategy varies greatly across tasks and datasets (On the Relationship between Data Efficiency and Error for Uncertainty Sampling, Mussmann et al.). Yet using such a simplistic method for sampling might not be the best idea in practice. Are there more flavours of it or alternatives? 
We’ll focus on two querying strategies here. The first one is the aforementioned uncertainty sampling, the one we have used in our example. 
Burr Settles calls uncertainty sampling the most common, simple and straight forward querying method in his review of the Active Learning literature (Burr Settles, Active Learning Literature Survey). Let’s go back to our sign function. Most probabilistic models in the context of binary classification will try to assign each instance a probability that said instance is of a given class. For example, if we input -0.103, our algorithm might output a 0.91 probability for class ‘+’ and 0.09 for class ‘-’. The final class assigned to the instance is the class with the maximum probability (argmax of all probabilities). In our binary case, we would say that the closer a probability for a label is to 0.5 (which would mean a 0.5 split between ‘+’ and ‘-’), the least confident our model is of its prediction. Hence the instance we’re trying to classify is more informative. If we are to generalize to a multi-class scenario we’d have: 
where ŷ is the class with the highest probability under model 𝜃, and x is the most informative instance under this least confident selection algorithm. 
Of course, if we only take the probability of the most likely label, we’re going to miss out on some information that might prove valuable: the probabilities assigned to the other labels. If we start taking the second most likely label into account too, we end up with margin sampling: 
where, once again x is the most informative instance under algorithm M and ŷ₁ and ŷ₂ are the first and second most probable class labels. Notice how this time we take the argmin, as the most informative sample is the one with the smallest difference between the top two label probabilities. In other words, instances with a small margin are more ambiguous, hence knowing their label will help the model improve its discrimination capacity on those two classes. 
But how about the other labels? Can’t we squeeze some more information out of them? No worries, active learning researchers and information theory experts have you covered: entropy as an uncertainty measure. This way we can measure the amount of information that the probability distribution over all the labels withholds. And we pick that entry which maximizes this information. 
Query-by-committee (QBC) is another popular and less naive querying strategy. As the name suggests, the approach involves having a set of distinct models, all of them trained on the labeled set X. This set of models is called a committee. When it comes to labeling an instance, each committee member has a vote and the label with the most votes wins. Instances on which the committee “disagrees” the most are considered the most informative. 
We can consider the learning process as being the search for the best model within something that’s called a version space of all possible models. The main idea behind QBC is to aid this search through constraining the size of the version space and then querying the most “controversial” samples. 
The simplest way to measure the disagreement within the committee is using vote entropy: 
Where yi goes through all possible labels, and V(yi) represents the number of votes for that label and C is committee’s size. 
Another way to measure how much the committee disagrees is using Kullback-Leibler (KL) divergence. The math gets a bit complicated, so I will not scare you away with the formula (feel free to search it up if interested). KL divergence is used to measure the divergence (difference, if you wish) between two probability distribution. In the QBC algorithm this divergence can be used on the probability distribution of labels given a certain instance. This measure how much the probabilities that are output by committee members diverge. 
There are a few other noteworthy algorithms that are a bit too theory heavy to cover in this blog post. Most of them seem to hint at concepts also present in meta-learning (on which we’ll write another article in the future). The main idea around these methods is to try and estimate the impact that labeling a certain instance will have on how the model changes (expected gradient length), how much the error reduces (expected error reduction) or how much choosing an instance will reduce the variance in the model’s output, resulting in more consistent predictions (variance reduction). 
As mentioned in one of the previous blog posts, Cognifeed uses the concept of active learning as a means of putting user’s (the teacher, or oracle) interaction with data at the core of the platform. This allows us to lower the number of labeled instances you need to train a machine learning model. Or, if you have a vast number of instances, it will help you label them extremely fast. Active learning is one of the many ML concepts that enable us to create a frictionless and fast transition from uploading a few unlabeled data points to having a solid annotated dataset and an instantly usable model. 
When it comes to Cognifeed we are using a querying strategy called query-by-boosting (Abe & Mamitsuka, Query Learning Strategies Using Boosting and Bagging) which is similar to query-by-committee. Query-by-boosting and query-by-bagging strategies piggyback on the fact that certain machine learning models and architectures (such as Random Forests, the AdaBoost meta-algorithm, XGBoost) are by themselves made out of “committees” of smaller, weaker models. These type of learners are called ensemble methods. Applying strategies as boosting or bagging when building the models within this ensemble (which is also our committee), results in a slight departure from your classical QBC. 
Cognifeed trains a boosted ensemble model on high-level features extracted from multidimensional data (such as images, sounds, text). What this model is, how it works and the representation learning methods used to extract these features will be the subjects of two future articles, so stay tuned! 
Written by 
Written by",Raul Incze,2019-03-12T16:12:51.076Z
"Role of AI in Cyber Security. As with other fields of use, artificial… | by Inc.Outlook | Medium","As with other fields of use, artificial intelligence will play a key role in cyber security innovation. Al could be useful in detecting and effectively fighting the latest cyber threats. As cyber threats are becoming increasingly sophisticated and complex, detecting them and addressing them is proving a difficult challenge for any software and app development company. But, artificial intelligence plays a key role, like a Quarterback that draws its team to victory. By examining the intelligence of threats from millions of research documents, blogs and news stories, artificial intelligence is able to provide instant insights to help you fight the noise of thousands of reports every day, drastically reducing response times. By examining the intelligence of threats from millions of research documents, blogs and news stories, artificial intelligence is able to provide instant insights to help you fight the noise of thousands of reports every day, drastically reducing response times. Enterprises are faced with the challenge of pushing for greater gains in business advantage while balancing the risk of cyber exposure. 
AI never takes a day off — One of the few advantages of Artificial Intelligence in cyber security is that AI never takes a day off or gets tired after hours of continuous work. Thus, we can conclude here, that AI helps in doing the job with the maximum efficiency at the maximum possible rate with the best quality products. Prevention of cyber-threats and the avoidance of attacks represent the ideal, but it is almost inevitable that stop these incidents from occurring. And once they do, a fast response is crucial, both in minimizing the harm caused by the assault and in recovering from its effects. With a “thinking machine”, a fast response might be written into its system. 
AI swiftly spots the threats — Adaptive or machine learning algorithms, designed into an intelligent security system have the potential to spot and answer threats as they occur — even dynamic ones. And these intelligent security devices might have the inherent ability to keep on learning, to check current pools of data and extrapolate from them, to anticipate future threats and acceptable responses. 
As AI is adding values to the security sectors of the corporations and individuals as well, it is also spreading more power in the wrong hands. In order to give AI more authority in the near future for the security purposes, we need to stay sure that it stays with the white hat people only. Though AI is still in the developing stage, there is a lot more to explore about it. With time we will be able to classify it as a boon or a bane. 
Written by 
Written by",Inc.Outlook,2019-08-06T13:51:16.324Z
How To Optimize Your Medium Articles for Google | by Christina M. Ward | Better Marketing | Medium,"No matter what type of writing you do and no matter where you choose to post your work online, you’re writing for search engines like Google. If SEO is not in your wheelhouse, you need to learn, adjust, and be deliberate with your methods. 
This article will cover stats analysis, a discussion on outside traffic to your Medium stories, and using the SEO settings options on your stories. 
Recently, I had an article “take off” on Medium. It’s the first article I’ve posted on the platform to receive such attention. Aside from it being a reasonably inflammatory personal story, I wanted to know why this article was doing so well. 
When you look at your top stories, you need to be asking yourself, “What do these stories have in common?” and, “How can I improve them?” SEO should be a part of the equation. 
Let’s take a look at the stats for the first story I shared in the screenshot. 
Go to your stats, click on the word “views,” and organize your stories into a list descending by numbers of views. From there, click on the most-viewed story to get to the internal stats for the story. You’ll see something like this at the bottom of the page: 
For this story, 84% of the views are from internal traffic, meaning other Medium readers are clicking on the story. It’s important to note two things: this story is from last July, and it was curated in only one topic. Yet, somehow, internal views are way up, and there are also 175 views from Google traffic, meaning that people are finding the article as they perform Google searches. 
The next story had much higher Google traffic. Let’s think about why. 
Clearly, Google users are clicking on this story, bringing 1.6k in traffic to my Medium page. I suspect this says a lot about our society — most of these clickers are thinking they’re getting a recipe for making drugs. Joke’s on them, huh? It really is a recipe for bath salts — for your bath. This accounts for the high views and low reads. (Note: This story was not curated.) 
But the story also has keywords like “CBD oil” in the title and throughout the story. This increases the story’s visibility online. This is important to remember when optimizing your stories. 
“12 Very Creative Ways to Promote Your Writing” was curated in two topics: Creativity and Writing. While the external views at 15% doesn’t seem like much, these extra views are helping to push the success forward. 
Keep in mind — once your article makes a “run” through Medium followers and Medium promotion, this external traffic can continue for much longer. 
“Skin Picking is a Compulsive Behavior” is curated in Mental Health and deals with a very specific issue: Dermatillomania, or Excoriation disorder. You can see that a large portion of the attention on this article is coming from outside sources. 
The topic, clearly, is of interest to search engine users who are looking for help with this issue. 
Also, many views come from Smartnews, which is an app that features news stories. Smartnews picked up this article and distributed it on their app. 
So what can we do with the information we find when we look into our top stories, and how can we use it to generate more interest in our work and set up future articles for success? 
With the “help” that certain topics (keywords) and outside traffic offers, you need to make SEO a priority. 
“Link to your article across your social media, networking, and institutional sitesThe more in-bound links to your article, the more search engines like Google will value and highlight your content 
Encourage colleagues to link to your article The more links from respected individuals/trusted sites the more powerful the effect. Don’t forget to do the same for them!”— Search Engine Optimization (SEO) for your article 
I discovered this hidden gem while doing a thorough analysis of my top story, My Father Was Asked to Choose — Save the Mother or the Baby. As I mentioned, the topic is personal, ethically inflammatory, and doing well with Medium readers. The Google views are up a bit, but I wanted to improve them. 
Go to your article (or do this in one you’re writing), click the gear in the top right corner, and then select “edit story.” 
From there, click the three dots to open the drop-down menu. 
Scroll down to “More settings” and choose that option. 
This will take you to a new page with more settings options. 
The screenshot shows this section prior to any editing on my part. As you can see, the SEO description automatically selects a portion of your text, typically the first few lines of your story, and uses it in place of your tagline to feed the search engine algorithms. 
(Collective *gasp*) 
Are you thinking of all the stories with a great “lead-in” but no real “meat” for search engines in the first few lines? What about poetry or fiction? Do you really want a few lines plucked out for this very important purpose? 
You need to edit this section before you publish. You need to include some keywords here for the search engines and give a very brief synopsis of the piece and what it has to offer. 
I made a few changes to this article’s SEO description (don’t forget to click “save”) and will be watching the Google views to see if it’s working. 
I invite you to use this tool and learn a bit more about SEO and keywords so that you give your work the views it deserves. 
The more you know, right? 
Written by 
Written by",Christina M. Ward,2020-01-17T18:55:04.109Z
Practical Guide to Outlier Detection Methods | by A. Tayyip Saka | Towards Data Science,"I am going to talk about the details of four outlier detection methodologies implemented in R Studio. I will be mentioning what the outlier is, why it is important and why outliers occur. These methods are as follows: 
To put simply, an outlier is a data point that differs greatly (much smaller or larger than) from other values in a dataset. Outliers may be because of random variation or may demonstrate something scientifically interesting. In any event, we should not simply eliminate the outlying observation before a careful investigation. It is significant to understand them and their occurrence in the right context of the study to be able to cope with them. 
Why do outliers occur? 
There are several reasons for outliers: 1. Some observations in the sample are extreme; 2. The data are inappropriately scaled; 3. Errors were made on data entry. 
Dataset and Pre-Processing 
My public dataset that shows weekly sales of the company from 2014 to 2017 provides “year”, “week” and “sales” columns as shown below. 
The dimension of a dataset is 201 x 3 and data includes NA values. I looked at the week of blank values then fulfilled blanks by taking an average of observations whose week is the same with blank values in the dataset. 
The line graph points out sales of beverage in Turkey starting from 2014 to 2017. 
Now, it is time to dive deeper into outlier detection methods. 
This method is particularly useful for indicating whether a distribution is skewed and whether there are potential unusual observations in the data set. 
“ggplot” is one of the powerful data visualization packages in R; hence I used “ggplot” package to draw box-plot of sales. As shown above, blue points are identified as outliers and red point shows the median of a dataset. The data is left-skewed. 
The model detected 8 outliers in the dataset as shown on the left table. I can clearly say that these data points differ from the remaining of the dataset. In fact, the sales value of outlier points is above 115,000. 
2. Twitter Anomaly Detection 
AnomalyDetection is an open-source R package to detect anomalies which is robust, from a statistical standpoint, in the presence of seasonality and an underlying trend. The AnomalyDetection package can be used in a wide variety of contexts such as new software release, user engagement posts, and financial engineering problems. The underlying algorithm — known as Seasonal Hybrid ESD builds upon the Generalized ESD test for detecting anomalies. It can be used to find both global as well as local anomalies. 
It can be seen below that I implemented Twitter Anomaly and then the model found 6 outliers in the dataset in comparison with Tukey’s method by taking the alpha value as 0.05. Outlier points are identified as a circle in the table. 
The table below shows which data points labeled as outliers. The index cumulatively shows week numbers of sales. These outliers belong to generally years 2014 and 2015. 
3. Z-score Method 
Z-score finds the distribution of data where mean is 0 and the standard deviation is 1. The Z-score method relies on the mean and standard deviation of data to gauge the central tendency and dispersion. This is problematic on several occasions since the mean and standard deviation are highly affected by outliers. 
From a statistical perspective, using a cut-off of 1.96 would give you an equivalent p-value of p = 0.05. As a result, this method detected 8 extreme points among 201 observations. The Z-score method accords with Box-Whisker on this dataset. Most of these points appeared in the year 2014. 
4. MADe Method 
In statistics, the median absolute deviation (MAD) is a robust measure of the variability of univariate data. Besides, MAD is similar to the standard deviation but it is less sensitive to extreme values in the data than the standard deviation. 
I first computed the median, and then for each data point, I computed the distance between that value and the median. The MAD is defined as the median of these distances. Then, this quantity (MAD) needs to be multiplied by 1.4826 to assure it approximates the actual standard deviation. 
Median and MADe are employed to detect outliers by applying formulas below. 
As a result of the MADe method, 9 outliers are found method as shown above. 
Final Notes 
I performed four outlier detection methods and each method may produce different results on a dataset. Therefore, you must select one of them to observe outliers or can label the most common points among all methods as extreme points. In that case, there are 6 common extreme points among methods I mentioned above. 
As a next step, outlier points will be transformed by considering different ways before forecasting modeling. This will result in a more accurate forecasting model. 
You can see all codes here and next story will be coming soon … 
Hands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look 
Written by 
Written by",A. Tayyip Saka,2019-09-12T12:36:19.252Z
Building the New Zero Trust Enterprise | by Alissa Knight | Data Driven Investor | Medium,"What Zero Trust Is 
It was 2010 when John Kindervag, then an analyst with Forrester Research first wrote about the idea of a zero trust security framework in which the idea of a network edge or perimeter was no longer the front lines of the cyber battlefield for an organization. 
Rather, that instead of an organization implicitly trusting anything inside its perimeter as being “friendly fire,” nothing inside or outside the perimeter should be trusted and that both users and devices should be authenticated, authorized, and determined based on who, what, when, and even where a user or device is from and what it’s trying to access should be scrutinized using IAM, orchestration, analytics, encryption, scoring, and filesystem permissions. 
The concept of zero trust security effectively erases the castle approach to security architecture in that the “castle has left the moat” with data now being everywhere, no longer behind the defenses of a network perimeter but instead extends to mobile devices, cloud drives, cloud servers and that the threat to those assets are everywhere, everyone, and everything. 
This approach to cybersecurity implements an inside-out mentality of protecting the organization’s most critical assets where security is designed as “micro-perimeters” around the assets being protected and that the “bad guy” is not trying to be kept out of the perimeter, but is already inside. 
Zero trust security is achieved through a litany of different security solutions with identity access management and micro-segmentation at the very heart. 
Implementing Zero Trust in 5 steps 
The implementation of zero trust in a pre-existing enterprise is a challenge but not impossible. Unfortunately, it isn’t as simple as throwing money at the problem and implementing a stack of security solutions that give you “zero trust.” It does, of course, involve a healthy amount of budgeting, but it also involves a re-architecture of the network and understanding of “who needs access to and from what” and “what needs access from and to where.” 
Step 1: Create an asset catalogue that includes devices and applications, making sure to document data transmission paths (ports and protocols) that the applications talk over. 
Step 2: Locate your data. What is it you are trying to protect? Where is it? Is it on a shared folder on a file server or on a NetApp server? Who needs access to it? Where are they going to access it from? 
“You can’t protect something when you don’t know where that something is or what that something is.” 
Step 3: Implement micro-segmentation. Segment the network up into different virtual local area networks (VLANs), not just moving devices based on their role to their own unique subnet, but also implementing VLAN Access Control Lists (VACLs) between those subnets or default route them directly to a firewall responsible for internal core traffic filtering. Servers should be in their own VLAN, users should be in their own VLAN, VOIP equipment in its own VLAN, and so-on. An ALLOW-ALL rule should not be set between the VLANs. Time should be taken to ensure what users need access to and what ports/protocols applications and users need to be able to communicate with in order to work. Moving database servers to their own VLANs will also increase security by only allowing ports such as MSSQL (1433/1434) from the application servers and setting up an administration-only VLAN where administrators such as database administrators (DBAs) must first connect to in order to remote in to the server VLAN rather than allowing them direct access from their desktops. 
Step 4: Implement IAM (Identity and Access Management) Solutions. No one should be trusted inside or outside the network. Eliminate forms of single-factor authentication inside and outside the AD environment, such as passwords by moving to multi-factor authentication using solutions such as Duo Security, Okta, or take it further with Yubico, StrongKey, or Trusona, which uniquely authenticates users by where they place their finger on the screen when pushing the authentication button and locks the user to a specific geographical location preventing man-in-the-middle (MITM) attacks. Also consider application security in zero trust environments of mobile devices and web apps by securing them with solutions such as Arxan. 
Step 5: Protect the data where it’s at. Implement role-based authentication to where the data is being stored using solutions such as Varonis to ensure that individual users are granted access to specific data based on their need to know as well as EDR (endpoint detection and response) solutions to detect and autonomously respond to those detected threats. 
Other solutions can be implemented to monitor and manage this framework, including security orchestration (SOAR) solutions, SIEM, UTM, and more. 
As usual, if you liked this article, please support me by clicking LIKE and share it to your own feed! This is the best possible way that you can support me and my continued research. If anyone has anything to add or comment on in this article, please feel free to share it with everyone below in the comments section! Learn more about me at my homepage at www.alissaknight.com, LinkedIn, watch my VLOGs on my YouTube channel, listen to my weekly podcast episodes, or follow me on Twitter @alissaknight. 
I am a senior analyst with Aite Group where I perform focused research into cybersecurity issues impacting the financial services, healthcare, and fintech industries through the assessment of sector trends, creation of segment taxonomies, market sizing, preparation of forecasts, and developing industry models. I provide these industries a combination of syndicated and bespoke market research, competitive intelligence, and consulting services in the cybersecurity market through unbiased, objective and accurate research and content development. Out of my research into the contemporary cybersecurity issues affecting these industries today, I produce research reports and white papers, as well as provide advisory services that include inquiries, briefings, consulting projects, and presentations on study findings as well as bespoke speaking engagements where I often keynote at cybersecurity conferences, seminars, and roundtables annually. 
In each issue we share the best stories from the Data-Driven Investor's expert community. Take a look 
Written by 
Written by",Alissa Knight,2019-01-30T04:51:15.458Z
Microsoft – Wortell – Medium,"With the digital transformation that is happening, more and more data is transferred to Microsoft Azure. A big part of that data is stored in SQL databases. It is only logical to use Microsofts PaaS service for that kind of data: Azure SQL… 
When working with the Service Bus in Azure, the Service Bus Explorer is the must-have tool because it has lots of functionality that is not available in the Azure Portal. In this blog I will describe step by step how to install the Service Bus Explorer. 
Blockchain has recently been mentioned a lot in relation to cryptocurrency. At…",NA,NA
AWS Machine Learning Certification Exam Tips | by Javier Ramos | Medium,"I just passed the AWS Machine Learning exam. It is the most difficult topic of all AWS certifications, however if you understand the concepts, you can easily pass it. You have 3 hours to answer 65 scenario based questions. Refer to the exam blueprint for more info. 
The exam covers lots of topics: Data Collection, Data Preparation, Data Engineering, Data Analysis and Visualization, Algorithms, Model Training, Model Tuning, Model Deployment, Operations and Optimizations. 
However, it builds up on the knowledge from other certifications, specially the Big Data Specialty. There are several questions on the exam regarding data analysis, collection and engineering which are copy on the AWS Big Data Certification. Having a associate level is also recommended so you are familiar with concepts such as IAM Roles, VPCs, EC2, S3, DynamoDB, RDS… which are also part of this certification. The Solutions Architect Associate and the Big Data specialty certification cover more than half of this exam, so I highly recommend taking these two certifications before taking this one. Also, you currently get 50% on the next exam. So if you take the associate which is cheaper you get 50% of this one which is more expensive. 
The exam also covers concepts regarding data analysis and preparation which rely heavily on statistics and math, you need to understand the different distributions, variance, standard deviation, etc. I highly recommend The Data Science Course 2019: Complete Data Science Bootcamp from Udemy, it is really long but very comprehensive and will help you build your analytic knowledge to be ready for the exam. 
So, if you have a machine learning background plus AWS and AWS big data knowledge, then this exam will be easy. The main focus is Sagemaker, a very big topic in the exam, so make sure you know what it is and how to use it. 
There are two main courses to prepare for the exam: 
I took the Cloud Guru course, so I cannot comment on the Linux Academy one. The Cloud Guru is great and very useful to prepare for the exam. The instructors did an incredible job at capturing so much content in the course including tips, quizzes and labs. However, as they mention, it is not fully comprehensive, the course itself will not be enough to pass the exam. You need to check additional materials. The Exam Simulator is great, some of the questions were very similar to the exam, but as always happens with all the practice exams I took so far, some are not clear enough and leave room for interpretation which is quite frustrating. The good news is that, this does not happen in the exam where many reviewers make sure that the questions are very clear. I always score better on the real exam than the practice exam, so do not worry if your score is low, and do not try to memorize the answers. 
I recommend this approach to train for the exam: 
2. Take one of the courses mentioned above. Important: Take notes! 
Depending on your experience spend 1–4 months doing this. Make sure you are comfortable with the labs. 
3. Big push on the last two weeks. Re watch reInvent videos (super helpful), review the FAQs and if you have time, read the white papers. 
4. In the last 2 days review your notes, do the practice exam and memorize everything, but not the answers!. 
AWS has a lot of courses and a Learning Path for the exam detailed below: 
Just register into AWS Training to do the courses for free! 
However, not all courses are that useful, I do not recommend doing all of them, I would focus on these ones which are really good: 
As I mentioned in my previous post, Take this approach to answer the questions: 
For this certification do not rely just on one course. Have a solid understanding of the basic statistics and math. Be familiar with AWS, you need to know IAM, VPCs, EC2, Kinesis, Glue, Athena…. I really recommend the big data certification. Read AWS documentation, specially for SageMaker, it is really good. Also, do the sample notebooks. Watch the AWS videos and play around with Python. 
For me, this was the most difficult AWS certification but it was also a lot of fun. Good Luck! 
I hope you enjoyed this article. Feel free to leave a comment or share this post. Follow me for future post. 
Written by 
Written by",Javier Ramos,2020-08-08T19:44:40.421Z
My Experience-AWS Machine Learning Certification Specialty Beta | by Manas Narkar | Medium,"AWS finally launched the much awaited Machine Learning Specialty Certification at reinvent 2018! Personally I was very much excited to learn about this exam and wanted to give it a shot at reinvent itself (and was discouraged to do so at the certification booth :) ). I finally attempted it this yesterday and wanted to share my experience without going in too much details and violating the NDA. 
Background: 
I have been professionally working with AWS Big Data stack for more than 5 years and my focus areas being Big Data platforms involving design, architecture and hands-on development. I have also been involved in architecting the end to end ML platform and have hands-on experience of working with AWS SageMaker (key service , more on this later). I was immediately on board when AWS released this beta exam as a perfect opportunity to test out my knowledge. I didn’t prepare for this exam specifically and relied heavily on my hands on experience with the platform and some knowledge of ML/AI. 
Exam Experience: 
It’s a 3 hr exam with total of 70 multiple choice questions. Most of the questions are medium to short length (with few exceptions of long ones) and to the point. Although, I did find wordings and options on the some of the questions confusing. Overall I found that AWS has significantly improved clarity on the exam questions. I found 3 hr time frame to be more than sufficient, it gave me ample opportunity to revisit the tricky questions. 
AWS has published the exam guide here. 
Here is the quick breakdown and commentary on each domain. 
Data Engineering ( 20%): 
There were few questions (i.e. felt it was less than 20% ) on AWS services that could be leveraged for data repository, data ingestion and data transformation. Services that came often included , AWS Kinesis, Glue, S3 ,Kinesis FiresHose , Kinesis Analytics, Lambda ,EMR , Athena etc. My experience with AWS Big Data platform definitely helped me to answer these questions. For folks without much exposure in this area, I would recommend to study these services and understand how they could be used for different use cases (i.e. for Ingest/Store/Transform). 
Exploratory Data Analysis (20%): 
I had couple of questions based on visualization technique that could be used for a particular scenario. Couple of questions where ML training related graph was presented and questions were asked based on that. These questions weren’t specifically geared towards any AWS services but in general were focused on how would you interpret particular graph related to ML activities and draw conclusions from it. Overall , felt like questions were much less than estimated 20%. 
Modelling (36%): 
This was the most fun and the challenging section of all and felt like 50% (or even more) of the questions were focused on this. This section heavily focuses on the Machine learning and deep learning algorithms aspect than the specific AWS services. Lot of questions asking you to chose particular ML algorithm given a business use case. SageMaker was featured in few questions along with the appropriate algorithm to chose so it will be worth while to get some hands on experience with it under your belt. 
Key Points: 
MachineLearning Implementations and Operations (20 %): 
Questions were focused on operationalizing the ML services. This one felt like 10–15% of the total questions. 
Key areas: 
Exam Preparation Guidelines: 
1 . Go through all ML Application level services such as comprehend, polly , lex, transcribe, translate and understand use cases for each. 
2. Get hands on experience with SageMaker and how integrates with other . AWS services. 
3. Understand various deep learning models and their use cases. 
4. Understand big-data ingestion patterns and recommended services. 
5. Understand data-prep, feature engineering activities in detail. 
Final Thoughts: 
I found that this exam is bit different (i.e. in a good way) than traditional AWS certifications and is very broad and deep. Very similar to Big Data certification and feels like a natural evolution of the same. ML/AI is very broad topic by design and there are not any courses available focused on this exam so getting hands on experience in some of the key technologies (like BigData, SageMaker) will help. I thoroughly enjoyed writing this exam and think it was a good learning experience overall. 
Written by 
Written by",Manas Narkar,2018-12-21T17:58:32.463Z
10 Critical Performance Optimization Steps You Should Take | by Ferenc Almasi | Better Programming | Medium,"As the web continues to evolve, performance is starting to become more and more important. Content-heavy web applications like Facebook or Instagram are dealing with megabytes of data. They provide rich user experiences that require heavy scripting in the background — all this at the cost of performance. 
But why you should care if your page takes three seconds to load instead of one? It’s not that much of a difference, is it? Why is it essential to optimize performance and reduce load times? And what can we do about it? 
Let’s first see the why and then we’ll see the how. 
We ideally want to provide the best user experience. Visitors expect a site to be snappy and responsive. They want it to load instantly. If your site takes too long to load, people won’t wait around long before they start to abandon it. Poor performance — because of extra network traffic — can also mean increased costs for users with a limited data plan. 
Another reason is improved conversions. Case studies shared on Web Fundamentals show that performance can have a serious positive impact: 
“Pinterest increased search engine traffic and sign-ups by 15% when they reduced perceived wait times by 40%.” 
But it can also have some serious negative effects: 
“The BBC found they lost an additional 10% of users for every additional second their site took to load.” 
This is why we must take some time to improve page load speed and performance. So let’s see how we can achieve that. 
Before starting to address the problems, we need to know how we can measure changes. There are two things you should keep in mind when dealing with performance improvements: 
With this in mind, let’s see the list. Everything starts on the server. We can make huge improvements before we even get our assets. 
If you haven’t already, enable server-side compression. This can greatly reduce the size of HTTP responses and has the most impact. Compress as many file types as possible. The only exceptions from this rule are some static assets, such as images, fonts, or PDF files. They are already compressed. Trying to compress them will only waste CPU resources, and in some cases, it can even increase file sizes. 
To see which type of compression your server supports, go to the Network tab in DevTools. Inspect the Accept-Encoding header in a request and you’ll see all available options: 
According to a benchmark test done by Akamai, Brotli seems to be a better option over gzip. They tested 1,000 websites and found that Brotli outperformed gzip by a median of 21%. 
While compression can save significant amounts of data, you can further cut down page load times by minimizing the number of HTTP requests. Some common techniques you can use are: 
Using a content delivery network can help you reduce response times. A CDN is a collection of servers distributed across the globe. It aims to help deliver content to users faster by choosing the server closest to the user’s location. 
Cache each resource to avoid unnecessary network trips. For static assets that rarely change — like images or fonts — you can use a long expiry date in the far future. For other resources, you can use an appropriate Cache-Control header. The time should depend on the frequency of your changes. The more frequently your assets change, the shorter your caches should be. 
Remember that redirects (301, 302) delay everything and slow down page load. Redirects most commonly occur when a trailing slash is missing from the URL. This causes the web page to reload everything with the correct URL. 
Now let’s move on to client-side optimizations and see how we can further improve things. 
When it comes to client-side optimization, we have some common strategies that we can use for all types of assets. These include things like compression/minification and bundling. You should implement both as part of your release process. 
I would like to close this article with some recommendations that can help you along the way. You can use them to audit your site and spot potential issues. You’ll also get useful tips on how to resolve them. 
All of the tools listed below are free to use. 
Lighthouse will probably be your first audit tool to use. It’s built right inside Chrome. You can reach it through the Audits tab. If you are not using Chrome, you can still use it online as the PageSpeed Insights tool on Google. If you also want to run audits for SEO or accessibility, web.dev will be the way to go. 
webhint provides you with deep details not only on performance but other aspects as well, such as common pitfalls or security issues. You can run your audits through its official website. You can use it in VS Code, as a browser extension, or even integrate it into your release process through CLI. It also gives you the ability to write and enforce your own set of rules. 
Just like the previous two, GTmetrix helps you discover performance issues and provides you with optimization opportunities. It generates a report for you with the most impactful issues at the top. 
The Performance tab in Chrome lets you analyze runtime performance. It provides you with all sorts of details, ranging from FPS drops to long-running scripts and how long it takes your site to render and paint everything. 
Lastly, to benchmark your JavaScript code you can use the Performance API. It provides you with a means of calculating how long it takes for your code to run. Previously, we could use console.time and console.timeEnd. However, performance gives more accurate results. Take the following code as an example: 
If you go through all steps mentioned above and fix any issues brought up by your audits, your site’s performance will be through the roof. 
What are some common performance pitfalls you come across from time to time? Let us know in the comments! Thanks for taking the time to read this article, happy debugging! 
Written by 
Written by",Ferenc Almasi,2020-09-09T11:28:49.432Z
Active Learning with PyTorch. Building Uncertainty Sampling… | by Robert Munro | PyTorch | Medium,"I have been building an Active Learning library with PyTorch to accompany my new book, Human-in-the-Loop Machine Learning. It addresses one of the most important problems in technology: how do human and machines combine their intelligence to solve problems? 
The code is open-source: 
Most deployed Machine Learning models use Supervised Learning powered by human training data. Selecting the right data for human review is known as Active Learning. Almost every company invents (or reinvents) the same Active Learning strategies and too often they repeat the same avoidable errors. 
Everyone gets taught how to build good Machine Learning algorithms in their college education, but not enough people are taught how to build in the human components. My book + code aims to plug that gap. 
Within the Knowledge Quadrant for Machine Learning, Active Learning combines Uncertainty Sampling which are your “known unknowns” (where you know your model is confused because of low confidence predictions) and Diversity Sampling which are your “unknown unknowns” (the gaps in your model knowledge that are not so obvious): 
Active Learning is the right column above: the Known Unknowns that you can address with Uncertainty Sampling, and the Unknown Unknowns that you can address with Diversity Sampling. 
People often underestimate how easy it is to build an ok Active Learning system and also underestimate how hard it is to get state-of-the-art results. You can implement Active Learning strategies at very different levels of sophistication and I’ll cover each in more detail in this article: 
This is a similar breakdown to what every Machine Learning Scientist is already familiar with for the predictive algorithm side of the problem. There’s often a good-enough method to get started with relatively little code. You can often deploy a good model with the right data and only a few 1,000 line of code. State-of-the-art models require much more work and often novel research. The same is true for Active Learning. 
My book and code supports each level of complexity. I get the readers started with a fully working system in the second chapter! 
Chapter 2 of the book implements a complete Human-in-the-Loop system in only ~500 lines of code, all in the active_learning_basics.py file in the library. The use case is to classify messages as being “disaster-related” or “not disaster-related”. 
This is a use case that I’ve worked in many times. I worked in post-conflict development for the UN before coming to the USA to complete a PhD focused on disaster response and health at Stanford. I’ve continued to work in disaster response in parallel to my Machine Learning career and I look for ways to being them together where-ever I can. 
The takeaway here is: create a complete Human-in-the-Loop Machine Learning system first, and then build on it. 
I often see data scientists hacking together Active Learning strategies: “Hactive Learning” (thanks to Jennifer Prendki for co-inventing this term). In many companies, I have seen scientists adding one hand-built filter after another to take care of all the edge cases as they pop up. By the end of it, they might have written 1,000s of lines of codes to process their data files, with no documentation about how to string their dozen or more scripts together to reproduce the result. As a result, they end up with non-reproducible Hactive Learning systems that miss important data, at best, or amplify biases at worst. 
The example in this chapter shows that you can build a more mathematically well-principled system with less code, provided that you design it right from the start. 
The code has examples of Uncertainty Sampling and Diversity Sampling that cover about 90% of what I have used in industry. These are in uncertainty_sampling.py and diversity_sampling.py in the library. 
The code to implement most of them is only a few 1,000 lines long. You could use the library or copy the code into your own code base, testing it within your framework. Any Data Scientist who wants to have a complete set of Machine Learning tools should understand the major Active Learning algorithms, so it is good practice to implement them yourself. The core algorithms are pretty simple — here’s a code snippet for Entropy-based sampling: 
Playing around with the different methods for Active Learning will also give you the intuition for which ones to start with for your projects. The graphic above is a visualization of the four most common Uncertainty Sampling methods. You can play around with this visualization at: http://robertmunro.com/uncertainty_sampling_example.html 
The takeaway here is: become familiar with the most common Active Learning strategies because they will be some of the most important tools in your Data Scientist tool-kit. 
Needless to say, it is hard to discover what you don’t know that you don’t know, and so the set of strategies for Diversity Sampling are more varied than for Uncertainty Sampling. The ones covered in the book are: 
State-of-the-art Human-in-the-Loop strategies use almost every new development in Machine Learning: Transfer Learning, Interpretability, Bias Detection, Bayesian Deep Learning, and Probing Neural Models to extract internal representations. 
Pushing the boundaries of Active Learning will often mean pushing the boundaries of other Machine Learning subfields. Sometimes, you can combine simpler methods, like the example above where you apply Uncertainty Sampling first and then Diversity Sampling. In other cases, you will experiment with new ways to interpret your model: 
For most advanced Active Learning methods, you need to understand what’s going on inside your model. You need to probe it for levels of activation (and then quantize with validation data); apply masks to sample the right parts of your data; and/or to introduce noise to simulate Bayesian dropouts. PyTorch makes this incredibly simple with the ability to pass the activation of every neuron back to other processes: 
The takeaway here is: the building blocks for innovation in Active Learning already exist in PyTorch, so you can concentrate on innovating. 
Writing this book was the first time I’d coded to PyTorch. Having written to other libraries for years, it was a joy to implement so many strategies so quickly. 
The most advanced techniques use Machine Learning to optimize the Active Learning strategies themselves. I will cover them in chapters that are still to come in the book! 
Active Learning is just one piece of a complete Human-in-the-Loop Machine Learning model: 
Every other step, from incorporating other models via Transfer Learning to Evaluating Quality for Annotation, can similarly be implemented simply or by pushing the state-of-the-art. 
The Human-in-the-Loop Machine Learning book is being published by Manning Publications as each chapter is being written, so stay tuned for more details about how each part of this puzzle fits together! 
Robert Munro 
October, 2019 
NOTE: to accompany this article, we are offering PyTorch users 40% off the book price! Please go to the Human-in-the-Loop Machine Learning page at Manning Publications, and check out with the code: pytorch40 
Written by 
Written by",Robert Munro,2019-10-21T15:41:55.247Z
Principal Component Analysis(PCA) — Dive deep | by Sanjay Ulsha | Analytics Vidhya | Medium,"The purpose of this article is to give the readers a flavor of geometric and mathematical intuition behind the dimensionality reduction technique, Principal Component Analysis(PCA). So, let’s keep going. 
Logistics for this article are as follows, 
First, let’s get familiar with some basic terms, 
Variance: Variance is a measure of how far a data set is spread out. It is mathematically defined as the average of the squared differences from the mean of a random variable(X). 
Co-Variance: Co-Variance is a measure of the relationship between two random variables. The metric evaluates how much — to what extent — the variables change together. 
1. Co-Variance matrix(C) : Represents the co-variances between the dimensions of a given data set(X) as a matrix. 
For example, consider a 3-dimensional data set(X) then its covariance matrix(C) is as, 
Here, co-variance along the diagonal represents the variance of each dimension. As C(i,j) = C(j,i) the Co-Variance matrix(C) is symmetric. We can write the co-variance matrix in the matrix notation as follows, 
If a typical form of normalization like column standardization is applied to the data set then the mean and variance of all dimensions will be zero and one. Hence we can write co-variance matrix as, 
Principal Component Analysis(PCA): 
Real-world data is too much messy, highly multi-dimensional and we perform data analysis, do plots on such data to find hidden patterns in it and use those to train the machine learning models. But as the # dimensions increase the difficulty to visualize and make computations on it also increases. 
So, we need to somehow reduce the # dimensions such that maximum information is preserved. Some ways we can think of are,* Remove the unnecessary/redundant dimensions and* Include only the most important dimensions. 
Principal Component Analysis(PCA) is one such type of the simplest dimensionality reduction techniques retaining maximum variance/spread along each orthogonal dimension called principal axes. Often used to make data easy to explore and visualize in lower-dimensional space. 
These new orthogonal dimensions are ranked based on the variance along with them i.e more important principal axis comes first that with maximum variance/spread. 
2. Geometric interpretation : Here we’ll go through a geometric interpretation of how the dimensions are reduced. 
Case — 1: Variance along one axis is more than other axes.Let’s consider a data set(X) with 2 dimensions/features say height and weight of a person, 
When we do a scatter plot of this data set we can observe that the variance along the F1(2–8) is more than the F2(4–6). 
Hence we can skip F2, and include only feature F1 in our data set preserving the direction with maximum information. 
Case — 2: Variance along all the axes is equal.Let’s consider a data set that which has equal variance along both the axis as, 
The variance along all the dimensions(F1, F2) is almost same. So, now we do find new dimensions(F1', F2') i.e vectors such that the variance is maximized along one of them as shown above figure using axis rotation techniques. 
— Here F1' is the first principal axis and F2' is the second principal axis. 
Now that we obtained the maximal variance axes in different dimensional space we can skip the dimensions in this space that with low variance and operate in the same. 
3. Objective function : Now that we’ve seen the geometric interpretation of PCA, we’ll now go through the mathematical perspective of it. 
Consider the previous example of geometric interpretation, from the figure (a) above we can see that spread is maximal along F1' and we need only that direction of maximal spread. So, we need to find a unit vector(u1) along that direction,i.e the magnitude is ||u1||=1. 
Let the original data set be, 
From figure (b) above we can see that x’(i) is the projection of x(i) on u1. 
Now the new projected data set in the lower dimensional space is, 
Goal: Finally we want to find ‘u1’ such that variance of projections of x(i) on to the unit vector(u1) is maximum as, 
Better be careful before proceeding for PCA we should always normalize our data because if we use the data(features here) of different scales, we’ll be getting misleading principal components. Typical form of normalization that is applied is column standardization. For the simplicity, I’m assuming we have already normalized data. 
Since we’ve applied the column standardization for data set(D) features then the mean vector values for each feature will be zero. 
Hence the mathematical objective function for finding the maximal spread principal components(ui’s) can be written as, 
We can write the above optimization problem in matrix notation using the co-variance matrix(C) that we’ve learned at the starting as, 
The above type of problem is called as constrained optimization problem with a constraint of each principal component(ui) to be a unit vector. 
Since the components are orthogonal to each other, the dot product between any two components is zero. 
4. Solution to the constrained optimization problem : 
Now that we’ve discussed the mathematical objective of the optimization function, we’ll now go through the solution part of it. 
We can solve this constrained optimization problem with our old friendly technique Lagrange Multipliers and change it to Lagrange form as, 
The equation (1) above is the definition of Eigen Values & Eigen Vectors where ‘u1’ is the Eigenvector and ‘λ’ is the corresponding Eigenvalue of ‘C’ i.e the co-variance matrix of ‘X’.Similarly, we can find the other Eigenvectors(ui) and Eigenvalues(λi) of the covariance matrix ‘C’ for each dimension(d). Hence PCA arrives at Eigen values and Eigenvectors. 
property : 
Now selecting the # dimensions such that maximum variance is captured in the other-dimensional space is the key thing of performing PCA. This can be achieved by the Eigenvalues(λi) that we get while solving the optimization problem. From this Eigenvalues we can generate the cumulative variance ratio as, 
The variance ratio tells us the percentage of information preserved in the new dimensional space for different # components. 
—This is a typical PCA cumulative variance ratio plot for different # dimensions got by solving the constrained optimization problem that we saw in the previous section.  — We can select # dimensions based on the maximum or required amount of info to be preserved in the new dimensional space. 
Once we select the # dimensions we can now project the original data set(D) points on to these top # Eigenvectors and get the new data set(D’) with reduced dimensions. 
Typically PCA is used for visualizing the high dimensional data(nD) in the lower dimensional space(2D) so that we get some sense of how data points are spread across in the higher dimensional space. 
Limitations: 
The red arrows denote the principal axes, which do not capture the complete variance as per the assumption. 
3. Orthogonality: PCA also assumes that the principal components are orthogonal to each other. From figure ‘B’ above we can see that there is some info loss as principal axes are orthogonal. 
Since PCA is a fairly old technique, there has been considerable work on improving it when the above assumptions fail. Depending on the use case, we may want to use one of the more advanced techniques out there. 
Conclusion : 
Principal component analysis(PCA) is an unsupervised technique used to pre-process and reduce the dimensionality of high dimensional data while preserving the original structure and relationships inherent to the original data set so that machine learning models can still learn from them and be used to make accurate predictions. 
For a very nice visual explanation and my favorite, you can go through,Principal component analysis(PCA). 
This brings us to the end of the discussion regarding intuition behind basic dimensionality reduction technique, Principal component analysis(PCA). Let’s discuss in the comments if you find anything wrong in the post or if you have anything to add :P. 
References:[1] https://www.appliedaicourse.com[2] https://en.wikipedia.org/wiki/Principal_component_analysis,[3] https://www.statisticshowto.datasciencecentral.com/[4] http://setosa.io/ev/eigenvectors-and-eigenvalues/[5] https://arxiv.org/pdf/1404.1100.pdf 
You can also find and connect with me on LinkedIn and GitHub. 
Have a look at my previous articles:Give them a read and your feedback is highly appreciated. 
[1] Sketch2Color anime translation using Generative Adversarial Networks(GANs).[2] A mathematical intuition on Stochastic Gradient Descent (SGD). 
Written by 
Written by",Sanjay Ulsha,2020-04-30T06:21:42.533Z
Anomaly Detection with MIDAS. Anomaly detection in graphs is a… | by Nunzio Logallo | Towards AI — Multidisciplinary Science Journal | Medium,"Anomaly detection in graphs is a severe problem finding strange behaviors in systems, like intrusion detection, fake ratings, and financial fraud. To minimize the effect of malicious activities as soon as possible, we need to detect anomalies in real-time to identify an incoming edge and decide if it is anomalous or not. Existing methods, process edge streams in an online manner and can miss a large amount of suspicious activity; in contrast to this, MIDAS detects microclusters anomalies in edge streams using constant time and memory, providing theoretical bounds on the false positive probability. 
MIDAS is a project made by Siddharth Bhatia, Bryan Hooi, Minji Yoon, Kijung Shin, and Christos Faloutsos. 
Main MIDAS contributions are:1. Streaming Microcluster Detection, novel streaming approach for detecting microcluster anomalies;2. Theoretical Guarantee, on the false positive probability of MIDAS;3. Effectiveness, MIDAS’ experimental results show that MIDAS outperforms the baseline approaches by 42%-48% accuracy and processes the data 162–644 times faster. 
If we compare MIDAS to previous approaches that detect anomalies in edge streams, we see that MIDAS includes more features like Microcluster Detection and Guarantee on false-positive probability, keeping the other elements of other approaches. 
There are two approaches proposed: MIDAS and MIDAS-R. Here is an overview: 
If you want to learn more about the algorithm, please visit the MIDAS repository. 
In the graph above, which plots the ROC curve for MIDAS, MIDAS-R, and SedanSpot (a consistent anomaly detection approach), we can see that MIDAS is 42% more accurate compared to the baseline, and also run significantly faster (644×). 
In the graph above, which plots the average precision score vs. the running time, we see that MIDAS is 27% more precise compared to the baseline. In comparison, MIDAS-R is 29% more precise, achieving the highest average precision score. We can say that both MIDAS and MIDAS-R outperform other anomaly detection approaches in edge streams. 
The graph above shows the scalability of MIDAS and MIDAS-R. As we can see, it confirms the scalability of them compared to the processing time per edge with an increase in the number of edges. Both MIDAS and MIDAS-R, allow real-time anomaly detection, processing 4M edges within 0.5s. 
One last time we compare MIDAS, MIDAS-R, and SedanSpot measuring their anomaly scores in a real-world example: TwitterSecurity dataset. The graph above plots anomaly scores vs. day, from May to September 2014. As we can see, we have different peaks of anomalies that coincide with significant events in the TwitterSecurity timeline for MIDAS. In contrast, SedanSpot simply outputs a lot of high anomalousness scores, thereby leading to low AUC. 
Let’s think about one application of MIDAS in the manufacturing sector, where there are a lot of working machines interconnected as a graph; if these machines have strange behavior, it can result in an overrun of costs in terms of power consumption and raw materials waste. An anomaly detection algorithm like MIDAS is capable of detecting these strange behaviors in real-time, reducing, and preventing a loss. There are many more applications of MIDAS, for example, as a detector of fake accounts for social networks like Twitter and Facebook, where there are people or bots who create false identities. MIDAS can help in detecting fake news too, deciding whether an article is real or it is just a clickbait. 
MIDAS and MIDAS-R make the detection of anomalies in edge streams, faster and more accurate, keeping high scalability and real-world effectiveness. 
If you want to learn more about MIDAS, check the MIDAS repository where you can find examples and a getting started guide. If you have any questions, please don’t hesitate to contact Siddharth Bhatia. 
Siddharth Bhatia, Bryan Hooi, Minji Yoon, Kijung Shin and Christos Faloutsos. “MIDAS: Microcluster-Based Detector of Anomalies in Edge Streams.” AAAI Conference on Artificial Intelligence (AAAI), 2020. https://arxiv.org/abs/1911.04464 
Nunzio Logallo 
Towards AI publishes the best of tech, science, and engineering. Subscribe with us to receive our newsletter right on your inbox. For sponsorship opportunities, please email us at pub@towardsai.net Take a look 
Written by 
Written by",Nunzio Logallo,2020-06-29T10:10:31.487Z
Building a real-time anomaly detection system for time series at Pinterest | by Pinterest Engineering | Pinterest Engineering Blog | Medium,"Kevin Chen | Software Engineer Intern, VisibilityBrian Overstreet | Software Engineer, Visibility 
In this post, we’ll share the algorithms and infrastructure that we developed to build a real-time, scalable anomaly detection system for Pinterest’s key operational timeseries metrics. Read on to hear about our learnings, lessons, and plans for the future. 
Background 
Pinterest uses an in-house metrics and dashboard system called Statsboard that allows teams to gain a real-time understanding of the state of their services. Using Statsboard, engineers can create alerts using a language that wraps common time series operations into a simple interface. These alerts include: 
However, many of Pinterest’s top line growth and traffic metrics (ex: site requests, user logins) exhibit dynamic patterns that make it difficult to set rule-based alerts. Without an anomaly detection system capable of building a model to handle these dynamic metrics, users are faced with three undesirable alert situations: 
Landscape of anomaly detection 
There is a great deal of literature on anomaly detection out in the world — from open-source packages like Twitter’s AnomalyDetection or Linkedin’s Luminol, to academic works like Rob Hyndman’s papers on feature-based anomaly detection. We can classify the most popular of these time series anomaly detection techniques into four broad categories (which are neither mutually exclusive nor all encompassing): 
In practice, we observed the best performance with decomposition models. Many machine learning models perform poorly for anomaly detection because of their tendency to overfit on the training data. For anomaly detection, we need to be able to distinguish and extract even the anomalies that we haven’t seen before. Decomposition models perform well at this task by explicitly removing correlated components (seasonality, trend) from the timeseries — giving us the opportunity to apply statistical tests with gaussian assumptions on the residuals whilst preserving the anomalies’ original attributes. 
Requirements of anomaly detection for observability 
To build an anomaly detection system for observability, we have to keep a number of requirements in mind: 
Next, we’ll dive into the challenges we faced deploying our models in real-time. 
How do we update our model in real-time? 
Much of the literature on anomaly detection deals with finding retrospective anomalies in a static dataset i.e fitting a model with a nonlinear optimization procedure, such as LBFGS or Nelder-Mead, and then identifying anomalies within the training set. 
However, if we want to find anomalies in real-time, training once is not enough — we have to continuously keep our model up to date to adapt to the latest behavior of our metric. Thus, we have to take one of four approaches to update our model parameters over time. 
In our case, we found that the operational overhead of debugging scheduled or event driven systems outweighed their benefits. Instead, we use online updates whenever available, and brute force updates otherwise. To recast our algorithms into the online setting, we use Online Gradient Descent as an optimizer. 
Seasonality and trend estimation 
The most commonly used form of decomposition today, called STL, makes use of a regression procedure called Loess, which repeatedly fits low-degree polynomials to subsets of the data. Unfortunately, this procedure is not scalable for real-time analytics. 
In lieu of estimating seasonality with iterated Loess, we instead use a simple ensemble of efficient seasonality estimation techniques (e.g fourier, historical sampling). We also replace the trend component of STL with robust statistics viz. median, inspired by Twitter’s S-H-ESD anomaly detection algorithm. 
Prediction intervals 
Once we have our predictions, we can then use prediction intervals to determine whether a data point is an anomaly, based on its distance from our forecast. To do this, we can take advantage of the three sigma rule, which states that 99.73% of values in a normal distribution lie within three standard deviations of the mean. If an incoming data point is more than three standard deviations of the residual from our forecast, we can reasonably classify the point as an anomaly. 
If the residual data is non-normal (e.g when the time series contains anomalies), we can use power transformations to attempt to recover a normal dataset. But if these transformations do not work, we can still resort to other measures — like sampling from the previous errors — to get an estimate of the uncertainty. 
Once we’ve generated these intervals, we can translate them into visual bands on the UI side for ease of interpretation and tweaking by our end users. 
Now, let’s take a look at the infrastructure that supports these algorithms. 
Anomaly detection architecture 
We have a forecasting server that is responsible for constructing one-step-ahead forecasts for Statsboard metrics in real-time and persisting them to our time series database (TSDB). 
The forecasting server consists of a set of autoscaling workers and a server with a job queue and a scheduler that submits one-ahead forecast jobs every minute. For batch jobs, we pre-merge overlapping data window requests in order to minimize network and I/O. We integrate with our existing Statsboard UI and alerting infrastructure to provide anomaly detection builders, alerts, and dashboards. 
Summary 
Anomaly detection plays an important role in obtaining visibility for metrics that exhibit complex patterns that can’t be modeled by traditional alerts. Uncovered anomalies show up in real-time dashboards and alert the relevant users when something goes wrong. 
Through the use of robust, scalable, and interpretable algorithms, our anomaly detection system helps engineers recognize and react to incidents as they happen, minimizing impact to the Pinterest business and our Pinners. 
Acknowledgments 
Dai Nguyen built the UI for this project. Special thanks to Naoman Abbas, Humsheen Geo, Colin Probasco, and Wei Zhu of the Visibility team as well as Yegor Gorshkov of the Traffic team for design recommendations and review. 
Written by 
Written by",Pinterest Engineering,2019-07-30T17:26:00.541Z
Azure Logic Apps & Graph Security API: easy integration of all security alerts to your ticketing system | by Jeroen Niesen | Wortell | Medium,"If you are working in the security business, you probably know solving alerts “in the wild” isn’t the best thing to do. You should follow a decent Incident Management process. In the more professional companies an Incident Management System is deployed that supports this process. These kind of systems will track the incident in the process of solving and will make life of the SOC engineers easier. 
Connecting all security services to an incident management system can be a tough job. You need to have a good understanding of the software development world (so you can implement the API using the program language of your like) or the implementation contains a “not so solid” alert-flow that includes mailboxes, various scripts etc. In order to connect applications/apis in a decent way, without having any software development knowledge, Microsoft has invented Logic Apps. 
According to Microsofts own documentation: “A Logic App is a cloud service that helps you schedule, automate, and orchestrate tasks, business processes, and workflows when you need to integrate apps, data, systems, and services across enterprises or organizations. Logic Apps simplifies how you design and build scalable solutions for app integration, data integration, system integration, enterprise application integration (EAI), and business-to-business (B2B) communication, whether in the cloud, on premises, or both.”In short: it is the magic glue that sits between your applications and you can build without hiring a software developer. 
Azure Logic Apps let you define a “flow.” This flow starts with a “trigger” and various activities can follow after this trigger. Each activity let you run logic that is provided by a connector (e.g. create a ticket in a ticketing system). 
Logic Apps work with connectors. For each application/service you want to integrate, a connector needs to be available. For the most common “ticketing systems” like ServiceNow or FreshDesk a connector is available. You can find a complete list of all connectors here. Connectors provide activities and triggers to interact with application/service they are build for. If you build your own Incident Management System you can easily integrate by building your own custom Logic App connector. 
One of the connectors you could use is the connector for the Microsoft Graph Security API. This connector lets you use activities that get data out of the Graph Security API. 
According to Microsoft, the Graph Security API is: “ Microsoft Graph Security API is an intermediary service (or broker) that provides a single programmatic interface to connect multiple Microsoft Graph Security providers (also called security providers or providers)” So the Microsoft Graph Security API is an programming interface that lets you: 
At the moment of writing, the following Microsoft Security products are supported by the Graph Security API: 
As stated earlier in this post, a Logic Apps connector is available for the Graph Security API. This means that you can integrate with the Graph Security API without having any software development/programming knowledge. 
Microsoft has made a schema that contains the alert data. This schema is the same for each alert. Please have a look at the Microsoft documentation for all properties that are available trough the Graph Security API. 
Learning and displaying what’s in the Graph API can be hard. Microsoft has build the Graph Explorer, this a web application that let you explore the Graph API, and see what’s in it. 
Pro Tip: You can use the Graph Explorer to explore what data of your organisation in the Graph API. You can modify the list of “example queries” and enable the security examples. By logging in and clicking on “modify permissions” you can make sure your account has the right permissions. 
You could use the following flow (logic app) to integrate with a ticketing system. In this case I used FreshDesk to integrate with the Graph Security API. 
Let me explain the steps used in the above example: 
— Recurrence: In this step I make sure that the logic app will every minute. Unfortunately the Graph Security API connector does not have a trigger available. By running the Logic App with an interval we can pull the Graph Security API. 
— Get Alerts: In this activity (step) I will collect all new alerts out of the Graph Security API. Using Status eq 'newAlert' I make sure only new alerts will be collected. In a later activity I will update the state of an alert as it is processed. 
— For Each: The result of the previous activity is an array (list) of alerts. In this activity I will loop trough al the alerts and execute some logic on them. 
— Create Ticket: In this activity I will create a ticket in the ticketing system (e.g. FreshDesk). Depending on the ticketing system of your like, you need to replace this activity with an activity that connects with you ticketing system. In these kinds of activities you can set the description, title, severity etc. of the ticket. In this activity you can reference the values that are coming from the Graph Security API. 
— Update Alert: This is the last activity of the Logic App. In this activity I will update the status of the Graph Security API alert status. I’ll update the status from newAlert to inProgress. Doing so will make sure the next time you run the Logic App, this alert is not collected anymore. 
Pro Tip: In the activity “Get Alerts” you could also apply a filter to only get alerts from a certain security products. For example, you could use the following filter vendorInformation/provider eq ‘ASC’&Status eq ‘newAlert' to only get new alerts from Azure Security Center. 
Using the Graph Security API together with Logic Apps make it quite easy to have all Microsoft products integrated in your ticketing system or incident management system. 
Integrating all security products in one time might cause alert fatigue. This means you probably will get a huge amount of alerts. This can be solved by using a lot of automation or tuning your security products. If you have not tuned your security products, I would rather start doing that before integrating using the Graph Security API 
Happy integrating the Microsoft security products by using Logic Apps and the Graph Security API! — Jeroen Niesen 
Written by 
Written by",Jeroen Niesen,2020-01-29T06:39:56.432Z
The law and cybersecurity: eight ways to help mitigate legal risks | by Mike Mullane | The Startup | Medium,"The European Union’s General Data Protection Regulation (GDPR) has transformed the way data is treated, as businesses around the world are avoiding the additional costs of managing different data regimes. Comparable laws giving local residents more control over their data are now starting to come into effect in other countries. For example, the California Consumer Privacy Act (CCPA) gives their residents the power to demand the deletion of information. 
Different kinds of legal risk 
Regulatory compliance is not the only challenge. Litigation poses a serious threat to organizations, especially in cases where customers, employees or business partners suffer actual financial losses — for example, in the case of criminals taking advantage of poor security to steal credit card information. The phrase ‘actual financial losses’ can also refer to a drop in a company’s share price. This was the case, for example, when Yahoo shareholders brought a class action lawsuit after the company’s market value dropped as a result of criminals taking advantage of poor security to steal sensitive data. Yahoo settled for USD 80 million in early 2018. 
It is essential, in terms of mitigating the risk of fines or litigation, that organizations are able to demonstrate that their services are safe and that they are taking reasonable care to protect the data of their customers and business partners. In the event of a data breach due to inadequate protection measures, some legal systems view unkept promises made to customers about safeguarding their data as tantamount to engaging in unfair and deceptive practices. In 2017, the US health insurance company Anthem settled a class action lawsuit for USD 115 million over a breach that had compromised the personal information of nearly 79 million people. 
It is important to seek out good advice, not least because not knowing the law has never worked as a defence for failing to comply. Fortunately, international standards, which are based on global best practices identified by the consensus of the world’s leading experts, provide invaluable help and support. There are more than 40 standards that comprise the ISO/IEC 27000 family of information security management standards. This family provides requirements and supporting guidance for establishing, implementing, maintaining and the continual improvement of an information security management system. These can be used to provide guidance and support to an organization to address the information security and privacy protection requirements of GDPR to help them achieve compliance, for example. 
Here are eight things organizations can do to help satisfy the most stringent legal regulations with the help of IEC and ISO standards. 
1. Establish an information management security system (ISMS) 
The ISMS requirements described in the ISO/IEC 27001 defines a cyber risk management-based approach to managing people, processes, services and technology. Using ISO/IEC 27001, helps organizations to manage their information security risks, including threats, vulnerabilities and impacts, as well as designing controls to protect the confidentiality, integrity and availability of data and for regulating access to critical information systems and networks. It emphasizes the importance of the ISO/IEC 27001 risk management process taking account of legal, regulatory and contractual requirements. (See point 8) 
2. Commission an independent audit 
In terms of mitigating cyber risk, the first step every organization should take is to implement the ISMS standard ISO/IEC 27001 and then commission an independent ISMS certification audit to ensure compliance with the requirements of ISO/IEC 27001. An ISMS certification will help organizations demonstrate their cyber-risk approach has considered local and international laws and regulations. ISO/IEC 27001. ISO/IEC 27014, which offers support on the governance of information security, recommends such an approach. Other standards in the family that support the implementation of ISO/IEC 27001 include: ISO/IEC 27005, which provides guidance on information risk management; and ISO/IEC 27004, which suggests metrics for evaluating the effectiveness and performance of information security systems. 
The aim of an ISMS certification audit is to verify that the organization has considered and assessed the cyber-risks it faces and that they have implemented an effective and appropriate set of controls to mitigate these risks, this includes both information security and privacy protection controls. This certification audit should verify that the organization has taken account of all business, contractual, legal and regulatory requirements (e.g. GDPR) in its risk assessment. ISO/IEC 27014 provides guidance on establishing an information security governance framework to ensure that the organization is properly addressing is internal governance requirements in compliance with external rules and regulations. 
3. Keep an accurate data inventory 
It is impossible to manage risk effectively or to comply with regulations about access and portability, without the implementation of an effective set of controls. For example, an organization should have an accurate inventory of data and network assets. ISO/IEC 27002 is a code of practice which is a collection of such information security controls with guidelines for implementing these controls, for example, for identifying information assets, defining appropriate protection responsibilities and maintaining an inventory that is up-to-date, consistent and aligned with an organization’s other inventories. ISO/IEC 27002 is a baseline control set supporting ISO/IEC 27001 and the mitigation of cyber risk. 
4. Implement a Privacy Information Management System (PIMS) 
ISO/IEC 27701 is an extension to ISO/IEC 27001 that provides a comprehensive set of operational controls for implementing, maintaining and continually improving a PIMS, including privacy processing controls. Implementing ISO/IEC 27701 and ISO/IEC 27001 helps to meet the EU GDPR’s requirement for “appropriate technical and organizational measures”. It maps its recommendations to the GDPR (Annex D). 
5. Facilitate portability and implement a data minimization process 
The GDPR gives individuals the right to access their data and find out how it is being used. ISO/IEC 19941 provides support to organizations who need to enable their customers to move their data or applications between non-cloud and cloud services, as well as between cloud services. Another important requirement of the GDPR is “data minimization”, which means keeping data that can identify individuals for no longer than necessary. ISO/IEC 27018, a code of practice for protection of personally identifiable information (PII) in public clouds, contains important advice for the secure erasure of temporary files within a specified, documented period, a complementary standard is ISO/IEC 27017 which addresses the information security in the cloud. Another standard, currently under development, ISO/IEC 27555 will provide guidelines on establishing a PII deletion concept in organizations. 
6. Implement an incident response plan 
An incident response plan is important in terms of mitigating the risk of litigation. It also helps to ensure that the breach notification requirements of the GDPR (72 hours) and of any other relevant laws or regulations are respected. The two-part ISO/IEC 27035 presents principles of incident management and a complete guide to planning and preparing for incident response. 
7. Don’t forget supplier relationships in your security strategy 
It is vital that an organization’s legal risk mitigation strategy takes into account third-party relationships, which take the security practices of the vendor into their own risk profiles. This was the case, for example, with the US retail giant, Target, after hackers used the network credentials of a heating, ventilation and air-conditioning company to steal personal data from tens of millions of credit and debit cards. Target has paid USD 18.5 million to settle multi-state claims, as well as another settlement of USD 10 million following a class action lawsuit in addition to compensation of up to USD 10,000 to customers who have suffered directly from the data breach. The four-part standard ISO/IEC 27036 provides guidance on supplier relationships, including supply chain and cloud service security. 
8. Take out cyber-insurance 
Organizations are strongly advised to have adequate cyber-insurance in place to cover any operational or legal costs, including possible fines, related to serious breaches. ISO/IEC 27102 provides guidelines on cyber-insurance to cover potential financial losses. The standard looks at the kind of losses covered and what measures need to be on place to satisfy the insurance providers. ISO/IEC 27102 notes that an ISMS “can provide the insured and insurer with data, information and documentation that can be used in cyber-insurance policy inception, cyber-insurance policy renewal and throughout the lifetime of that cyber-insurance policy”. 
Written by 
Written by",Mike Mullane,2020-04-03T14:19:24.715Z
Zero Trust Architecture -“An alternative security model” | by Pyramid Solutions | Medium,"Zero Trust Architecture is an alternative security model created when organizations began exposing their data to the World Wide Web. The initiative (Zero Trust Architecture) was implemented to place a higher level of trust in data transferred within the network and executed at the application level which helps to make an application “aware” of the Data, Assets, Applications, and Services (DAAS) that interact with it. For example, when you sign in to your work email from a new device, and the platform prompts you to verify your identity, and that’s Zero Trust Architecture at work. 
The motive to adopt Zero Trust 
With the rise of IoT and machine learning, businesses require more resilient cybersecurity strategies to avoid data breach and regulatory fines. Considering that 84% of CISOs believe cybersecurity breaches are inevitable, and Zero Trust Architecture can help them have peace of mind while the workflow is in action (StreetInsider.com). With this architecture, businesses will have a smarter, more powerful, and cost-effective data security strategy due to effective operational efficiencies. And as you move to the public cloud, you can take Zero Trust Architecture with you by inserting a Virtual Segmentation Gateway into the virtualization stack of the public cloud service. As you think to move ahead with adopting zero trust architecture, you need to know the core elements of it. 
Identify traffic and data that map to actual business flows and establish visibility into the applications that drive these communications. Your IT needs to know who’s user, what applications they’re using, and the best way to connect those apps to the network so that you can enforce security policy effectively and secure access to your data. 
Reduce the number of pathways for attackers and malware to latch your devices and applications. By assuming devices aren’t trustworthy, you’ll be able to stop dormant hackers from consistently accessing information under the radar. 
Build “inspection points” into popular junctions to spot in-network attackers as they navigate your systems. Create security rules that can be used to identify, allow, or deny traffic that moves through these junctions. This strategy can help you form trust boundaries and prevent the risk of highly sensitive data. 
Adding multiple user-authentication gateways can help you identify the actual user before they gain access to your network. 
Hence the purpose of zero trust architecture is to limit the trust you have in your own network. It may seem simple or complex at times but Zero Trust Architecture is something that gives more visibility on security concerns. 
Written by 
Written by",Pyramid Solutions,2020-02-10T10:23:18.074Z
Responses – Harris Brakmic – Medium,"Senior Software Gardener @advarics 
Back then, in 2015, Data Science and the rest (Machine Learning etc.) were praised as the next big thing. You maybe remember the quote: “Statistician will be the sexiest job of the 21st century”. It was like the whole web was filled with articles on how to make “intelligent this, intelligent that, machine learning my hamster, data science your cat”…",NA,NA
A Beginner’s Guide to Latent Dirichlet Allocation(LDA) | by Ria Kulshrestha | Towards Data Science,"Topic modeling is a method for unsupervised classification of documents, similar to clustering on numeric data, which finds some natural groups of items (topics) even when we’re not sure what we’re looking for. 
A document can be a part of multiple topics, kind of like in fuzzy clustering(soft clustering) in which each data point belongs to more than one cluster. 
Topic modeling provides methods for automatically organizing, understanding, searching, and summarizing large electronic archives. It can help with the following: 
For example, let’s say a document belongs to the topics food, dogs and health. So if a user queries “dog food”, they might find the above-mentioned document relevant because it covers those topics(among other topics). We are able to figure its relevance with respect to the query without even going through the entire document. 
Therefore, by annotating the document, based on the topics predicted by the modeling method, we are able to optimize our search process. 
It is one of the most popular topic modeling methods. Each document is made up of various words, and each topic also has various words belonging to it. The aim of LDA is to find topics a document belongs to, based on the words in it. Confused much? Here is an example to walk you through it. 
We have 5 documents each containing the words listed in front of them( ordered by frequency of occurrence). 
What we want to figure out are the words in different topics, as shown in the table below. Each row in the table represents a different topic and each column a different word in the corpus. Each cell contains the probability that the word(column) belongs to the topic(row). 
Let’s say we have 2 topics that can be classified as CAT_related and DOG_related. A topic has probabilities for each word, so words such as milk, meow, and kitten, will have a higher probability in the CAT_related topic than in the DOG_related one. The DOG_related topic, likewise, will have high probabilities for words such as puppy, bark, and bone. 
If we have a document containing the following sentences: 
“Dogs like to chew on bones and fetch sticks”. “Puppies drink milk.” “Both like to bark.” 
We can easily say it belongs to topic DOG_related because it contains words such as Dogs, bones, puppies, and bark. Even though it contains the word milk which belongs to the topic CAT_related, the document belongs to DOG_related as more words match with it. 
There are 2 parts in LDA: 
Suppose you have various photographs(documents) with captions(words). You want to display them in a gallery so you decide to categorize the photographs on various themes(topics) based on which you will create different sections in your gallery. 
You decide to create k=2 sections in your album — nature and city. Naturally, the classification isn’t so clear as some photographs with city have trees and flowers while the nature ones might have some buildings in it. You, as a start, decide to assign the photographs which only have nature or city elements in them into their respective categories while you randomly assigned the rest. 
You notice that a lot of photographs in nature have the word tree in their captions. So you concluded that the word tree and topic nature must be closely related. 
Next, you pick the word building and check how many photographs are in nature because they have the word building in their caption. You don’t find many and now are less sure about building belonging to the topic nature and associate it more strongly with the topic city. 
You then pick a photograph which has the caption “The tree is in front of the building and behind a car” and see that it is in the category nature currently.You then chose the word tree, and calculate the first probability p(topic t | document d): other words in the caption are building and car, most photographs having captions with building or car in it are in city, so you get a low probability.Now the second probability p(word w| topic t): we know that a lot of photographs in nature have the word trees in it. So you get a high score here.You update the probability of tree belonging in nature by multiplying the two. You get a lower value than before for tree in topic nature because now you have seen that tree and words such as building/car in the same caption, implying that trees can also be found in cities. For the same reason, when you update the probability for tree belonging in topic city, you will notice that it will be greater than what it was before. 
After multiple iterations over all the photographs and for each topic, you will have accurate scores for each word with respect to each topic. Your guesses will keep getting better and better because you’ll conclude from the context that words such as buildings, sidewalk, subway appear together and hence must belong to the same topic, which we can easily guess is city. Words such as mountains, fields, beach which might not appear together in a lot of captions but they do appear often without city words and hence will have higher scores for nature. While words such as trees, flowers, dogs, sky will have almost the same probability of being in either as they occur in both topics. 
As for the photograph, you see that it has 1 word (with average probability) from category nature and 2 words (with high probability) from city, you conclude, it belongs to city more strongly than it does to nature and hence you decide to add it in city. 
The applications of LDA need not be restricted to Natural Language Processing. I recently implemented a paper where we use LDA( along with a Neural Networks) to extract the scene-specific context of an image. If you are interested in learning more about that please leave a comment or a message. 
I‘m glad you made it till the end of this article.🎉I hope your reading experience was as enriching as the one I had writing this. 💖 
Do check out my other articles here. 
If you want to reach out to me, my medium of choice would be Twitter. 
Hands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look 
Written by 
Written by",Ria Kulshrestha,2020-09-28T08:35:14.452Z
Medium SEO: Medium Article Search Engine Optimization | by Casey Botticello | Blogging Guide | Medium,"I have received literally hundreds of requests for an article on Medium SEO, so I am happy to be finally writing this! 
Because many writers are unfamiliar with Search Engine Optimization, this guide will attempt to distill relatively complex topics into understandable bits. 
This article contains three main sections: 
If you are already well-versed in SEO, you can probably skip the first section and go straight to the section on Medium-specific SEO. 
SEO stands for Search Engine Optimization, which is the practice of increasing the quantity and quality of traffic to your website through organic search engine results. 
To expand on the terms used in this definition: 
This is illustrated below by the (successful) differentiation of content for the two sample similar key word phrases from above: 
As an example, I wrote an article for my publication, Strategic Communications, called 5 High Authority Social Media Platforms to Boost SEO (That You Don’t Already Have): 
This article appears first in the Google search results for the phrase: 
high authority social media platforms 
So without spending money on ads, this article organically generates traffic. 
On-page SEO is the practice of optimizing individual web pages in order to rank higher and earn more relevant traffic in search engines. 
On-page refers to both the content and HTML source code of a page that can be optimized, as opposed to off-page SEO which refers to links and other external signals. 
This guide will focus on the On Page SEO factor’s that Medium easily allows users to modify or create. 
I based the topics on the 16 On Page SEO concepts in the image above. All the explanations below have been tailored to Medium articles. Some require a lengthy explanation, others require little explanation as Medium automatically completes some portion or all of a specific on-page SEO tip. 
Throughout this article I reference SEO keywords. Your SEO keywords are the keywords and phrases in your web content that make it possible for people to find your site via search engines. A website that is well optimized for search engines “speaks the same language” as its potential visitor base with keywords for SEO that help connect searchers to your site. Keywords are one of the main elements of SEO. 
For the purposes of most Medium articles, the keywords are the word or phrases that best describe the topic you are writing about. 
Medium makes this process easy, since it automatically makes your article’s URL match its title by default (which, typically, includes your keyword). 
For example, in the Google search results for the phrase “how to create a medium publication” my article How to Create a Medium Publication appears with a URL that clearly articulates what readers can expect when they click on the link. 
URL Example 
Optimized: http://example.com/blog/url-optimization-tips 
Not Optimized: http://example.com/blog/url_optimization_tips 
You do not need to always start your title with your keyword, but if you have some flexibility in your article title, it is worth doing. It is important to put the most important keywords in the beginning of the URL because search engine spiders do not give as much significance to words toward the end of a URL. 
Keyword modifiers are words that you add to your main keyword to produce a more specific longtail keyword. 
Using modifiers like “2019”, “best”, “guide”, “checklist”, “fast” and “review” can help you rank for long tail versions of your target keyword. 
In the structure of a keyword, modifiers usually come at the beginning and the end of the keyword: 
modifier | head | modifier 
The modifiers change an aspect of the keyword but not its meaning. 
Some examples (modifiers in bold): 
Best-selling money making mobile apps in 2020 
Beginner stock photography guide 
DSLR cameras under $1000 
For example, if we are trying to come up with modifiers for the keyword marketing, based on search history of Google users, we can simply enter the term “marketing” in the search bar. The bolded words for each result are almost always some type of modifier: 
Medium automatically formats your Title with the H1 tag so there is nothing to worry about for this SEO aspect! 
However, Medium does let you customize your title, which can definitely be useful for boosting an article’s appearance: 
Adding multimedia to your website is an effective way of drawing in more viewers and building your brand. People have become extremely visual. It would be nearly impossible effectively position your brand today without including audio, video and supporting images to build brand recognition. Put simply, building your brand requires the use of every multimedia asset available to you. 
Images — They are easy to share and can spice up your content. If you have high-quality images included on your website, you can guarantee they will be seen, and possibly shared. 
Slideshows — Slideshows enable you to feature multiple images, video and even audio. A well-organized slideshow generates a strong visual impact and can keep visitors fully engaged. Viewers have become so accustomed to seeing slideshows that it has to be unique and eye-catching in order to make an impact. 
Videos — Video content is more likely than text to appear on the first page of search engine results. Additionally, just having the word ‘video’ improves click-through rates by an estimated 55%. This is the easiest way to make your content more appealing and shareable. 
Include most of your keywords in the first 100–150 words. Don’t just create a long intro — use all this space to include as many keywords as possible. Remember — keywords should be relevant and included organically in the content. Like with a keyword-rich title, keywords in your content should be logically connected and should briefly describe the topic of your article. 
Google has designed a mobile ranking system favoring mobile-friendly websites, making it almost necessary to invest in responsive design. Google has favoritism towards responsive design for a few reasons. 
Luckily, one of the many advantages of using Medium as your publishing platform is that pages are optimized for mobile viewing and tend to load quickly compared to many blogs (this is not an SEO factor you need to worry about). 
Internal linking is linking to one page to another page within your site. It helps to optimize your site for search engine. Because Medium is such a large site and your articles index relatively quickly, this is not a major concern, SEO-wise. A good example of this is when Medium writers reference a previous article they wrote, in one of their other articles on Medium. 
For example, my article, The Best Medium Formatting Guide, contains links to other Medium articles I have written. In the example below, I reference a related article about Medium Canonical Links (which I also wrote and helps link together my articles. 
If you have a publication, this is also demonstrated. If you Google “medium blogging guide” you will see that underneath the main link to my publication, are links of articles found in that publication. This took months of linking new articles before Google finally indexed the content in this format: 
When it’s come On Page SEO site speed is very important. Luckily when it comes to site speed, Medium has already taken care of that (unlike a self hosted blog). 
LSI (Latent Semantic Indexing) keyword is a keyword it’s semantically related to your target keyword. 
This topic is a bit complex, and not necessary for most writers to worry about. But if you are really looking at all the relevant SEO factors, you cannot ignore LSI keywords. 
Simply put — semantic web is the aim to prepare web contents in a way that they’re not only understandable by human readers, but also to machine readers. 
Semantic Web aims to organize web contents as the entities they represent instead of text, image and codes. 
Machine readers includes things like search engines, web crawlers or even web browsers, especially Google Chrome. 
Making your contents understandable to machine readers will grant significant benefits. Few examples: 
First of all, you need a way to gather LSI keywords for your content. There are various tools and methods available for finding LSI Keywords. Some of the examples: 
One of the most important steps to take under the SEO umbrella is image optimization. Google itself acknowledges the importance of imagery and shares some helpful tips on image optimization on their guidelines for image publishing. 
One of the most overlooked features of Medium article’s is the ability to add alt text to images. 
Also called “alt tags” and “alt descriptions,” alt text is the written copy that appears in place of an image on a webpage if the image fails to load on a user’s screen. This text helps screen-reading tools describe images to visually impaired readers and allows search engines to better crawl and rank your website. 
Whether or not you perform SEO for your Medium articles, optimizing your article’s image alt text is your ticket to creating a better user experience for your visitors, no matter how they first found you. 
If you’re creating content on a topic that requires the support of visuals, consider how your audience might prefer to find answers to their questions on that topic. In many cases, Google searchers don’t want the classic blue, hyperlinked search result — they want the image itself, embedded inside your webpage. 
One of the most important things image alt text can do for you is turn your images into hyperlinked search results — giving your website yet another way to receive organic visitors. 
2. Click on the image you wish to add alt text to. Click on the new “Alt-text” button. 
3. Write a brief description of this image for readers with visual impairments or to optimize SEO. 
4. Click the “save” button and you are done. 
Long form content has proven to be more engaging, more shareable, and better for SEO. Simply put, studies show that longer content dominates page one of search rankings. 
SerpIQ ran a study charting the top 10 results in search queries by content length. 
Here’s the Graph: 
The first result typically has 2,416 words and the 10th result has 2,032 words. This shows that Google prefers content rich sites. 
The fact that not one of the results on an average page falls below 2,000 words is pretty compelling on its own. 
Does this mean you need to make all your Medium articles this long? 
Absolutely, not! 
But it is a good reminder that readers and search engines toward to gravitate toward longer content (assuming all else equal). 
Dwell time, the amount of time a user spends on your page before going back to the search results to find something else is an important user experience metric. It gives website owners an idea of how useful readers find their content and whether or not it meets their needs. 
Search engines take this user behavior into account although it’s not officially a ranking factor. When users spend very little time on a website after clicking on its search, it gives search engines reason to demote it in SERPs. They could potentially do this because it signals that the search result wasn’t able to answer the visitor’s question and they had to go back and look for a different result. On the other hand, if a user does spend a considerable amount of time on a website, the search engine would improve its ranking in SERPs because it indicates that the visitor was satisfied with the content on that page. 
One creative way to boost dwell time, that I have personally utilized, is creating a narrated version of of your article. 
In addition to increasing dwell time, anecdotally, Google also seems to favor content that exists in multiple formats (illustrative images, written content, audio, etc.). 
Medium takes care of social share buttons so there is no need to worry about programming them. 
— Casey Botticello 
Thanks for reading this article! Leave a comment below if you have any questions. Be sure to sign up for the Blogging Guide newsletter, to get the latest tips, tricks, and news about writing on Medium and to join our Facebook group, Medium Writing, to share your latest Medium posts and connect with other writers. 
Casey Botticello is a partner at Black Edge Consulting. Black Edge Consulting is a strategic communications firm, specializing in online reputation management, digital marketing, and crisis management. Prior to founding Black Edge Consulting, he worked for BGR Group, a bipartisan lobbying and strategic communications firm. 
Casey is the founder of the Cryptocurrency Alliance, a Super PAC dedicated to cryptocurrency and blockchain advocacy. He is a graduate of The University of Pennsylvania, where he received his B.A. in Urban Studies. 
You can connect with him on LinkedIn, Twitter, Facebook, or by visiting his website, Blogging Guide. 
Medium Blogging Guide is the premier publication dedicated to helping writers achieve success on Medium! Take a look 
Written by 
Written by",Casey Botticello,2020-07-10T00:06:09.873Z
Latent Dirichlet Allocation. What the heck is LDA and how is it used… | by J.P. Rinfret | The Startup | Medium,"What the heck is LDA and how is it used for topic modeling? 
Humans have at least two advantages over computers: we understand context and can pick up on the emotions attached to text (“K.”). Computers on the other hand do none of these things particularly well without a little help. So how does a computer cluster words into topics like it does in say sentiment analysis and topic models? 
Thats where LDA comes in handy. By assuming documents are nothing more than a probability distribution of topics, and topics are nothing more than a probability distribution of words, LDA calculates a probability that a document is mostly this topic or mostly that topic (e.g., Document N is 77% topic 1, 10% topic 2, 8% topic 5, and 5% topic 7) based on the words it contains. 
Basically what this boils down to is this: documents with similar topics have a higher probability of using similar groups of words. 
But before we can take a look at how this done, we most let go of all preconceived notions for how we humans draft text documents in order to understand how a computer thinks we draft text documents. 
As defined by Wikipedia: 
In Bayesian inference, plate notation is a method of representing variables that repeat in a graphical model. Instead of drawing each repeated variable individually, a plate or rectangle is used to group variables into a subgraph that repeat together, and a number is drawn on the plate to represent the number of repetitions of the subgraph in the plate. The assumptions are that the subgraph is duplicated that many times, the variables in the subgraph are indexed by the repetition number, and any links that cross a plate boundary are replicated once for each subgraph repetition. 
Let’s define each model parameter: 
The goal of LDA is to use N, W, Z, and 𝚹 to describe α and β. 
Simply put, according to LDA, if we wanted to create a new document X, we would first determine the number of words in document X (which is N), choose a topic mixture for X over a fixed number of topics (i.e., set 𝚹), and then come up with the words of X by: 
Once we have each word (W) and the topic of each word (Z), as well as the actual distribution of the topics in each document (𝚹), we can calculate the word distribution of each topic (β) and the topic distribution of each document (α). When viewed at the corpus level, we have our probability distributions per-document and per-topic. 
Of course, this is not how an actual human being drafts a text document, though it has proven to be a useful generative process for topic modeling. 
Say we have a group of newspaper articles that we assume to be only about sports, politics, and tech. Each of these topics are describe by the following words: 
LDA would assume that these articles were drafted by first choosing the length of the article (a 1,000 word column would have N=1,000). Then we would determine the topic mix of our article, or 𝚹. Let’s say our one-thousand word column is 50% politics, 20% sports, and 30% tech. That means we would choose 50 words from politics, 20 words from sports, and 30 words from tech. Finally, as we are writing our article, we would chose a word (W) with topic (Z) to met this 50/20/30 split based on the distribution defined above (𝚹). If we were to fill the December 17th edition of this newspaper with various articles following the same creation algorithm, we could easily compute the newspaper’s latent word probability distribution for each topic (β), as well as the newspaper’s topic distribution for each article (α). 
Remember syntax, context, emotion, etc. are lost on a computer! While we would never be able to comprehend this newspaper, a computer would have no problem modeling out the per-topic word probability distribution and the per-document topic probability distribution. 
Humans actual draft text documents in the opposite direction of LDA. We write a collection of words (phrases) that have context and meaning, but also follow basic rules or syntax. So, in order to use LDA to solve for α and β of an article written by a human (with context, syntax, emotion, etc.), we will follow the creation algorithm in reverse order. 
First, let’s randomly assign each word W in each document X to one of K pre-determined topics (i.e., set Z for every W in every document X). Again, LDA assumes that every corpus has a latent per-document probabilistic topic distribution and a latent per-topic probabilistic word distribution. In other words, we assume that each document in a corpus can be mostly one of say 7 topics. 
For each document X, we will assume that all randomly generated topic assignments Z for each word W are correct except for the current word W we are analyzing. 
In order to find the correct topic assignment Z for this word W, we will need to calculate two probabilities: 
We can then assign word W and new topic assignment Z based on this per-document topic probability distribution relative to all documents that contain W, or 𝚹 * β = α. In other words, we are assuming that the current topic assignment of a word is wrong and using the inherent features of the document to assign the “correct” topic to a word based on probability. 
If we iterate through each word W and repeat this process enough times, we should reach a steady state where all assignments make sense based on the assumed latent probability distributions and model parameters (i.e., α will approach the true latent value of α). 
And that my friends, is LDA! 
I highly recommend everyone watch Scott Sullivan’s YouTube video (here) on LDA, especially his summary of core assumptions and conclusions. His video was absolute the basis for this blog and the foundation of my understanding of LDA. 
Written by 
Written by",J.P. Rinfret,2019-12-18T06:35:11.538Z
"Machine Learning for Recommender systems — Part 1 (algorithms, evaluation and cold start) | by Pavel Kordík | Recombee blog | Medium","Recommender systems are one of the most successful and widespread application of machine learning technologies in business. There were many people on waiting list that could not attend our MLMU talk so I am sharing slides and comments here. 
You can apply recommender systems in scenarios where many users interact with many items. 
You can find large scale recommender systems in retail, video on demand, or music streaming. In order to develop and maintain such systems, a company typically needs a group of expensive data scientist and engineers. That is why even large corporates such as BBC decided to outsource its recommendation services. 
Our company Recombee is based in Prague and develops an universal automated recommendation engine capable of adapting to business needs in multiple domains. Our engine is used by hundreds of businesses all over the world. 
Surprisingly, recommendation of news or videos for media, product recommendation or personalization in travel and retail can be handled by similar machine learning algorithms. Furthermore, these algorithms can be adjusted by using our special query language in each recommendation request. 
Machine learning algorithms in recommender systems are typically classified into two categories — content based and collaborative filtering methods although modern recommenders combine both approaches. Content based methods are based on similarity of item attributes and collaborative methods calculate similarity from interactions. Below we discuss mostly collaborative methods enabling users to discover new content dissimilar to items viewed in the past. 
Collaborative methods work with the interaction matrix that can also be called rating matrix in the rare case when users provide explicit rating of items. The task of machine learning is to learn a function that predicts utility of items to each user. Matrix is typically huge, very sparse and most of values are missing. 
The simplest algorithm computes cosine or correlation similarity of rows (users) or columns (items) and recommends items that k — nearest neighbors enjoyed. 
Matrix factorization based methods attempt to reduce dimensionality of the interaction matrix and approximate it by two or more small matrices with k latent components. 
By multiplying corresponding row and column you predict rating of item by user. Training error can be obtained by comparing non empty ratings to predicted ratings. One can also regularize training loss by adding a penalty term keeping values of latent vectors low. 
Most popular training algorithm is a stochastic gradient descent minimizing loss by gradient updates of both columns and rows of p a q matrices. 
Alternatively, one can use Alternating Least Squares method that iteratively optimizes matrix p and matrix q by general least squares step. 
Association rules can also be used for recommendation. Items that are frequently consumed together are connected with an edge in the graph. You can see clusters of best sellers (densely connected items that almost everybody interacted with) and small separated clusters of niche content. 
Rules mined from the interaction matrix should have at least some minimal support and confidence. Support is related to frequency of occurrence — implications of bestsellers have high support. High confidence means that rules are not often violated. 
Mining rules is not very scalable. The APRIORI algorithm explores the state space of possible frequent itemsets and eliminates branches of the search space, that are not frequent. 
Frequent itemsets are used to generate rules and these rules generate recommendations. 
As an example we show rules extracted from bank transactions in the Czech Republic. Nodes (interactions) are terminals and edges are frequent transactions. You can recommend bank terminals that are relevant based on past withdrawals / payments. 
Penalizing popular items and extracting long tail rules with lower support leads to interesting rules that diversify recommendations and help to discover new content. 
Rating matrix can be also compressed by a neural network. So called autoencoder is very similar to the matrix factorization. Deep autoencoders, with multiple hidden layers and nonlinearities are more powerful but harder to train. Neural net can be also used to preprocess item attributes so we can combine content based and collaborative approaches. 
User-KNN top N recommendation pseudocode is given above. 
Associations rules can be mined by multiple different algorithms. Here we show the Best-Rule recommendations pseudocode. 
The pseudocode of matrix factorization is given above. 
In collaborative deep learning, you train matrix factorization simultaneously with autoencoder incorporating item attributes. There are of course many more algorithms you can use for recommendation and the next part of the presentation introduces some methods based on deep and reinforcement learning. 
Recommenders can be evaluated similarly as classical machine learning models on historical data (offline evaluation). 
Interactions of randomly selected testing users are cross validated to estimate the performance of recommender on unseen ratings. 
Root mean squared error (RMSE) is still widely used despite many studies showed that RMSE is poor estimator of online performance. 
More practical offline evaluation measure is recall or precision evaluating percentage of correctly recommended items (out of recommended or relevant items). DCG takes also the position into consideration assuming that relevance of items logarithmically decreases. 
One can use additional measure that is not so sensitive to bias in offline data. Catalog coverage together with recall or precision can be used for multiobjective optimization. We have introduced regularization parameters to all algorithms allowing to manipulate their plasticity and penalize recommendation of popular items. 
Both recall and coverage should be maximized so we drive recommender towards accurate and diverse recommendations enabling users to explore new content. 
Sometimes interactions are missing. Cold start products or cold start users do not have enough interactions for reliable measurement of their interaction similarity so collaborative filtering methods fail to generate recommendations. 
Cold start problem can be reduced when attribute similarity is taken into account. You can encode attributes into binary vector and feed it to recommender. 
Items clustered based on their interaction similarity and attribute similarity are often aligned. 
You can use neural network to predict interaction similarity from attributes similarity and vice versa. 
There are many more approaches enabling us to reduce the cold start problem and improve the quality of recommendation. In the second part of our talk we discussed session based recommendation techniques, deep recommendation, ensembling algorithms and AutoML that enables us to run and optimize thousands of different recommendation algorithms in production. 
Continue to the second part of the presentation discussing Deep Recommendation, Sequence Prediction, AutoML and Reinforcement Learning in Recommendation. 
Or check our recent updates in Recombee in 2019: New Features and Improvements. 
Written by 
Written by",Pavel Kordík,2019-12-15T22:34:48.875Z
Tower Street – Medium,"Robust industry-specific cyber insurance for large corporates; Fast because it leverages existing security assessments and questionnaires 
The CIS standard is a set of Critical Security Controls. The recently announced Cyber Risk Dataset is a historical breach dataset with manual annotations for which CIS controls were…",NA,NA
A Scoop of SEO: How to optimize your Website’s Page Rank? | by Spreeha Dutta | Code To Express | Medium,"Most of us are aware that PageRank is the algorithm that is used by Google Search to rank web pages in order of their priority against the search queries fired by the users. But a few basic criteria that this very powerful algorithm incorporated by the world’s most popular search engine stands upon are, (let us consider page A to be the page under examination) : 
To understand it better, let us view Page Rank as a social networking platform where people can be substituted by websites. Just like the prestige of a person on Twitter, Instagram can be analyzed by the number of followers he has, its the same case here too where the prestige of a site is determined by the number and priority of web pages that link to it. 
“Focusing on quality (backlinks) over quantity is what can help to protect your site as Google updates.” - Adam Riemer from the SearchEngineJournal 
PR(A) = (1-d) + d [ PR(T1)/O(T1) + PR(T1)/O(T1) +...+ PR(Tn)/O(Tn)] 
PR(A) → Page Rank of A T1...Tn → Pages that link to page APR(Ti) → Page Rank of the T pages that have links to A on themO(Ti) → Total number of outbound links from the page Ti d → damping factor that lies between 0 to 1. 
In the simplest of terms, the probability that a person surfing the web arrives at page A would be equal to the sum of the probabilities of his clicking on links on other pages that direct him to page A. This sum is - PR(T1)/O(T1) + PR(T1)/O(T1) +…+ PR(Tn)/O(Tn).This probability is further reduced by a factor d in order to normalize between 0 and 1. The ( 1 - d ) signifies that the person will sometime end up on a page that has no outgoing links from it. 
Let us take a look at a simple example: 
Let us consider a web of 4 pages, 
Note that since no other page links to D, page D has the lowest priority of them all and A has the highest priority. 
1. In order to get a fair approximation of the Page Rank values of the whole web, about a 100 iterations of subsequent RageRank calculation using the steps mentioned above are needed. 
2. The sum of the Page Rank of all pages will still combine to give the total number of pages, so the average Page Rank of a page will remain 1. 
Apart from these, Google considers several other factors for ranking. Only the major ones have been mentioned here. You can find a detailed description of the 200 Ranking factors considered by Google as of 2019 here. 
Note:Even if the pages that link to your page have low priority, they are still helpful in elevating your page rank since it is directly dependent on the sum of values from all inbound pages. 
Thus, this is all that Search Engine Optimization encapsulates. SEO is all about maximizing the number of visitors to your site to boost its ranking in the results. In the world we live in today, the rank 1 search result plays the same role as the most popular people we are following on Instagram. Thus to thrive in a society where majority of the population stays online, to only have your own website is not enough, how well connected your site is to the rest of the web is what matters. And SEO helps you do just that! 
Do you want to write for CodeToExpress? We would love to have you as a technical writer. Send us an email with a link to your draft at codetoexpress@gmail.com 
Written by 
Written by",Spreeha Dutta,2019-03-19T04:44:59.446Z
How To Start A Career in Cyber Security | by Dennis Chow | The Startup | Medium,"I’ve decided to take moment to reflect on who and what were beneficial to me during my journey to become a cyber security professional. In this brief guide, I go over security expertise requirements, best practices, and recommendations for individuals looking to transition into security professionals. This is also a useful pocket guide for recruiters in selecting talented candidates and snuffing out less than credible or incompetent professionals diluting our field. We’re going to cover: 
I’m going to provide my background so that you may consider it as you read and compare it with your own as you think about my transition recommendations. My name is Dennis Chow and presently, I’m the CISO of a small consulting group in the Texas region (SCIS Security). I’ve also worked for and with many different employers of varying verticals and sizes. My journey has been moving mostly up from IT general practitioner work and through the the varying stages of security professional titles ultimately winding up in the red team side after many years. In tandem for a time; I was a grunt systems technologist in the Air Force (nothing to do with cyber security). 
Note: In our slide you will also see our co-founder and CEO James Davidson of SCIS Security as well. (We work together.) 
I’ve always wanted to be some sort of security professional and no one really wanted to take a chance on me despite my interest and projects involving security administration because I was only “IT.” In 2009, I made the public facing transition officially into security as a SOC analyst working rotating graveyard shifts at a company called Alert Logic. Unfortunately, I also had to take a 30K pay-hit to be there. 
Luckily, I was still single at the time; but it made it stressful to save, finish school, and pay rent at the same time. The hiring manager threw me a pcap and said “tell me everything you can find about this file.” — it had MS08–67 exploitation and man-in-the-middle attacks as I would later investigate and report to even get an in-person interview. 
Today, I’m glad to have made the investment during that limited opportunity window and kept hammering away at rounding out my skills as being a holistic and robust security professional. I’ve been lucky to have been part of multiple industry verticals including: healthcare, oil and gas, defense, e-commerce, airlines, and Fortune 100 consulting. And yes, my present salary more than makes up for the first year’s pay cut. 
This section is going to be brief, but direct. Just like how everyone as a kid wants to become “xyz” such as an Astronaut; many don’t make the cut. It’s not for a lack of effort, or a dream. It’s a combination of opportunity, your existing skill, and aptitude. You need to be ready with self reflected reality check— and if you can even afford to transition right now. 
Some hard questions others and you need to ask yourself involve having enough background skill, experience, and aptitude in the following (that you can self-study and achieve business class results without having to be hand held): 
This is a critical junction point: If you said no to any of the above or would have trouble learning all of the above; the hard truth is you aren’t ready for a role in security at all. Some candidates get lucky and have either friend-referrals or can navigate enough interviews with non-technical hiring teams to “get in.” My recommendations assume you have neither going forward. Don’t fret! There is light for those who want to work hard at getting the above skills whom may have a little trouble along the way. 
The above are baseline skills you need to set expectation wise before trying to enter this field. Now if you’re wondering why do you see ‘incompetent’ security professionals in our industry — the short of it is no one ever gave those individuals the proper scrutiny. They’re decreasing the value and brand that true security professionals work hard to maintain and elevate. 
If you’re good with the above bulleted questions and can perform them; skip this section and move on to the next section examining security specific industry standards and trends. 
For those of you who arrived at this section still highly motivated to meet basic system level and scripting requirements; please make your way, study, and genuinely master the skills using the following resources. Please be cognizant of yourself and understand that *memorizing is not learning.* 
At the end of each exercise, lab, or lesson: if you can’t look at a different scenario or piece of news and cannot apply the concepts in the form of system deployment, actions, or conceptually in a conversation with a lay person — you did not learn the material. 
Here are some FREE resources to help you catch up and get your general IT skills up to snuff before you considering to study security topics. Why is that? It’s because you can’t really secure systems and technologies without knowing how to plan, architect, deploy, use, and tune them yourself. You wouldn’t want a surgeon operating on you without hands-on experience right? Neither do employers or customers for security folk. 
Of course, there’s also the paid routes and you can help shore up skills by achieving certifications like so: 
The world as a security professional is just like in IT where it starts off as a general practitioner. There are lots of different wording and distinctions that we’d like to address though. I’ve taken this from a slide deck we created for a recruiting agency to help illustrate further. In short, the current industry is broken up into 3 major areas: Infosec, Cybersec, and AppSec. Read our infographic for further details: 
It’s not really a question of getting into “security”and automatically becoming a penetration tester (hacking techniques). Some immediately grab their OSCP and sometimes get a major break. However, it’s more likely you’re going to be starting off in the defender or blue team side of the house. Even though you may want to “hack” things for a living; you’ve got a long way to go. 
Penetration testing is actually a specialization within security and one that just like IT foundations; you can’t skip foundational defender skills. Sorry, great red teamers need to know the blue side. How else will you evade security controls as a “hacker” if you’ve never designed and implemented them? 
No matter what your end-goal or title desire in security: The well-rounded practitioner will have spent time in all 3 of the above security general areas or have had skills exercised in the above before starting to specialize. 
So, how do we know trend wise that the above categories and technologies depicted are relevant? Let’s examine what we estimated back in 2018 also taken from our deck: 
In our brief timeline we depict some popular certification vendors and paths over time starting with the 90’s and the learning topics that featured the most demand and “bang for your buck” from a role to get a high paying salary. As we’ve moved and matured varying technologies and filled roles; the requirement of learning steepened. 
In the 90’s the first generation of security professionals paved the way from ground zero. So much of the focus was on documentation, practices, and conceptual procedures that have to be implemented in a program stand point. Today, many of these first gen security professionals that did not transition into other technical skills eventually became architects.The ISC2 CISSP became very popular during this era. 
Fast forward into second generation of security; and that’s where I came in on the tail end of it. Network Security was very focused such as firewalls, IPS, and SIEM. In between we also had a sub-movement of trying to learn and extend skills into ICS which was still heavily network security focused but required additional knowledge of industrial professionals to properly monitor and respond on. EC-Council’s CEH is still heavily sought after and the GIAC and Offensive Security certifications such as the OSCP now are starting to take off. 
Today, we’re nearing the end of the third generation where the hot commodity skills focused on AppSec roles which include static code analysis, reverse engineering, web apps, API’s, and other heavy automation focused engineering. Between this generation and the next; we estimate that more focus will be on AI and data science driven integration efforts and skills which is a half-step in between. With the focus of data science and AI you see more vendor specific certifications and training which can include Splunk, Azure, and other EdX course ware. 
What is beyond security integration and AI efforts in the fourth generation? The highest demand will be us leveraging what we learned from AI and the major skills demand will shift towards security researcher roles where you find vulnerabilities and create exploit proof of concepts. 
As you can see; to maintain top pay you’ll continue progressing and be required to learn new and complex skills to stay ahead in our industry. That is the special breed person. A real security professional. Your learning will never stop until you’re ready to start getting paid less over time. 
Don’t believe me? Remember in the 90’s and early 2000’s when a mid range desktop support or junior IT system administrator with the A+, Network+ came out making 80K+ a year? For anyone looking now; those roles in many areas are almost paying half! See for yourself: just type in a certification in indeed and examine what the salary ranges are. 
Now, compared to our security search results for a ‘CISSP’ in Indeed in 2018 as well as wage employments from O*NET from other DOL data you can see many security roles are on a broad spectrum much higher with many landing 85–130K and if you specialize; this number can easily hit the 200’s. For recruiters, this information was pulled a couple of years ago but isn’t too far off the mark today and will help you determine what are industry general rates for most candidates: 
One of the key takeaways from this article, if nothing else; is that obtaining and maintaining security certifications matter. Highly recognized education entities include EC-Council, ISC2, SANS (GIAC), and Offensive Security. There’s many others out there include product specific vendor training out there; but you’ll find the above are the ‘big 4’ equivalent for our industry. 
It’s not that we’re saying that certifications are your only means of obtaining knowledge. You can do that on your own by joining various study groups, reading tutorials, and practicing with your own lab. 
However, just like a degree for many professions — you’re going to need some well known industry standard certifications to attest to your skillsets. And just as important, but rarely discussed — getting specific certifications in a certain order matters as well. I’ll explain more after our next slide below to iterate where most specialization paths lead to in security: 
As you can see, the higher the pay you want as a non-managerial specialist; the more skills and spread of knowledge you will have to obtain. But which credentials can help you get noticed if you want to transition in? 
My top two recommendations based on recruiter searches and general demand is the CISSP and the GCIH. The CISSP has a wide breadth of learning and is really more of a managerial and architecture position focused certification. Yet, for whatever reason many roles open require or are looking for it. The cost to the exam as I recall was somewhere in the $500 USD range. 
The GCIH is for SOC analyst defenders that need to recognize the different type of hacker attacks and be able to properly orchestrate response and handling of the data. That exam depending if you challenge it or take it as part of the official course ware range from $900- $2,000 USD. 
Coming in third place is the OSCP and largely because it’s a high value return for the money spent. It does not teach many of the other security foundations and concepts but it’s a great way to get noticed and extend the GCIH knowledge. Keep in mind, the OSCP is not usually for the faint of heart. There are no ‘test questions’; it’s a 24 hour hands on live CTF and another 24 hours of report writing. You could get away with the entire cert for as little as $1,000 USD but that’s all dependent on the individual. 
One very important thing to note is that this article is very centered around my experiences in the U.S. market place. As you traverse the globe, the EU and APJ regions seem to recognize CISSP and CEH over GIAC and OSCP. This could be due to a number of geo-political factors including age of the institution operating in the area, cost, and the general presence of the cert. It’s important that you account for your own market’s preference and weigh what you can afford for the cost of course material and examinations in your region. 
Does getting the above automatically make you a great candidate? No, not by a long shot. They are just more likely to get you called about your experience and resume. I’ve screen snipped an excerpt from the Training Roadmap from SANS which shows baseline foundations on their own recommended path. I personally recommend the GCIA, GCIH, and GCFA as the trifecta for becoming a solid security analyst before trying to become an engineer in focused roles like pen testing; so you have some minimal hands on expertise. 
Regardless of what you can afford or which path you want to choose as your own personal preference; certifications make a huge difference in getting your application noticed. Try to get your existing employer to pay for them and point out how the security administration topics align to your day-to-day work and use the big ‘C’ word — compliance. Certifications are one thing, but also, what do you need to do in regards to experience? 
Ethically, the only way you can obtain 1–3 years of experience of security related work without being in a paid FTE role is to really be a volunteer first. Sometimes you may get hired, and sometimes you may be retained for project work. But getting a name with a specific “security” specific title such as “security analyst” or “security administrator” even if it is volunteer work can help get you noticed. Here are some great ways to get connected to varying chapters and get some experience in: 
There’s plenty of work to be done and as I stated before; without connections and friends to help you with either work or tag you into an interview — this is starting with ground zero and something you can put on your resume. 
Inexperienced candidates don’t realize recruiters are your allies and friends. Not all recruiters are created equal and you’ll want to use your best judgement to work with ones that you both can benefit from growing your professional networks and evolve your career for the long-term. Here are some common complaints that we’ve collected from other security professionals about recruiters they do not wish to be in communication with again: 
While many recruiting agencies come and go, some of the large ones that can find alot of open requisitions for are from Tek Systems and Robert Half Technology 
There’s no secret that many security professionals have a smart-mouthed undertone in some of their communications. We suggest to keep in mind the following when you work with some security candidates: 
We often work with recruiting agencies to help them better understand where and who to source for talent in security. Based on our experience, we’ve come up with our own thoughts on where you can “phish” for them (pun intended). Sometimes the path of finding a security candidate has to start with a client trusting a potential transitioning IT professional. Here is a general path as we discussed earlier in the article: 
Many high demand and frequent roles of talent pools that we noticed in 2018 seem to be around certain areas of the country in the outer rims of the U.S.: 
Each star represents a general metro area that we felt had a large amount of demand and well paid roles for security professionals. For ourselves, we were presenting this original deck in Houston and so we had to acknowledge that Houston did not have a great source presence for security talent nor job opportunities compared to the rest of the states. However, it doesn’t mean it won’t change in the future. 
Recruiters can also find candidates at varying meetup chapters including Infragard, ISACA, and other local conventions such as B-Sides that bring communities together and lots of entry to mid level talent looking to exchange ideas and prove themselves. 
We should also mention that due diligence on all candidates are key. Identifying different levels of embellishment are key to determine if that person will be successful in certain roles or not. We’ve come up with (3) typical types of “fakers” trying to transition to and from security on a regular basis: 
Part of trying to weed out fakers that just don’t know their material happens in the pre-processing stage. Check out their OSINT as it relates to the individual such a screen name or email and just punch into a Google search. You would be amazed at what some candidates come back with. Other checks involve a little more work I highly employ anyone reading this article to always include your certification number when you send in a CV. 
Recruiters and employers need to cross validate certifications to ensure you’re 1) Have been certified and 2) Maintain that certification. Here are some links that can help for the any CISSP, GIAC/SANS, and A+, Network+, Linux+, Security+ type certifications: 
ISC2 Verifications 
CompTIA Certifications 
SANS/GIAC Certifications 
Security professionals are among the biggest opportunists one can come across and may come across as job “hoppers.” As I’ve traversed varying agencies, employers, and different roles in the space; I’ve got some ideas on how recruiters can ‘tune’ their communication to help smooth this process over with perspective employers: 
And of course in hard sells; it’s not always the candidate! If you need to help a candidate determine if a role is a good fit or not. Consider providing some other key points for roles that are hard to sell: 
I’m a fan of early screening and transparency so that neither candidate or recruiter wastes time for a requisition. I strongly suggest candidates and recruiters prepare for technical screenings at early stages of their applications prior to submitting to the client (perspective employer). Here are some sample questions from our deck and visual answers to consider when preparing a candidate for submission: 
Full animated version at MalwareUnicorn’s RE101 Course 
I hope you’ve enjoyed some of our perspectives about transitioning into the security industry and how recruiters and candidates can bridge communication differences between themselves. We’re all in it to make our communities safer to operate within cyberspace. As a final resource, I’d like to share a recent series of podcasts from SANS called: “Trust Me, I’m Certified” which airs weekly. It features many well known industry experts and how they entered the field along with some of the struggles they’ve faced, before, during, and sometimes after the security profession. 
Please feel free to let me know what you thought of our brief; and as always if you are in need of security expertise consulting or services, please visit us online at: www.scissecurity.com 
Written by 
Written by",Dennis Chow,2020-06-04T17:07:32.382Z
"What is Unsupervised Learning?. August 10, 2017 | by Shubham Mishra | Medium","August 10, 2017 
visit us 
Machine Learning (ML) uses powerful algorithms, statistical and numerical models to make machines learn without being explicitly programmed. The design of the ML systems, however, may differ depending on the skill set we want our machines to build, a form of training data, time constraints, and computing resources at our disposal. As a result, we have four major types of ML: supervised, unsupervised, semi-supervised, and reinforcement learning. 
In this article, we’re going to discuss unsupervised learning — an approach to the design of ML systems that has become very popular over the last several years. In a nutshell, in the unsupervised learning our inputs (training data) are unlabeled and we have no output results to validate efficiency of the learning process. However, once the training process is complete, we are able to label our data. The described process is similar to how humans acquire knowledge through experience. Like a child who makes the first steps in the world, our unsupervised machine lacks guidance and experience. Even though the machine works in the dark, it somehow manages to extract features and patterns from the probability distributions of data (e.g images, texts) which are fed to it. 
However, why would we even need unsupervised learning if we have so many efficient and tested supervised ML methods around? Actually, there are several reasons for the growing popularity of unsupervised methods: 
To illustrate how unsupervised learning actually works, let us take two popular implementations of this method with neural networks: autoencoders and GANs (Generative Adversarial Networks). 
First things first, autoencoder is a neural network with a specialized architecture suitable for unsupervised learning. In a way, it is similar to a standard neural network (Multilayer Perceptron). As an MLP, an autoencoder has an input layer that receives data, a number of hidden layers that process it using activation functions, and an output layer. Unlike a standard neural network, though, autoencoders have the same number of nodes (neurons) in their input and output layers. Such design is useful because the purpose of autoencoders is to reconstruct their own inputs rather than predict the target value (Y) of training samples. In other words, autoencoders learn how to represent and generate data, rather than how to classify and predict it. 
This feature makes unsupervised learning extremely efficient in generative models: ML techniques used to simulate and create virtual environments, and multimedia content. A recent advance in unsupervised learning with generative models are GANs (Generative Adversarial Networks) developed by Ian Goodfellow and his colleagues in 2014i. A GAN setup consists of two networks, Generative and Discriminatory, that compete against each other. A generative network tries to create model samples that look similar to the original data. In its turn, a discriminatory network tries to identify this counterfeit. The resultant competition drives both parties to improve their skills, which ultimately ends up in model data samples virtually indistinguishable from the training data. GAN’s learning techniques have been already effectively used in DCGAN (Deep Convolutional Generative Adversarial Networks) to create hi-fi images of human faces and landscapes. 
With all these fantastic applications one could hardly imagine even ten years ago, unsupervised learning firmly establishes itself as one of the main fields of innovation and experimentation in Artificial Intelligence and Machine Learning. 
Originally published at norah.ai. 
Written by 
Written by",Shubham Mishra,2017-08-16T18:04:28.413Z
Word Embedding in NLP: One-Hot Encoding and Skip-Gram Neural Network | by Jianna Park | Towards Data Science,"I’m a poet-turned-programmer who has just begun learning about the wonderful world of natural language processing. In this post, I’ll be sharing what I’ve come to understand about word embedding, with the focus on two embedding methods: one-hot encoding and skip-gram neural network model. 
Last year, OpenAI released a (restricted) version of GPT-2, an AI system that generates texts. When the user inputs a prompt — a word, sentence, or even paragraph — the system “predicts” and produces the next words. What is striking about GPT-2 is that the resulting passages, more often than not, could easily pass for texts written by a (rather rambling) human. 
It is not too hard to find poetry generators that output random words with or without regard to syntactic logic. One can write a fairly simple algorithm that selects a randomized word from a pool of nouns, then selects a randomized word from a pool of verbs, and so on. (Some say poetry generation is relatively “easy” because of the inherent literary license of the genre.) 
But how do machines generate coherent sentences that seem to know about their surrounding context similarly as humans? 
Enter natural language processing (NLP). NLP is a branch in the field of artificial intelligence that aims to make sense of everyday (thus natural) human languages. Numerous applications of NLP have been around for quite a while now, from text auto completion and chatbots to voice assistants and spot-on music recommendation. 
As NLP models are showing state-of-the-art performance more than ever, it might be worthwhile to take a closer look at one of the most commonly used methods in NLP: word embedding. 
At its core, word embedding is a means of turning texts into numbers. We do this because machine learning algorithms can only understand numbers, not plain texts. 
In order for a computer to be able to read texts, they have to be encoded as a continuous vector of numeric values. You might be familiar with the concept of vector in Euclidean geometry, which indicates an object with magnitude and direction. In computer science, a vector means a one-dimensional array. 
(Note: array is a type of data structure notated with square brackets ([]). An example of a one-dimensional array could be something like [.44, .26, .07, -.89, -.15]. A two-dimensional, nested array could look like [0, 1, [2, 3], 4]. Arrays hold values stored in a continuous set.) 
Okay, then how do we embed — or vectorize — words? The simplest method is called one-hot encoding, also known as “1-of-N” encoding (meaning the vector is composed of a single one and a number of zeros). 
Let’s take a look at the following sentence: “I ate an apple and played the piano.” We can begin by indexing each word’s position in the given vocabulary set. 
The word “I” is at position 1, so its one-hot vector representation would be [1, 0, 0, 0, 0, 0, 0, 0]. Similarly, the word “ate” is at position 2, so its one-hot vector would be [0, 1, 0, 0, 0, 0, 0, 0]. The number of words in the source vocabulary signifies the number of dimensions — here we have eight. 
The one-hot embedding matrix for the example text would look like this: 
There are two major issues with this approach for word embedding. 
First issue is the curse of dimensionality, which refers to all sorts of problems that arise with data in high dimensions. Even with relatively small eight dimensions, our example text requires exponentially large memory space. Most of the matrix is taken up by zeros, so useful data becomes sparse. Imagine we have a vocabulary of 50,000. (There are roughly a million words in English language.) Each word is represented with 49,999 zeros and a single one, and we need 50,000 squared = 2.5 billion units of memory space. Not computationally efficient. 
Second issue is that it is hard to extract meanings. Each word is embedded in isolation, and every word contains a single one and N zeros where N is the number of dimensions. The resulting set of vectors do not say much about one another. If our vocabulary had “orange,” “banana,” and “watermelon,” we can see the similarity between those words, such as the fact that they are types of fruit, or that they usually follow some form of the verb “eat.” We can easily form a mental map or cluster where these words exist close to each other. But with one-hot vectors, all words are equal distance apart. 
There are different ways to address the two issues mentioned above, but in this article, we’ll be looking at the skip-gram neural network model. “Skip-gram” is about guessing — given an input word — its context words (words that occur nearby), and “neural network” is about a network of nodes and the weights (strength of connection) between those nodes. 
This model of word embedding helps reduce dimensionality and retain information on contextual similarity, to which we’ll get back later. The famous Word2vec developed by Tomas Mikolov uses this model (along with another one called Continuous Bag of Words or CBOW model, which does the opposite of skip-gram by guessing a word based on its context words). 
To illustrate the skip-gram method, let’s go back to our sample text, “I ate an apple and played the piano.” We’ll use the word “ate” as an input to our model. Again, skip-gram is about predicting context words appearing near the input word. Say we define the window size to be two words (the window size can vary) and look at two words that come before the input word and two words that come after it. Then we’ll be looking at “I”, “an”, and “apple”. From here, our training sample of input-output pairs would be (“ate”, “I”), (“ate”, “an”), and (“ate”, “apple”). See the following image for reference: 
Imagine we are starting off with a model that has not been trained yet and therefore does not know the context words for each word. When we take the first pair of our training samples, (“ate”, “I”), and input “ate” into our model expecting to see “I,” our model might randomly spit out “played.” More precisely, what’s actually happening is that the model is outputting a prediction vector with a probability of each word in the vocabulary to occur near “ate”, then selecting the word with the highest probability (in this case, “played’). At the early training stage, it’s pretty much a random prediction. 
How do we tune the prediction to be more accurate? The fact that the model thought the word “played” would probably appear near the word “ate” tells something about the relationship between “played” and “ate” — they currently have a relatively strong connection, or weight, between them, though inaccurate to their actual relationship. How do we know that this connection should have been weaker? Because our sample pairs only had context words of “I”, “an”, or “apple”, and not “played”. 
Note that our target output should look like this in this particular pair of (“ate”, “I”): a probability of 1 at “I” and 0 at every other word, such that [1, 0, 0, 0, 0, 0, 0, 0]. We get a vector of errors values by comparing the actual output (where “played” has a value closest to 1) with the expected target output (where “I” has a value closest to 1), and use those values in back-propagation to adjust the weights between “ate” and the rest. 
Plainly put, “I” should have occurred with 100% probability near “ate” but it only occurred with (for instance) 10% probability, so we refactor the “ate-to-I” weight to be closer to 100%, using the 90% difference. As you might have guessed, since there are two other words (“an” and “apple”) that also appear near “ate”, adding these pairs into calculation will reduce the weight of “I” occurring near “ate” to be closer to 33%. 
The resulting weights at the end of the training are the word embeddings we are looking for — we want to get the tool that predicts relationships, not necessarily what each individual input returns. 
In fact, this is the basic idea of neural network training. It is similar to how we remember things by association, in that certain associations become stronger with repeated co-occurrence (e.g. “New” and “York”), while others become weaker as they are used less and less together. The neurons in our brain keep adjusting weights between themselves to better represent reality. 
We’ve now seen how the skip-gram neural network model works. As mentioned before, this has two advantages over one-hot embedding: dimensionality reduction and context similarity. 
Without going into the detail, I’d like to bring our attention back to the above diagram, in particular on the hidden layer in the middle. To give you a context, the source article uses a vocabulary of 10,000 words. You’ll see that the hidden layer says 300 neurons, which means it has an embedding size of 300, as compared to the embedding size of 10,000. 
Remember how one-hot encoding created vectors composed of as many values as the total number of words in the training set, mostly filled with zeros? Instead of doing that, the skip-gram neural network model selects a smaller number of features, or neurons, that say something more useful about that word. Think of features as character traits — we are made of multiple traits (e.g. introverted vs. extroverted) that have a value on a spectrum. We only retain what describes us best. 
As a side note, I can’t stress enough how much I love this quote from Pathmind: 
“Just as Van Gogh’s painting of sunflowers is a two-dimensional mixture of oil on canvas that represents vegetable matter in a three-dimensional space in Paris in the late 1880s, so 500 numbers arranged in a vector can represent a word or group of words. Those numbers locate each word as a point in 500-dimensional vectorspace.” 
The skip-gram model lets you keep information about the context of each word based on their proximity. In the example of “I ate an apple,” “ate” is a context word of “apple.” Context allows for grouping of words based on their syntactic and/or semantic similarity. If we were given additional input of “I ate a banana” and “I ate an orange,” we will soon find out that “ate” is also a context word of “banana” and “orange,” and therefore can infer that “apple,” “banana” and “orange” must share some commonality. 
The vectors of “apple,” “banana,” and “orange,” since they have similar context, are then adjusted to be closer to each other, forming a cluster on some multidimensional geometric space. On this note, linguist J.R. Firth said, “you shall know a word by the company it keeps.” 
(A caveat: context isn’t always most strongly correlated with the closest words; think of a noun or verb phrase, separated by a lot of miscellaneous words in between. This is only one way of language modeling.) 
In this article, we’ve looked at what I understood about the concept of word embedding in NLP and two common embedding methods, one-hot encoding and skip-gram. 
I also briefly touched upon the advantages of particular word embedding methods over another, namely dimensionality reduction and context similarity. 
Moving forward, I’d like to gain a better understanding of transformers in language modeling and the workings of GPT-2 (as well as the most recently released GPT-3) which initially prompted this writing. 
The idea that machines can compose texts, increasingly sensible ones more so, still awes me. I have a long journey ahead to get a fuller picture of how different NLP models work, and there will probably be (many) misunderstandings along the way. I’d very much appreciate any kind of feedback on how I could be a better learner and writer. 
For those who are not familiar with the topic, I hope this piece has sparked your interest in the field of NLP, the way the following articles did mine. 
Hands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look 
Written by 
Written by",Jianna Park,2020-07-31T16:48:40.584Z
Supervised and Unsupervised Learning | by Vineet Maheshwari | Data Driven Investor | Medium,"In the previous blog, we have seen the overview of ML techniques and some of the applications of Machine Learning. Now we will gather more information about 2 techniques out of the 3 and will also learn about one more technique, i.e Semi-Supervised Learning. 
SUPERVISED LEARNING : What do we understand about Supervision? Supervision is learning something under the guidance of a teacher or some supervisor, who can judge us whether we are doing things right or not. Similarly, in supervised learning, we have a set of labeled data while we are training an algorithm. Now what do we understand by labeled data? 
Labeled data mean that the target data is tagged with the answer, that the algorithm on which we are working, should come up with. So, for example, a labeled dataset of bikes would tell the algorithm to tell us about R15, Pulsar, Splendor. And whenever a new image is being shown, the algorithm compares it to the training data set to come up with a result and to predict the correct label. 
This learning is mainly useful in 2 areas, Classification and Regression problems. 
Classification problems ask the algorithm to predict discrete values, identify input data as a member of particular class or group. For example, if there is a training data set of bikes, that means each image is pre-labelled as R15, Pulsar, Splendor, e.t.c. Now the algorithm is being tested to correctly classify the new images of Pulsar, Splendor and other images. 
And on the other hand, the regression problems look for continuous data. 
Therefore supervised learning is best suited to problems where there is a set of available reference points. 
UNSUPERVISED LEARNING: This technique is used where deep learning model is handed the data set with no explicit instructions, that what to do with it. The model then tries to automatically find the structure in data by extracting the features and analyzing the structure. 
It can organize the data in various ways like Clustering, Anomaly Detection, Association, Auto-Encoders. 
It’s difficult to calculate the accuracy of the algorithm under unsupervised learning. 
Now we will talk about Semi-Supervised Learning, Semi-Supervised learning is the training data set with both labeled and unlabeled data. This method is useful when it is quite difficult to extract features and labeling the examples is a time taking process, even for some great experts. 
Common examples for this kind of situation is a medical situation that many people come through, MRI, CT Scan. Now it would be a time taking process to manually label each and every scan, but the deep learning network can still work in more accurate way by working on a small proportion of labeled data and improving the accuracy in comparison to unsupervised learning. 
In each issue we share the best stories from the Data-Driven Investor's expert community. Take a look 
Written by 
Written by",Vineet Maheshwari,2019-01-05T02:22:13.204Z
Explanatory Model Analysis with modelStudio | by Przemyslaw Biecek | Medium,"Version 0.2.1 of the modelStudio package reached CRAN this week. Let me shortly overview coolest new features (list of all changes). 
Short reminder: The modelStudio package creates an interactive serverless D3.js dashboard for exploration of predictive models, which is based on principles of Explanatory Model Analysis. With modelStudio interface, one can juxtapose instance-level model explanations (Break Down, Shapley values and Ceteris Paribus profiles), dataset-level model explanations (Partial Dependence Plots, Feature Importance, Accumulated Local Effects Plot) and data exploration plots (histogram and scatterplots). Examples below use a GBM model trained on the kaggle FIFA 19 dataset to predict player’s value based on selected 40 player’s characteristics. Play with the live demo here. 
There are two new plots in modelStudio for exploratory data analysis: Target vs Feature and Average Target vs Feature (useful for classification problems). They are especially helpful when examining Partial Dependence profiles. Both show the relation between target and a selected feature, but first shows the raw relation in the data, while the latter shows a relationship learned by a model. 
Some defaults changed in version v0.2.1 to improve the general usability. 
If no new instances are provided for local explanations, then by default, a small sample is taken at random from the training data. 
When modelStudio is plotted, by default the first panel is set to a Break Down plot while the second panel is clicked. This saves 3 clicks! 
All plots for categorical variables now have the same order of levels. 
modelStudio is serverless, so all computations need to be done in advance, which may take some time. By default, the whole process is more verbose, shows a progress bar and information about the current calculations. 
The try-catch blocks reassure that even if some parts fail, the rest will finish and plots will show up in the dashboard. 
Feature importance plots now have boxplots that show how stable are calculations for individual variables. 
The code chunk below creates a random forest model for the apartments dataset and then creates a modelStudio dashboard. 
library(DALEX)library(randomForest)model <- randomForest(m2.price ~., data = apartments)explainer <- explain(model, data = apartments[,-1], y = apartments[,1]) 
library(modelStudio)modelStudio(explainer) 
Find more information in the modelStudio GitHub repo. 
Written by 
Written by",Przemyslaw Biecek,2020-01-22T08:34:30.700Z
Role of Distance Metrics in Machine Learning | by Writuparna Banerjee | Analytics Vidhya | Medium,"Distance metrics play an important role in machine learning. They provide a strong foundation for several machine learning algorithms like k-nearest neighbors for supervised learning and k-means clustering for unsupervised learning. Different distance metrics are chosen depending upon the type of the data. So, it is important to know the various distance metrics and the intuitions behind it. 
An effective distance metric improves the performance of our machine learning model, whether that’s for classification tasks or clustering. 
Let’s say we want to create clusters using the k-Nearest Neighbor algorithm to solve a classification or regression problem. How can we say that two points are similar to one another? 
This will happen if their features are similar. When we plot these points, they will be closer to each other in distance. 
Hence, we can calculate the distance between points and then define the similarity between them. Now, the question arises— how do we calculate this distance and what are the different distance metrics in machine learning? 
That’s what we will discuss in this article. We will go through 6 types of distance metrics in machine learning. 
2. Manhattan Distance 
3. Minkowski distance 
4. Hamming Distance 
5. Cosine Distance 
Euclidean Distance represents the shortest distance between two points. 
Euclidean distance formula can be used to calculate the distance between two data points in a plane. 
Euclidean distance is generally used when calculating the distance between two rows of data that have numerical values, such as floating point or integer values. 
If columns have values with differing scales, it should be normalized or standardized before calculating the Euclidean distance. Otherwise, columns that have large values will dominate the distance measure. 
Euclidean distance is calculated as the square root of the sum of the squared differences between the two vectors. 
Where, 
n = number of dimensions 
pi, qi = data points 
This calculation is related to the L2 vector norm(discussed later). 
Now, let’s stop and look carefully! Does this formula look familiar? Well yes, this formula comes from the “Pythagorean Theorem”. 
Let’s write the code of Euclidean Distance in Python. We will first import the the SciPy library that contains pre-written codes for most of the distance functions used in Python: 
This is how we can calculate the Euclidean Distance between two points in Python. 
Manhattan Distance is the sum of absolute differences between points across all the dimensions. 
We use Manhattan distance, also known as city block distance, or taxicab geometry if we need to calculate the distance between two data points in a grid-like path just like a chessboard or city blocks. 
The name taxicab refers to the intuition for what the measure calculates: the shortest path that a taxicab would take between city blocks (coordinates on the grid). 
Let’s say, we want to calculate the distance, d, between two data points- A and B. 
Distance d will be calculated using an absolute sum of difference between its cartesian co-ordinates as below : 
And the generalized formula for an n-dimensional space is given as: 
Where, 
n = number of dimensions 
pi, qi = data points 
The Manhattan distance is related to the L1 vector norm (discussed later). 
If you try to visualize the distance calculation, it will look something like below : 
In the above picture, imagine each cell to be a building, and the grid lines to be roads. Now if we want to travel from Point P to Point Q marked in the image and follow the sky-blue and navy-blue paths , we see that the path is not straight and there are turns. In this case, we use the Manhattan distance metric to calculate the distance walked. The pink line joining the two points P and Q is the Manhattan distance. Now the distance d will be calculated as shown by the yellow line . 
When is Manhattan distance metric preferred in ML? 
The Manhattan Distance is preferred over the Euclidean distance metric as the dimension of the data increases. This occurs due to something known as the ‘curse of dimensionality’. For further details, please visit this link. 
Now, we will calculate the Manhattan Distance between the two points. SciPy has a function called cityblock that returns the Manhattan Distance between two points. 
Minkowski Distance is the generalized form of Euclidean and Manhattan Distance. 
Minkowski Distance calculates the distance between two points. 
It is a generalization of the Euclidean and Manhattan distance measures and adds a parameter, called the “order” or “p“, that allows different distance measures to be calculated. 
The Minkowski distance measure is calculated as follows: 
Where “p” is the order parameter. 
When p is set to 1, the calculation is the same as the Manhattan distance. When p is set to 2, it is the same as the Euclidean distance. 
Intermediate values provide a controlled balance between the two measures. 
The Minkowski distance is related to the Lp vector norm (discussed later). 
It is common to use Minkowski distance when implementing a machine learning algorithm that uses distance measures as it gives control over the type of distance measure used for real-valued vectors via a hyperparameter “p” that can be tuned. 
Let’s calculate the Minkowski Distance of the order 3: 
When the order(p) is 1, it will represent Manhattan Distance and when the order in the above formula is 2, it will represent Euclidean Distance. Let’s verify that in Python: 
Here, we can see that when the order is 1, both Minkowski and Manhattan Distance are the same. 
Let’s verify the Euclidean Distance as well: 
When the order is 2, we can see that Minkowski and Euclidean distances are the same. 
While reading this article you must have come across the words L1 norms, L2 norms. So, lets discuss them in details. 
Calculating the size or length of a vector is often required either directly or as part of a vector-metric operation. 
The length of the vector is referred to as the vector norm or the vector’s magnitude. 
The length of a vector is a non-negative number that describes the extent of the vector in space, and is sometimes referred to as the vector’s magnitude or the norm. 
The length of the vector is always a positive number, except for a vector of all zero values. It is calculated using some distance metrics that summarizes the distance of the vector from the origin of the vector space. For example, the origin of a vector space for a vector with 3 elements is (0, 0, 0). 
We will take a look at a few common vector norm calculations used in machine learning. 
The length of a vector can be calculated using the L1 norm. The L1 norm, represented as ||v||1 is calculated as the sum of the absolute vector values, where the absolute value of a scalar uses the notation |a1|. Clearly, the norm is a calculation of the Manhattan distance from the origin of the vector space. 
||v||1 = |a1| + |a2| + |a3| 
The L1 norm of a vector can be calculated in NumPy using the norm() function with a parameter to specify the norm order, in this case 1. 
The L2 norm, represented as ||v||2 is calculated as the square root of the sum of the squared vector values.Clearly, the norm is a calculation of the Euclidean distance from the origin of the vector space. 
||v||2 = sqrt(a1² + a2² + a3²) 
The L2 norm of a vector can be calculated in NumPy using the norm() function with default parameters. 
The Lp norm, represented as ||v||p is calculated as follows from the origin of the vector space: 
||v||p=(a1^p + a2^p + a3^p)^(1/p) 
Clearly, the norm is a calculation of the Minkowski distance from the origin of the vector space. 
So far, we have covered the distance metrics that are used when we are dealing with continuous or numerical variables. But what if we have categorical variables? How can we decide the similarity between categorical variables? This is where we can make use of another distance metric called Hamming Distance. 
Hamming Distance measures the similarity between two strings of the same length. The Hamming Distance between two strings of the same length is the number of positions at which the corresponding characters are different. 
Let’s understand the concept using an example. Let’s say we have two strings: 
“euclidean” and “manhattan” 
Since the length of these strings is equal, we can calculate the Hamming Distance. We will go character by character and match the strings. The first character of both the strings (e and m respectively) is different. Similarly, the second character of both the strings (u and a) is different. and so on. 
Look carefully — seven characters are different whereas two characters (the last two characters) are similar: 
Hence, the Hamming Distance here will be 7. Note that larger the Hamming Distance between two strings, more dissimilar will be those strings (and vice versa). 
Let’s see how we can compute the Hamming Distance of two strings in Python. 
As we saw in the example above, the Hamming Distance between “euclidean” and “manhattan” is 7. We also saw that Hamming Distance only works when we have strings of the same length. 
Let’s see what happens when we have strings of different lengths: 
This throws an error saying that the lengths of the arrays must be the same. Hence, Hamming distance only works when we have strings or arrays of the same length. 
These are some of the similarity measures or the distance matrices that are generally used in Machine Learning. 
Cosine similarity is a metric used to measure how similar the documents are irrespective of their size. Mathematically, it measures the cosine of the angle between two vectors projected in a multi-dimensional space. The cosine similarity is advantageous because even if the two similar documents are far apart by the Euclidean distance (due to the size of the document), chances are they may still be oriented closer together. The smaller the angle, higher the cosine similarity. 
Cosine similarity formula can be derived from the equation of dot products :- 
So, cosine similarity is given by Cos θ, and cosine distance is 1- Cos θ. Example:- 
In the above image, there are two data points shown in blue, the angle between these points is 90 degrees, and Cos 90 = 0. Therefore, the shown two points are not similar, and their cosine distance is 1 — Cos 90 = 1. 
Now if the angle between the two points is 0 degrees in the above figure, then the cosine similarity, Cos 0 = 1 and Cosine distance is 1- Cos 0 = 0. Then we can interpret that the two points are 100% similar to each other. 
In the above figure, imagine the value of θ to be 60 degrees, then by cosine similarity formula, Cos 60 =0.5 and Cosine distance is 1- 0.5 = 0.5. Therefore the points are 50% similar to each other. 
Let’s code for a better understanding. We have to import the cosine_similarity library for this purpose. 
Note that the first value of the array is 1.0 because it is the Cosine Similarity between the first document with itself. Also note that due to the presence of similar words on the third document (“The sun in the sky is bright”), it achieved a better score. 
In this article, we got to know about few popular distance/similarity metrics and how these can be used in order to solve complicated machine learning problems.We studied about Minkowski, Euclidean, Manhattan, Hamming, and Cosine distance metrics and their uses. 
Manhattan distance is usually preferred over the more common Euclidean distance when there is high dimensionality in the data. Hamming distance is used to measure the distance between categorical variables, and the Cosine distance metric is mainly used to find the amount of similarity 
Thanks for reading! 😊 If you enjoyed it, hit 👏 icon . It’s a great exercise for your fingers and will help other people see the story. 
Written by 
Written by",Writuparna Banerjee,2020-06-16T15:07:34.977Z
Unsupervised Learning – The Startup – Medium,"Disclaimer : The steps explained can give you a high-level understanding in model building, but one can perform in much more detail as per the data in hand. It is expected that all Data… 
In the previous post I tried to give a simple explanation on Machine Learning. In this post I…",NA,NA
Intro to Automatic Keyword Extraction | by Nattapong Ousirimaneechai | Medium,"ก่อนจะเริ่มพูดถึงเรื่องนี้ ก็ขอเกริ่นซักนิดว่าทำไมถึงเขียนบทความเรื่องนี้ละกันเนอะ 
แรกเริ่มเดิมที ก็อยากให้บทความแรกมีสาระอะนะ เลยไม่ได้เขียนซักที 
ข้ออ้างชัดๆ จริงๆขี้เกียจซินะ เออะ … ยอม (ฮา) 
ทีนี้ตอนนี้ฝึกงานอยู่ เพื่อนหลายคนก็เขียนบล็อคกันเยอะละ หยิบเรื่องที่อ่านๆอยู่มาเขียนบ้างละกัน จะได้ไม่หลับตอนอ่าน … จะอู้ก็บอกตรงๆก็ได้ เออะ นั่นแหละ (ฮา) 
เข้าเรื่องเหอะ 555 
ก็พอดีว่าได้มาอยู่แลปอ.ที่วิจัยด้าน NLP (Natural Language Processing) ซึ่ง NLP เนี่ยสามารถแบ่งย่อยลงไปได้หลายทางมาก และใช้ศาสตร์หลายแขนงอีก OTL ก็ศาสตร์ที่เกี่ยวข้องกับด้านที่สนใจอยู่คงหนีไม่พ้น AI/ML (Artificial Intelligence / Machine Learning) ซึ่งก็ใช้โมเดลทางคณิตศาสตร์ ทั้งด้านสถิติก็ได้ หรือจะใช้ Neural Network ก็ได้ มาทำ Supervised Learning หรือ Unsupervised Learning 
แต่ก็นะ แค่คิดก็เหนื่อยละ เอาเป็นว่า วันนี้เราจะมาเริ่มจากอะไรง่ายๆกันก่อนดีกว่า 
เพื่อให้เห็นภาพ เราขอพูดถึงปัญหาก่อน 
สมมติว่าเรามี text book อยู่เล่มนึง เนื่องจาก text book มีการทำ index ไว้แล้ว เราจึงสามารถหา keyword ของเรื่องที่เราสนใจได้ทันทีว่าอยู่หน้าไหนใน text book 
แต่ถ้ามันไม่มีการทำ index อะ 
เช่น content ในเว็บต่างๆเนี่ย เราอยากรู้ว่ามันพูดถึงเรื่องอะไรบ้าง แน่นอน เราก็ดูจากหัวเรื่องก่อน แล้วก็ดูเรื่องย่อยๆ หรือ เรื่องที่เกี่ยวข้องนั้น เราก็จะพยายามมองหาตัวอักษรหนาจริงมะ มันก็คือการหา keyword ในแบบฉบับของมนุษย์ ทีนี้ปัญหาคือ มันทำให้ automatic ได้มั้ย 
ถ้าคนมีหลักการในการหา เราก็แปลงหลักการนั้นให้คอมทำซิ #มาทำ AI กันเถอะ 
ตัวอย่างง่ายๆก็เช่น มีข่าว วันนี้ลุงตู่มาให้สัมภาษณ์กับสื่อว่า… keyword ที่ได้คือ ลุงตู่ และเรื่องที่สัมภาษณ์ รวมถึง keyword ‘การเมือง’ ‘ไทย’ ด้วยแม้จะไม่ปรากฏในข่าว 
แน่นอน จะเห็นได้ว่าประเด็นที่ 2 นั้นยากและซับซ้อน เพราะต้องเข้าใจเนื้อหาของข้อความนั้น และจัดประเภทข้อความว่าสอดคล้องกับเรื่องใดบ้าง จึงจะได้ keyword เหล่านี้มา ซึ่งต้องใช้ ML มาช่วยล้วนๆ ซึ่งจะยังไม่ขอพูดถึงในพาร์ทนี้ 
แน่นอนนี่เป็นงานเอกสารด้านข้อมูล เราก็ต้องเริ่มจาก clean data กันก่อน เพื่อจัดให้อยู่ในรูปแบบที่เราจะใช้ คือ คำ หรือ วลี ที่น่าจะเป็น keyword ได้ทั้งหมด โดยใช้ตัวแบ่งคำต่างๆ (stopwords) เช่น the, and, because และอีกมากมาย รวมถึงใช้วรรคตอน (punctuation) เช่น spacebar, tab, enter, comma, semicolon และถ้าจะให้ดี ก็ควรจะเลือกคำที่เป็นคำศัพท์ด้วย และ เป็นคำที่ชัดเจนไม่กำกวม (disambiguate) 
ต่อมาก็นำ คำและวลี ทั้งหมด ที่ได้มาประมวลผลบางอย่างก่อน เช่น นับความถี่ที่ปรากฏ ดูตำแหน่งที่มันอยู่ ดูความยาวของวลี และอื่นๆ ตามอัลกอที่จะใช้เลือกเลย 
เมื่อประมวลผลเสร็จแล้วก็ โยนคำกับผล เข้าฟังก์ชั่นให้คะแนนความเป็น keyword เลย แล้วเอามาเรียงจากมากไปน้อย ตั้งคะแนนที่เป็นเกณฑ์ผ่านให้มัน ก็ได้ผลลัพท์เป็น คำหรือวลีที่น่าจะสำคัญในบทความนั้นแหละ ง่ายเนอะ หรืออยากทำ ML ก็มา ฟังก์ชั่นประเมิน(Heuristic) ที่เราคิดขึ้นมาเองก็ได้ easy (เรอะะะะะะะ) 
ปิดท้ายบทความแรกด้วยอะไรง่ายๆ ตามที่พิมพ์ไว้แต่แรกละกัน … RAKE !!! 
RAKE ย่อมาจาก Rapid Automatic Keyword Extraction เป็น 1 ใน Algorithm ที่มีคนคิดมาให้แล้วสำหรับเรื่องนี้นั่นเอง ซึ่งทางด้าน NLP ก็จะใช้ Python กันเป็นหลัก โดยมี lib ดังๆเช่น NLTK ดังนั้น ถ้าคิดจะทำเรื่อง NLP ก็หัด Python เถอะครัช ภาษาง่ายๆที่ชีวิตดีมากๆเลยนะ 
RAKE นั้นจะรับ text file เข้ามา แล้วทำการแบ่งประโยคด้วยตัวแบ่งประโยค (punctuation) ก่อนเลย (ใช้ regular expression อะนะ) ต่อมาก็ต้องใส่ stopwords ลงไปให้มัน โดยจะใช้ stopwords พื้นฐานที่มีคนทำให้ไว้แล้วก็ได้นะ แล้วมันก็เอาไปตัดเป็นวลีเลย ต่อมามันจะหาค่า degree ของแต่ละคำในวลีซึ่งมาจากสูตร 
degree[word]=ซิกม่า(จำนวนคำในแต่ละวลี-1)จากทุกวลีที่มีคำนี้ปรากฎ + ความถี่ของคำนี้จากทุกวลี 
หลังจากได้ค่าดีกรีแล้วก็ไปเข้าฟังก์ชั่นประเมินคะแนนง่ายๆของมัน 
score[word] = degree[word]/(k*freq[word]) โดยปกติ k=1 
score[phrase] = sigma(score[word] in phrase) 
อ่านก็พอเดาหลักการง่ายๆได้ ถ้าวลีนั้นยาวๆ ปรากฎไม่บ่อยนัก ก็น่าจะเป็น keyword มากกว่าวลีสั้นๆที่เจอบ่อยๆเพราะน่าจะเป็น คำศัพท์ธรรมดานั่นเอง 
เสร็จแล้วก็เรียงคะแนนจากมากไปน้อยเลย แล้วจะตั้งเกณฑ์ผ่านยังไงก็แล้วแต่เลย เช่น 1 ใน 3 ที่มากสุดเป็น keyword ก็ได้ ง่ายปะหละ 
https://www.airpair.com/nlp/keyword-extraction-tutorialhttps://github.com/zelandiya/RAKE-tutorialhttp://textminingonline.com/getting-started-with-keyword-extractionhttps://en.wikipedia.org/wiki/Automatic_summarization#Keyphrase_extractionhttp://www.diva-portal.org/smash/get/diva2:352481/FULLTEXT01.pdf 
ในที่สุด บทความแรกก็จบลงแล้ว สำหรับตอนต่อไปก็ … รอขยันต่อไป (ฮา) 
Written by 
Written by",Nattapong Ousirimaneechai,2017-07-11T06:36:59.482Z
Hidden Markov Model — Part 1 of the HMM series | by Raymond Kwok | Analytics Vidhya | Medium,"How is it structured and what to do with it? 
Note: In this series of articles I will cover what I have learnt about HMM in the past few days. I have tried to make the articles very simple to understand. Although this is not a systematic way a text book would offer you, but I hope my article could give you a hand to get through some bottlenecks in your journey. Good luck! 
Part 1: Architecture of the Hidden Markov ModelPart 2: Algorithm to train a HMM: Baum-Welch algorithmPart 3: Algorithm to predict with a trained HMM: Viterbi algorithm 
In very simple terms, the HMM is a probabilistic model to infer unobserved information from observed data. Take mobile phone’s on-screen keyboard as an example, you may sometimes mistype the character next to what you wanted to. Here, the character you mistype is the observed data, and the one that you intended to type in your mind is the unobserved one. As another example, your GPS readings (observed) could jump around your actual location (unobserved) due to some sources of random noise. 
Interesting? There are many cases that what is observed is not the truth, and the Hidden Markov Model is one way that could give us the hidden truth back. However, it is not a magic for everything, and to use it, it has to satisfy with some assumptions, on which the HMM was founded. My approach here is to discuss the HMM with words, and supplemented with some Maths which could be skipped if it is not your cup of tea this time. Let’s start! 
In either of my examples above, we are actually dealing with data sequences. For example, we may mistype ‘miscellaneous’ as ‘miscellameous’ which are actually both sequences of characters. And in the GPS example, if we stand still at the location (22.3525°N 113.8475°E), then this should be repeated for five times if we record the reading once every second for continuously five seconds. However, as the reading jumps around, we could end up seeing (22.3535°N 113.8455°E), (22.3585°N 113.8325°E), (22.3123°N 113.8600°E), (22.3212°N 113.8397°E), (22.3421°N 113.7989°E). 
Here, for each observed data, we have an associated hidden truth. In formal HMM discussion, people call each ‘hidden truth’ as a ‘state variable’, and each ‘observed data’ as a ‘observed variable’. 
Let’s look at the figure above. In this HMM architecture, we have a sequence of states (hidden truths) ‘linked’ with each other. The arrow has a meaning of dependence here. For example, the state at time = 1 depends on the state at time = 0. This is the very important assumption in the Markov model that makes it so simple — any state depends ONLY on some of its previous state(s). I put a (s) after state because it is optional. We call it a simple Markov chain if a state depends on its first previous state, whereas it is a n-th order Markov chain if a state depends on its first n previous states. 
The consideration of the dependence between states is natural, because we think there are some relations between them. For instance, the combination of the characters in a word is not random, and it has to be in the right order to be correctly understood. And the simplest dependence is that a state depends only on one previous state, which is what the discussion will be focused on. 
Now we have put the states,the observed variables, and their dependence relations in the HMM architecture. We will go one step further to quantify the dependence, and begin that with state transition, which is the transition from the state at time k-1 to the time k, or the change from one orange box to the next. 
Assume there are M possible states to choose from, namely, the state could be any one of {1, 2, 3, …, m}. And we could quantify the transition probability from state i to state j as aᵢⱼ. Since the transition could happen from any one of the M possible states to another one of the M possible states, there are in total M × M possibilities. And we could arrange them in the following matrix representation, which is called the state transition matrix A. 
Because all aᵢⱼ are probability values, they are all bounded to be between 0 and 1, and each row has to be summed to 1. Generally speaking, we could have one such matrix A for each time step t. However, we will consider a stationary Markov chain, meaning that the matrix A is constant over time. 
Similarly, we could construct the emission matrix B for storing the probabilities of a state i resulting in an observed value j. Here, the observable is assumed to have K possible values, meaning it could take any one of the {1, 2, …, k }. The stationarity is also assumed here so we have one constant emission matrix. 
We are close to the full picture, and now we have the architecture, the state transition matrix A, and the emission matrix B. The last thing that we will need is the probability distribution π₀ of the initial state X₀. We need it to start the first state, so the upcoming states in the Markov chain could be estimated by the state transition matrix. 
To give you some actual numbers, let us look at the following example about words. I took the 3000 words list from the Oxford Advanced Learner’s Dictionary and tabulate the probability of having a character as the starting character, and the probability of a character coming after another. 
The first row of the table gives us the initial state distribution π₀, and the rest form the state transition matrix A. For the emission matrix B, let’s argue that 70% of time a user type a character correctly, otherwise a neighbor character is typed. For example, if we intend to type ‘G’ (state), then there is 70% we correctly type ‘G’ (observed), and there is 30% to get any one of {‘T’, ‘Y’, ‘F’, ‘H’, ‘C’, ‘V’, ‘B’}, or ~4.2% each. By this argument, we could construct the emission matrix. 
Let’s take a break here. By now, we have seen that a HMM model is constructed by state variables and the associated observed variables, and for each state, how many previous states it depends on. Then the model is parameterized by the state transition matrix A, the emission matrix B, and the initial state distribution π₀. Please be noted that, the HMM we have been talking about is a stationary, simple Hidden Markov Model that takes discrete state variables, discrete observed variables, and the variables are discrete in time, therefore, it is one special case in the HMM family. 
Lastly, although I have used the words list to do counting, and presented the matrices with some actual numbers, we don’t actually do it that way, nor we could always construct the emission matrix by arguing that. In the coming articles, we will talk about the Baum-Welch algorithm for training (or tuning the parameter matrices), as well as the Viterbi algorithm for inferring the whole sequence of hidden states from observed data. 
Written by 
Written by",Raymond Kwok,2019-07-24T04:13:19.296Z
Illustrated Guide to Transformers- Step by Step Explanation | by Michael Phi | Towards Data Science,"Transformers are taking the natural language processing world by storm. These incredible models are breaking multiple NLP records and pushing the state of the art. They are used in many applications like machine language translation, conversational chatbots, and even to power better search engines. Transformers are the rage in deep learning nowadays, but how do they work? Why have they outperform the previous king of sequence problems, like recurrent neural networks, GRU’s, and LSTM’s? You’ve probably heard of different famous transformers models like BERT, GPT, and GPT2. In this post, we’ll focus on the one paper that started it all, “Attention is all you need”. 
Check out the link below if you’d like to watch the video version instead. 
To understand transformers we first must understand the attention mechanism. The Attention mechanism enables the transformers to have extremely long term memory. A transformer model can “attend” or “focus” on all previous tokens that have been generated. 
Let’s walk through an example. Say we want to write a short sci-fi novel with a generative transformer. Using Hugging Face’s Write With Transformer application, we can do just that. We’ll prime the model with our input, and the model will generate the rest. 
Our input: “As Aliens entered our planet”. 
Transformer output: “and began to colonized Earth, a certain group of extraterrestrials began to manipulate our society through their influences of a certain number of the elite to keep and iron grip over the populace.” 
Ok, so the story is a little dark but what’s interesting is how the model generated it. As the model generates the text word by word, it can “attend” or “focus” on words that are relevant to the generated word. The ability to know what words to attend too is all learned during training through backpropagation. 
Recurrent neural networks (RNN) are also capable of looking at previous inputs too. But the power of the attention mechanism is that it doesn’t suffer from short term memory. RNN’s have a shorter window to reference from, so when the story gets longer, RNN’s can’t access words generated earlier in the sequence. This is still true for Gated Recurrent Units (GRU’s) and Long-short Term Memory (LSTM’s) networks, although they do a bigger capacity to achieve longer-term memory, therefore, having a longer window to reference from. The attention mechanism, in theory, and given enough compute resources, have an infinite window to reference from, therefore being capable of using the entire context of the story while generating the text. 
The attention mechanism’s power was demonstrated in the paper “Attention Is All You Need”, where the authors introduced a new novel neural network called the Transformers which is an attention-based encoder-decoder type architecture. 
On a high level, the encoder maps an input sequence into an abstract continuous representation that holds all the learned information of that input. The decoder then takes that continuous representation and step by step generates a single output while also being fed the previous output. 
Let’s walk through an example. The paper applied the Transformer model on a neural machine translation problem. In this post, we’ll demonstrate how it’ll work for a conversational chatbot. 
Our Input: “Hi how are you” 
Transformer Output: “I am fine” 
The first step is feeding out input into a word embedding layer. A word embedding layer can be thought of as a lookup table to grab a learned vector representation of each word. Neural networks learn through numbers so each word maps to a vector with continuous values to represent that word. 
The next step is to inject positional information into the embeddings. Because the transformer encoder has no recurrence like recurrent neural networks, we must add some information about the positions into the input embeddings. This is done using positional encoding. The authors came up with a clever trick using sin and cosine functions. 
We won’t go into the mathematical details of positional encoding, but here are the basics. For every odd index on the input vector, create a vector using the cos function. For every even index, create a vector using the sin function. Then add those vectors to their corresponding input embeddings. This successfully gives the network information on the position of each vector. The sin and cosine functions were chosen in tandem because they have linear properties the model can easily learn to attend to. 
Now we have the encoder layer. The Encoders layers job is to map all input sequences into an abstract continuous representation that holds the learned information for that entire sequence. It contains 2 sub-modules, multi-headed attention, followed by a fully connected network. There are also residual connections around each of the two sublayers followed by a layer normalization. 
To break this down, let’s first look at the multi-headed attention module. 
Multi-headed attention in the encoder applies a specific attention mechanism called self-attention. Self-attention allows the models to associate each word in the input, to other words. So in our example, it’s possible that our model can learn to associate the word “you”, with “how” and “are”. It’s also possible that the model learns that words structured in this pattern are typically a question so respond appropriately. 
To achieve self-attention, we feed the input into 3 distinct fully connected layers to create the query, key, and value vectors. 
What are these vectors exactly? I found a good explanation on stack exchange stating…. 
“The query key and value concept come from retrieval systems. For example, when you type a query to search for some video on Youtube, the search engine will map your query against a set of keys (video title, description etc.) associated with candidate videos in the database, then present you the best matched videos (values). 
After feeding the query, key, and value vector through a linear layer, the queries and keys undergo a dot product matrix multiplication to produce a score matrix. 
The score matrix determines how much focus should a word be put on other words. So each word will have a score that corresponds to other words in the time-step. The higher the score the more focus. This is how the queries are mapped to the keys. 
Then, the scores get scaled down by getting divided by the square root of the dimension of query and key. This is to allow for more stable gradients, as multiplying values can have exploding effects. 
Next, you take the softmax of the scaled score to get the attention weights, which gives you probability values between 0 and 1. By doing a softmax the higher scores get heighten, and lower scores are depressed. This allows the model to be more confident about which words to attend too. 
Then you take the attention weights and multiply it by your value vector to get an output vector. The higher softmax scores will keep the value of words the model learns is more important. The lower scores will drown out the irrelevant words. Then you feed the output of that into a linear layer to process. 
To make this a multi-headed attention computation, you need to split the query, key, and value into N vectors before applying self-attention. The split vectors then go through the self-attention process individually. Each self-attention process is called a head. Each head produces an output vector that gets concatenated into a single vector before going through the final linear layer. In theory, each head would learn something different therefore giving the encoder model more representation power. 
To sum it up, multi-headed attention is a module in the transformer network that computes the attention weights for the input and produces an output vector with encoded information on how each word should attend to all other words in the sequence. 
The multi-headed attention output vector is added to the original positional input embedding. This is called a residual connection. The output of the residual connection goes through a layer normalization. 
The normalized residual output gets projected through a pointwise feed-forward network for further processing. The pointwise feed-forward network is a couple of linear layers with a ReLU activation in between. The output of that is then again added to the input of the pointwise feed-forward network and further normalized. 
The residual connections help the network train, by allowing gradients to flow through the networks directly. The layer normalizations are used to stabilize the network which results in substantially reducing the training time necessary. The pointwise feedforward layer is used to project the attention outputs potentially giving it a richer representation. 
That wraps up the encoder layer. All of these operations are to encode the input to a continuous representation with attention information. This will help the decoder focus on the appropriate words in the input during the decoding process. You can stack the encoder N times to further encode the information, where each layer has the opportunity to learn different attention representations therefore potentially boosting the predictive power of the transformer network. 
The decoder’s job is to generate text sequences. The decoder has a similar sub-layer as the encoder. it has two multi-headed attention layers, a pointwise feed-forward layer, and residual connections, and layer normalization after each sub-layer. These sub-layers behave similarly to the layers in the encoder but each multi-headed attention layer has a different job. The decoder is capped off with a linear layer that acts as a classifier, and a softmax to get the word probabilities. 
The decoder is autoregressive, it begins with a start token, and it takes in a list of previous outputs as inputs, as well as the encoder outputs that contain the attention information from the input. The decoder stops decoding when it generates a token as an output. 
Let’s walk through the decoding steps. 
The beginning of the decoder is pretty much the same as the encoder. The input goes through an embedding layer and positional encoding layer to get positional embeddings. The positional embeddings get fed into the first multi-head attention layer which computes the attention scores for the decoder’s input. 
This multi-headed attention layer operates slightly differently. Since the decoder is autoregressive and generates the sequence word by word, you need to prevent it from conditioning to future tokens. For example, when computing attention scores on the word “am”, you should not have access to the word “fine”, because that word is a future word that was generated after. The word “am” should only have access to itself and the words before it. This is true for all other words, where they can only attend to previous words. 
We need a method to prevent computing attention scores for future words. This method is called masking. To prevent the decoder from looking at future tokens, you apply a look ahead mask. The mask is added before calculating the softmax, and after scaling the scores. Let’s take a look at how this works. 
The mask is a matrix that’s the same size as the attention scores filled with values of 0’s and negative infinities. When you add the mask to the scaled attention scores, you get a matrix of the scores, with the top right triangle filled with negativity infinities. 
The reason for the mask is because once you take the softmax of the masked scores, the negative infinities get zeroed out, leaving zero attention scores for future tokens. As you can see in the figure below, the attention scores for “am”, has values for itself and all words before it but is zero for the word “fine”. This essentially tells the model to put no focus on those words. 
This masking is the only difference in how the attention scores are calculated in the first multi-headed attention layer. This layer still has multiple heads, that the mask is being applied to, before getting concatenated and fed through a linear layer for further processing. The output of the first multi-headed attention is a masked output vector with information on how the model should attend on the decoder’s input. 
The second multi-headed attention layer. For this layer, the encoder’s outputs are the queries and the keys, and the first multi-headed attention layer outputs are the values. This process matches the encoder’s input to the decoder’s input, allowing the decoder to decide which encoder input is relevant to put a focus on. The output of the second multi-headed attention goes through a pointwise feedforward layer for further processing. 
The output of the final pointwise feedforward layer goes through a final linear layer, that acts as a classifier. The classifier is as big as the number of classes you have. For example, if you have 10,000 classes for 10,000 words, the output of that classier will be of size 10,000. The output of the classifier then gets fed into a softmax layer, which will produce probability scores between 0 and 1. We take the index of the highest probability score, and that equals our predicted word. 
The decoder then takes the output, add’s it to the list of decoder inputs, and continues decoding again until a token is predicted. For our case, the highest probability prediction is the final class which is assigned to the end token. 
The decoder can also be stacked N layers high, each layer taking in inputs from the encoder and the layers before it. By stacking the layers, the model can learn to extract and focus on different combinations of attention from its attention heads, potentially boosting its predictive power. 
And that’s it! That’s the mechanics of the transformers. Transformers leverage the power of the attention mechanism to make better predictions. Recurrent Neural networks try to achieve similar things, but because they suffer from short term memory. Transformers can be better especially if you want to encode or generate long sequences. Because of the transformer architecture, the natural language processing industry can achieve unprecedented results. 
Check out michaelphi.com for more content like this. 
✍🏽 Want more Content? Check out my blog at https://www.michaelphi.com 
📺 Like to watch project-based videos? Check out my Youtube! 
🥇 Stay up to date on articles and videos by signing up for my email newsletter! 
Hands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual. Take a look 
Written by 
Written by",Michael Phi,2020-06-28T17:26:45.109Z
Human-in-the-Loop Label Generation with Active Learning and Weak Supervision | by ODSC - Open Data Science | Medium,"One of the key challenges of utilizing supervised machine learning for real-world use cases is that most algorithms and models require lots of data with labels. Such labels will be used as the target variable in the training of your predictive model. 
How do we efficiently improve the labeling process to save money and time then and get the labels we need? Well we can use active learning combined with weak supervision in a guided analytics interactive application. 
In active learning settings, the human is placed back in the loop and helps guide the algorithm. The idea is simple: not all examples are equally valuable for learning, so the process picks the examples using active learning sampling and the human provides the labels for these, so that the algorithm can learn from them. This cycle (or loop) continues until the learned model converges or the user decides to quit the application. 
A diagram depicting the human-in-the-loop cycle of active learning. 
Active learning sampling is the selection of unlabeled data points at the real core of the active learning strategy. We are going to use two strategies to do our active sampling: label density for exploring the feature space and model uncertainty for exploiting data points near the decision boundary. 
Label density is about comparing the prior distribution with one of the already labeled data points. When labeling data points the user might wonder “is this data point representative of the distribution?” and “are there still many other data points quite similar to this one I just labeled? How do I skip them?”. Label density can address these concerns by ranking the unlabeled data points to give a feeling of how a data point is representative of the unlabeled distribution. Furthermore, label density also ranks penalizing data points that are too similar to what has already been labeled in past iterations. 
Model Uncertainty is based on the prediction probabilities of the model on the sill unlabeled data points. Besides selecting data points based on the overall distribution, we should also prioritize missing labels based on the attached model predictions. In every iteration, we can score the data that still needs to be labeled with the retrained model. What can we infer given those predictions by the constantly re-trained model? One of the things we can do is to compute the uncertainty of the model for each prediction. Uncertainty gives a feeling of where the model needs human input. 
Beside active learning, you can also use weak supervision, a revolutionary technique where a model can be trained by providing a good set of labeling functions. Adopting weak supervision speeds, even more, the generation of labels but it’s even more powerful if interactively controlled via a user interface! 
Join my presentation at ODSC East 2020 to learn how I could combine active learning and weak supervision in a single guided analytics application using the free and open-source tool KNIME Analytics Platform. 
The user interface of the Guided Labeling application implementing an interactive guided analytics using both active learning and weak supervision for document classification 
About the ODSC East Speaker/Author: 
Paolo Tamagnini is a data scientist at KNIME, holds a master’s degree in data science from the Sapienza University of Rome and has research experience from NYU in data visualization techniques for machine learning interpretability. Follow Paolo on LinkedIn. 
Written by 
Written by",ODSC - Open Data Science,2020-03-23T13:01:01.363Z
Machine Learning (including Deep Learning and Reinforcement Learning) for Engineers — A Technical Primer (Part 2) | by Arun Rao | Medium,"How should a software developer fascinated by AI but with no background build a base? Below are the resources that I used and would recommend. 
I offer six stages to jump in: 
I also have another post (Part 1) on AI/ML for less technical people (without math and CS backgrounds) here (it also has my general reader book and paper recommendations). 
1) Math for Machine Learning 
2) Intro ML Courses and Books 
3) Project Ideas and Fun Videos 
4) Advanced RL and DL Classes 
5) Ethics and AI/ML 
6) Papers to Read and People to Follow 
ML and Data Science Newsletters: 
“Academic Twitter” is a great way to follow ML experts and research orgs: 
Written by 
Written by",Arun Rao,2020-06-25T07:16:11.827Z
What is User Experience Strategy. Good UX in product development is… | by MentorMate | Medium,"Good UX in product development is accessible to all types of users in as many use cases as possible. Product teams work hard to map, build, and test quality digital experiences. 
But a digital product can’t be everything to everyone. 
By putting equal emphasis into solving for every use case and every problem, teams risk stretching themselves too thin and arriving at a solution that works well for no one. 
So how should teams decide which problems to solve or what features to prioritize? 
The answer is a UX Strategy. 
A UX strategy is the plan and approach for a digital product. 
UX strategies help businesses translate their intended user experience to every touchpoint where people interact with or experience its products or services. A solid UX strategy ensures that the business vision, user needs, and technical capabilities are aligned and helps to prioritize a team’s attention and resources by keeping them focused on solving the right problems for target users. 
There are a number of methods product teams can use to create a UX strategy. At the core, it involves researching, planning, testing and validating ideas before the implementation of design or development begins. 
Some common approaches include: 
Often, a UX Strategy can take the form of a document containing the information learned during the discovery phase: This document can be used to guide the product team and keep everyone working towards the same goal. 
Businesses with strict timelines and budgets must deal with the reality that product development is neither fast nor cheap. 
Product owners with previous exposure to outdated methods of integrating design in software development might mistakenly assume that involving designers is a surefire way to spend more than they want. 
But this isn’t actually the case. 
Coming up with a UX Strategy at the outset of a project can help make sure digital product teams don’t waste time, money, and energy developing products or features they aren’t sure people will want or use. 
Here are some of the main ways developing a UX Strategy can give businesses a leg up on their competition: 
New products are almost always working under conditions of uncertainty. Product owners might not yet know exactly who their target users are or exactly what problems the software needs to solve for them. 
Through the process of researching and talking with potential users, businesses can discover if the product and features they intend to develop are actually necessary. Potential users are able to provide valuable insight into how their problems are (or are not) currently being solved — which can then help businesses gauge if their proposed solutions will have a competitive advantage. 
People are creatures of habit. If new digital products want to succeed, their value proposition must be enough to entice people to switch to a new product. 
Many apps and websites fail because they attempt to solve problems that real people don’t actually have or already have a good solution for. UX Strategists can’t be afraid to challenge clients or stakeholders about their understanding of the target users and competitors, especially if the research contradicts their assumptions. 
Meanwhile, product owners and business leaders who want to succeed must be open to feedback that challenges their ideas. Innovation happens through conversations — not one-way channels. Plus, this compels them to back up their decisions and justify their intentions with concrete data. 
Takeaway: Rather than defining a UX Strategy around assumptions, let factual user data pave the way. 
Every idea is an assumption until validated. Assumptions equal risks. 
A UX Strategy can help businesses eliminate risk by spending time up front to research, plan, test, and validate ideas. When conversations with users invalidate early assumptions, businesses must pivot and can easily do so with minimal impact to the overall time and budget of a project. 
As Jaime Levy stated in her book, UX Strategy: How to Devise Innovative Digital Products that People Want: 
You don’t want a North Star to guide your UX Strategy; instead, you want a goal or point towards which to steer every time you pivot. 
Takeaway: Businesses can minimize risk by identifying user needs at the outset of a project. Involving design in your software development strategy isn’t about aesthetics so much as it is keeping the user at the forefront of your value delivery. After all, it’s the user who will determine whether your product is a success. 
Without a UX Strategy, product owners may have difficulty prioritizing which problems to solve. 
It is easy to go down the rabbit hole of trying to solve for every use case and every problem, but that is not practical or effective. Budgets and time constraints may demand that the most important problems are addressed first. 
This is especially true for a MVP, or a Minimum Viable Product. An MVP is the first complete version of a product, but has a limited feature set. The end-result is the leanest first version of the app that satisfies client needs while offering users value. 
MVPs are critical to businesses that must get their products to market as soon as possible. Having a solid UX Strategy to keep the product team focused and aligned is essential to delivering the best first version of a product. 
Takeaway: A UX Strategy will help to prioritize the most important problems. 
Great digital products focus on resolving specific needs for as many kinds people who experience those needs. It doesn’t mean building something that can be everything to everyone. 
UX engagements — no matter the budget — entail complicated processes and require extensive research, creative iteration, and testing. By talking with stakeholders, users, and analyzing the competitive space, product teams can provide crucial boundaries to what could be potentially endless problem-solving and iteration. 
Image Source: Patrick Hendry on Unsplash 
Original post can be found here. 
Innovate with us. Click here to access all of our free resources. Authored by Kate Tolmie. 
Kate is an enthusiastic and passionate designer who enjoys the challenge of transforming complex workflows into elegant solutions. Her adaptability, analytical mindset and strategic approach combined with a stellar eye for design means that Kate consistently delivers intuitive and enjoyable experiences across iOS, Android and responsive web. When she isn’t designing, Kate loves traveling, trying new recipes and working on her side hustle. 
Written by 
Written by",MentorMate,2018-07-26T20:42:58.731Z
Pam Hester O'Neal – Medium,"B2B growth marketer with experience in infosec, data analytics, enterprise software, IT and network management, and marketing technology. 
See more",NA,NA
Machine Learning in Cyber Security | by Ebubekir Büber | Deep Learning Turkey | Medium,"In recent years, attackers have been developing more sophisticated ways to attack systems. Thus, recognizing these attacks is getting more complicated in time. Most of the time, network administrators were not capable of recognizing these attacks effectively or response quickly. experiences via interdependent metrics. Globally pontificate innovative expertise via competitive manufactured products. Rapidiously restore. 
Therefore, there is a lot of software has been developed to support the human race in order to be able to manage and protect their systems effectively. Initially, these software has been developed to handle some operations like mathematical calculations which seem very complex for the human being. And then we need more. Next step was extending the ability of software using artificial intelligence and machine learning techniques. As technology advances, a huge amount of data is being produced to be processed every day and every hour. Finally, the concept of “Big Data” was born and people began to need a more intelligent system for processing and getting make sense of these data. For this purpose, there are a lot of algorithms have been developed until today. These algorithms are used for many research area such as; image processing, speech recognition, biomedical area, and of course cyber security domain.   Besides all of these, basically, the main purpose of Machine Learning techniques is providing decision mechanism to software as people do. Cybersecurity domain is one of the most important research area worked on. The Centre for Strategic and International Studies in 2014 estimated annual costs to the global economy caused by cyber crimes was between $375 billion and $575 billion. Although sources differ, the average cost of a data breach incident to large companies is over $3 million. Researchers have developed some intelligent systems for cyber security domain with the purpose of reducing this cost. 
Written by 
Written by",Ebubekir Büber,2018-02-25T09:47:14.345Z
USER-USER Collaborative filtering Recommender System in Python | by Ankur Tomar | Medium,"INTRODUCTION 
In the previous article, we learned about the content based recommender system which takes the user input and provides with an output that matches most closely to the user’s input. Although it uses some context of the user to provide the recommendation, largely it is still a non-personalized recommender system. In this article, we will start building a system that uses the profile of the given user and provide recommendation completely based on that user’s preference and liking. 
Collaborative filtering 
Let’s start with the idea of what would I do if I want to do a non-personalized collaborative filtering and calculate the prediction of an item i for the user u. Simply, I would calculate the average of the rating of that item i by adding all the rating values of the item i and divide it by the total number of users U. 
Let’s move forward from this intuition and incorporate the behaviour of other users and provide more weight to the ratings of those users who are like me. But how do we check how much a user is similar to me? 
To answer this, we will use Karl Pearson’s correlation and see how similar two users are. It is usually calculated over the items that both the users have rated in the past. But there is a problem with this approach. When the number of common ratings are not very large, the similarity value gets biased. It might be possible that 2 users have only 2 ratings in common but the value of correlation is very high or even very close to 1. 
To remove this, we weight the similarity. One way to do this is to calculate the numerator at the common ratings only but calculate denominator for all the ratings of both the users. Doing this makes it similar to the cosine similarity function that we discussed in the last article. 
Another problem in this approach of predicting the rating of a new item is, not all the users have same level of optimism while rating an item. For a user, an average movie lies at a scale of 2 while another user rates an average movie as 4. So first user’s 2 is similar to second user’s 4. To incorporate this inconsistency, we will calculate the mean of the ratings of the user and then subtract this mean from each of the ratings provided by the user. This will tell us how much above or below average a user rated the movie. After incorporating this, the final rating formula looks like this : 
And the Pearson’s correlation looks like this: 
With the above understanding, let’s get to the coding part. 
The representation of the code below might not be very easy to read, so please go to my GitHub repository to access all the codes of Recommender Systems of this series. 
Again, let’s start by firing up the required libraries and importing the datasets. 
import pandas as pdimport numpy as npimport math 
Ratings=pd.read_csv(“C:\Users\Ankur.Tomar\Desktop\Courses\Recommender\User-User\uu-assignment\data\\ratings.csv”)Movies=pd.read_csv(“C:\Users\Ankur.Tomar\Desktop\Courses\Recommender\User-User\uu-assignment\data\\movies.csv”)Tags=pd.read_csv(“C:\Users\Ankur.Tomar\Desktop\Courses\Recommender\User-User\uu-assignment\data\\tags.csv”) 
Calculating the mean rating and subtracting from each rating of a user to calculate the adjusted rating. 
Mean= Ratings.groupby([‘userId’], as_index = False, sort = False).mean().rename(columns = {‘rating’: ‘rating_mean’})[[‘userId’,’rating_mean’]]Ratings = pd.merge(Ratings,Mean,on = ‘userId’, how = ‘left’, sort = False)Ratings[‘rating_adjusted’]=Ratings[‘rating’]-Ratings[‘rating_mean’]Ratings 
Finding the top 30 similar user profiles for each user. 
distinct_users=np.unique(Ratings[‘userId’]) 
user_data_append=pd.DataFrame() 
user_data_all=pd.DataFrame() user1_data= Ratings[Ratings[‘userId’]==320]user1_mean=user1_data[“rating”].mean()user1_data=user1_data.rename(columns={‘rating_adjusted’:’rating_adjusted1'})user1_data=user1_data.rename(columns={‘userId’:’userId1'})user1_val=np.sqrt(np.sum(np.square(user1_data[‘rating_adjusted1’]), axis=0)) 
distinct_movie=np.unique(Ratings[‘movieId’]) 
i=1 
for movie in distinct_movie[604:605]: 
item_user = Ratings[Ratings[‘movieId’]==movie] 
distinct_users1=np.unique(item_user[‘userId’])  j=1  for user2 in distinct_users1: 
if j%200==0: 
print j , “out of “, len(distinct_users1), i , “out of “, len(distinct_movie) 
user2_data= Ratings[Ratings[‘userId’]==user2] user2_data=user2_data.rename(columns={‘rating_adjusted’:’rating_adjusted2'}) user2_data=user2_data.rename(columns={‘userId’:’userId2'}) user2_val=np.sqrt(np.sum(np.square(user2_data[‘rating_adjusted2’]), axis=0)) 
user_data = pd.merge(user1_data,user2_data[[‘rating_adjusted2’,’movieId’,’userId2']],on = ‘movieId’, how = ‘inner’, sort = False) user_data[‘vector_product’]=(user_data[‘rating_adjusted1’]*user_data[‘rating_adjusted2’]) 
user_data= user_data.groupby([‘userId1’,’userId2'], as_index = False, sort = False).sum() 
user_data[‘dot’]=user_data[‘vector_product’]/(user1_val*user2_val) 
user_data_all = user_data_all.append(user_data, ignore_index=True) 
j=j+1 
user_data_all= user_data_all[user_data_all[‘dot’]<1] user_data_all = user_data_all.sort([‘dot’], ascending=False) user_data_all = user_data_all.head(30) user_data_all[‘movieId’]=movie user_data_append = user_data_append.append(user_data_all, ignore_index=True) i=i+1 
Calculating the predicted rating for each item and ignoring the item if less than 2 similar neighbours. 
User_dot_adj_rating_all=pd.DataFrame() 
distinct_movies=np.unique(Ratings[‘movieId’]) 
j=1for movie in distinct_movies:  user_data_append_movie=user_data_append[user_data_append[‘movieId’]==movie] User_dot_adj_rating = pd.merge(Ratings,user_data_append_movie[[‘dot’,’userId2',’userId1']], how = ‘inner’,left_on=’userId’, right_on=’userId2', sort = False)  if j%200==0:  print j , “out of “, len(distinct_movies)  User_dot_adj_rating1=User_dot_adj_rating[User_dot_adj_rating[‘movieId’]==movie]  if len(np.unique(User_dot_adj_rating1[‘userId’]))>=2:  User_dot_adj_rating1[‘wt_rating’]=User_dot_adj_rating1[‘dot’]*User_dot_adj_rating1[‘rating_adjusted’]  User_dot_adj_rating1[‘dot_abs’]=User_dot_adj_rating1[‘dot’].abs() User_dot_adj_rating1= User_dot_adj_rating1.groupby([‘userId1’], as_index = False, sort = False).sum()[[‘userId1’,’wt_rating’,’dot_abs’]] User_dot_adj_rating1[‘Rating’]=(User_dot_adj_rating1[‘wt_rating’]/User_dot_adj_rating1[‘dot_abs’])+user1_mean User_dot_adj_rating1[‘movieId’]=movie User_dot_adj_rating1 = User_dot_adj_rating1.drop([‘wt_rating’, ‘dot_abs’], axis=1)  User_dot_adj_rating_all = User_dot_adj_rating_all.append(User_dot_adj_rating1, ignore_index=True)  j=j+1 User_dot_adj_rating_all = User_dot_adj_rating_all.sort([‘Rating’], ascending=False) 
With this we are done with making a recommendation system based on the user similarities. For more in depth understanding, please go through the University of Minnesota’s Recommender system specialisation courses on Coursera. 
In the next article, we will see another form of collaborative filtering called ITEM-ITEM Collaborative filtering based Recommender System. 
Please go to my GitHub repository to access all the codes of Recommender Systems of this series. 
Thanks! 
Written by 
Written by",Ankur Tomar,2019-02-14T01:22:10.833Z
How Google Search Works? Page Rank Algorithm using Python | by Sai Durga Kamesh Kota | Analytics Vidhya | Medium,"PageRank (PR) is an algorithm used by Google Search to rank websites in their search engine results. PageRank was named after Larry Page, one of the founders of Google. PageRank is a way of measuring the importance of website pages. According to Google: 
PageRank works by counting the number and quality of links to a page to determine a rough estimate of how important the website is. The underlying assumption is that more important websites are likely to receive more links from other websites. 
It is not the only algorithm used by Google to order search engine results, but it is the first algorithm that was used by the company, and it is the best-known. 
6. Now generate a graph with 25 nodes using networkx library. Here we are setting probability as 0.6 which is the probability of having edge between 2 nodes in a graph. 
The generated output graph looks like: 
7. Now let's store no of nodes and neighbors of a particular node which will be required for further implementation of the algorithm. 
The output of the above cell is: 
8. Now we need to generate a random walk score for each and every node by starting with a random node and doing a walk through its neighbor nodes and increasing score. Now for implementation, I am iterating the process for 500000 times. 
8. Once we got the scores stored in the dictionary we need to normalize score by dividing the random walk score by no of iterations. 
9. Now let's get the random scores for the graph by using built-in function pagerank in networkx library and sort the obtained dictionary based on the scores. 
10. Now sort the dictionary we generated by an algorithm and store it. 
Now, let's compare both results:- 
11. The output is:- 
The order generated by our implementation algorithm is 
22 11 23 10 3 6 18 14 5 21 19 15 13 8 0 4 9 17 24 1 12 16 2 7 20 
The order generated by networkx library is 
22 11 23 10 3 6 18 5 14 21 19 15 13 8 4 0 9 17 24 1 12 16 2 7 20 
which is almost same. 
In this way, google search ranking works by page rank algorithm. 
For any further assistance in source code, you can check out in below gist 
Thank you. 
Written by 
Written by",Sai Durga Kamesh Kota,2020-05-17T16:31:06.105Z
Enhancing a Facial Recognition System via a Deep Learning Model as a Similarity Metric | by Fortune Okorji | Seamfix Engineering | Medium,"Facial recognition (FR) is a deep learning approach capable of identifying or verifying an individual from a digital image or a video. Quite a number of approaches exist capable of identifying faces, but we would focus on improving an FR system by replacing our traditional similarity metric with an intelligent model as a similarity learner. We would also evaluate the characteristics of using some traditional similarity metrics. 
Facial recognition, verification or clustering are all ways in which faces are compared for different purposes. These purposes vary with applications ranging from attendance checks for employees to forensics in criminal detection. When compared to other biometric traits (such as fingerprint and iris), FR has transformed over time. FR systems capture an image without the user’s consent and further uses it for security-based applications — take an example of surveillance systems and security checks in airports. In addition, companies that focus on building virtual reality systems like VironIT and Oculus VR utilize FR for face tracking in their applications. 
For our pipeline, the pre-trained Google’s FaceNet model would be responsible for generating some sort of number vector. FaceNet seemed sufficient compared to other models available to the community due to its ability to generate representations of unique facial features. This is as a result of the large dataset it was trained on. A point to note is our focus on ‘improving the similarity learning metric and not on FaceNet itself’. Also, a significant implementation we adopted from FaceNet was the one-shot training approach. The existing FR model to be improved utilizes a traditional similarity learning metric for classification, therefore the need for a different approach was necessary to improve the model. 
Facial biometrics such as — the distance between the eyes and nose tip positioning — vary in all humans. These unique features are therefore commonly used for identification. 
Face verification has received adequate attention from researchers over the past decade, the process involves capturing a face image and then, comparing the image concurrently against a previously taken image or those stored in a database. Face recognition is a tasking field of research in artificial intelligence with various limitations imposed for an intelligent system to recognize a face — These include variations in head pose, change in lighting effect, facial expression, aging faces, occlusion due to accessories, and so on. Quality research work has been done to correct these effects with significant progress made. 
The FaceNet model as proposed by Schroff et al solves the face verification problem. It takes face images in batches and trains on them using the Triple Loss Function to calculate loss. A batch contains images as positive, negative and anchor pairs. While computing loss, the function minimises the distance between an anchor and a positive, i.e images of the same identity, and maximises the distance between the anchor and a negative i.e images of different identities. It learns one deep CNN, then transforms a face image to an embedding. The embedding can be used to compare faces in three ways: 
Similarity learning is the process of training a mathematical function or metric to measure the degree of relationship between elements. It simply measures a metric across both elements under observation, also known as metric learning. Therefore, vector embeddings generated from our FaceNet model could be compared against each other to validate for similarity. 
Traditional algorithms for similarity learning metrics can be: 
The need to improve our model’s accuracy led us to research and experiment on better approaches that could make our FR system robust. First, we considered experimenting on traditional similarity learning metrics as discussed previously. It seemed viable initially, but results showed little or no improvements in relation to our previous implementation. Implementing other similarity metrics did not suffice our need to scale up model accuracy. 
Before the decision of experimenting with a deep learning model, we first considered the use of statistical machine learning models, support vector machine (SVM) and the logistic regression model. The SVM classifier from experiments has proven to be a viable approach when the training data consists of long dimensional arrays. Therefore, it could serve this purpose but since the breakthrough in deep learning models, they have proven to be better than statistical models with regards to accuracy and performance. For a clearer picture, the previous FR pipeline and the improved has been represented diagrammatically below. 
Comparison function 
A comparison function is a mathematical function that quantifies a “metric” between pairs of elements in two sets or more. This is achieved through a mathematical computation of some sort. In choosing a comparison function, it must satisfy the following properties for all x, y, z belonging to the set: 
Below are experimental results carried out on some comparison functions with respect to our implementation. Do note that these results might differ for different projects with respect to the dataset and implementation. 
Data Constraint 
Some of the major constraints encountered in deep learning today are data availability. For training, we needed pairs of images for each example; a large number of similar pairs belonging to one class (i.e. ‘match’ class) and multiples of dissimilar pairs belonging to another class (i.e ‘not a match’ class). 
The solution was to augment the available dataset. Data augmentation has proven to be useful in solving deep learning problems today in areas where data is insufficient. There are vast methods available ranging from the use of sophisticated GANS to mere image translation. To keep things simple, we applied augmentation processes ranging from distributed noise addition to image flipping. The code snippet is as displayed below: 
Embedding Generator and Storage 
For an efficient training pipeline, the researcher has to make a decision on how to process the data generated. Data can either be stored to be accessed later or trained on-the-go as it is generated. 
To store the embedding generated, the storage format has to be light-weight. The .npz format is a zipped archive of files named after the variables they contain. It saves NumPy arrays in a single file in an uncompressed format. On the other hand, for on-the-go training, the generator's function yields a chosen amount of embedding obtained from the desired comparison function chosen. 
In building our deep learning model network architecture, the input dimension, logits and label were taken into consideration. We implemented a number of dense layers, trained with Adam optimizer and binary cross entropy as loss function. 
Finally, the keras model “fit_generator” fits the data yielded batch-by-batch by our embedding generator. The generator runs parallel to the model for better results. For our generator, it enables us to do real-time data augmentation on images on the system in parallel to the model training handled by the GPU. 
This work presented an overview of the development of an enhanced facial recognition system by using a deep learning model in place of a traditional distance metric as a similarity learning metric. We would further extend this work in the future to see the possibility of using a deep learning network trained via triplet loss to generate a scalar value to base our decision on. 
Is this paper sufficient? I would love to hear your opinions, suggestions and comments. Please leave them below. Thanks. 
References 
Florian Schroff, Dmitry Kalenichenko, James Philbin. FaceNet: A Unified Embedding for Face Recognition and Clustering. 2015 
Yaniv Taigman, Ming Yang, Marc’Aurelio Ranzato, Lior Wolf. DeepFace: Closing the Gap to Human-Level Performance in Face Verification. 2014. 4 
Rajalingappaa Shanmugamani. Deep Learning for Computer Vision: Expert techniques to train advanced neural networks using TensorFlow and Keras. Packt Publishing Ltd, January 2018 
Written by 
Written by",Fortune Okorji,2019-05-27T07:26:08.083Z
Machine Learning — Singular Value Decomposition (SVD) & Principal Component Analysis (PCA) | by Jonathan Hui | Medium,"In machine learning (ML), some of the most important linear algebra concepts are the singular value decomposition (SVD) and principal component analysis (PCA). With all the raw data collected, how can we discover structures? For example, with the interest rates of the last 6 days, can we understand its composition to spot trends? 
This becomes even harder for high-dimensional raw data. It is like finding a needle in a haystack. SVD allows us to extract and untangle information. In this article, we will detail SVD and PCA. We assume you have basic linear algebra knowledge including rank and eigenvectors. If you experience difficulties in reading this article, I will suggest refreshing those concepts first. At the end of the article, we will answer some questions in the interest rate example above. This article also contains optional sections. Feel free to skip it according to your interest level. 
I realize a few common questions that non-beginners may ask. Let me address the elephant in the room first. Is PCA dimension reduction? PCA reduces dimension but it is far more than that. I like the Wiki description (but if you don’t know PCA, this is just gibberish): 
Principal component analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables (entities each of which takes on various numerical values) into a set of values of linearly uncorrelated variables called principal components. 
From a simplified perspective, PCA transforms data linearly into new properties that are not correlated with each other. For ML, positioning PCA as feature extraction may allow us to explore its potential better than dimension reduction. 
What is the difference between SVD and PCA? SVD gives you the whole nine-yard of diagonalizing a matrix into special matrices that are easy to manipulate and to analyze. It lay down the foundation to untangle data into independent components. PCA skips less significant components. Obviously, we can use SVD to find PCA by truncating the less important basis vectors in the original SVD matrix. 
In the article on eigenvalue and eigenvectors, we describe a method to decompose an n × n square matrix A into 
For example, 
However, this is possible only if A is a square matrix and A has n linearly independent eigenvectors. Now, it is time to develop a solution for all matrices using SVD. 
The matrix AAᵀ and AᵀA are very special in linear algebra. Consider any m × n matrix A, we can multiply it with Aᵀ to form AAᵀ and AᵀA separately. These matrices are 
In addition, the covariance matrices that we often use in ML are in this form. Since they are symmetric, we can choose its eigenvectors to be orthonormal (perpendicular to each other with unit length) — this is a fundamental property for symmetric matrices. 
Let’s introduce some terms that frequently used in SVD. We name the eigenvectors for AAᵀ as uᵢ and AᵀA as vᵢ here and call these sets of eigenvectors u and v the singular vectors of A. Both matrices have the same positive eigenvalues. The square roots of these eigenvalues are called singular values. 
Not too many explanations so far but let’s put everything together first and the explanations will come next. We concatenate vectors uᵢ into U and vᵢ into V to form orthogonal matrices. 
Since these vectors are orthonormal, it is easy to prove that U and V obey 
Let’s start with the hard part first. SVD states that any matrix A can be factorized as: 
where U and V are orthogonal matrices with orthonormal eigenvectors chosen from AAᵀ and AᵀA respectively. S is a diagonal matrix with r elements equal to the root of the positive eigenvalues of AAᵀ or Aᵀ A (both matrics have the same positive eigenvalues anyway). The diagonal elements are composed of singular values. 
i.e. an m× n matrix can be factorized as: 
We can arrange eigenvectors in different orders to produce U and V. To standardize the solution, we order the eigenvectors such that vectors with higher eigenvalues come before those with smaller values. 
Comparing to eigendecomposition, SVD works on non-square matrices. U and V are invertible for any matrix in SVD and they are orthonormal which we love it. Without proof here, we also tell you that singular values are more numerical stable than eigenvalues. 
Example (Source of the example) 
Before going too far, let’s demonstrate it with a simple example. This will make things very easy to understand. 
We calculate: 
These matrices are at least positive semidefinite (all eigenvalues are positive or zero). As shown, they share the same positive eigenvalues (25 and 9). The figure below also shows their corresponding eigenvectors. 
The singular values are the square root of positive eigenvalues, i.e. 5 and 3. Therefore, the SVD composition is 
To proof SVD, we want to solve U, S, and V with: 
We have 3 unknowns. Hopefully, we can solve them with the 3 equations above. The transpose of A is 
Knowing 
We compute AᵀA, 
The last equation is equilvant to the eigenvector definition for the matrix (AᵀA). We just put all eigenvectors in a matrix. 
with VS² equals 
V hold all the eigenvectors vᵢ of AᵀA and S hold the square roots of all eigenvalues of AᵀA. We can repeat the same process for AAᵀ and come back with a similar equation. 
Now, we just solve U, V and S for 
and prove the theorem. 
The following is a recap of SVD. 
where 
Since matrix V is orthogonal, VᵀV equals I. We can rewrite the SVD equation as: 
This equation establishes an important relationship between uᵢ and vᵢ. 
Recall 
Apply AV = US, 
This can be generalized as 
Recall, 
and 
The SVD decomposition can be recognized as a series of outer products of uᵢ and vᵢ. 
This formularization of SVD is the key to understand the components of A. It provides an important way to break down an m × n array of entangled data into r components. Since uᵢ and vᵢ are unit vectors, we can even ignore terms (σᵢuᵢvᵢᵀ) with very small singular value σᵢ. (We will come back to this later.) 
Let’s first reuse the example before and show how it works. 
The matrix A above can be decomposed as 
Next, we will take a look at what U & V composed of. Let’s say A is an m × n matrix of rank r. AᵀA will be an n× n symmetric matrix. All symmetric matrices can choose n orthonormal eigenvectors vⱼ. Because of Avᵢ = σᵢuᵢ and vⱼ are orthonormal eigenvectors of AᵀA, we can calculate the value of uᵢᵀuⱼ as 
It equals zero. i.e. uᵢ and uⱼ are orthogonal with each other. As shown previously, they are also eigenvectors of AAᵀ. 
From Avᵢ = σuᵢ, we can recognize that uᵢ is a column vector of A. 
Because A has a rank of r, we can choose these r uᵢ vectors to be orthonormal. So what are the remaining m - r orthogonal eigenvectors for AAᵀ? Since left nullspace of A is orthogonal to the column space, it is very natural to pick them as the remaining eigenvector. (The left nullsapce N(Aᵀ) is the space span by x in Aᵀx=0.) A similar argument will work for the eigenvectors for AᵀA. Therefore, 
To get back to the former SVD equation from 
We simply put back the eigenvectors in the left nullspace and nullspace. 
For a linear equation system, we can compute the inverse of a square matrix A to solve x. 
But not all matrices are invertible. Also, in ML, it will be unlikely to find an exact solution with the presence of noise in data. Our objective is to find the model that best fit the data. To find the best-fit solution, we compute a pseudoinverse 
which minimizes the least square error below. 
And the solution for x can be estimated as, 
In a linear regression problem, x is our linear model, A contains the training data and b contains the corresponding labels. We can solve x by 
Here is an example. 
In ML, we identify patterns and relationship. How do we identify the correlation of properties in data? Let’s start the discussion with an example. We sample the height and weight of 12 people and compute their means. We zero-center the original values by subtracting them with its mean. For example, Matrix A below holds the adjusted zero-centered height and weight. 
As we plot the data points, we can recognize height and weight are positively related. But how can we quantify such a relationship? 
First, how does a property vary? We probably learn the variance from high school. Let’s introduce its cousin. Sample variance is defined as : 
Note, it is divided by n-1 instead of n in the variance. With a limited size of the samples, the sample mean is biased and correlated with the samples. The average square distance from this mean will be smaller than that from the general population. The sample covariance S², divided by n-1, compensates for the smaller value and can be proven to be an unbiased estimate for variance σ². (The proof is not very important so I will simply provide a link for the proof here.) 
Variance measures how a variable varies between itself while covariance is between two variables (a and b). 
We can hold all these possible combinations of covariance in a matrix called the covariance matrix Σ. 
We can rewrite this in a simple matrix form. 
The diagonal elements hold the variances of individual variables (like height) and the non-diagonal elements hold the covariance between two variables. Let’s compute the sample covariance now. 
The positive sample covariance indicates weight and height are positively correlated. It will be negative if they are negatively correlated and zero if they are independent. 
Covariance matrix & SVD 
We can use SVD to decompose the sample covariance matrix. Since σ₂ is relatively small compared with σ₁, we can even ignore the σ₂ term. When we train an ML model, we can perform a linear regression on the weight and height to form a new property rather than treating them as two separated and correlated properties (where entangled data usually make model training harder). 
u₁ has one significant importance. It is the principal component of S. 
There are a few properties about a sample covariance matrix under the context of SVD: 
Property 
Covariance matrices are not only symmetric but they are also positive semidefinite. Because variance is positive or zero, uᵀVu below is always greater or equal zero. By the energy test, V is positive semidefinite. 
Therefore, 
Often, after some linear transformation A, we want to know the covariance of the transformed data. This can be calculated with the transformation matrix A and the covariance of the original data. 
Correlation matrix 
A correlation matrix is a scaled version of the covariance matrix. A correlation matrix standardizes (scale) the variables to have a standard deviation of 1. 
Correlation matrix will be used if variables are in scales of very different magnitudes. Bad scaling may hurt ML algorithms like gradient descent. 
So far, we have a lot of equations. Let’s visualize what SVD does and develop the insight gradually. SVD factorizes a matrix A into USVᵀ. Applying A to a vector x (Ax) can be visualized as performing a rotation (Vᵀ), a scaling (S) and another rotation (U) on x. 
As shown above, the eigenvector vᵢ of V is transformed into: 
Or in the full matrix form 
As described before, the SVD can be formulated as 
Since uᵢ and vᵢ have unit length, the most dominant factor in determining the significance of each term is the singular value σᵢ. We purposely sort σᵢ in the descending order. If the eigenvalues become too small, we can ignore the remaining terms (+ σᵢuᵢvᵢᵀ + …). 
This formularization has some interesting implications. For example, we have a matrix contains the return of stock yields traded by different investors. 
As a fund manager, what information can we get out of it? Finding patterns and structures will be the first step. Maybe, we can identify the combination of stocks and investors that have the largest yields. SVD decompose an n × n matrix into r components with the singular value σᵢ demonstrating its significant. Consider this as a way to extract entangled and related properties into fewer principal directions with no correlations. 
If data is highly correlated, we should expect many σᵢ values to be small and can be ignored. 
In our previous example, weight and height are highly related. If we have a matrix containing the weight and height of 1000 people, the first component in the SVD decomposition will dominate. The u₁ vector indeed demonstrates the ratio between weight and height among these 1000 people as we discussed before. 
Technically, SVD extracts data in the directions with the highest variances respectively. PCA is a linear model in mapping m-dimensional input features to k-dimensional latent factors (k principal components). If we ignore the less significant terms, we remove the components that we care less but keep the principal directions with the highest variances (largest information). 
Consider the 3-dimensional data points that displayed as blue dots below. It can be approximated by a plane easily. 
You may quickly realize that we can use SVD to find the matrix W. Consider the data points below that lie on a 2-D space. 
SVD selects a projection that maximizes the variance of their output. Hence, PCA will pick the blue line over the green line if it has a higher variance. 
As indicated below, we keep the eigenvectors that have the top kth highest singular value. 
Interest rate 
Let’s illustrate the concept deeper by retracing an example here with the interest rate data originated from the US Treasurer Department. The basis points for 9 different interest rates (from 3 months, 6 months, … to 20 years) over 6 consecutive business days are collected with A stored the difference from the previous date. A also has its elements subtracted by its mean over this period already. i.e. it is zero-centered (across its row). 
The sample covariance matrix equals S = AAᵀ/(5–1). 
Now we have the covariance matrix S that we want to factorize. The SVD decomposition is 
From the SVD decomposition, we realize that we can focus on the first three principal components. 
As shown, the first principal component is related to a weighted average of the daily change for all maturity lengths. The second principal component adjusts the daily change sensitive to the maturity length of the bond. (The third principal component is likely the curvature — a second-degree derivative.) 
We understand the relationship between the interesting rate change and maturity well in our daily life. So the principal components reconfirm what we believe how interest rates behave. But when we are presented with unfamiliar raw data, PCA is very helpful to extract the principal components of your data to find the underneath information structure. This may answer some questions on how to find a needle in a haystack. 
Scale features before performing SVD. 
Say, we want to retain 99% variance, we can choose k such that 
Written by 
Written by",Jonathan Hui,2020-02-08T17:01:48.826Z
Credit Card Fraud Detection: Neural Network vs. Anomaly Detection Algorithms | by Harsh Bansal | Analytics Vidhya | Medium,"In data mining, anomaly detection means to search or scan for a data point, item or record which do not match or conform to expected pattern, trend or to other data points in dataset. So, most of the time these data points or records are considered as defects, outliers, errors or frauds. There are various machine learning anomaly detection algorithms which enhance the speed of detecting of these outliers. These anomaly detection algorithms are used for detecting invasions, as while detecting outliers and can also prevent attacks, defects, faults and so on. Various companies, organization or institution adapted and implement these algorithms with simple yet effective approach for detecting and classifying these anomalies. Machine learning algorithms have the ability to learn from data and make predictions based on that data. Since basic machine learning involves learning from data and predict the data but anomaly detection algorithms specifically learn or work on these outliers. It provides an alternative for detection and classification of anomalies based on an initially large set of features. Anomaly detection or outlier detection is the recognition of unalike data, records or observations which raise doubts by differing significantly from the majority of the data. 
Credit card fraud is a serious and global issue or crime committed by frauds using payment card such as credit card or debit card. The purpose of these fraudsters is to acquire goods without paying, or to acquire unauthorized funds from an account. Credit card fraud also give rise to identity theft. According to the some reports and statistics, while the rate of identity theft has been steadily increased by 21 percent in 2008. However, credit card fraud, that crime which leads to ID theft, decreasing as a percentage of all ID theft complaints for many years. Although only 0.1% of card holders are aware of credit card frauds. These credit card frauds have resulted in huge financial losses as the fraudulent transactions have been large value transactions. In the year 1999, 10 million transactions out of 12 billion turned out to be fraudulent. Also, every 4 out of every 10,000 active accounts are fraudulent. Current fraud detection systems are only able to prevent 1/12th of 1% of all transactions processed which still leads billions of dollars in losses. 
The major aspect of this project to develop a best suited algorithm to find the outliers or frauds in case of credit cards. We will implement several machine learning and deep learning algorithms and compare them and choose the best algorithm. 
We will implement algorithms like: 
For this purpose, we used an existing dataset. The dataset contains information about transactions made by various cardholders. The dataset composed of around 300,000 records out of which there are only around only 500 fraudsters. So, this shows that dataset is highly imbalanced as the positive class or frauds are only 0.172% of all transactions. All the features columns are numeric which are result of PCA transformation. Hence their value ranges from -1 to 1. Features columns V1, V2, V3… V28 are obtained as result of PCA transformation. Columns like Time and Amount have not been transformed. Feature Class column is the classification variable which contain value 0 (Normal Case) and 1 (Fraud). 
For Dataset click here 
ANN are concept of deep learning which are implement using keras (in this case). ANN are composed of neurons. First layer or Input layer is the input neuron which consist the transaction and amount of each customer. The hidden layer consists of weights, bias and activation function. We can add as much hidden layer for tuning the performance. In this case we are using 3 layers. The output layer is the final layer where we get the classified output. The output either be 1 or 0 where 1 indicate fraud case and 0 indicate normal. 
1) Data Processing: 
• Libraries Used for Data Preprocessing: Pandas, NumPy 
• Operations applied: Featuring Scaling, PCA 
• Columns Dropped: Time 
2) Visualization: 
• Libraries Used for Visualization: Matplotlib 
• Histogram Plot of Class Columns 
Class 0: 284315 Class 1: 492 
3) Model (Neural Network): 
• Framework Used for Neural Network: Keras 
4) Model Evaluation: 
• Libraries Used for Model Evaluation: Confusion Matrix, Classification Report 
• Accuracy Score: 
99% of accuracy in predicting the cases, 0.4% of cost 
5) Final Report: 
• No. of lines of code: 80 
• Source Code Memory: 108 kb 
• Time Taken: 6:16:46 (6 minute 16 seconds 46 nanoseconds) (May vary in different system) 
• Accuracy in Predicting Normal Cases: 99% 
• Accuracy in Predicting Outliers: 76% 
6) Source Code: 
• For ipynb file, click here 
Anomaly Detection algorithms are used to detect or identify unusual patterns that are different from expected trend and behavior. These cases are called outliers. It has been used in business, from invasion detection (identifying strange patterns in network traffic that could signal a hack) to system health monitoring like detecting malignant tumor in an MRI scan and also help in fraud detection in credit card transactions to fault detection in operating environments. 
1) Data Processing: 
• Library used for data processing and manipulation: Pandas, Numpy 
• We will divide the dataset into 3 parts- Normal outliers, Normal Train, Normal Test 
2) Visualization: 
• Library used: Matplotlib 
3) Model Implementation: 
a) Isolation Forest: Isolation forest is a tree algorithm which detects anomalies by randomly partitioning not on basis of information gain as like of decision tree. Partitions are created by randomly selecting a feature and then randomly creating a split value between the maximum and the minimum value of the feature. We keep on creating the partitions until we isolate all the points. In most cases we also set a limit on number of partitions/heights of the tree. 
Evaluation and Conclusion: 
b) Local Outlier Factor: It is an anomaly detection unsupervised algorithm which computes the local density deviation of a given data point with respect to its neighbors. It recognizes data point or record as outliers which have a substantially lower density than their neighbors. When LOF is used for outlier detection it has no predict, decision function and score samples methods. The number of neighbors considered (parameter n_neighbors) is typically set 1) greater than the minimum number of samples a cluster has to contain, so that other samples can be local outliers relative to this cluster, and smaller than the maximum number of close by samples that can potentially be local outliers. 
Evaluation and Conclusion: 
c)OneClassSVM: A One-Class Support Vector Machine is an unsupervised learning algorithm that is trained only on the one type of class. This module is particularly useful in scenarios where you have a lot of “normal” data and not many cases of the anomalies you are trying to detect. An SVM model is based on dividing the training sample points into separate categories by as wide a gap as possible, while penalizing training samples that fall on the wrong side of the gap. The SVM model then makes predictions by assigning points to one side of the gap or the other. In one-class learning we train the model only on the positive class data-set and take judgments from it 
• Libraries Used for Implementing Algorithm: OneClassSVM from svm 
• Hyperparametres and Values used: 
Evaluation and Conclusion: 
Accuracy in predicting Outliers: 91% 
Time Taken: 30 Min 
Source Code Memory: 70 kb 
Source Code: 
d)DBSCAN: Density-Based Spatial Clustering of Applications with Noise. DBSCAN is anomaly detection algorithm which uses clustering. In this method, we calculate the distance between points (the Euclidean distance or some other distance) and look for points which are far away from others. DBSCAN works bottom up approach that consider those data point which are close to each other. Clusters with few points in them are considered outliers. DBSCAN detect the outliers on time series in simplified form. We consider each host to be a point in d-dimensions, where d is the number of elements in the time series. After calculating the distance between data points a cluster is formed and data points that are not in the largest cluster will be considered an outlier. 
Evaluation and Conclusion: 
Source Code: 
Several algorithms have been implemented on same data set to detect the credit cards frauds. All the algorithms have been analyzed and compared on basis of accuracy on basis of predicting normal cases and outliers or frauds. We implemented different type of algorithms which include neural network from deep learning, anomaly detection algorithms like isolation forest, OneClassSVM, Local Outlier Factor, supervised algorithm like DBSCAN. This was done to attain the best approach for the purpose. 
Upon analyzing we get to know that 3-Layer Neural Network and DBSCAN is spot on predicting the normal cases with accuracy of 99% but in case of predicting the outliers they are not as good as anomaly detection algorithms. Isolation forest and OneClassSVM algorithm is giving impressive accuracy of 91% on predicting the outliers but in case of predicting the normal case they have less accuracy as compared to neural networks. In case of time taken neural networks and isolation forest algorithms are very impressive. In future this type of algorithm can be used in different cases. For better performance we can change the layers properties neural networks for better results. 
Written by 
Written by",Harsh Bansal,2019-11-06T04:31:40.012Z
